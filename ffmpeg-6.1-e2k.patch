From a931b09738e265fd3568152cefa85bbb0186d663 Mon Sep 17 00:00:00 2001
From: Ilya Kurdyukov <jpegqs@gmail.com>
Date: Thu, 28 Dec 2023 16:41:30 +0700
Subject: [PATCH] ffmpeg-6.1 e2k support

---
 configure                            |   27 +-
 libavcodec/audiodsp.c                |    2 +
 libavcodec/audiodsp.h                |    1 +
 libavcodec/blockdsp.c                |    2 +
 libavcodec/blockdsp.h                |    1 +
 libavcodec/e2k/Makefile              |   31 +
 libavcodec/e2k/audiodsp.c            |   61 +
 libavcodec/e2k/blockdsp.c            |   71 +
 libavcodec/e2k/dctdsp.h              |   29 +
 libavcodec/e2k/fdctdsp.c             |  389 ++++
 libavcodec/e2k/fft.c                 | 1043 ++++++++++
 libavcodec/e2k/fft.h                 |   29 +
 libavcodec/e2k/fft_init.c            |  151 ++
 libavcodec/e2k/fmtconvert.c          |   53 +
 libavcodec/e2k/h264chroma.c          |   56 +
 libavcodec/e2k/h264chroma_template.c |  209 ++
 libavcodec/e2k/h264dsp.c             | 1011 +++++++++
 libavcodec/e2k/h264qpel.c            |   82 +
 libavcodec/e2k/h264qpel_template.c   |  504 +++++
 libavcodec/e2k/hevcdsp.c             | 2893 ++++++++++++++++++++++++++
 libavcodec/e2k/hpeldsp.c             |  489 +++++
 libavcodec/e2k/hpeldsp.h             |   30 +
 libavcodec/e2k/idctdsp.c             |  324 +++
 libavcodec/e2k/lossless_audiodsp.c   |   74 +
 libavcodec/e2k/lossless_videodsp.c   |   58 +
 libavcodec/e2k/me_cmp.c              |  465 +++++
 libavcodec/e2k/mpeg4videodsp.c       |   85 +
 libavcodec/e2k/mpegaudiodsp.c        |  143 ++
 libavcodec/e2k/mpegvideo.c           |   99 +
 libavcodec/e2k/mpegvideoencdsp.c     |   74 +
 libavcodec/e2k/pixblockdsp.c         |   82 +
 libavcodec/e2k/svq1enc.c             |   67 +
 libavcodec/e2k/vc1dsp.c              |  297 +++
 libavcodec/e2k/videodsp.c            |   36 +
 libavcodec/e2k/vorbisdsp.c           |   61 +
 libavcodec/e2k/vp3dsp.c              |  168 ++
 libavcodec/e2k/vp8dsp.c              |  429 ++++
 libavcodec/e2k/vp9dsp.c              | 1739 ++++++++++++++++
 libavcodec/e2k/xvididct.c            |  198 ++
 libavcodec/fdctdsp.c                 |    2 +
 libavcodec/fdctdsp.h                 |    2 +
 libavcodec/fmtconvert.c              |    2 +
 libavcodec/fmtconvert.h              |    1 +
 libavcodec/h264chroma.c              |    2 +
 libavcodec/h264chroma.h              |    1 +
 libavcodec/h264dsp.c                 |    2 +
 libavcodec/h264dsp.h                 |    2 +
 libavcodec/h264qpel.c                |    2 +
 libavcodec/h264qpel.h                |    1 +
 libavcodec/hevcdsp.c                 |    2 +
 libavcodec/hevcdsp.h                 |    1 +
 libavcodec/hpeldsp.c                 |    2 +
 libavcodec/hpeldsp.h                 |    1 +
 libavcodec/idctdsp.c                 |    2 +
 libavcodec/idctdsp.h                 |    2 +
 libavcodec/librsvgdec.c              |    2 +
 libavcodec/lossless_audiodsp.c       |    2 +
 libavcodec/lossless_audiodsp.h       |    1 +
 libavcodec/lossless_videodsp.c       |    2 +
 libavcodec/lossless_videodsp.h       |    1 +
 libavcodec/me_cmp.c                  |    2 +
 libavcodec/me_cmp.h                  |    1 +
 libavcodec/mpeg4videodsp.c           |    2 +
 libavcodec/mpeg4videodsp.h           |    1 +
 libavcodec/mpegaudiodsp.c            |    2 +
 libavcodec/mpegaudiodsp.h            |    1 +
 libavcodec/mpegvideo.c               |    2 +
 libavcodec/mpegvideo.h               |    1 +
 libavcodec/mpegvideoencdsp.c         |    2 +
 libavcodec/mpegvideoencdsp.h         |    2 +
 libavcodec/pixblockdsp.c             |    2 +
 libavcodec/pixblockdsp.h             |    2 +
 libavcodec/svq1enc.c                 |    2 +
 libavcodec/svq1encdsp.h              |    1 +
 libavcodec/tests/dct.c               |    2 +
 libavcodec/tests/e2k/dct.c           |   33 +
 libavcodec/vc1dsp.c                  |    2 +
 libavcodec/vc1dsp.h                  |    1 +
 libavcodec/videodsp.c                |    2 +
 libavcodec/videodsp.h                |    1 +
 libavcodec/vorbisdsp.c               |    2 +
 libavcodec/vorbisdsp.h               |    1 +
 libavcodec/vp3dsp.c                  |    2 +
 libavcodec/vp3dsp.h                  |    1 +
 libavcodec/vp8dsp.c                  |    2 +
 libavcodec/vp8dsp.h                  |    1 +
 libavcodec/vp9dsp.c                  |    2 +
 libavcodec/vp9dsp.h                  |    1 +
 libavcodec/xvididct.c                |    2 +
 libavcodec/xvididct.h                |    2 +
 libavfilter/vf_drawtext.c            |   12 +
 libavutil/cpu.c                      |    6 +
 libavutil/cpu.h                      |    2 +
 libavutil/cpu_internal.h             |    2 +
 libavutil/e2k/Makefile               |    2 +
 libavutil/e2k/cpu.c                  |   41 +
 libavutil/e2k/cpu.h                  |   27 +
 libavutil/e2k/float_dsp.c            |  187 ++
 libavutil/e2k/intreadwrite.h         |   54 +
 libavutil/e2k/timer.h                |   35 +
 libavutil/e2k/util_e2k.h             |  146 ++
 libavutil/float_dsp.c                |    2 +
 libavutil/float_dsp.h                |    1 +
 libavutil/intreadwrite.h             |    2 +
 libavutil/tests/cpu.c                |    2 +
 libavutil/timer.h                    |    2 +
 libswresample/audioconvert.c         |    2 +
 libswresample/e2k/Makefile           |    1 +
 libswresample/e2k/audio_convert.c    |  109 +
 libswresample/swresample_internal.h  |    4 +
 libswscale/e2k/Makefile              |    3 +
 libswscale/e2k/swscale.c             | 2075 ++++++++++++++++++
 libswscale/e2k/yuv2rgb.c             |  248 +++
 libswscale/e2k/yuv2rgb.h             |   52 +
 libswscale/e2k/yuv2yuv.c             |  146 ++
 libswscale/swscale.c                 |    2 +
 libswscale/swscale_internal.h        |    5 +
 libswscale/swscale_unscaled.c        |    2 +
 libswscale/utils.c                   |   13 +
 libswscale/yuv2rgb.c                 |    2 +
 tests/checkasm/checkasm.c            |    2 +
 tests/checkasm/huffyuvdsp.c          |    8 +-
 122 files changed, 14869 insertions(+), 6 deletions(-)
 create mode 100644 libavcodec/e2k/Makefile
 create mode 100644 libavcodec/e2k/audiodsp.c
 create mode 100644 libavcodec/e2k/blockdsp.c
 create mode 100644 libavcodec/e2k/dctdsp.h
 create mode 100644 libavcodec/e2k/fdctdsp.c
 create mode 100644 libavcodec/e2k/fft.c
 create mode 100644 libavcodec/e2k/fft.h
 create mode 100644 libavcodec/e2k/fft_init.c
 create mode 100644 libavcodec/e2k/fmtconvert.c
 create mode 100644 libavcodec/e2k/h264chroma.c
 create mode 100644 libavcodec/e2k/h264chroma_template.c
 create mode 100644 libavcodec/e2k/h264dsp.c
 create mode 100644 libavcodec/e2k/h264qpel.c
 create mode 100644 libavcodec/e2k/h264qpel_template.c
 create mode 100644 libavcodec/e2k/hevcdsp.c
 create mode 100644 libavcodec/e2k/hpeldsp.c
 create mode 100644 libavcodec/e2k/hpeldsp.h
 create mode 100644 libavcodec/e2k/idctdsp.c
 create mode 100644 libavcodec/e2k/lossless_audiodsp.c
 create mode 100644 libavcodec/e2k/lossless_videodsp.c
 create mode 100644 libavcodec/e2k/me_cmp.c
 create mode 100644 libavcodec/e2k/mpeg4videodsp.c
 create mode 100644 libavcodec/e2k/mpegaudiodsp.c
 create mode 100644 libavcodec/e2k/mpegvideo.c
 create mode 100644 libavcodec/e2k/mpegvideoencdsp.c
 create mode 100644 libavcodec/e2k/pixblockdsp.c
 create mode 100644 libavcodec/e2k/svq1enc.c
 create mode 100644 libavcodec/e2k/vc1dsp.c
 create mode 100644 libavcodec/e2k/videodsp.c
 create mode 100644 libavcodec/e2k/vorbisdsp.c
 create mode 100644 libavcodec/e2k/vp3dsp.c
 create mode 100644 libavcodec/e2k/vp8dsp.c
 create mode 100644 libavcodec/e2k/vp9dsp.c
 create mode 100644 libavcodec/e2k/xvididct.c
 create mode 100644 libavcodec/tests/e2k/dct.c
 create mode 100644 libavutil/e2k/Makefile
 create mode 100644 libavutil/e2k/cpu.c
 create mode 100644 libavutil/e2k/cpu.h
 create mode 100644 libavutil/e2k/float_dsp.c
 create mode 100644 libavutil/e2k/intreadwrite.h
 create mode 100644 libavutil/e2k/timer.h
 create mode 100644 libavutil/e2k/util_e2k.h
 create mode 100644 libswresample/e2k/Makefile
 create mode 100644 libswresample/e2k/audio_convert.c
 create mode 100644 libswscale/e2k/Makefile
 create mode 100644 libswscale/e2k/swscale.c
 create mode 100644 libswscale/e2k/yuv2rgb.c
 create mode 100644 libswscale/e2k/yuv2rgb.h
 create mode 100644 libswscale/e2k/yuv2yuv.c

diff --git a/configure b/configure
index 0407af2..f94404d 100755
--- a/configure
+++ b/configure
@@ -2078,6 +2078,7 @@ ARCH_LIST="
     parisc
     ppc
     ppc64
+    e2k
     riscv
     s390
     sh4
@@ -2153,6 +2154,10 @@ ARCH_EXT_LIST_PPC="
     vsx
 "
 
+ARCH_EXT_LIST_E2K="
+    e2k_simd
+"
+
 ARCH_EXT_LIST_RISCV="
     rvv
 "
@@ -2166,6 +2171,7 @@ ARCH_EXT_LIST_X86="
 ARCH_EXT_LIST="
     $ARCH_EXT_LIST_ARM
     $ARCH_EXT_LIST_PPC
+    $ARCH_EXT_LIST_E2K
     $ARCH_EXT_LIST_RISCV
     $ARCH_EXT_LIST_X86
     $ARCH_EXT_LIST_MIPS
@@ -2678,6 +2684,8 @@ ppc4xx_deps="ppc"
 vsx_deps="altivec"
 power8_deps="vsx"
 
+e2k_simd_deps="e2k"
+
 rvv_deps="riscv"
 
 loongson2_deps="mips"
@@ -2732,10 +2740,10 @@ for ext in $(filter_out mmx $ARCH_EXT_LIST_X86_SIMD); do
 done
 
 aligned_stack_if_any="aarch64 ppc x86"
-fast_64bit_if_any="aarch64 alpha ia64 mips64 parisc64 ppc64 riscv64 sparc64 x86_64"
-fast_clz_if_any="aarch64 alpha avr32 mips ppc x86"
-fast_unaligned_if_any="aarch64 ppc x86"
-simd_align_16_if_any="altivec neon sse"
+fast_64bit_if_any="aarch64 alpha ia64 mips64 parisc64 ppc64 e2k riscv64 sparc64 x86_64"
+fast_clz_if_any="aarch64 alpha avr32 mips ppc e2k x86"
+fast_unaligned_if_any="aarch64 ppc e2k x86"
+simd_align_16_if_any="altivec e2k neon sse"
 simd_align_32_if_any="avx"
 simd_align_64_if_any="avx512"
 
@@ -5117,6 +5125,9 @@ case "$arch" in
     riscv*)
         arch="riscv"
     ;;
+    e2k|elbrus)
+        arch="e2k"
+    ;;
     s390|s390x)
         arch="s390"
     ;;
@@ -5426,6 +5437,11 @@ elif enabled riscv; then
         enable fast_clz
     fi
 
+elif enabled e2k; then
+
+    cpu="e2k"
+    cpuflags="-msse4.1 -mno-avx"
+
 elif enabled sparc; then
 
     case $cpu in
@@ -5533,6 +5549,9 @@ case "$arch" in
         check_64bit ppc ppc64
         enabled shared && enable_weak pic
     ;;
+    e2k)
+        enabled shared && enable_weak pic
+    ;;
     riscv)
         check_64bit riscv32 riscv64
         enabled shared && enable_weak pic
diff --git a/libavcodec/audiodsp.c b/libavcodec/audiodsp.c
index c5427d3..74bbf40 100644
--- a/libavcodec/audiodsp.c
+++ b/libavcodec/audiodsp.c
@@ -113,6 +113,8 @@ av_cold void ff_audiodsp_init(AudioDSPContext *c)
     ff_audiodsp_init_arm(c);
 #elif ARCH_PPC
     ff_audiodsp_init_ppc(c);
+#elif ARCH_E2K
+    ff_audiodsp_init_e2k(c);
 #elif ARCH_RISCV
     ff_audiodsp_init_riscv(c);
 #elif ARCH_X86
diff --git a/libavcodec/audiodsp.h b/libavcodec/audiodsp.h
index 485b512..e3c8c40 100644
--- a/libavcodec/audiodsp.h
+++ b/libavcodec/audiodsp.h
@@ -55,6 +55,7 @@ typedef struct AudioDSPContext {
 void ff_audiodsp_init(AudioDSPContext *c);
 void ff_audiodsp_init_arm(AudioDSPContext *c);
 void ff_audiodsp_init_ppc(AudioDSPContext *c);
+void ff_audiodsp_init_e2k(AudioDSPContext *c);
 void ff_audiodsp_init_riscv(AudioDSPContext *c);
 void ff_audiodsp_init_x86(AudioDSPContext *c);
 
diff --git a/libavcodec/blockdsp.c b/libavcodec/blockdsp.c
index 98f06c5..f1bd06c 100644
--- a/libavcodec/blockdsp.c
+++ b/libavcodec/blockdsp.c
@@ -69,6 +69,8 @@ av_cold void ff_blockdsp_init(BlockDSPContext *c)
     ff_blockdsp_init_arm(c);
 #elif ARCH_PPC
     ff_blockdsp_init_ppc(c);
+#elif ARCH_E2K
+    ff_blockdsp_init_e2k(c);
 #elif ARCH_X86
     ff_blockdsp_init_x86(c);
 #elif ARCH_MIPS
diff --git a/libavcodec/blockdsp.h b/libavcodec/blockdsp.h
index d853ada..9452720 100644
--- a/libavcodec/blockdsp.h
+++ b/libavcodec/blockdsp.h
@@ -41,6 +41,7 @@ void ff_blockdsp_init(BlockDSPContext *c);
 void ff_blockdsp_init_alpha(BlockDSPContext *c);
 void ff_blockdsp_init_arm(BlockDSPContext *c);
 void ff_blockdsp_init_ppc(BlockDSPContext *c);
+void ff_blockdsp_init_e2k(BlockDSPContext *c);
 void ff_blockdsp_init_x86(BlockDSPContext *c);
 void ff_blockdsp_init_mips(BlockDSPContext *c);
 
diff --git a/libavcodec/e2k/Makefile b/libavcodec/e2k/Makefile
new file mode 100644
index 0000000..a673607
--- /dev/null
+++ b/libavcodec/e2k/Makefile
@@ -0,0 +1,31 @@
+# subsystems
+OBJS-$(CONFIG_AUDIODSP)                += e2k/audiodsp.o
+OBJS-$(CONFIG_BLOCKDSP)                += e2k/blockdsp.o
+OBJS-$(CONFIG_FFT)                     += e2k/fft_init.o e2k/fft.o
+OBJS-$(CONFIG_FDCTDSP)                 += e2k/fdctdsp.o
+OBJS-$(CONFIG_FMTCONVERT)              += e2k/fmtconvert.o
+OBJS-$(CONFIG_H264CHROMA)              += e2k/h264chroma.o
+OBJS-$(CONFIG_H264DSP)                 += e2k/h264dsp.o e2k/hpeldsp.o
+OBJS-$(CONFIG_H264QPEL)                += e2k/h264qpel.o
+OBJS-$(CONFIG_HPELDSP)                 += e2k/hpeldsp.o
+OBJS-$(CONFIG_IDCTDSP)                 += e2k/idctdsp.o
+OBJS-$(CONFIG_LLVIDDSP)                += e2k/lossless_videodsp.o
+OBJS-$(CONFIG_ME_CMP)                  += e2k/me_cmp.o
+OBJS-$(CONFIG_MPEGAUDIODSP)            += e2k/mpegaudiodsp.o
+OBJS-$(CONFIG_MPEGVIDEO)               += e2k/mpegvideo.o e2k/mpeg4videodsp.o
+OBJS-$(CONFIG_MPEGVIDEOENC)            += e2k/mpegvideoencdsp.o
+OBJS-$(CONFIG_PIXBLOCKDSP)             += e2k/pixblockdsp.o
+OBJS-$(CONFIG_VC1DSP)                  += e2k/vc1dsp.o
+OBJS-$(CONFIG_VIDEODSP)                += e2k/videodsp.o
+OBJS-$(CONFIG_VP3DSP)                  += e2k/vp3dsp.o
+OBJS-$(CONFIG_VP8DSP)                  += e2k/vp8dsp.o
+
+# decoders/encoders
+OBJS-$(CONFIG_HEVC_DECODER)            += e2k/hevcdsp.o
+OBJS-$(CONFIG_LLAUDDSP)                += e2k/lossless_audiodsp.o
+OBJS-$(CONFIG_SVQ1_ENCODER)            += e2k/svq1enc.o
+OBJS-$(CONFIG_VORBIS_DECODER)          += e2k/vorbisdsp.o
+OBJS-$(CONFIG_VP7_DECODER)             += e2k/vp8dsp.o
+OBJS-$(CONFIG_VP9_DECODER)             += e2k/vp9dsp.o
+OBJS-$(CONFIG_MPEG4_DECODER)           += e2k/xvididct.o
+
diff --git a/libavcodec/e2k/audiodsp.c b/libavcodec/e2k/audiodsp.c
new file mode 100644
index 0000000..471c45b
--- /dev/null
+++ b/libavcodec/e2k/audiodsp.c
@@ -0,0 +1,61 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2007 Luca Barbato <lu_zero@gentoo.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * miscellaneous audio operations
+ */
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/audiodsp.h"
+
+static int32_t scalarproduct_int16_e2k(const int16_t *v1, const int16_t *v2, int order)
+{
+    int i;
+    vec_s16 vec1, vec2;
+    vec_s32 res = _mm_setzero_si128(), tmp;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < order; i += 8) {
+        vec1 = VEC_LD(v1);
+        vec2 = VEC_LD(v2);
+        tmp = _mm_madd_epi16(vec1, vec2);
+        res = _mm_add_epi32(res, tmp);
+        v1 += 8;
+        v2 += 8;
+    }
+
+    res = _mm_hadd_epi32(res, res);
+    return _mm_extract_epi32(res, 0) + _mm_extract_epi32(res, 1);
+}
+
+av_cold void ff_audiodsp_init_e2k(AudioDSPContext *c)
+{
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    c->scalarproduct_int16 = scalarproduct_int16_e2k;
+}
diff --git a/libavcodec/e2k/blockdsp.c b/libavcodec/e2k/blockdsp.c
new file mode 100644
index 0000000..fd194cb
--- /dev/null
+++ b/libavcodec/e2k/blockdsp.c
@@ -0,0 +1,71 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2002 Brian Foley
+ * Copyright (c) 2002 Dieter Shirley
+ * Copyright (c) 2003-2004 Romain Dolbeau <romain@dolbeau.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include <string.h>
+
+#include "libavutil/attributes.h"
+#include "libavutil/mem.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/blockdsp.h"
+
+static void clear_block_e2k(int16_t *block)
+{
+    LOAD_ZERO;
+    VEC_ST(block, zerov);
+    VEC_ST(block + 8, zerov);
+    VEC_ST(block + 8 * 2, zerov);
+    VEC_ST(block + 8 * 3, zerov);
+    VEC_ST(block + 8 * 4, zerov);
+    VEC_ST(block + 8 * 5, zerov);
+    VEC_ST(block + 8 * 6, zerov);
+    VEC_ST(block + 8 * 7, zerov);
+}
+
+static void clear_blocks_e2k(int16_t *blocks)
+{
+    int i;
+    LOAD_ZERO;
+    for (i = 0; i < 6; i++, blocks += 64) {
+        VEC_ST(blocks, zerov);
+        VEC_ST(blocks + 8, zerov);
+        VEC_ST(blocks + 8 * 2, zerov);
+        VEC_ST(blocks + 8 * 3, zerov);
+        VEC_ST(blocks + 8 * 4, zerov);
+        VEC_ST(blocks + 8 * 5, zerov);
+        VEC_ST(blocks + 8 * 6, zerov);
+        VEC_ST(blocks + 8 * 7, zerov);
+    }
+}
+
+av_cold void ff_blockdsp_init_e2k(BlockDSPContext *c)
+{
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    c->clear_block = clear_block_e2k;
+    c->clear_blocks = clear_blocks_e2k;
+}
diff --git a/libavcodec/e2k/dctdsp.h b/libavcodec/e2k/dctdsp.h
new file mode 100644
index 0000000..d1ba599
--- /dev/null
+++ b/libavcodec/e2k/dctdsp.h
@@ -0,0 +1,29 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_E2K_DCTDSP_H
+#define AVCODEC_E2K_DCTDSP_H
+
+#include <stdint.h>
+
+void ff_fdct_e2k(int16_t *block);
+void ff_simple_idct_e2k(int16_t *block);
+void ff_fast_idct_e2k(int16_t *block);
+void ff_xvid_idct_e2k(int16_t *block);
+
+#endif /* AVCODEC_E2K_DCTDSP_H */
diff --git a/libavcodec/e2k/fdctdsp.c b/libavcodec/e2k/fdctdsp.c
new file mode 100644
index 0000000..e5b568e
--- /dev/null
+++ b/libavcodec/e2k/fdctdsp.c
@@ -0,0 +1,389 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (C) 2003  James Klicman <james@klicman.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/avcodec.h"
+#include "libavcodec/fdctdsp.h"
+
+#include "dctdsp.h"
+
+#define C1     0.98078528040323044912618224 /* cos(1 * PI / 16) */
+#define C2     0.92387953251128675612818319 /* cos(2 * PI / 16) */
+#define C3     0.83146961230254523707878838 /* cos(3 * PI / 16) */
+#define C4     0.70710678118654752440084436 /* cos(4 * PI / 16) */
+#define C5     0.55557023301960222474283081 /* cos(5 * PI / 16) */
+#define C6     0.38268343236508977172845998 /* cos(6 * PI / 16) */
+#define C7     0.19509032201612826784828487 /* cos(7 * PI / 16) */
+
+#define W0 -(2 * C2)
+#define W1  (2 * C6)
+#define W2 (M_SQRT2 * C6)
+#define W3 (M_SQRT2 * C3)
+#define W4 (M_SQRT2 * (-C1 + C3 + C5 - C7))
+#define W5 (M_SQRT2 *  (C1 + C3 - C5 + C7))
+#define W6 (M_SQRT2 *  (C1 + C3 + C5 - C7))
+#define W7 (M_SQRT2 *  (C1 + C3 - C5 - C7))
+#define W8 (M_SQRT2 *  (C7 - C3))
+#define W9 (M_SQRT2 * (-C1 - C3))
+#define WA (M_SQRT2 * (-C3 - C5))
+#define WB (M_SQRT2 *  (C5 - C3))
+
+#define LD_W0 _mm_set1_ps(W0)
+#define LD_W1 _mm_set1_ps(W1)
+#define LD_W2 _mm_set1_ps(W2)
+#define LD_W3 _mm_set1_ps(W3)
+#define LD_W4 _mm_set1_ps(W4)
+#define LD_W5 _mm_set1_ps(W5)
+#define LD_W6 _mm_set1_ps(W6)
+#define LD_W7 _mm_set1_ps(W7)
+#define LD_W8 _mm_set1_ps(W8)
+#define LD_W9 _mm_set1_ps(W9)
+#define LD_WA _mm_set1_ps(WA)
+#define LD_WB _mm_set1_ps(WB)
+
+#define _mm_madd_ps(a, b, c) _mm_add_ps(_mm_mul_ps(a, b), c)
+
+#define FDCTROW(b0, b1, b2, b3, b4, b5, b6, b7) /* {{{ */           \
+    x0 = _mm_add_ps(b0, b7);            /* x0 = b0 + b7; */         \
+    x7 = _mm_sub_ps(b0, b7);            /* x7 = b0 - b7; */         \
+    x1 = _mm_add_ps(b1, b6);            /* x1 = b1 + b6; */         \
+    x6 = _mm_sub_ps(b1, b6);            /* x6 = b1 - b6; */         \
+    x2 = _mm_add_ps(b2, b5);            /* x2 = b2 + b5; */         \
+    x5 = _mm_sub_ps(b2, b5);            /* x5 = b2 - b5; */         \
+    x3 = _mm_add_ps(b3, b4);            /* x3 = b3 + b4; */         \
+    x4 = _mm_sub_ps(b3, b4);            /* x4 = b3 - b4; */         \
+                                                                    \
+    b7 = _mm_add_ps(x0, x3);            /* b7 = x0 + x3; */         \
+    b1 = _mm_add_ps(x1, x2);            /* b1 = x1 + x2; */         \
+    b0 = _mm_add_ps(b7, b1);            /* b0 = b7 + b1; */         \
+    b4 = _mm_sub_ps(b7, b1);            /* b4 = b7 - b1; */         \
+                                                                    \
+    b2   = _mm_sub_ps(x0, x3);          /* b2 = x0 - x3; */         \
+    b6   = _mm_sub_ps(x1, x2);          /* b6 = x1 - x2; */         \
+    b5   = _mm_add_ps(b6, b2);          /* b5 = b6 + b2; */         \
+    cnst = LD_W2;                                                   \
+    b5   = _mm_mul_ps(cnst, b5);        /* b5 = b5 * W2; */         \
+    cnst = LD_W1;                                                   \
+    b2   = _mm_madd_ps(cnst, b2, b5);   /* b2 = b5 + b2 * W1; */    \
+    cnst = LD_W0;                                                   \
+    b6   = _mm_madd_ps(cnst, b6, b5);   /* b6 = b5 + b6 * W0; */    \
+                                                                    \
+    x0   = _mm_add_ps(x4, x7);          /* x0 = x4 + x7; */         \
+    x1   = _mm_add_ps(x5, x6);          /* x1 = x5 + x6; */         \
+    x2   = _mm_add_ps(x4, x6);          /* x2 = x4 + x6; */         \
+    x3   = _mm_add_ps(x5, x7);          /* x3 = x5 + x7; */         \
+    x8   = _mm_add_ps(x2, x3);          /* x8 = x2 + x3; */         \
+    cnst = LD_W3;                                                   \
+    x8   = _mm_mul_ps(cnst, x8);        /* x8 = x8 * W3; */         \
+                                                                    \
+    cnst = LD_W8;                                                   \
+    x0   = _mm_mul_ps(cnst, x0);        /* x0 *= W8; */             \
+    cnst = LD_W9;                                                   \
+    x1   = _mm_mul_ps(cnst, x1);        /* x1 *= W9; */             \
+    cnst = LD_WA;                                                   \
+    x2   = _mm_madd_ps(cnst, x2, x8);   /* x2 = x2 * WA + x8; */    \
+    cnst = LD_WB;                                                   \
+    x3   = _mm_madd_ps(cnst, x3, x8);   /* x3 = x3 * WB + x8; */    \
+                                                                    \
+    cnst = LD_W4;                                                   \
+    b7   = _mm_madd_ps(cnst, x4, x0);   /* b7 = x4 * W4 + x0; */    \
+    cnst = LD_W5;                                                   \
+    b5   = _mm_madd_ps(cnst, x5, x1);   /* b5 = x5 * W5 + x1; */    \
+    cnst = LD_W6;                                                   \
+    b3   = _mm_madd_ps(cnst, x6, x1);   /* b3 = x6 * W6 + x1; */    \
+    cnst = LD_W7;                                                   \
+    b1   = _mm_madd_ps(cnst, x7, x0);   /* b1 = x7 * W7 + x0; */    \
+                                                                    \
+    b7 = _mm_add_ps(b7, x2);            /* b7 = b7 + x2; */         \
+    b5 = _mm_add_ps(b5, x3);            /* b5 = b5 + x3; */         \
+    b3 = _mm_add_ps(b3, x2);            /* b3 = b3 + x2; */         \
+    b1 = _mm_add_ps(b1, x3)             /* b1 = b1 + x3; */         \
+    /* }}} */
+
+#define FDCTCOL(b0, b1, b2, b3, b4, b5, b6, b7) /* {{{ */           \
+    x0 = _mm_add_ps(b0, b7);            /* x0 = b0 + b7; */         \
+    x7 = _mm_sub_ps(b0, b7);            /* x7 = b0 - b7; */         \
+    x1 = _mm_add_ps(b1, b6);            /* x1 = b1 + b6; */         \
+    x6 = _mm_sub_ps(b1, b6);            /* x6 = b1 - b6; */         \
+    x2 = _mm_add_ps(b2, b5);            /* x2 = b2 + b5; */         \
+    x5 = _mm_sub_ps(b2, b5);            /* x5 = b2 - b5; */         \
+    x3 = _mm_add_ps(b3, b4);            /* x3 = b3 + b4; */         \
+    x4 = _mm_sub_ps(b3, b4);            /* x4 = b3 - b4; */         \
+                                                                    \
+    b7 = _mm_add_ps(x0, x3);            /* b7 = x0 + x3; */         \
+    b1 = _mm_add_ps(x1, x2);            /* b1 = x1 + x2; */         \
+    b0 = _mm_add_ps(b7, b1);            /* b0 = b7 + b1; */         \
+    b4 = _mm_sub_ps(b7, b1);            /* b4 = b7 - b1; */         \
+                                                                    \
+    b2   = _mm_sub_ps(x0, x3);          /* b2 = x0 - x3; */         \
+    b6   = _mm_sub_ps(x1, x2);          /* b6 = x1 - x2; */         \
+    b5   = _mm_add_ps(b6, b2);          /* b5 = b6 + b2; */         \
+    cnst = LD_W2;                                                   \
+    b5   = _mm_mul_ps(cnst, b5);        /* b5 = b5 * W2; */         \
+    cnst = LD_W1;                                                   \
+    b2   = _mm_madd_ps(cnst, b2, b5);   /* b2 = b5 + b2 * W1; */    \
+    cnst = LD_W0;                                                   \
+    b6   = _mm_madd_ps(cnst, b6, b5);   /* b6 = b5 + b6 * W0; */    \
+                                                                    \
+    x0   = _mm_add_ps(x4, x7);          /* x0 = x4 + x7; */         \
+    x1   = _mm_add_ps(x5, x6);          /* x1 = x5 + x6; */         \
+    x2   = _mm_add_ps(x4, x6);          /* x2 = x4 + x6; */         \
+    x3   = _mm_add_ps(x5, x7);          /* x3 = x5 + x7; */         \
+    x8   = _mm_add_ps(x2, x3);          /* x8 = x2 + x3; */         \
+    cnst = LD_W3;                                                   \
+    x8   = _mm_mul_ps(cnst, x8);        /* x8 = x8 * W3; */         \
+                                                                    \
+    cnst = LD_W8;                                                   \
+    x0   = _mm_mul_ps(cnst, x0);        /* x0 *= W8; */             \
+    cnst = LD_W9;                                                   \
+    x1   = _mm_mul_ps(cnst, x1);        /* x1 *= W9; */             \
+    cnst = LD_WA;                                                   \
+    x2   = _mm_madd_ps(cnst, x2, x8);   /* x2 = x2 * WA + x8; */    \
+    cnst = LD_WB;                                                   \
+    x3   = _mm_madd_ps(cnst, x3, x8);   /* x3 = x3 * WB + x8; */    \
+                                                                    \
+    cnst = LD_W4;                                                   \
+    b7   = _mm_madd_ps(cnst, x4, x0);   /* b7 = x4 * W4 + x0; */    \
+    cnst = LD_W5;                                                   \
+    b5   = _mm_madd_ps(cnst, x5, x1);   /* b5 = x5 * W5 + x1; */    \
+    cnst = LD_W6;                                                   \
+    b3   = _mm_madd_ps(cnst, x6, x1);   /* b3 = x6 * W6 + x1; */    \
+    cnst = LD_W7;                                                   \
+    b1   = _mm_madd_ps(cnst, x7, x0);   /* b1 = x7 * W7 + x0; */    \
+                                                                    \
+    b7 = _mm_add_ps(b7, x2);            /* b7 += x2; */             \
+    b5 = _mm_add_ps(b5, x3);            /* b5 += x3; */             \
+    b3 = _mm_add_ps(b3, x2);            /* b3 += x2; */             \
+    b1 = _mm_add_ps(b1, x3)             /* b1 += x3; */             \
+    /* }}} */
+
+/* two dimensional discrete cosine transform */
+void ff_fdct_e2k(int16_t *block)
+{
+    vec_f b00, b10, b20, b30, b40, b50, b60, b70;
+    vec_f b01, b11, b21, b31, b41, b51, b61, b71;
+    vec_f cnst;
+    vec_f x0, x1, x2, x3, x4, x5, x6, x7, x8;
+    vec_s16 a0, a1, a2, a3, a4, a5, a6, a7;
+    vec_s16 z0, z1, z2, z3, z4, z5, z6, z7;
+
+    a0 = VEC_LD(block + 8 * 0);
+    a4 = VEC_LD(block + 8 * 4);
+    a1 = VEC_LD(block + 8 * 1);
+    a5 = VEC_LD(block + 8 * 5);
+    a2 = VEC_LD(block + 8 * 2);
+    a6 = VEC_LD(block + 8 * 6);
+    a3 = VEC_LD(block + 8 * 3);
+    a7 = VEC_LD(block + 8 * 7);
+
+    TRANSPOSE8(a0, a1, a2, a3, a4, a5, a6, a7);
+
+    /* Some of the initial calculations can be done as vector short
+     * before conversion to vec_f.  The following code section
+     * takes advantage of this. */
+
+    /* fdct rows {{{ */
+    z0 = _mm_add_epi16(a0, a7);
+    z7 = _mm_sub_epi16(a0, a7);
+    z1 = _mm_add_epi16(a1, a6);
+    z6 = _mm_sub_epi16(a1, a6);
+    z2 = _mm_add_epi16(a2, a5);
+    z5 = _mm_sub_epi16(a2, a5);
+    z3 = _mm_add_epi16(a3, a4);
+    z4 = _mm_sub_epi16(a3, a4);
+
+#define CTF0(n) \
+    b##n##0 = _mm_cvtepi32_ps(_mm_srai_epi32(_mm_unpacklo_epi16(a##n, a##n), 16));\
+    b##n##1 = _mm_cvtepi32_ps(_mm_srai_epi32(_mm_unpackhi_epi16(a##n, a##n), 16));
+
+    a7 = _mm_add_epi16(z0, z3);
+    a1 = _mm_add_epi16(z1, z2);
+    a0 = _mm_add_epi16(a7, a1);
+    a4 = _mm_sub_epi16(a7, a1);
+    CTF0(0);
+    CTF0(4);
+
+    a2 = _mm_sub_epi16(z0, z3);
+    a6 = _mm_sub_epi16(z1, z2);
+    CTF0(2);
+    CTF0(6);
+
+#undef CTF0
+
+    x0 = _mm_add_ps(b60, b20);
+    x1 = _mm_add_ps(b61, b21);
+
+    cnst = LD_W2;
+    x0   = _mm_mul_ps(cnst, x0);
+    x1   = _mm_mul_ps(cnst, x1);
+    cnst = LD_W1;
+    b20  = _mm_madd_ps(cnst, b20, x0);
+    b21  = _mm_madd_ps(cnst, b21, x1);
+    cnst = LD_W0;
+    b60  = _mm_madd_ps(cnst, b60, x0);
+    b61  = _mm_madd_ps(cnst, b61, x1);
+
+#define CTFX(x, b) \
+    b##0 = _mm_cvtepi32_ps(_mm_srai_epi32(_mm_unpacklo_epi16(x, x), 16)); \
+    b##1 = _mm_cvtepi32_ps(_mm_srai_epi32(_mm_unpackhi_epi16(x, x), 16));
+
+    CTFX(z4, b7);
+    CTFX(z5, b5);
+    CTFX(z6, b3);
+    CTFX(z7, b1);
+
+#undef CTFX
+
+    x0   = _mm_add_ps(b70, b10);
+    x1   = _mm_add_ps(b50, b30);
+    x2   = _mm_add_ps(b70, b30);
+    x3   = _mm_add_ps(b50, b10);
+    x8   = _mm_add_ps(x2, x3);
+    cnst = LD_W3;
+    x8   = _mm_mul_ps(cnst, x8);
+
+    cnst = LD_W8;
+    x0   = _mm_mul_ps(cnst, x0);
+    cnst = LD_W9;
+    x1   = _mm_mul_ps(cnst, x1);
+    cnst = LD_WA;
+    x2   = _mm_madd_ps(cnst, x2, x8);
+    cnst = LD_WB;
+    x3   = _mm_madd_ps(cnst, x3, x8);
+
+    cnst = LD_W4;
+    b70  = _mm_madd_ps(cnst, b70, x0);
+    cnst = LD_W5;
+    b50  = _mm_madd_ps(cnst, b50, x1);
+    cnst = LD_W6;
+    b30  = _mm_madd_ps(cnst, b30, x1);
+    cnst = LD_W7;
+    b10  = _mm_madd_ps(cnst, b10, x0);
+
+    b70 = _mm_add_ps(b70, x2);
+    b50 = _mm_add_ps(b50, x3);
+    b30 = _mm_add_ps(b30, x2);
+    b10 = _mm_add_ps(b10, x3);
+
+    x0   = _mm_add_ps(b71, b11);
+    x1   = _mm_add_ps(b51, b31);
+    x2   = _mm_add_ps(b71, b31);
+    x3   = _mm_add_ps(b51, b11);
+    x8   = _mm_add_ps(x2, x3);
+    cnst = LD_W3;
+    x8   = _mm_mul_ps(cnst, x8);
+
+    cnst = LD_W8;
+    x0   = _mm_mul_ps(cnst, x0);
+    cnst = LD_W9;
+    x1   = _mm_mul_ps(cnst, x1);
+    cnst = LD_WA;
+    x2   = _mm_madd_ps(cnst, x2, x8);
+    cnst = LD_WB;
+    x3   = _mm_madd_ps(cnst, x3, x8);
+
+    cnst = LD_W4;
+    b71  = _mm_madd_ps(cnst, b71, x0);
+    cnst = LD_W5;
+    b51  = _mm_madd_ps(cnst, b51, x1);
+    cnst = LD_W6;
+    b31  = _mm_madd_ps(cnst, b31, x1);
+    cnst = LD_W7;
+    b11  = _mm_madd_ps(cnst, b11, x0);
+
+    b71 = _mm_add_ps(b71, x2);
+    b51 = _mm_add_ps(b51, x3);
+    b31 = _mm_add_ps(b31, x2);
+    b11 = _mm_add_ps(b11, x3);
+    /* }}} */
+
+    /* 8x8 matrix transpose (vec_f[8][2]) {{{ */
+    x0 = _mm_unpacklo_ps(b00, b10);
+    x1 = _mm_unpackhi_ps(b00, b10);
+    x2 = _mm_unpacklo_ps(b20, b30);
+    x3 = _mm_unpackhi_ps(b20, b30);
+    b00 = _mm_unpacklo_ps2(x0, x2);
+    b10 = _mm_unpackhi_ps2(x0, x2);
+    b20 = _mm_unpacklo_ps2(x1, x3);
+    b30 = _mm_unpackhi_ps2(x1, x3);
+
+    x4 = _mm_unpacklo_ps(b41, b51);
+    x5 = _mm_unpackhi_ps(b41, b51);
+    x6 = _mm_unpacklo_ps(b61, b71);
+    x7 = _mm_unpackhi_ps(b61, b71);
+    b41 = _mm_unpacklo_ps2(x4, x6);
+    b51 = _mm_unpackhi_ps2(x4, x6);
+    b61 = _mm_unpacklo_ps2(x5, x7);
+    b71 = _mm_unpackhi_ps2(x5, x7);
+
+    x0 = _mm_unpacklo_ps(b01, b11);
+    x1 = _mm_unpackhi_ps(b01, b11);
+    x2 = _mm_unpacklo_ps(b21, b31);
+    x3 = _mm_unpackhi_ps(b21, b31);
+    x4 = _mm_unpacklo_ps(b40, b50);
+    x5 = _mm_unpackhi_ps(b40, b50);
+    x6 = _mm_unpacklo_ps(b60, b70);
+    x7 = _mm_unpackhi_ps(b60, b70);
+    b40 = _mm_unpacklo_ps2(x0, x2);
+    b50 = _mm_unpackhi_ps2(x0, x2);
+    b60 = _mm_unpacklo_ps2(x1, x3);
+    b70 = _mm_unpackhi_ps2(x1, x3);
+    b01 = _mm_unpacklo_ps2(x4, x6);
+    b11 = _mm_unpackhi_ps2(x4, x6);
+    b21 = _mm_unpacklo_ps2(x5, x7);
+    b31 = _mm_unpackhi_ps2(x5, x7);
+    /* }}} */
+
+    FDCTCOL(b00, b10, b20, b30, b40, b50, b60, b70);
+    FDCTCOL(b01, b11, b21, b31, b41, b51, b61, b71);
+
+    /* round, convert back to short */
+#define CTS(n) \
+    a##n = _mm_packs_epi32(_mm_cvtps_epi32(b##n##0), _mm_cvtps_epi32(b##n##1)); \
+    VEC_ST(block + 8 * n, a##n)
+
+    CTS(0); CTS(1); CTS(2); CTS(3);
+    CTS(4); CTS(5); CTS(6); CTS(7);
+
+#undef CTS
+}
+
+av_cold void ff_fdctdsp_init_e2k(FDCTDSPContext *c, AVCodecContext *avctx,
+                                 unsigned high_bit_depth)
+{
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+    // libavcodec/tests/dct
+
+    if (!high_bit_depth) {
+        if (avctx->dct_algo == FF_DCT_AUTO ||
+            avctx->dct_algo == FF_DCT_ALTIVEC) {
+            c->fdct = ff_fdct_e2k;
+        }
+    }
+}
diff --git a/libavcodec/e2k/fft.c b/libavcodec/e2k/fft.c
new file mode 100644
index 0000000..28383a9
--- /dev/null
+++ b/libavcodec/e2k/fft.c
@@ -0,0 +1,1043 @@
+/*
+ * FFT transform
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2014 Rong Yan
+ * Copyright (c) 2009 Loren Merritt
+ *
+ * This algorithm (though not any of the implementation details) is
+ * based on libdjbfft by D. J. Bernstein.
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+
+#include "config.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+#include "libavcodec/fft.h"
+#include "libavcodec/fft-internal.h"
+#include "fft.h"
+
+#define _mm_madd_ps(a, b, c) _mm_add_ps(_mm_mul_ps(a, b), c)
+#define _mm_msub_ps(a, b, c) _mm_sub_ps(_mm_mul_ps(a, b), c)
+#define _mm_nmsub_ps(a, b, c) _mm_sub_ps(c, _mm_mul_ps(a, b))
+
+static av_always_inline
+void pass_e2k_interleave(FFTComplex *z, const FFTSample *wre, unsigned int n)
+{
+    int i1 = n * 4, i2 = n * 8, i3 = n * 12;
+    FFTSample *out = (FFTSample*)z;
+    const FFTSample *wim = wre + n * 2;
+    vec_f vz0, vzo1, vzo2, vzo3;
+    vec_f x0, x1, x2, x3;
+    vec_f x4, x5, x6, x7;
+    vec_f x8, x9, x10, x11;
+    vec_f x12, x13, x14, x15;
+    vec_f x16, x17, x18, x19;
+    vec_f x20, x21, x22, x23;
+    vec_f vz0plus1, vzo1plus1, vzo2plus1, vzo3plus1;
+    vec_f y0, y1, y2, y3;
+    vec_f y4, y5, y8, y9;
+    vec_f y10, y13, y14, y15;
+    vec_f y16, y17, y18, y19;
+    vec_f y20, y21, y22, y23;
+    vec_f wr1, wi1, wr0, wi0;
+    vec_f wr2, wi2, wr3, wi3;
+    vec_f xmulwi0, xmulwi1, ymulwi2, ymulwi3;
+
+    n = n - 2;
+    vzo2 = _mm_loadu_ps(out + i2);  // zo2.r  zo2.i  z(o2+1).r  z(o2+1).i
+    vzo2plus1 = _mm_loadu_ps(out + i2 + 4);
+    vzo3 = _mm_loadu_ps(out + i3);  // zo3.r  zo3.i  z(o3+1).r  z(o3+1).i
+    vzo3plus1 = _mm_loadu_ps(out + i3 + 4);
+    vz0 = _mm_loadu_ps(out);    // z0.r  z0.i  z1.r  z1.i
+    vz0plus1 = _mm_loadu_ps(out + 4);
+    vzo1 = _mm_loadu_ps(out + i1);  // zo1.r  zo1.i  z(o1+1).r  z(o1+1).i
+    vzo1plus1 = _mm_loadu_ps(out + i1 + 4);
+
+    x0 = _mm_add_ps(vzo2, vzo3);
+    x1 = _mm_sub_ps(vzo2, vzo3);
+    y0 = _mm_add_ps(vzo2plus1, vzo3plus1);
+    y1 = _mm_sub_ps(vzo2plus1, vzo3plus1);
+
+    wr1 = _mm_set1_ps(wre[1]);
+    wi1 = _mm_set1_ps(wim[-1]);
+    wi2 = _mm_set1_ps(wim[-2]);
+    wi3 = _mm_set1_ps(wim[-3]);
+    wr2 = _mm_set1_ps(wre[2]);
+    wr3 = _mm_set1_ps(wre[3]);
+
+    x2 = _mm_unpackhi_ps(x0, x1);
+    x3 = _mm_shuffle_ps(x2, x2, 0x1b);
+
+    y2 = _mm_unpacklo_ps(y0, y1);
+    y3 = _mm_unpackhi_ps(y0, y1);
+    y4 = _mm_shuffle_ps(y2, y2, 0x1b);
+    y5 = _mm_shuffle_ps(y3, y3, 0x1b);
+
+    ymulwi2 = _mm_mul_ps(y4, wi2);
+    ymulwi3 = _mm_mul_ps(y5, wi3);
+    x4 = _mm_mul_ps(x2, wr1);
+    x5 = _mm_mul_ps(x3, wi1);
+    y8 = _mm_madd_ps(y2, wr2, ymulwi2);
+    y9 = _mm_msub_ps(y2, wr2, ymulwi2);
+    x6 = _mm_add_ps(x4, x5);
+    x7 = _mm_sub_ps(x4, x5);
+    y13 = _mm_madd_ps(y3, wr3, ymulwi3);
+    y14 = _mm_msub_ps(y3, wr3, ymulwi3);
+
+    x8 = _mm_shuffle_ps(x6, x7, 0xe4);
+    y10 = _mm_shuffle_ps(y8, y9, 0xe4);
+    y15 = _mm_shuffle_ps(y13, y14, 0xe4);
+
+    x9 = _mm_shuffle_ps(x0, x8, 0x84);
+    x10 = _mm_shuffle_ps(x1, x8, 0x71);
+
+    y16 = _mm_shuffle_ps(y10, y15, 0x88);
+    y17 = _mm_shuffle_ps(y10, y15, 0x77);
+
+    x11 = _mm_add_ps(vz0, x9);
+    x12 = _mm_sub_ps(vz0, x9);
+    x13 = _mm_add_ps(vzo1, x10);
+    x14 = _mm_sub_ps(vzo1, x10);
+
+    y18 = _mm_add_ps(vz0plus1, y16);
+    y19 = _mm_sub_ps(vz0plus1, y16);
+    y20 = _mm_add_ps(vzo1plus1, y17);
+    y21 = _mm_sub_ps(vzo1plus1, y17);
+
+    x15 = _mm_blend_ps(x13, x14, 0xa);
+    x16 = _mm_blend_ps(x14, x13, 0xa);
+    y22 = _mm_blend_ps(y20, y21, 0xa);
+    y23 = _mm_blend_ps(y21, y20, 0xa);
+
+    _mm_storeu_ps(out, x11);
+    _mm_storeu_ps(out + 4, y18);
+    _mm_storeu_ps(out + i1, x15);
+    _mm_storeu_ps(out + i1 + 4, y22);
+    _mm_storeu_ps(out + i2, x12);
+    _mm_storeu_ps(out + i2 + 4, y19);
+    _mm_storeu_ps(out + i3, x16);
+    _mm_storeu_ps(out + i3 + 4, y23);
+
+    do {
+        out += 8;
+        wre += 4;
+        wim -= 4;
+        wr0 = _mm_set1_ps(wre[0]);
+        wr1 = _mm_set1_ps(wre[1]);
+        wi0 = _mm_set1_ps(wim[0]);
+        wi1 = _mm_set1_ps(wim[-1]);
+
+        wr2 = _mm_set1_ps(wre[2]);
+        wr3 = _mm_set1_ps(wre[3]);
+        wi2 = _mm_set1_ps(wim[-2]);
+        wi3 = _mm_set1_ps(wim[-3]);
+
+        vzo2 = _mm_loadu_ps(out + i2);  // zo2.r  zo2.i  z(o2+1).r  z(o2+1).i
+        vzo2plus1 = _mm_loadu_ps(out + i2 + 4);
+        vzo3 = _mm_loadu_ps(out + i3);  // zo3.r  zo3.i  z(o3+1).r  z(o3+1).i
+        vzo3plus1 = _mm_loadu_ps(out + i3 + 4);
+        vz0 = _mm_loadu_ps(out + 0);    // z0.r  z0.i  z1.r  z1.i
+        vz0plus1 = _mm_loadu_ps(out + 4);
+        vzo1 = _mm_loadu_ps(out + i1); // zo1.r  zo1.i  z(o1+1).r  z(o1+1).i
+        vzo1plus1 = _mm_loadu_ps(out + i1 + 4);
+
+        x0 = _mm_add_ps(vzo2, vzo3);
+        x1 = _mm_sub_ps(vzo2, vzo3);
+
+        y0 = _mm_add_ps(vzo2plus1, vzo3plus1);
+        y1 = _mm_sub_ps(vzo2plus1, vzo3plus1);
+
+        x2 = _mm_unpacklo_ps(x0, x1);
+        x3 = _mm_unpackhi_ps(x0, x1);
+        x4 = _mm_shuffle_ps(x2, x2, 0x1b);
+        x5 = _mm_shuffle_ps(x3, x3, 0x1b);
+
+        y2 = _mm_unpacklo_ps(y0, y1);
+        y3 = _mm_unpackhi_ps(y0, y1);
+        y4 = _mm_shuffle_ps(y2, y2, 0x1b);
+        y5 = _mm_shuffle_ps(y3, y3, 0x1b);
+
+        xmulwi0 = _mm_mul_ps(x4, wi0);
+        xmulwi1 = _mm_mul_ps(x5, wi1);
+        x8 = _mm_madd_ps(x2, wr0, xmulwi0);
+        x9 = _mm_msub_ps(x2, wr0, xmulwi0);
+
+        ymulwi2 = _mm_mul_ps(y4, wi2);
+        ymulwi3 = _mm_mul_ps(y5, wi3);
+        x13 = _mm_madd_ps(x3, wr1, xmulwi1);
+        x14 = _mm_msub_ps(x3, wr1, xmulwi1);
+
+        y8 = _mm_madd_ps(y2, wr2, ymulwi2);
+        y9 = _mm_msub_ps(y2, wr2, ymulwi2);
+        y13 = _mm_madd_ps(y3, wr3, ymulwi3);
+        y14 = _mm_msub_ps(y3, wr3, ymulwi3);
+
+        x10 = _mm_shuffle_ps(x8, x9, 0xe4);
+        x15 = _mm_shuffle_ps(x13, x14, 0xe4);
+
+        y10 = _mm_shuffle_ps(y8, y9, 0xe4);
+        y15 = _mm_shuffle_ps(y13, y14, 0xe4);
+
+        x16 = _mm_shuffle_ps(x10, x15, 0x88);
+        x17 = _mm_shuffle_ps(x10, x15, 0x77);
+
+        y16 = _mm_shuffle_ps(y10, y15, 0x88);
+        y17 = _mm_shuffle_ps(y10, y15, 0x77);
+
+        x18 = _mm_add_ps(vz0, x16);
+        x19 = _mm_sub_ps(vz0, x16);
+        x20 = _mm_add_ps(vzo1, x17);
+        x21 = _mm_sub_ps(vzo1, x17);
+
+        y18 = _mm_add_ps(vz0plus1, y16);
+        y19 = _mm_sub_ps(vz0plus1, y16);
+        y20 = _mm_add_ps(vzo1plus1, y17);
+        y21 = _mm_sub_ps(vzo1plus1, y17);
+
+        x22 = _mm_blend_ps(x20, x21, 0xa);
+        x23 = _mm_blend_ps(x21, x20, 0xa);
+        y22 = _mm_blend_ps(y20, y21, 0xa);
+        y23 = _mm_blend_ps(y21, y20, 0xa);
+
+        _mm_storeu_ps(out, x18);
+        _mm_storeu_ps(out + 4, y18);
+        _mm_storeu_ps(out + i1, x22);
+        _mm_storeu_ps(out + i1 + 4, y22);
+        _mm_storeu_ps(out + i2, x19);
+        _mm_storeu_ps(out + i2 + 4, y19);
+        _mm_storeu_ps(out + i3, x23);
+        _mm_storeu_ps(out + i3 + 4, y23);
+    } while (n -= 2);
+}
+
+static av_always_inline
+void fft2_e2k_interleave(FFTComplex *z)
+{
+#if 1
+    vec_f a, b, c;
+    float *out = (float*)z;
+    vec_f sign = _mm_castsi128_ps(_mm_set1_epi32(-1 << 31));
+
+    a = _mm_loadu_ps(out);
+    b = _mm_unpacklo_ps2(a, a);
+    c = _mm_unpackhi_ps2(a, _mm_xor_ps(a, sign));
+    a = _mm_add_ps(b, c);
+    _mm_storeu_ps(out, a);
+#else
+    FFTSample r0, i0, r1, i1;
+    r0 = z[0].re; i0 = z[0].im;
+    r1 = z[1].re; i1 = z[1].im;
+    z[0].re = r0 + r1;
+    z[0].im = i0 + i1;
+    z[1].re = r0 - r1;
+    z[1].im = i0 - i1;
+#endif
+}
+
+static av_always_inline
+void fft4_e2k_interleave(FFTComplex *z)
+{
+    vec_f a, b, c, d;
+    float *out = (float*)z;
+    a = _mm_loadu_ps(out);
+    b = _mm_loadu_ps(out + 4);
+
+    c = _mm_shuffle_ps(a, b, 0x64);
+    d = _mm_shuffle_ps(a, b, 0xce);
+    a = _mm_add_ps(c, d);
+    b = _mm_sub_ps(c, d);
+
+    c = _mm_unpacklo_ps2(a, b);
+    d = _mm_shuffle_ps(a, b, 0xbe);
+
+    a = _mm_add_ps(c, d);
+    b = _mm_sub_ps(c, d);
+    _mm_storeu_ps(out, a);
+    _mm_storeu_ps(out + 4, b);
+}
+
+static av_always_inline
+void fft8_e2k_interleave(FFTComplex *z)
+{
+    vec_f vz0, vz1, vz2, vz3;
+    vec_f x0, x1, x2, x3;
+    vec_f x4, x5, x6, x7;
+    vec_f x8, x9, x10, x11;
+    vec_f x12, x13, x14, x15;
+    vec_f x16, x17, x18, x19;
+    vec_f x20, x21, x22, x23;
+    vec_f x24, x25, x26, x27;
+    vec_f x28, x29, x30, x31;
+    vec_f x32, x33, x34;
+
+    float *out = (float*)z;
+    vec_f vc1 = _mm_set1_ps(sqrthalf);
+
+    vz0 = _mm_loadu_ps(out);
+    vz1 = _mm_loadu_ps(out + 4);
+    vz2 = _mm_loadu_ps(out + 8);
+    vz3 = _mm_loadu_ps(out + 12);
+
+    x0 = _mm_shuffle_ps(vz0, vz1, 0x64);
+    x1 = _mm_shuffle_ps(vz0, vz1, 0xce);
+    x2 = _mm_shuffle_ps(vz2, vz3, 0x46);
+    x3 = _mm_shuffle_ps(vz2, vz3, 0xec);
+
+    x4 = _mm_add_ps(x0, x1);
+    x5 = _mm_sub_ps(x0, x1);
+    x6 = _mm_add_ps(x2, x3);
+    x7 = _mm_sub_ps(x2, x3);
+
+    x8 = _mm_unpacklo_ps2(x4, x5);
+    x9 = _mm_shuffle_ps(x4, x5, 0xbe);
+    x10 = _mm_shuffle_ps(x6, x7, 0x66);
+    x11 = _mm_shuffle_ps(x6, x7, 0xcc);
+
+    x12 = _mm_add_ps(x8, x9);
+    x13 = _mm_sub_ps(x8, x9);
+    x14 = _mm_add_ps(x10, x11);
+    x15 = _mm_sub_ps(x10, x11);
+    x16 = _mm_unpacklo_ps(x12, x13);
+    x17 = _mm_unpacklo_ps(x14, x15);
+    x18 = _mm_shuffle_ps(x17, x17, 0x6c);
+    x19 = _mm_add_ps(x16, x18); // z0.r  z2.r  z0.i  z2.i
+    x20 = _mm_sub_ps(x16, x18); // z4.r  z6.r  z4.i  z6.i
+
+    x21 = _mm_unpackhi_ps(x12, x13);
+    x22 = _mm_unpackhi_ps(x14, x15);
+    x23 = _mm_unpackhi_ps2(x22, x22);
+    x24 = _mm_add_ps(x22, x23);
+    x25 = _mm_sub_ps(x22, x23);
+
+    x26 = _mm_unpacklo_ps2(x24, x25);
+    x26 = _mm_mul_ps(_mm_shuffle_ps(x26, x26, 0x8d), vc1); // 1,s1,0,s0
+
+    x27 = _mm_add_ps(x21, x26); // z1.r  z7.r  z1.i  z3.i
+    x28 = _mm_sub_ps(x21, x26); // z5.r  z3.r  z5.i  z7.i
+
+    x29 = _mm_shuffle_ps(x19, x27, 0x88); // z0.r  z0.i  z1.r  z1.i
+    x30 = _mm_shuffle_ps(x19, x27, 0xdd); // z2.r  z2.i  z7.r  z3.i
+    x31 = _mm_shuffle_ps(x20, x28, 0x88); // z4.r  z4.i  z5.r  z5.i
+    x32 = _mm_shuffle_ps(x20, x28, 0xdd); // z6.r  z6.i  z3.r  z7.i
+    x33 = _mm_blend_ps(x30, x32, 0x4); // z2.r  z2.i  z3.r  z3.i
+    x34 = _mm_blend_ps(x32, x30, 0x4); // z6.r  z6.i  z7.r  z7.i
+
+    _mm_storeu_ps(out, x29);
+    _mm_storeu_ps(out + 4, x33);
+    _mm_storeu_ps(out + 8, x31);
+    _mm_storeu_ps(out + 12, x34);
+}
+
+static av_always_inline
+void fft16_e2k_interleave(FFTComplex *z)
+{
+    float *out = (float*)z;
+    vec_f vc0 = _mm_set1_ps(sqrthalf);
+    vec_f vc1 = _mm_set1_ps(ff_cos_16[1]);
+    vec_f vc2 = _mm_set1_ps(ff_cos_16[3]);
+    vec_f vz0, vz1, vz2, vz3;
+    vec_f vz4, vz5, vz6, vz7;
+    vec_f x0, x1, x2, x3;
+    vec_f x4, x5, x6, x7;
+    vec_f x8, x9, x10, x11;
+    vec_f x12, x13, x14, x15;
+    vec_f x16, x17, x18, x19;
+    vec_f x20, x21, x22, x23;
+    vec_f x24, x25, x26, x27;
+    vec_f x28, x29, x30, x31;
+    vec_f x32, x33, x34, x35;
+    vec_f x36, x37, x38, x39;
+    vec_f x40, x41, x42, x43;
+    vec_f x44, x45, x46, x47;
+    vec_f x48, x49, x50, x51;
+    vec_f x52, x53, x54, x55;
+    vec_f x56, x57, x58, x59;
+    vec_f x60, x61, x62, x63;
+    vec_f x64, x65, x66, x67;
+    vec_f x68, x69, x70, x71;
+    vec_f x72, x73, x74, x75;
+    vec_f x76, x77, x78, x79;
+    vec_f x80, x81, x82, x83;
+    vec_f x84, x85, x86;
+
+    vz0 = _mm_loadu_ps(out);
+    vz1 = _mm_loadu_ps(out + 4);
+    vz2 = _mm_loadu_ps(out + 8);
+    vz3 = _mm_loadu_ps(out + 12);
+    vz4 = _mm_loadu_ps(out + 16);
+    vz5 = _mm_loadu_ps(out + 20);
+    vz6 = _mm_loadu_ps(out + 24);
+    vz7 = _mm_loadu_ps(out + 28);
+
+    x0 = _mm_shuffle_ps(vz0, vz1, 0x64);
+    x1 = _mm_shuffle_ps(vz0, vz1, 0xce);
+    x2 = _mm_unpacklo_ps2(vz2, vz3);
+    x3 = _mm_unpackhi_ps2(vz2, vz3);
+
+    x4 = _mm_shuffle_ps(vz4, vz5, 0x64);
+    x5 = _mm_shuffle_ps(vz4, vz5, 0xce);
+    x6 = _mm_shuffle_ps(vz6, vz7, 0x64);
+    x7 = _mm_shuffle_ps(vz6, vz7, 0xce);
+
+    x8 = _mm_add_ps(x0, x1);
+    x9 = _mm_sub_ps(x0, x1);
+    x10 = _mm_add_ps(x2, x3);
+    x11 = _mm_sub_ps(x2, x3);
+
+    x12 = _mm_add_ps(x4, x5);
+    x13 = _mm_sub_ps(x4, x5);
+    x14 = _mm_add_ps(x6, x7);
+    x15 = _mm_sub_ps(x6, x7);
+
+    x16 = _mm_unpacklo_ps2(x8, x9);
+    x17 = _mm_shuffle_ps(x8, x9, 0xbe);
+    x18 = _mm_shuffle_ps(x10, x11, 0x96);
+    x19 = _mm_shuffle_ps(x10, x11, 0xcc);
+    x20 = _mm_unpacklo_ps2(x12, x14);
+    x21 = _mm_unpackhi_ps2(x12, x14);
+    x22 = _mm_unpacklo_ps2(x13, x15);
+    x23 = _mm_shuffle_ps(x13, x15, 0xbb);
+
+    x24 = _mm_add_ps(x16, x17);
+    x25 = _mm_sub_ps(x16, x17);
+    x26 = _mm_add_ps(x18, x19);
+    x27 = _mm_sub_ps(x18, x19);
+    x28 = _mm_add_ps(x20, x21);
+    x29 = _mm_sub_ps(x20, x21);
+    x30 = _mm_add_ps(x22, x23);
+    x31 = _mm_sub_ps(x22, x23);
+
+    x32 = _mm_add_ps(x24, x26);
+    x33 = _mm_sub_ps(x24, x26);
+    x34 = _mm_unpacklo_ps2(x32, x33);
+
+    x35 = _mm_shuffle_ps(x28, x29, 0x96);
+    x36 = _mm_shuffle_ps(x28, x29, 0xcc);
+    x37 = _mm_add_ps(x35, x36);
+    x38 = _mm_sub_ps(x35, x36);
+    x39 = _mm_shuffle_ps(x37, x38, 0x14);
+
+    x40 = _mm_shuffle_ps(x27, x38, 0xeb);
+    x41 = _mm_shuffle_ps(x26, x37, 0xbe);
+    x42 = _mm_add_ps(x40, x41);
+    x43 = _mm_sub_ps(x40, x41);
+    x44 = _mm_mul_ps(x42, vc0);
+    x45 = _mm_mul_ps(x43, vc0);
+
+    x46 = _mm_add_ps(x34, x39);  // z0.r  z0.i  z4.r  z4.i
+    x47 = _mm_sub_ps(x34, x39);  // z8.r  z8.i  z12.r  z12.i
+
+    x48 = _mm_shuffle_ps(x30, x31, 0x96);
+    x49 = _mm_shuffle_ps(x30, x31, 0x3c);
+    x50 = _mm_add_ps(x48, x49);
+    x51 = _mm_sub_ps(x48, x49);
+    x52 = _mm_mul_ps(x50, vc1);
+    x53 = _mm_mul_ps(x50, vc2);
+    x54 = _mm_mul_ps(x51, vc1);
+    x55 = _mm_mul_ps(x51, vc2);
+
+    x56 = _mm_unpackhi_ps2(x24, x25);
+    x57 = _mm_shuffle_ps(x44, x45, 0x14);
+    x58 = _mm_add_ps(x56, x57);
+    x59 = _mm_sub_ps(x56, x57);
+
+    x60 = _mm_shuffle_ps(x54, x54, 0xb1);
+    x61 = _mm_shuffle_ps(x55, x55, 0xb1);
+    x62 = _mm_add_ps(x52, x61);
+    x63 = _mm_sub_ps(x52, x61);
+    x64 = _mm_add_ps(x60, x53);
+    x65 = _mm_sub_ps(x60, x53);
+    x66 = _mm_shuffle_ps(x62, x64, 0xb4);
+    x67 = _mm_shuffle_ps(x65, x63, 0xb4);
+
+    x68 = _mm_add_ps(x58, x66); // z1.r    z1.i  z3.r    z3.i
+    x69 = _mm_sub_ps(x58, x66); // z9.r    z9.i  z11.r  z11.i
+    x70 = _mm_add_ps(x59, x67); // z5.r    z5.i  z15.r  z15.i
+    x71 = _mm_sub_ps(x59, x67); // z13.r  z13.i z7.r   z7.i
+
+    x72 = _mm_shuffle_ps(x27, x27, 0xe1);
+    x73 = _mm_add_ps(x25, x72);
+    x74 = _mm_sub_ps(x25, x72);
+    x75 = _mm_unpacklo_ps2(x73, x74);
+    x76 = _mm_shuffle_ps(x44, x45, 0xeb);
+    x77 = _mm_add_ps(x75, x76); // z2.r   z2.i    z6.r    z6.i
+    x78 = _mm_sub_ps(x75, x76); // z10.r  z10.i  z14.r  z14.i
+
+    x79 = _mm_unpacklo_ps2(x46, x68); // z0.r  z0.i  z1.r  z1.i
+    x80 = _mm_shuffle_ps(x77, x68, 0xe4); // z2.r  z2.i  z3.r  z3.i
+    x81 = _mm_shuffle_ps(x46, x70, 0x4e); // z4.r  z4.i  z5.r  z5.i
+    x82 = _mm_unpackhi_ps2(x77, x71); // z6.r  z6.i  z7.r  z7.i
+    _mm_storeu_ps(out, x79);
+    _mm_storeu_ps(out + 4, x80);
+    _mm_storeu_ps(out + 8, x81);
+    _mm_storeu_ps(out + 12, x82);
+    x83 = _mm_unpacklo_ps2(x47, x69); // z8.r  z8.i  z9.r  z9.i
+    x84 = _mm_shuffle_ps(x78, x69, 0xe4); // z10.r  z10.i  z11.r  z11.i
+    x85 = _mm_shuffle_ps(x47, x71, 0x4e); // z12.r  z12.i  z13.r  z13.i
+    x86 = _mm_unpackhi_ps2(x78, x70); // z14.r  z14.i  z15.r  z15.i
+    _mm_storeu_ps(out + 16, x83);
+    _mm_storeu_ps(out + 20, x84);
+    _mm_storeu_ps(out + 24, x85);
+    _mm_storeu_ps(out + 28, x86);
+}
+
+static av_always_inline
+void fft4_e2k(FFTComplex *z)
+{
+    vec_f a, b, c, d;
+    float *out = (float*)z;
+    a = _mm_loadu_ps(out);
+    b = _mm_loadu_ps(out + 4);
+
+    c = _mm_shuffle_ps(a, b, 0x64);
+    d = _mm_shuffle_ps(a, b, 0xce);
+    a = _mm_add_ps(c, d);
+    b = _mm_sub_ps(c, d);
+
+    c = _mm_unpacklo_ps(a, b);
+    d = _mm_unpackhi_ps(a, _mm_shuffle_ps(b, b, 0xb1));
+
+    a = _mm_add_ps(c, d);
+    b = _mm_sub_ps(c, d);
+
+    c = _mm_unpacklo_ps2(a, b);
+    d = _mm_unpackhi_ps2(a, b);
+
+    _mm_storeu_ps(out, c);
+    _mm_storeu_ps(out + 4, d);
+}
+
+static av_always_inline
+void fft8_e2k(FFTComplex *z)
+{
+    vec_f vz0, vz1, vz2, vz3;
+    vec_f vz4, vz5, vz6, vz7, vz8;
+
+    float *out = (float*)z;
+    vec_f vc0 = _mm_setzero_ps();
+    vec_f vc1 = _mm_setr_ps(-sqrthalf, sqrthalf, sqrthalf, -sqrthalf);
+    vec_f vc2 = _mm_set1_ps(sqrthalf);
+
+    vz0 = _mm_loadu_ps(out);
+    vz1 = _mm_loadu_ps(out + 4);
+    vz2 = _mm_loadu_ps(out + 8);
+    vz3 = _mm_loadu_ps(out + 12);
+
+    vz6 = _mm_unpacklo_ps(vz2, vz3);
+    vz7 = _mm_unpackhi_ps(vz2, vz3);
+    vz4 = _mm_shuffle_ps(vz0, vz1, 0x64);
+    vz5 = _mm_shuffle_ps(vz0, vz1, 0xce);
+
+    vz2 = _mm_add_ps(vz6, vz7);
+    vz3 = _mm_sub_ps(vz6, vz7);
+    vz8 = _mm_shuffle_ps(vz3, vz3, 0x4e);
+
+    vz0 = _mm_add_ps(vz4, vz5);
+    vz1 = _mm_sub_ps(vz4, vz5);
+
+    vz3 = _mm_madd_ps(vz3, vc1, vc0);
+    vz3 = _mm_madd_ps(vz8, vc2, vz3);
+
+    vz4 = _mm_unpacklo_ps(vz0, vz1);
+    vz5 = _mm_unpackhi_ps(vz0, _mm_shuffle_ps(vz1, vz1, 0xb1));
+    vz6 = _mm_shuffle_ps(vz2, vz3, 0x39);
+    vz7 = _mm_shuffle_ps(vz2, vz3, 0x6c);
+
+    vz0 = _mm_add_ps(vz4, vz5);
+    vz1 = _mm_sub_ps(vz4, vz5);
+    vz2 = _mm_add_ps(vz6, vz7);
+    vz3 = _mm_sub_ps(vz6, vz7);
+
+    vz4 = _mm_unpacklo_ps2(vz0, vz1);
+    vz5 = _mm_unpackhi_ps2(vz0, vz1);
+    vz6 = _mm_shuffle_ps(vz2, vz3, 0xd8);
+    vz7 = _mm_shuffle_ps(vz2, vz3, 0x8d);
+
+    vz2 = _mm_sub_ps(vz4, vz6);
+    vz3 = _mm_sub_ps(vz5, vz7);
+
+    vz0 = _mm_add_ps(vz4, vz6);
+    vz1 = _mm_add_ps(vz5, vz7);
+
+    _mm_storeu_ps(out, vz0);
+    _mm_storeu_ps(out + 4, vz1);
+    _mm_storeu_ps(out + 8, vz2);
+    _mm_storeu_ps(out + 12, vz3);
+}
+
+static av_always_inline
+void fft16_e2k(FFTComplex *z)
+{
+    float *out = (float*)z;
+    vec_f vc0 = _mm_setzero_ps();
+    vec_f vc1 = _mm_setr_ps(-sqrthalf, sqrthalf, sqrthalf, -sqrthalf);
+    vec_f vc2 = _mm_set1_ps(sqrthalf);
+    vec_f vc3 = _mm_setr_ps(1.0, 0.92387953, sqrthalf, 0.38268343);
+    vec_f vc4 = _mm_setr_ps(0.0, 0.38268343, sqrthalf, 0.92387953);
+    vec_f vc5 = _mm_setr_ps(-0.0, -0.38268343, -sqrthalf, -0.92387953);
+
+    vec_f vz0, vz1, vz2, vz3;
+    vec_f vz4, vz5, vz6, vz7;
+    vec_f vz8, vz9, vz10, vz11;
+    vec_f vz12, vz13;
+
+    vz0 = _mm_loadu_ps(out + 16);
+    vz1 = _mm_loadu_ps(out + 20);
+    vz2 = _mm_loadu_ps(out + 24);
+    vz3 = _mm_loadu_ps(out + 28);
+
+    vz4 = _mm_shuffle_ps(vz0, vz1, 0x64);
+    vz5 = _mm_shuffle_ps(vz0, vz1, 0xce);
+    vz6 = _mm_shuffle_ps(vz2, vz3, 0x64);
+    vz7 = _mm_shuffle_ps(vz2, vz3, 0xce);
+
+    vz0 = _mm_add_ps(vz4, vz5);
+    vz1= _mm_sub_ps(vz4, vz5);
+    vz2 = _mm_add_ps(vz6, vz7);
+    vz3 = _mm_sub_ps(vz6, vz7);
+
+    vz4 = _mm_unpacklo_ps(vz0, vz1);
+    vz5 = _mm_unpackhi_ps(vz0, _mm_shuffle_ps(vz1, vz1, 0xb1));
+    vz6 = _mm_unpacklo_ps(vz2, vz3);
+    vz7 = _mm_unpackhi_ps(vz2, _mm_shuffle_ps(vz3, vz3, 0xb1));
+
+    vz0 = _mm_add_ps(vz4, vz5);
+    vz1 = _mm_sub_ps(vz4, vz5);
+    vz2 = _mm_add_ps(vz6, vz7);
+    vz3 = _mm_sub_ps(vz6, vz7);
+
+    vz4 = _mm_unpacklo_ps2(vz0, vz1);
+    vz5 = _mm_unpackhi_ps2(vz0, vz1);
+
+    vz6 = _mm_unpacklo_ps2(vz2, vz3);
+    vz7 = _mm_unpackhi_ps2(vz2, vz3);
+
+    vz0 = _mm_loadu_ps(out);
+    vz1 = _mm_loadu_ps(out + 4);
+    vz2 = _mm_loadu_ps(out + 8);
+    vz3 = _mm_loadu_ps(out + 12);
+    vz10 = _mm_unpacklo_ps(vz2, vz3);
+    vz11 = _mm_unpackhi_ps(vz2, vz3);
+    vz8 = _mm_shuffle_ps(vz0, vz1, 0x64);
+    vz9 = _mm_shuffle_ps(vz0, vz1, 0xce);
+
+    vz2 = _mm_add_ps(vz10, vz11);
+    vz3 = _mm_sub_ps(vz10, vz11);
+    vz12 = _mm_shuffle_ps(vz3, vz3, 0x4e);
+    vz0 = _mm_add_ps(vz8, vz9);
+    vz1 = _mm_sub_ps(vz8, vz9);
+
+    vz3 = _mm_madd_ps(vz3, vc1, vc0);
+    vz3 = _mm_madd_ps(vz12, vc2, vz3);
+    vz8 = _mm_unpacklo_ps(vz0, vz1);
+    vz9 = _mm_unpackhi_ps(vz0, _mm_shuffle_ps(vz1, vz1, 0xb1));
+    vz10 = _mm_shuffle_ps(vz2, vz3, 0x39);
+    vz11 = _mm_shuffle_ps(vz2, vz3, 0x6c);
+
+    vz0 = _mm_add_ps(vz8, vz9);
+    vz1 = _mm_sub_ps(vz8, vz9);
+    vz2 = _mm_add_ps(vz10, vz11);
+    vz3 = _mm_sub_ps(vz10, vz11);
+
+    vz8 = _mm_unpacklo_ps2(vz0, vz1);
+    vz9 = _mm_unpackhi_ps2(vz0, vz1);
+    vz10 = _mm_shuffle_ps(vz2, vz3, 0xd8);
+    vz11 = _mm_shuffle_ps(vz2, vz3, 0x8d);
+
+    vz2 = _mm_sub_ps(vz8, vz10);
+    vz3 = _mm_sub_ps(vz9, vz11);
+    vz0 = _mm_add_ps(vz8, vz10);
+    vz1 = _mm_add_ps(vz9, vz11);
+
+    vz8 = _mm_madd_ps(vz4, vc3, vc0);
+    vz9 = _mm_madd_ps(vz5, vc3, vc0);
+    vz10 = _mm_madd_ps(vz6, vc3, vc0);
+    vz11 = _mm_madd_ps(vz7, vc3, vc0);
+
+    vz8 = _mm_madd_ps(vz5, vc4, vz8);
+    vz9 = _mm_madd_ps(vz4, vc5, vz9);
+    vz10 = _mm_madd_ps(vz7, vc5, vz10);
+    vz11 = _mm_madd_ps(vz6, vc4, vz11);
+
+    vz12 = _mm_sub_ps(vz10, vz8);
+    vz10 = _mm_add_ps(vz10, vz8);
+
+    vz13 = _mm_sub_ps(vz9, vz11);
+    vz11 = _mm_add_ps(vz9, vz11);
+
+    vz4 = _mm_sub_ps(vz0, vz10);
+    vz0 = _mm_add_ps(vz0, vz10);
+
+    vz7 = _mm_sub_ps(vz3, vz12);
+    vz3 = _mm_add_ps(vz3, vz12);
+
+    vz5 = _mm_sub_ps(vz1, vz11);
+    vz1 = _mm_add_ps(vz1, vz11);
+
+    vz6 = _mm_sub_ps(vz2, vz13);
+    vz2 = _mm_add_ps(vz2, vz13);
+
+    _mm_storeu_ps(out, vz0);
+    _mm_storeu_ps(out + 4, vz1);
+    _mm_storeu_ps(out + 8, vz2);
+    _mm_storeu_ps(out + 12, vz3);
+    _mm_storeu_ps(out + 16, vz4);
+    _mm_storeu_ps(out + 20, vz5);
+    _mm_storeu_ps(out + 24, vz6);
+    _mm_storeu_ps(out + 28, vz7);
+}
+
+static av_always_inline
+void pass_e2k(FFTComplex *z, const FFTSample *wre, unsigned int n)
+{
+    int i1 = n * 4, i2 = n * 8, i3 = n * 12;
+    FFTSample *out = (FFTSample*)z;
+    const FFTSample *wim = wre + n * 2;
+    vec_f v0, v1, v2, v3;
+    vec_f v4, v5, v6, v7;
+    vec_f v8, v9, v10, v11;
+    vec_f v12, v13;
+
+    n = n - 2;
+
+    v8 = _mm_loadu_ps(wre);
+#if 0
+    v9 = _mm_loadu_ps(wim - 3);
+#else
+    v10 = _mm_loadu_ps(wim);
+    v9 = _mm_loadu_ps(wim - 4);
+    v9 = _mm_castsi128_ps(_mm_alignr_epi8(_mm_castps_si128(v10), _mm_castps_si128(v9), 4));
+#endif
+    v9 = _mm_shuffle_ps(v9, v9, 0x1b);
+
+    v4 = _mm_loadu_ps(out + i2);
+    v5 = _mm_loadu_ps(out + i2 + 4);
+    v6 = _mm_loadu_ps(out + i3);
+    v7 = _mm_loadu_ps(out + i3 + 4);
+    v10 = _mm_mul_ps(v4, v8); // r2*wre
+    v11 = _mm_mul_ps(v5, v8); // i2*wre
+    v12 = _mm_mul_ps(v6, v8); // r3*wre
+    v13 = _mm_mul_ps(v7, v8); // i3*wre
+
+    v0 = _mm_loadu_ps(out); // r0
+    v3 = _mm_loadu_ps(out + i1 + 4); // i1
+    v10 = _mm_madd_ps(v5, v9, v10); // r2*wim
+    v11 = _mm_nmsub_ps(v4, v9, v11); // i2*wim
+    v12 = _mm_nmsub_ps(v7, v9, v12); // r3*wim
+    v13 = _mm_madd_ps(v6, v9, v13); // i3*wim
+
+    v1 = _mm_loadu_ps(out + 4); // i0
+    v2 = _mm_loadu_ps(out + i1); // r1
+    v8 = _mm_sub_ps(v12, v10);
+    v12 = _mm_add_ps(v12, v10);
+    v9 = _mm_sub_ps(v11, v13);
+    v13 = _mm_add_ps(v11, v13);
+    v4 = _mm_sub_ps(v0, v12);
+    v0 = _mm_add_ps(v0, v12);
+    v7 = _mm_sub_ps(v3, v8);
+    v3 = _mm_add_ps(v3, v8);
+
+    _mm_storeu_ps(out, v0); // r0
+    _mm_storeu_ps(out + i1 + 4, v3); // i1
+    _mm_storeu_ps(out + i2, v4); // r2
+    _mm_storeu_ps(out + i3 + 4, v7);// i3
+
+    v5 = _mm_sub_ps(v1, v13);
+    v1 = _mm_add_ps(v1, v13);
+    v6 = _mm_sub_ps(v2, v9);
+    v2 = _mm_add_ps(v2, v9);
+
+    _mm_storeu_ps(out + 4, v1); // i0
+    _mm_storeu_ps(out + i1, v2); // r1
+    _mm_storeu_ps(out + i2 + 4, v5); // i2
+    _mm_storeu_ps(out + i3, v6); // r3
+
+    do {
+        out += 8;
+        wre += 4;
+        wim -= 4;
+
+        v8 = _mm_loadu_ps(wre);
+#if 0
+        v9 = _mm_loadu_ps(wim - 3);
+#else
+        v10 = _mm_loadu_ps(wim);
+        v9 = _mm_loadu_ps(wim - 4);
+        v9 = _mm_castsi128_ps(_mm_alignr_epi8(_mm_castps_si128(v10), _mm_castps_si128(v9), 4));
+#endif
+        v9 = _mm_shuffle_ps(v9, v9, 0x1b);
+
+        v4 = _mm_loadu_ps(out + i2); // r2
+        v5 = _mm_loadu_ps(out + i2 + 4); // i2
+        v6 = _mm_loadu_ps(out + i3); // r3
+        v7 = _mm_loadu_ps(out + i3 + 4);// i3
+        v10 = _mm_mul_ps(v4, v8); // r2*wre
+        v11 = _mm_mul_ps(v5, v8); // i2*wre
+        v12 = _mm_mul_ps(v6, v8); // r3*wre
+        v13 = _mm_mul_ps(v7, v8); // i3*wre
+
+        v0 = _mm_loadu_ps(out); // r0
+        v3 = _mm_loadu_ps(out + i1 + 4); // i1
+        v10 = _mm_madd_ps(v5, v9, v10); // r2*wim
+        v11 = _mm_nmsub_ps(v4, v9, v11); // i2*wim
+        v12 = _mm_nmsub_ps(v7, v9, v12); // r3*wim
+        v13 = _mm_madd_ps(v6, v9, v13); // i3*wim
+
+        v1 = _mm_loadu_ps(out + 4); // i0
+        v2 = _mm_loadu_ps(out + i1); // r1
+        v8 = _mm_sub_ps(v12, v10);
+        v12 = _mm_add_ps(v12, v10);
+        v9 = _mm_sub_ps(v11, v13);
+        v13 = _mm_add_ps(v11, v13);
+        v4 = _mm_sub_ps(v0, v12);
+        v0 = _mm_add_ps(v0, v12);
+        v7 = _mm_sub_ps(v3, v8);
+        v3 = _mm_add_ps(v3, v8);
+
+        _mm_storeu_ps(out, v0); // r0
+        _mm_storeu_ps(out + i1 + 4, v3); // i1
+        _mm_storeu_ps(out + i2, v4); // r2
+        _mm_storeu_ps(out + i3 + 4, v7); // i3
+
+        v5 = _mm_sub_ps(v1, v13);
+        v1 = _mm_add_ps(v1, v13);
+        v6 = _mm_sub_ps(v2, v9);
+        v2 = _mm_add_ps(v2, v9);
+
+        _mm_storeu_ps(out + 4, v1); // i0
+        _mm_storeu_ps(out + i1, v2); // r1
+        _mm_storeu_ps(out + i2 + 4, v5); // i2
+        _mm_storeu_ps(out + i3, v6); // r3
+    } while (n -= 2);
+}
+
+static void fft32_e2k_interleave(FFTComplex *z)
+{
+    fft16_e2k_interleave(z);
+    fft8_e2k_interleave(z+16);
+    fft8_e2k_interleave(z+24);
+    pass_e2k_interleave(z,ff_cos_32,4);
+}
+
+static void fft64_e2k_interleave(FFTComplex *z)
+{
+    fft32_e2k_interleave(z);
+    fft16_e2k_interleave(z+32);
+    fft16_e2k_interleave(z+48);
+    pass_e2k_interleave(z,ff_cos_64, 8);
+}
+
+static void fft128_e2k_interleave(FFTComplex *z)
+{
+    fft64_e2k_interleave(z);
+    fft32_e2k_interleave(z+64);
+    fft32_e2k_interleave(z+96);
+    pass_e2k_interleave(z,ff_cos_128,16);
+}
+
+static void fft256_e2k_interleave(FFTComplex *z)
+{
+    fft128_e2k_interleave(z);
+    fft64_e2k_interleave(z+128);
+    fft64_e2k_interleave(z+192);
+    pass_e2k_interleave(z,ff_cos_256,32);
+}
+
+static void fft512_e2k_interleave(FFTComplex *z)
+{
+    fft256_e2k_interleave(z);
+    fft128_e2k_interleave(z+256);
+    fft128_e2k_interleave(z+384);
+    pass_e2k_interleave(z,ff_cos_512,64);
+}
+
+static void fft1024_e2k_interleave(FFTComplex *z)
+{
+    fft512_e2k_interleave(z);
+    fft256_e2k_interleave(z+512);
+    fft256_e2k_interleave(z+768);
+    pass_e2k_interleave(z,ff_cos_1024,128);
+}
+
+static void fft2048_e2k_interleave(FFTComplex *z)
+{
+    fft1024_e2k_interleave(z);
+    fft512_e2k_interleave(z+1024);
+    fft512_e2k_interleave(z+1536);
+    pass_e2k_interleave(z,ff_cos_2048,256);
+}
+
+static void fft4096_e2k_interleave(FFTComplex *z)
+{
+    fft2048_e2k_interleave(z);
+    fft1024_e2k_interleave(z+2048);
+    fft1024_e2k_interleave(z+3072);
+    pass_e2k_interleave(z,ff_cos_4096, 512);
+}
+
+static void fft8192_e2k_interleave(FFTComplex *z)
+{
+    fft4096_e2k_interleave(z);
+    fft2048_e2k_interleave(z+4096);
+    fft2048_e2k_interleave(z+6144);
+    pass_e2k_interleave(z,ff_cos_8192,1024);
+}
+
+static void fft16384_e2k_interleave(FFTComplex *z)
+{
+    fft8192_e2k_interleave(z);
+    fft4096_e2k_interleave(z+8192);
+    fft4096_e2k_interleave(z+12288);
+    pass_e2k_interleave(z,ff_cos_16384,2048);
+}
+
+static void fft32768_e2k_interleave(FFTComplex *z)
+{
+    fft16384_e2k_interleave(z);
+    fft8192_e2k_interleave(z+16384);
+    fft8192_e2k_interleave(z+24576);
+    pass_e2k_interleave(z,ff_cos_32768,4096);
+}
+
+static void fft65536_e2k_interleave(FFTComplex *z)
+{
+    fft32768_e2k_interleave(z);
+    fft16384_e2k_interleave(z+32768);
+    fft16384_e2k_interleave(z+49152);
+    pass_e2k_interleave(z,ff_cos_65536,8192);
+}
+
+static void fft32_e2k(FFTComplex *z)
+{
+    fft16_e2k(z);
+    fft8_e2k(z+16);
+    fft8_e2k(z+24);
+    pass_e2k(z,ff_cos_32,4);
+}
+
+static void fft64_e2k(FFTComplex *z)
+{
+    fft32_e2k(z);
+    fft16_e2k(z+32);
+    fft16_e2k(z+48);
+    pass_e2k(z,ff_cos_64, 8);
+}
+
+static void fft128_e2k(FFTComplex *z)
+{
+    fft64_e2k(z);
+    fft32_e2k(z+64);
+    fft32_e2k(z+96);
+    pass_e2k(z,ff_cos_128,16);
+}
+
+static void fft256_e2k(FFTComplex *z)
+{
+    fft128_e2k(z);
+    fft64_e2k(z+128);
+    fft64_e2k(z+192);
+    pass_e2k(z,ff_cos_256,32);
+}
+
+static void fft512_e2k(FFTComplex *z)
+{
+    fft256_e2k(z);
+    fft128_e2k(z+256);
+    fft128_e2k(z+384);
+    pass_e2k(z,ff_cos_512,64);
+}
+
+static void fft1024_e2k(FFTComplex *z)
+{
+    fft512_e2k(z);
+    fft256_e2k(z+512);
+    fft256_e2k(z+768);
+    pass_e2k(z,ff_cos_1024,128);
+
+}
+
+static void fft2048_e2k(FFTComplex *z)
+{
+    fft1024_e2k(z);
+    fft512_e2k(z+1024);
+    fft512_e2k(z+1536);
+    pass_e2k(z,ff_cos_2048,256);
+}
+
+static void fft4096_e2k(FFTComplex *z)
+{
+    fft2048_e2k(z);
+    fft1024_e2k(z+2048);
+    fft1024_e2k(z+3072);
+    pass_e2k(z,ff_cos_4096, 512);
+}
+
+static void fft8192_e2k(FFTComplex *z)
+{
+    fft4096_e2k(z);
+    fft2048_e2k(z+4096);
+    fft2048_e2k(z+6144);
+    pass_e2k(z,ff_cos_8192,1024);
+}
+
+static void fft16384_e2k(FFTComplex *z)
+{
+    fft8192_e2k(z);
+    fft4096_e2k(z+8192);
+    fft4096_e2k(z+12288);
+    pass_e2k(z,ff_cos_16384,2048);
+}
+
+static void fft32768_e2k(FFTComplex *z)
+{
+    fft16384_e2k(z);
+    fft8192_e2k(z+16384);
+    fft8192_e2k(z+24576);
+    pass_e2k(z,ff_cos_32768,4096);
+}
+
+static void fft65536_e2k(FFTComplex *z)
+{
+    fft32768_e2k(z);
+    fft16384_e2k(z+32768);
+    fft16384_e2k(z+49152);
+    pass_e2k(z,ff_cos_65536,8192);
+}
+
+static void (* const fft_dispatch_e2k[])(FFTComplex*) = {
+    fft4_e2k, fft8_e2k, fft16_e2k, fft32_e2k, fft64_e2k, fft128_e2k, fft256_e2k, fft512_e2k, fft1024_e2k,
+    fft2048_e2k, fft4096_e2k, fft8192_e2k, fft16384_e2k, fft32768_e2k, fft65536_e2k,
+};
+
+static void (* const fft_dispatch_e2k_interleave[])(FFTComplex*) = {
+    fft4_e2k_interleave, fft8_e2k_interleave, fft16_e2k_interleave, fft32_e2k_interleave, fft64_e2k_interleave,
+    fft128_e2k_interleave, fft256_e2k_interleave, fft512_e2k_interleave, fft1024_e2k_interleave,
+    fft2048_e2k_interleave, fft4096_e2k_interleave, fft8192_e2k_interleave, fft16384_e2k_interleave, fft32768_e2k_interleave, fft65536_e2k_interleave,
+};
+
+void ff_fft_calc_interleave_e2k(FFTContext *s, FFTComplex *z)
+{
+     fft_dispatch_e2k_interleave[s->nbits-2](z);
+}
+
+void ff_fft_calc_e2k(FFTContext *s, FFTComplex *z)
+{
+     fft_dispatch_e2k[s->nbits-2](z);
+}
+
diff --git a/libavcodec/e2k/fft.h b/libavcodec/e2k/fft.h
new file mode 100644
index 0000000..62ae2f3
--- /dev/null
+++ b/libavcodec/e2k/fft.h
@@ -0,0 +1,29 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_E2K_FFT_H
+#define AVCODEC_E2K_FFT_H
+
+#include "config.h"
+#include "libavcodec/fft.h"
+#include "libavcodec/fft-internal.h"
+
+void ff_fft_calc_interleave_e2k(FFTContext *s, FFTComplex *z);
+void ff_fft_calc_e2k(FFTContext *s, FFTComplex *z);
+
+#endif /* AVCODEC_E2K_FFT_H */
diff --git a/libavcodec/e2k/fft_init.c b/libavcodec/e2k/fft_init.c
new file mode 100644
index 0000000..92a3009
--- /dev/null
+++ b/libavcodec/e2k/fft_init.c
@@ -0,0 +1,151 @@
+/*
+ * FFT/IFFT transforms
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2009 Loren Merritt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+#include "libavcodec/fft.h"
+
+#include "fft.h"
+
+/**
+ * Do a complex FFT with the parameters defined in ff_fft_init().
+ * The input data must be permuted before with s->revtab table.
+ * No 1.0 / sqrt(n) normalization is done.
+ *
+ * This code assumes that the 'z' pointer is 16 bytes-aligned.
+ * It also assumes all FFTComplex are 8 bytes-aligned pairs of floats.
+ */
+
+static void imdct_half_e2k(FFTContext *s, FFTSample *output, const FFTSample *input)
+{
+    int j, k;
+    int n = 1 << s->mdct_bits;
+    int n4 = n >> 2;
+    int n8 = n >> 3;
+    int n32 = n >> 5;
+    const uint16_t *revtabj = s->revtab;
+    const uint16_t *revtabk = s->revtab + n4;
+    const vec_f *tcos = (const vec_f*)(s->tcos + n8);
+    const vec_f *tsin = (const vec_f*)(s->tsin + n8);
+    const vec_f *pin = (const vec_f*)(input + n4);
+    vec_f *pout = (vec_f*)(output + n4);
+
+    /* pre rotation */
+    k = n32 - 1;
+    do {
+        vec_f cos, sin, cos0, sin0, cos1, sin1;
+        vec_f re, im, r0, i0, r1, i1, a, b;
+#define CMULA(p, perm) \
+    a = pin[ k*2+p];                 /* { z[k].re,    z[k].im,    z[k+1].re,  z[k+1].im  } */ \
+    b = pin[-k*2-p-1];               /* { z[-k-2].re, z[-k-2].im, z[-k-1].re, z[-k-1].im } */ \
+    re = _mm_shuffle_ps(a, b, 0x88); /* { z[k].re,    z[k+1].re,  z[-k-2].re, z[-k-1].re } */ \
+    im = _mm_shuffle_ps(b, a, 0x77); /* { z[-k-1].im, z[-k-2].im, z[k+1].im,  z[k].im    } */ \
+    cos = _mm_shuffle_ps(cos0, cos1, perm);  /* { cos[k], cos[k+1], cos[-k-2], cos[-k-1] } */ \
+    sin = _mm_shuffle_ps(sin0, sin1, perm); \
+    r##p = _mm_sub_ps(_mm_mul_ps(im, cos), _mm_mul_ps(re, sin)); \
+    i##p = _mm_add_ps(_mm_mul_ps(re, cos), _mm_mul_ps(im, sin));
+#define STORE2(L, v, dst) \
+    VEC_ST##L(output + dst * 2, _mm_castps_si128(v));
+#define STORE8(p) \
+    a = _mm_unpacklo_ps(r##p, i##p); \
+    b = _mm_unpackhi_ps(r##p, i##p); \
+    STORE2(L, a, revtabk[ p*2-4]); \
+    STORE2(H, a, revtabk[ p*2-3]); \
+    STORE2(L, b, revtabj[-p*2+2]); \
+    STORE2(H, b, revtabj[-p*2+3]);
+
+        cos0 = tcos[k];
+        sin0 = tsin[k];
+        cos1 = tcos[-k-1];
+        sin1 = tsin[-k-1];
+        CMULA(0, 0xe4);
+        CMULA(1, 0x4e);
+        STORE8(0);
+        STORE8(1);
+        revtabj += 4;
+        revtabk -= 4;
+        k--;
+    } while (k >= 0);
+
+    ff_fft_calc_e2k(s, (FFTComplex*)output);
+
+    /* post rotation + reordering */
+    j = -n32;
+    k = n32 - 1;
+    do {
+        vec_f cos, sin, re, im, a, b, c, d;
+#define CMULB(d0, d1, o)                  \
+    re = pout[o*2]; im = pout[o*2+1]; \
+    cos = tcos[o];  sin = tsin[o];    \
+    d0 = _mm_sub_ps(_mm_mul_ps(im, sin), _mm_mul_ps(re, cos)); \
+    d1 = _mm_add_ps(_mm_mul_ps(re, sin), _mm_mul_ps(im, cos));
+
+        CMULB(a, b, j);
+        CMULB(c, d, k);
+        d = _mm_shuffle_ps(d, d, 0x1b);
+        b = _mm_shuffle_ps(b, b, 0x1b);
+        pout[2*j]   = _mm_unpacklo_ps(a, d);
+        pout[2*j+1] = _mm_unpackhi_ps(a, d);
+        pout[2*k]   = _mm_unpacklo_ps(c, b);
+        pout[2*k+1] = _mm_unpackhi_ps(c, b);
+        j++;
+        k--;
+    } while (k >= 0);
+}
+
+static void imdct_calc_e2k(FFTContext *s, FFTSample *output, const FFTSample *input)
+{
+    int k;
+    int n = 1 << s->mdct_bits;
+    int n4 = n >> 2;
+    int n16 = n >> 4;
+    vec_u32 sign = _mm_set1_epi32(-1 << 31);
+    vec_u32 *p0 = (vec_u32*)(output + n4);
+    vec_u32 *p1 = (vec_u32*)(output + n4 * 3);
+
+    imdct_half_e2k(s, output + n4, input);
+
+    for (k = 0; k < n16; k++) {
+        vec_u32 a = p0[k] ^ sign;
+        vec_u32 b = p1[-1 - k];
+        p0[-1 - k] = _mm_shuffle_epi32(a, 0x1b);
+        p1[k]      = _mm_shuffle_epi32(b, 0x1b);
+    }
+}
+
+av_cold void ff_fft_init_e2k(FFTContext *s)
+{
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+    // libavcodec/tests/fft -n 2..14 [-i]
+    // libavcodec/tests/fft -{m|d|r} -n 4..14 [-i]
+
+    s->fft_calc = ff_fft_calc_interleave_e2k;
+
+    if (s->mdct_bits >= 5) {
+        s->imdct_calc = imdct_calc_e2k;
+        s->imdct_half = imdct_half_e2k;
+    }
+}
diff --git a/libavcodec/e2k/fmtconvert.c b/libavcodec/e2k/fmtconvert.c
new file mode 100644
index 0000000..9a5d646
--- /dev/null
+++ b/libavcodec/e2k/fmtconvert.c
@@ -0,0 +1,53 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2006 Luca Barbato <lu_zero@gentoo.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+#include "libavutil/attributes.h"
+#include "libavutil/mem.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+#include "libavcodec/fmtconvert.h"
+
+static void int32_to_float_fmul_scalar_e2k(float *dst, const int32_t *src,
+                                           float mul, int len)
+{
+    int i;
+    __m128 src1, src2, dst1, dst2, mul_v;
+    mul_v = _mm_set1_ps(mul);
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < len; i += 8) {
+        src1 = _mm_cvtepi32_ps(VEC_LD(src + i));
+        src2 = _mm_cvtepi32_ps(VEC_LD(src + i + 4));
+        dst1 = _mm_mul_ps(src1, mul_v);
+        dst2 = _mm_mul_ps(src2, mul_v);
+        _mm_storeu_ps(dst + i, dst1);
+        _mm_storeu_ps(dst + i + 4, dst2);
+    }
+}
+
+av_cold void ff_fmt_convert_init_e2k(FmtConvertContext *c)
+{
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    c->int32_to_float_fmul_scalar = int32_to_float_fmul_scalar_e2k;
+}
diff --git a/libavcodec/e2k/h264chroma.c b/libavcodec/e2k/h264chroma.c
new file mode 100644
index 0000000..422c8ae
--- /dev/null
+++ b/libavcodec/e2k/h264chroma.c
@@ -0,0 +1,56 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2004 Romain Dolbeau <romain@dolbeau.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/h264chroma.h"
+
+#define OP8_E2K(d, s, dst) d = s
+#define OP16_E2K(d, s, dst) d = s
+#define PREFIX_h264_chroma_mc4_e2k put_h264_chroma_mc4_e2k
+#define PREFIX_h264_chroma_mc8_e2k put_h264_chroma_mc8_e2k
+#include "h264chroma_template.c"
+
+#define OP8_E2K(d, s, dst) d = _mm_avg_pu8(dst, s)
+#define OP16_E2K(d, s, dst) d = _mm_avg_epu8(dst, s)
+#define PREFIX_h264_chroma_mc4_e2k avg_h264_chroma_mc4_e2k
+#define PREFIX_h264_chroma_mc8_e2k avg_h264_chroma_mc8_e2k
+#include "h264chroma_template.c"
+
+av_cold void ff_h264chroma_init_e2k(H264ChromaContext *c, int bit_depth)
+{
+    const int high_bit_depth = bit_depth > 8;
+
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+    if (!high_bit_depth) {
+        c->put_h264_chroma_pixels_tab[0] = put_h264_chroma_mc8_e2k;
+        c->avg_h264_chroma_pixels_tab[0] = avg_h264_chroma_mc8_e2k;
+        c->put_h264_chroma_pixels_tab[1] = put_h264_chroma_mc4_e2k;
+        c->avg_h264_chroma_pixels_tab[1] = avg_h264_chroma_mc4_e2k;
+    }
+}
diff --git a/libavcodec/e2k/h264chroma_template.c b/libavcodec/e2k/h264chroma_template.c
new file mode 100644
index 0000000..0a961d1
--- /dev/null
+++ b/libavcodec/e2k/h264chroma_template.c
@@ -0,0 +1,209 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/mem.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#define GET_VSRC1(v0) v0 = _mm_cvtsi32_si64(*(uint32_t*)src); src += stride;
+#define GET_VSRC2(v0, v1) \
+    v0 = _mm_cvtsi32_si64(*(uint32_t*)src); \
+    v1 = _mm_cvtsi32_si64(*(uint32_t*)(src + 1)); \
+    v0 = _mm_unpacklo_pi8(v0, v1); src += stride;
+
+#define CHROMA_MC4_CORE2() \
+    v1 = _mm_add_pi16(_mm_maddubs_pi16(v0, vA), bias); \
+    v1 = _mm_add_pi16(_mm_maddubs_pi16(v2, vB), v1); \
+    v1 = _mm_srai_pi16(v1, 6); \
+    v1 = _mm_packs_pu16(v1, v1); \
+    OP8_E2K(v1, v1, _mm_cvtsi32_si64(*(uint32_t*)dst)); \
+    *(uint32_t*)dst = _mm_cvtsi64_si32(v1); \
+    dst += stride; \
+    v0 = v2;
+
+#define CHROMA_MC4_CORE1(v1) \
+    v1 = _mm_add_pi16(_mm_maddubs_pi16(v1, vA), bias); \
+    v1 = _mm_srai_pi16(v1, 6); \
+    v1 = _mm_packs_pu16(v1, v1); \
+    OP8_E2K(v1, v1, _mm_cvtsi32_si64(*(uint32_t*)dst)); \
+    *(uint32_t*)dst = _mm_cvtsi64_si32(v1); \
+    dst += stride;
+
+#ifdef PREFIX_h264_chroma_mc4_e2k
+static void PREFIX_h264_chroma_mc4_e2k(uint8_t *dst, const uint8_t *src,
+                                       ptrdiff_t stride, int h,
+                                       int x, int y)
+{
+    int xm = (8 - x) | x << 8;
+    __m64 vA, vB, bias = _mm_set1_pi16(32);
+    __m64 v0, v1, v2, v3;
+
+    if (y) {
+        if (x) {
+            vA = _mm_set1_pi16(xm * (8 - y));
+            vB = _mm_set1_pi16(xm * y);
+            GET_VSRC2(v0, v1)
+            PRAGMA_E2K("ivdep")
+            do {
+                GET_VSRC2(v2, v3)
+                CHROMA_MC4_CORE2()
+            } while (--h > 0);
+        } else {
+            vA = _mm_set1_pi16(((8 - y) | y << 8) * 8);
+            GET_VSRC1(v0)
+            PRAGMA_E2K("ivdep")
+            do {
+                GET_VSRC1(v2)
+                v1 = _mm_unpacklo_pi8(v0, v2);
+                CHROMA_MC4_CORE1(v1)
+                v0 = v2;
+            } while (--h > 0);
+        }
+    } else {
+        if (x) {
+            vA = _mm_set1_pi16(xm * 8);
+            PRAGMA_E2K("ivdep")
+            do {
+                GET_VSRC2(v0, v1)
+                CHROMA_MC4_CORE1(v0)
+            } while (--h > 0);
+        } else {
+            PRAGMA_E2K("ivdep")
+            do {
+                v0 = _mm_cvtsi32_si64(*(uint32_t*)src);
+                v1 = _mm_cvtsi32_si64(*(uint32_t*)(src + stride));
+                src += stride << 1;
+                OP8_E2K(v0, v0, _mm_cvtsi32_si64(*(uint32_t*)dst));
+                OP8_E2K(v1, v1, _mm_cvtsi32_si64(*(uint32_t*)(dst + stride)));
+                *(uint32_t*)dst = _mm_cvtsi64_si32(v0);
+                *(uint32_t*)(dst + stride) = _mm_cvtsi64_si32(v1);
+                dst += stride << 1;
+            } while ((h -= 2) > 0);
+        }
+    }
+}
+#undef PREFIX_h264_chroma_mc4_e2k
+#endif
+
+#undef GET_VSRC1
+#undef GET_VSRC2
+
+#define GET_VSRC1(v0) v0 = VEC_LD8(src); src += stride;
+#define GET_VSRC2(v0, v1) \
+    v0 = VEC_LD8(src); v1 = VEC_LD8(src + 1); \
+    v0 = _mm_unpacklo_epi8(v0, v1); src += stride;
+
+#define CHROMA_MC8_CORE2() \
+    v1 = _mm_add_epi16(_mm_maddubs_epi16(v0, vA), bias); \
+    v1 = _mm_add_epi16(_mm_maddubs_epi16(v2, vB), v1); \
+    v1 = _mm_srai_epi16(v1, 6); \
+    h0 = _mm_movepi64_pi64(_mm_packus_epi16(v1, v1)); \
+    OP8_E2K(h0, h0, *(__m64*)dst); \
+    *(__m64*)dst = h0; dst += stride; \
+    v0 = v2;
+
+#define CHROMA_MC8_CORE1(v1) \
+    v1 = _mm_add_epi16(_mm_maddubs_epi16(v1, vA), bias); \
+    v1 = _mm_srai_epi16(v1, 6); \
+    h0 = _mm_movepi64_pi64(_mm_packus_epi16(v1, v1)); \
+    OP8_E2K(h0, h0, *(__m64*)dst); \
+    *(__m64*)dst = h0; dst += stride;
+
+#ifdef PREFIX_h264_chroma_mc8_e2k
+static void PREFIX_h264_chroma_mc8_e2k(uint8_t *dst, const uint8_t *src,
+                                       ptrdiff_t stride, int h,
+                                       int x, int y)
+{
+    int xm = (8 - x) | x << 8;
+    __m128i vA, vB, bias = _mm_set1_epi16(32);
+    __m128i v0, v1, v2, v3; __m64 h0, h1;
+
+    if (y) {
+        if (x) {
+            vA = _mm_set1_epi16(xm * (8 - y));
+            vB = _mm_set1_epi16(xm * y);
+            GET_VSRC2(v0, v1)
+            PRAGMA_E2K("ivdep")
+            do {
+                GET_VSRC2(v2, v3)
+                CHROMA_MC8_CORE2()
+            } while (--h > 0);
+        } else {
+            vA = _mm_set1_epi16(((8 - y) | y << 8) * 8);
+            GET_VSRC1(v0)
+            PRAGMA_E2K("ivdep")
+            do {
+                GET_VSRC1(v2)
+                v1 = _mm_unpacklo_epi8(v0, v2);
+                CHROMA_MC8_CORE1(v1)
+                v0 = v2;
+            } while (--h > 0);
+        }
+    } else {
+        if (x) {
+            vA = _mm_set1_epi16(xm * 8);
+            PRAGMA_E2K("ivdep")
+            do {
+                GET_VSRC2(v0, v1)
+                CHROMA_MC8_CORE1(v0)
+            } while (--h > 0);
+        } else {
+            PRAGMA_E2K("ivdep")
+            do {
+                h0 = *(__m64*)src;
+                h1 = *(__m64*)(src + stride);
+                src += stride << 1;
+                OP8_E2K(h0, h0, *(__m64*)dst);
+                OP8_E2K(h1, h1, *(__m64*)(dst + stride));
+                *(__m64*)dst = h0;
+                *(__m64*)(dst + stride) = h1;
+                dst += stride << 1;
+            } while ((h -= 2) > 0);
+        }
+    }
+}
+#undef PREFIX_h264_chroma_mc8_e2k
+#endif
+
+#ifdef PREFIX_no_rnd_vc1_chroma_mc8_e2k
+static void PREFIX_no_rnd_vc1_chroma_mc8_e2k(uint8_t *dst, const uint8_t *src,
+                                             ptrdiff_t stride, int h,
+                                             int x, int y)
+{
+    int i, xm = (8 - x) | x << 8;
+    __m128i vA = _mm_set1_epi16(xm * (8 - y));
+    __m128i vB = _mm_set1_epi16(xm * y);
+    __m128i bias = _mm_set1_epi16(28);
+    __m128i v0, v1, v2, v3; __m64 h0;
+
+    GET_VSRC2(v0, v1)
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+         GET_VSRC2(v2, v3);
+         CHROMA_MC8_CORE2()
+    }
+}
+#undef PREFIX_no_rnd_vc1_chroma_mc8_e2k
+#endif
+
+#undef GET_VSRC1
+#undef GET_VSRC2
+#undef OP8_E2K
+#undef OP16_E2K
+
diff --git a/libavcodec/e2k/h264dsp.c b/libavcodec/e2k/h264dsp.c
new file mode 100644
index 0000000..01e5f44
--- /dev/null
+++ b/libavcodec/e2k/h264dsp.c
@@ -0,0 +1,1011 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2004 Romain Dolbeau <romain@dolbeau.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include <stdint.h>
+#include <string.h>
+
+#include "libavutil/attributes.h"
+#include "libavutil/intreadwrite.h"
+#include "libavutil/mem.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/h264dec.h"
+#include "libavcodec/h264dsp.h"
+
+/****************************************************************************
+ * IDCT transform:
+ ****************************************************************************/
+
+#define VEC_1D_DCT(vb0, vb1, vb2, vb3, va0, va1, va2, va3)         \
+    /* 1st stage */                                                \
+    vz0 = _mm_add_epi16(vb0, vb2); /* temp[0] = Y[0] + Y[2] */     \
+    vz1 = _mm_sub_epi16(vb0, vb2); /* temp[1] = Y[0] - Y[2] */     \
+    vz2 = _mm_srai_epi16(vb1, 1);                                  \
+    vz2 = _mm_sub_epi16(vz2, vb3); /* temp[2] = Y[1].1/2 - Y[3] */ \
+    vz3 = _mm_srai_epi16(vb3, 1);                                  \
+    vz3 = _mm_add_epi16(vb1, vz3); /* temp[3] = Y[1] + Y[3].1/2 */ \
+    /* 2nd stage: output */                                        \
+    va0 = _mm_add_epi16(vz0, vz3); /* x[0] = temp[0] + temp[3] */  \
+    va1 = _mm_add_epi16(vz1, vz2); /* x[1] = temp[1] + temp[2] */  \
+    va2 = _mm_sub_epi16(vz1, vz2); /* x[2] = temp[1] - temp[2] */  \
+    va3 = _mm_sub_epi16(vz0, vz3)  /* x[3] = temp[0] - temp[3] */
+
+#define VEC_TRANSPOSE_4(a0, a1, a2, a3, b0, b1, b2, b3) \
+    b0 = _mm_unpacklo_epi16(a0, a0); \
+    b1 = _mm_unpacklo_epi16(a1, a0); \
+    b2 = _mm_unpacklo_epi16(a2, a0); \
+    b3 = _mm_unpacklo_epi16(a3, a0); \
+    a0 = _mm_unpacklo_epi16(b0, b2); \
+    a1 = _mm_unpackhi_epi16(b0, b2); \
+    a2 = _mm_unpacklo_epi16(b1, b3); \
+    a3 = _mm_unpackhi_epi16(b1, b3); \
+    b0 = _mm_unpacklo_epi16(a0, a2); \
+    b1 = _mm_unpackhi_epi16(a0, a2); \
+    b2 = _mm_unpacklo_epi16(a1, a3); \
+    b3 = _mm_unpackhi_epi16(a1, a3)
+
+#define VEC_LOAD_U8_ADD_S16_STORE_U8(va)               \
+    va = _mm_srai_epi16(va, 6);                        \
+    vdst = _mm_cvtsi32_si128(*(uint32_t*)dst);         \
+    vdst = _mm_unpacklo_epi8(vdst, zerov);             \
+    va = _mm_add_epi16(va, vdst);                      \
+    va = _mm_packus_epi16(va, va);                     \
+    *(uint32_t*)dst = _mm_extract_epi32(va, 0);        \
+    dst += stride;
+
+static void h264_idct_add_e2k(uint8_t *dst, int16_t *block, int stride)
+{
+    vec_s16 va0, va1, va2, va3;
+    vec_s16 vz0, vz1, vz2, vz3;
+    vec_s16 vtmp0, vtmp1, vtmp2, vtmp3;
+    vec_u8 vdst;
+    LOAD_ZERO;
+
+    block[0] += 32;  /* add 32 as a DC-level for rounding */
+
+    vtmp0 = VEC_LD(block);
+    vtmp1 = VEC_ALIGNR8(vtmp0, vtmp0);
+    vtmp2 = VEC_LD(block + 8);
+    vtmp3 = VEC_ALIGNR8(vtmp2, vtmp2);
+    VEC_ST(block, zerov);
+    VEC_ST(block + 8, zerov);
+
+    VEC_1D_DCT(vtmp0, vtmp1, vtmp2, vtmp3, va0, va1, va2, va3);
+    VEC_TRANSPOSE_4(va0, va1, va2, va3, vtmp0, vtmp1, vtmp2, vtmp3);
+    VEC_1D_DCT(vtmp0, vtmp1, vtmp2, vtmp3, va0, va1, va2, va3);
+
+    VEC_LOAD_U8_ADD_S16_STORE_U8(va0);
+    VEC_LOAD_U8_ADD_S16_STORE_U8(va1);
+    VEC_LOAD_U8_ADD_S16_STORE_U8(va2);
+    VEC_LOAD_U8_ADD_S16_STORE_U8(va3);
+}
+
+#define IDCT8_1D_E2K(s0, s1, s2, s3, s4, s5, s6, s7, d0, d1, d2, d3, d4, d5, d6, d7) { \
+    /* a0 = SRC(0) + SRC(4); */      \
+    /* a2 = SRC(0) - SRC(4); */      \
+    /* a4 = (SRC(2)>>1) - SRC(6); */ \
+    /* a6 = (SRC(6)>>1) + SRC(2); */ \
+    vec_s16 a0v = _mm_add_epi16(s0, s4); \
+    vec_s16 a2v = _mm_sub_epi16(s0, s4); \
+    vec_s16 a4v = _mm_sub_epi16(_mm_srai_epi16(s2, 1), s6); \
+    vec_s16 a6v = _mm_add_epi16(_mm_srai_epi16(s6, 1), s2); \
+    /* b0 = a0 + a6; */ \
+    /* b2 = a2 + a4; */ \
+    /* b4 = a2 - a4; */ \
+    /* b6 = a0 - a6; */ \
+    vec_s16 b0v = _mm_add_epi16(a0v, a6v);  \
+    vec_s16 b2v = _mm_add_epi16(a2v, a4v);  \
+    vec_s16 b4v = _mm_sub_epi16(a2v, a4v);  \
+    vec_s16 b6v = _mm_sub_epi16(a0v, a6v);  \
+    /* a1 = (SRC(5)-SRC(3)) - (SRC(7) + (SRC(7)>>1)); */ \
+    /* a3 = (SRC(7)+SRC(1)) - (SRC(3) + (SRC(3)>>1)); */ \
+    /* a5 = (SRC(7)-SRC(1)) + (SRC(5) + (SRC(5)>>1)); */ \
+    /* a7 = (SRC(5)+SRC(3)) + (SRC(1) + (SRC(1)>>1)); */ \
+    vec_s16 a1v = _mm_sub_epi16(_mm_sub_epi16(s5, s3), _mm_add_epi16(s7, _mm_srai_epi16(s7, 1))); \
+    vec_s16 a3v = _mm_sub_epi16(_mm_add_epi16(s7, s1), _mm_add_epi16(s3, _mm_srai_epi16(s3, 1))); \
+    vec_s16 a5v = _mm_add_epi16(_mm_sub_epi16(s7, s1), _mm_add_epi16(s5, _mm_srai_epi16(s5, 1))); \
+    vec_s16 a7v = _mm_add_epi16(_mm_add_epi16(s5, s3), _mm_add_epi16(s1, _mm_srai_epi16(s1, 1))); \
+    /* b1 = (a7>>2) + a1; */ \
+    /* b3 = a3 + (a5>>2); */ \
+    /* b5 = (a3>>2) - a5; */ \
+    /* b7 = a7 - (a1>>2); */ \
+    vec_s16 b1v = _mm_add_epi16(_mm_srai_epi16(a7v, 2), a1v); \
+    vec_s16 b3v = _mm_add_epi16(a3v, _mm_srai_epi16(a5v, 2)); \
+    vec_s16 b5v = _mm_sub_epi16(_mm_srai_epi16(a3v, 2), a5v); \
+    vec_s16 b7v = _mm_sub_epi16(a7v, _mm_srai_epi16(a1v, 2)); \
+    /* DST(0, b0 + b7); */ \
+    /* DST(1, b2 + b5); */ \
+    /* DST(2, b4 + b3); */ \
+    /* DST(3, b6 + b1); */ \
+    /* DST(4, b6 - b1); */ \
+    /* DST(5, b4 - b3); */ \
+    /* DST(6, b2 - b5); */ \
+    /* DST(7, b0 - b7); */ \
+    d0 = _mm_add_epi16(b0v, b7v); \
+    d1 = _mm_add_epi16(b2v, b5v); \
+    d2 = _mm_add_epi16(b4v, b3v); \
+    d3 = _mm_add_epi16(b6v, b1v); \
+    d4 = _mm_sub_epi16(b6v, b1v); \
+    d5 = _mm_sub_epi16(b4v, b3v); \
+    d6 = _mm_sub_epi16(b2v, b5v); \
+    d7 = _mm_sub_epi16(b0v, b7v); \
+}
+
+#define E2K_STORE_SUM_CLIP(dest, idctv) {     \
+    /* unaligned load */                      \
+    __m128i dstv = VEC_LD8(dest);             \
+    dstv = _mm_unpacklo_epi8(dstv, zerov);    \
+    idctv = _mm_srai_epi16(idctv, 6);         \
+    dstv = _mm_add_epi16(dstv, idctv);        \
+    dstv = _mm_packus_epi16(dstv, dstv);      \
+    /* unaligned store */                     \
+    VEC_STL(dest, dstv);                      \
+}
+
+static void h264_idct8_add_e2k(uint8_t *dst, int16_t *dct, int stride)
+{
+    vec_s16 s0, s1, s2, s3, s4, s5, s6, s7;
+    vec_s16 d0, d1, d2, d3, d4, d5, d6, d7;
+    vec_s16 idct0, idct1, idct2, idct3, idct4, idct5, idct6, idct7;
+
+    LOAD_ZERO;
+
+    dct[0] += 32; // rounding for the >>6 at the end
+
+    s0 = VEC_LD(dct + 8 * 0);
+    s1 = VEC_LD(dct + 8 * 1);
+    s2 = VEC_LD(dct + 8 * 2);
+    s3 = VEC_LD(dct + 8 * 3);
+    s4 = VEC_LD(dct + 8 * 4);
+    s5 = VEC_LD(dct + 8 * 5);
+    s6 = VEC_LD(dct + 8 * 6);
+    s7 = VEC_LD(dct + 8 * 7);
+    VEC_ST(dct + 8 * 0, zerov);
+    VEC_ST(dct + 8 * 1, zerov);
+    VEC_ST(dct + 8 * 2, zerov);
+    VEC_ST(dct + 8 * 3, zerov);
+    VEC_ST(dct + 8 * 4, zerov);
+    VEC_ST(dct + 8 * 5, zerov);
+    VEC_ST(dct + 8 * 6, zerov);
+    VEC_ST(dct + 8 * 7, zerov);
+
+    IDCT8_1D_E2K(s0, s1, s2, s3, s4, s5, s6, s7,
+                     d0, d1, d2, d3, d4, d5, d6, d7);
+
+    TRANSPOSE8(d0, d1, d2, d3, d4, d5, d6, d7);
+
+    IDCT8_1D_E2K(d0, d1, d2, d3, d4, d5, d6, d7,
+                     idct0, idct1, idct2, idct3, idct4, idct5, idct6, idct7);
+
+    E2K_STORE_SUM_CLIP(&dst[0*stride], idct0);
+    E2K_STORE_SUM_CLIP(&dst[1*stride], idct1);
+    E2K_STORE_SUM_CLIP(&dst[2*stride], idct2);
+    E2K_STORE_SUM_CLIP(&dst[3*stride], idct3);
+    E2K_STORE_SUM_CLIP(&dst[4*stride], idct4);
+    E2K_STORE_SUM_CLIP(&dst[5*stride], idct5);
+    E2K_STORE_SUM_CLIP(&dst[6*stride], idct6);
+    E2K_STORE_SUM_CLIP(&dst[7*stride], idct7);
+}
+
+static void h264_idct_dc_add_e2k(uint8_t *dst, int16_t *block, int stride)
+{
+    __m64 dc16, zerov = _mm_setzero_si64();
+    __m64 dcplus, dcminus, v0, v1, v2, v3;
+    int i, dc;
+
+    dc = (block[0] + 32) >> 6;
+    block[0] = 0;
+    dc16 = _mm_set1_pi16(dc);
+    dcplus = _mm_packs_pu16(dc16, dc16);
+    dc16 = _mm_sub_pi16(zerov, dc16);
+    dcminus = _mm_packs_pu16(dc16, dc16);
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < 4; i += 4) {
+        v0 = _mm_cvtsi32_si64(*(uint32_t*)(dst + 0 * stride));
+        v1 = _mm_cvtsi32_si64(*(uint32_t*)(dst + 1 * stride));
+        v2 = _mm_cvtsi32_si64(*(uint32_t*)(dst + 2 * stride));
+        v3 = _mm_cvtsi32_si64(*(uint32_t*)(dst + 3 * stride));
+
+        v0 = _mm_unpacklo_pi32(v0, v1);
+        v2 = _mm_unpacklo_pi32(v2, v3);
+        v0 = _mm_adds_pu8(v0, dcplus);
+        v2 = _mm_adds_pu8(v2, dcplus);
+        v0 = _mm_subs_pu8(v0, dcminus);
+        v2 = _mm_subs_pu8(v2, dcminus);
+
+        *(uint32_t*)(dst + 0 * stride) = _mm_extract_pi32(v0, 0);
+        *(uint32_t*)(dst + 1 * stride) = _mm_extract_pi32(v0, 1);
+        *(uint32_t*)(dst + 2 * stride) = _mm_extract_pi32(v2, 0);
+        *(uint32_t*)(dst + 3 * stride) = _mm_extract_pi32(v2, 1);
+        dst += 4 * stride;
+    }
+}
+
+static void h264_idct8_dc_add_e2k(uint8_t *dst, int16_t *block, int stride)
+{
+    vec_s16 dc16;
+    vec_u8 dcplus, dcminus, v0, v1, v2, v3;
+    LOAD_ZERO;
+    int i, dc;
+
+    dc = (block[0] + 32) >> 6;
+    block[0] = 0;
+    dc16 = _mm_set1_epi16(dc);
+    dcplus = _mm_packus_epi16(dc16, dc16);
+    dc16 = _mm_sub_epi16(zerov, dc16);
+    dcminus = _mm_packus_epi16(dc16, dc16);
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < 8; i += 4) {
+        v0 = VEC_LD8(dst + 0 * stride);
+        v1 = VEC_LD8(dst + 1 * stride);
+        v2 = VEC_LD8(dst + 2 * stride);
+        v3 = VEC_LD8(dst + 3 * stride);
+
+        v0 = _mm_unpacklo_epi64(v0, v1);
+        v2 = _mm_unpacklo_epi64(v2, v3);
+        v0 = _mm_adds_epu8(v0, dcplus);
+        v2 = _mm_adds_epu8(v2, dcplus);
+        v0 = _mm_subs_epu8(v0, dcminus);
+        v2 = _mm_subs_epu8(v2, dcminus);
+
+        VEC_STL(dst + 0 * stride, v0);
+        VEC_STH(dst + 1 * stride, v0);
+        VEC_STL(dst + 2 * stride, v2);
+        VEC_STH(dst + 3 * stride, v2);
+        dst += 4 * stride;
+    }
+}
+
+static void h264_idct_add16_e2k(uint8_t *dst, const int *block_offset,
+                                    int16_t *block, int stride,
+                                    const uint8_t nnzc[15 * 8])
+{
+    int i;
+    for (i = 0; i < 16; i++) {
+        int nnz = nnzc[scan8[i]];
+        if (nnz) {
+            if (nnz == 1 && block[i * 16])
+                h264_idct_dc_add_e2k(dst + block_offset[i], block + i * 16, stride);
+            else                           
+                h264_idct_add_e2k(dst + block_offset[i], block + i * 16, stride);
+        }
+    }
+}
+
+static void h264_idct_add16intra_e2k(uint8_t *dst, const int *block_offset,
+                                         int16_t *block, int stride,
+                                         const uint8_t nnzc[15 * 8])
+{
+    int i;
+    for (i = 0; i < 16; i++) {
+        if (nnzc[scan8[i]])
+            h264_idct_add_e2k(dst + block_offset[i], block + i * 16, stride);
+        else if(block[i * 16])
+            h264_idct_dc_add_e2k(dst + block_offset[i], block + i * 16, stride);
+    }
+}
+
+static void h264_idct8_add4_e2k(uint8_t *dst, const int *block_offset,
+                                    int16_t *block, int stride,
+                                    const uint8_t nnzc[15 * 8])
+{
+    int i;
+    for (i = 0; i < 16; i += 4){
+        int nnz = nnzc[scan8[i]];
+        if (nnz) {
+            if (nnz == 1 && block[i * 16]) 
+                h264_idct8_dc_add_e2k(dst + block_offset[i], block + i * 16, stride);
+            else
+                h264_idct8_add_e2k(dst + block_offset[i], block + i * 16, stride);
+        }
+    }
+}
+
+static void h264_idct_add8_e2k(uint8_t **dest, const int *block_offset,
+                                   int16_t *block, int stride,
+                                   const uint8_t nnzc[15 * 8])
+{
+    int i, j;
+    for (j = 1; j < 3; j++) {
+        for (i = j * 16; i < j * 16 + 4; i++) {
+            if (nnzc[scan8[i]])
+                h264_idct_add_e2k(dest[j - 1] + block_offset[i], block + i * 16, stride);
+            else if (block[i * 16])
+                h264_idct_dc_add_e2k(dest[j - 1] + block_offset[i], block + i * 16, stride);
+        }
+    }
+}
+
+/* performs a 6x16 transpose of data in src, and stores it to dst */
+#define read_and_transpose16x6(src, st, r8, r9, r10, r11, r12, r13) { \
+    vec_u8 r0, r1, r2, r3, r4, r5, r6, r7, r14, r15;                  \
+    r0 = VEC_LD8(src);                                                \
+    r1 = VEC_LD8(src + st);                                           \
+    r2 = VEC_LD8(src + st * 2);                                       \
+    r3 = VEC_LD8(src + st * 3);                                       \
+    r4 = VEC_LD8(src + st * 4);                                       \
+    r5 = VEC_LD8(src + st * 5);                                       \
+    r6 = VEC_LD8(src + st * 6);                                       \
+    r7 = VEC_LD8(src + st * 7);                                       \
+    r8 = VEC_LD8(src + st * 8);                                       \
+    r9 = VEC_LD8(src + st * 9);                                       \
+    r10 = VEC_LD8(src + st * 10);                                     \
+    r11 = VEC_LD8(src + st * 11);                                     \
+    r12 = VEC_LD8(src + st * 12);                                     \
+    r13 = VEC_LD8(src + st * 13);                                     \
+    r14 = VEC_LD8(src + st * 14);                                     \
+    r15 = VEC_LD8(src + st * 15);                                     \
+                                                                      \
+    /* Merge first pairs */                                           \
+    r0 = _mm_unpacklo_epi8(r0, r4);    /*  0, 4 */                    \
+    r1 = _mm_unpacklo_epi8(r1, r5);    /*  1, 5 */                    \
+    r2 = _mm_unpacklo_epi8(r2, r6);    /*  2, 6 */                    \
+    r3 = _mm_unpacklo_epi8(r3, r7);    /*  3, 7 */                    \
+    r4 = _mm_unpacklo_epi8(r8, r12);   /*  8,12 */                    \
+    r5 = _mm_unpacklo_epi8(r9, r13);   /*  9,13 */                    \
+    r6 = _mm_unpacklo_epi8(r10, r14);  /* 10,14 */                    \
+    r7 = _mm_unpacklo_epi8(r11, r15);  /* 11,15 */                    \
+                                                                      \
+    /* Merge second pairs */                                          \
+    r8  = _mm_unpacklo_epi8(r0, r2);   /* 0, 2, 4, 6 set 0 */         \
+    r9  = _mm_unpackhi_epi8(r0, r2);   /* 0, 2, 4, 6 set 1 */         \
+    r10 = _mm_unpacklo_epi8(r1, r3);   /* 1, 3, 5, 7 set 0 */         \
+    r11 = _mm_unpackhi_epi8(r1, r3);   /* 1, 3, 5, 7 set 1 */         \
+    r12 = _mm_unpacklo_epi8(r4, r6);   /* 8,10,12,14 set 0 */         \
+    r13 = _mm_unpackhi_epi8(r4, r6);   /* 8,10,12,14 set 1 */         \
+    r14 = _mm_unpacklo_epi8(r5, r7);   /* 9,11,13,15 set 0 */         \
+    r15 = _mm_unpackhi_epi8(r5, r7);   /* 9,11,13,15 set 1 */         \
+                                                                      \
+    /* Third merge */                                                 \
+    r0 = _mm_unpacklo_epi8(r8, r10);   /* 0..7 set 0  */              \
+    r1 = _mm_unpackhi_epi8(r8, r10);   /* 0..7 set 1  */              \
+    r2 = _mm_unpacklo_epi8(r9, r11);   /* 0..7 set 2  */              \
+    r4 = _mm_unpacklo_epi8(r12, r14);  /* 8..15 set 0 */              \
+    r5 = _mm_unpackhi_epi8(r12, r14);  /* 8..15 set 1 */              \
+    r6 = _mm_unpacklo_epi8(r13, r15);  /* 8..15 set 2 */              \
+    /* Don't need to compute 3 and 7*/                                \
+                                                                      \
+    /* Final merge */                                                 \
+    r8  = _mm_unpacklo_epi64(r0, r4);   /* all set 0 */               \
+    r9  = _mm_unpackhi_epi64(r0, r4);   /* all set 1 */               \
+    r10 = _mm_unpacklo_epi64(r1, r5);   /* all set 2 */               \
+    r11 = _mm_unpackhi_epi64(r1, r5);   /* all set 3 */               \
+    r12 = _mm_unpacklo_epi64(r2, r6);   /* all set 4 */               \
+    r13 = _mm_unpackhi_epi64(r2, r6);   /* all set 5 */               \
+    /* Don't need to compute 14 and 15*/                              \
+}
+
+#define read_and_transpose16x8(src, st, r8, r9, r10, r11, r12, r13, r14, r15) { \
+    vec_u8 r0, r1, r2, r3, r4, r5, r6, r7;                            \
+    r0 = VEC_LD8(src);                                                \
+    r1 = VEC_LD8(src + st);                                           \
+    r2 = VEC_LD8(src + st * 2);                                       \
+    r3 = VEC_LD8(src + st * 3);                                       \
+    r4 = VEC_LD8(src + st * 4);                                       \
+    r5 = VEC_LD8(src + st * 5);                                       \
+    r6 = VEC_LD8(src + st * 6);                                       \
+    r7 = VEC_LD8(src + st * 7);                                       \
+    r8 = VEC_LD8(src + st * 8);                                       \
+    r9 = VEC_LD8(src + st * 9);                                       \
+    r10 = VEC_LD8(src + st * 10);                                     \
+    r11 = VEC_LD8(src + st * 11);                                     \
+    r12 = VEC_LD8(src + st * 12);                                     \
+    r13 = VEC_LD8(src + st * 13);                                     \
+    r14 = VEC_LD8(src + st * 14);                                     \
+    r15 = VEC_LD8(src + st * 15);                                     \
+                                                                      \
+    /* Merge first pairs */                                           \
+    r0 = _mm_unpacklo_epi8(r0, r4);    /*  0, 4 */                    \
+    r1 = _mm_unpacklo_epi8(r1, r5);    /*  1, 5 */                    \
+    r2 = _mm_unpacklo_epi8(r2, r6);    /*  2, 6 */                    \
+    r3 = _mm_unpacklo_epi8(r3, r7);    /*  3, 7 */                    \
+    r4 = _mm_unpacklo_epi8(r8, r12);   /*  8,12 */                    \
+    r5 = _mm_unpacklo_epi8(r9, r13);   /*  9,13 */                    \
+    r6 = _mm_unpacklo_epi8(r10, r14);  /* 10,14 */                    \
+    r7 = _mm_unpacklo_epi8(r11, r15);  /* 11,15 */                    \
+                                                                      \
+    /* Merge second pairs */                                          \
+    r8  = _mm_unpacklo_epi8(r0, r2);   /* 0, 2, 4, 6 set 0 */         \
+    r9  = _mm_unpackhi_epi8(r0, r2);   /* 0, 2, 4, 6 set 1 */         \
+    r10 = _mm_unpacklo_epi8(r1, r3);   /* 1, 3, 5, 7 set 0 */         \
+    r11 = _mm_unpackhi_epi8(r1, r3);   /* 1, 3, 5, 7 set 1 */         \
+    r12 = _mm_unpacklo_epi8(r4, r6);   /* 8,10,12,14 set 0 */         \
+    r13 = _mm_unpackhi_epi8(r4, r6);   /* 8,10,12,14 set 1 */         \
+    r14 = _mm_unpacklo_epi8(r5, r7);   /* 9,11,13,15 set 0 */         \
+    r15 = _mm_unpackhi_epi8(r5, r7);   /* 9,11,13,15 set 1 */         \
+                                                                      \
+    /* Third merge */                                                 \
+    r0 = _mm_unpacklo_epi8(r8, r10);   /* 0..7 set 0  */              \
+    r1 = _mm_unpackhi_epi8(r8, r10);   /* 0..7 set 1  */              \
+    r2 = _mm_unpacklo_epi8(r9, r11);   /* 0..7 set 2  */              \
+    r3 = _mm_unpackhi_epi8(r9, r11);   /* 0..7 set 3  */              \
+    r4 = _mm_unpacklo_epi8(r12, r14);  /* 8..15 set 0 */              \
+    r5 = _mm_unpackhi_epi8(r12, r14);  /* 8..15 set 1 */              \
+    r6 = _mm_unpacklo_epi8(r13, r15);  /* 8..15 set 2 */              \
+    r7 = _mm_unpackhi_epi8(r13, r15);  /* 8..15 set 3 */              \
+                                                                      \
+    /* Final merge */                                                 \
+    r8  = _mm_unpacklo_epi64(r0, r4);   /* all set 0 */               \
+    r9  = _mm_unpackhi_epi64(r0, r4);   /* all set 1 */               \
+    r10 = _mm_unpacklo_epi64(r1, r5);   /* all set 2 */               \
+    r11 = _mm_unpackhi_epi64(r1, r5);   /* all set 3 */               \
+    r12 = _mm_unpacklo_epi64(r2, r6);   /* all set 4 */               \
+    r13 = _mm_unpackhi_epi64(r2, r6);   /* all set 5 */               \
+    r14 = _mm_unpacklo_epi64(r3, r7);   /* all set 6 */               \
+    r15 = _mm_unpackhi_epi64(r3, r7);   /* all set 7 */               \
+}
+
+#define read_and_transpose8x4(src, st, r8, r9, r10, r11) {            \
+    __m64 r0, r1, r2, r3, r4, r5, r6, r7;                             \
+    r0 = _mm_cvtsi32_si64(*(uint32_t*)(src));                         \
+    r1 = _mm_cvtsi32_si64(*(uint32_t*)(src + st));                    \
+    r2 = _mm_cvtsi32_si64(*(uint32_t*)(src + st * 2));                \
+    r3 = _mm_cvtsi32_si64(*(uint32_t*)(src + st * 3));                \
+    r4 = _mm_cvtsi32_si64(*(uint32_t*)(src + st * 4));                \
+    r5 = _mm_cvtsi32_si64(*(uint32_t*)(src + st * 5));                \
+    r6 = _mm_cvtsi32_si64(*(uint32_t*)(src + st * 6));                \
+    r7 = _mm_cvtsi32_si64(*(uint32_t*)(src + st * 7));                \
+                                                                      \
+    r0 = _mm_unpacklo_pi8(r0, r4);                                    \
+    r1 = _mm_unpacklo_pi8(r1, r5);                                    \
+    r2 = _mm_unpacklo_pi8(r2, r6);                                    \
+    r3 = _mm_unpacklo_pi8(r3, r7);                                    \
+                                                                      \
+    r4 = _mm_unpacklo_pi8(r0, r2);                                    \
+    r5 = _mm_unpackhi_pi8(r0, r2);                                    \
+    r6 = _mm_unpacklo_pi8(r1, r3);                                    \
+    r7 = _mm_unpackhi_pi8(r1, r3);                                    \
+                                                                      \
+    r8 = _mm_unpacklo_pi8(r4, r6);                                    \
+    r9 = _mm_unpackhi_pi8(r4, r6);                                    \
+    r10 = _mm_unpacklo_pi8(r5, r7);                                   \
+    r11 = _mm_unpackhi_pi8(r5, r7);                                   \
+}
+
+#define DEF_HELPERS(vec, p, si) \
+/* out: o = |x-y| < a */ \
+static av_always_inline vec diff_lt_##si(vec x, vec y, vec a) { \
+    vec o = _mm_or_##si(_mm_subs_##p##u8(x, y), _mm_subs_##p##u8(y, x)); /* |x-y| */ \
+    return _mm_cmpgt_##p##i8(a, _mm_xor_##si(o, _mm_set1_##p##i8(-128))); \
+} \
+static av_always_inline vec deblock_mask_##si(vec p0, vec p1, vec q0, \
+                                            vec q1, vec alpha, vec beta) { \
+    vec mask, tempmask; \
+    mask = diff_lt_##si(p0, q0, alpha); \
+    tempmask = diff_lt_##si(p1, p0, beta); \
+    mask = _mm_and_##si(mask, tempmask); \
+    tempmask = diff_lt_##si(q1, q0, beta); \
+    return _mm_and_##si(mask, tempmask); \
+}
+
+DEF_HELPERS(__m128i, ep, si128)
+DEF_HELPERS(__m64, p, si64)
+
+// out: newp1 = clip((p2 + ((p0 + q0 + 1) >> 1)) >> 1, p1-tc0, p1+tc0)
+static av_always_inline __m128i h264_deblock_q1(__m128i p0_q0, __m128i p1,
+                                                __m128i p2, __m128i tc0) {
+    __m128i t0, t1, t2, c1 = _mm_set1_epi8(1);
+
+    t0 = _mm_xor_si128(p0_q0, p2);
+    t1 = _mm_avg_epu8(p0_q0, p2);   /* avg(p2, avg(p0, q0)) */
+    t0 = _mm_and_si128(t0, c1);     /* (p2 ^ avg(p0, q0)) & 1 */
+    t1 = _mm_subs_epu8(t1, t0);     /* (p2 + ((p0 + q0 + 1) >> 1)) >> 1 */
+    t0 = _mm_subs_epu8(p1, tc0);
+    t2 = _mm_adds_epu8(p1, tc0);
+    return _mm_min_epu8(t2, _mm_max_epu8(t0, t1));
+}
+
+#define h264_deblock_p0_q0(p0, p1, q0, q1, tc0masked, vec, x, si) {                                 \
+    vec pq0bit = _mm_xor_##si(p0, q0);                                                              \
+    vec q1minus, p0minus, stage1, stage2, c127 = _mm_set1_##x##i8(127);                             \
+    vec vec160 = _mm_set1_##x##i8(160), delta, deltaneg, notv = _mm_set1_##x##i8(-1);               \
+                                                                                                    \
+    q1minus = _mm_xor_##si(q1, notv);            /* 255 - q1 */                                     \
+    stage1 = _mm_avg_##x##u8(p1, q1minus);       /* (p1 - q1 + 256) >> 1 */                         \
+    stage2 = _mm_srli_##x##i16(stage1, 1);                                                          \
+    stage2 = _mm_and_##si(stage2, c127);         /* (p1 - q1 + 256) >> 2 = 64 + (p1 - q1) >> 2 */   \
+    p0minus = _mm_xor_##si(p0, notv);            /* 255 - p0 */                                     \
+    stage1 = _mm_avg_##x##u8(q0, p0minus);       /* (q0 - p0 + 256) >> 1 */                         \
+    pq0bit = _mm_and_##si(pq0bit, _mm_set1_##x##i8(1));                                             \
+    stage2 = _mm_avg_##x##u8(stage2, pq0bit);    /* 32 + ((q0 - p0)&1 + (p1 - q1) >> 2 + 1) >> 1 */ \
+    stage2 = _mm_adds_##x##u8(stage2, stage1);   /* 160 + ((p0 - q0) + (p1 - q1) >> 2 + 1) >> 1 */  \
+    deltaneg = _mm_subs_##x##u8(vec160, stage2); /* -d */                                           \
+    delta = _mm_subs_##x##u8(stage2, vec160);    /*  d */                                           \
+    deltaneg = _mm_min_##x##u8(tc0masked, deltaneg);                                                \
+    delta = _mm_min_##x##u8(tc0masked, delta);                                                      \
+    p0 = _mm_subs_##x##u8(p0, deltaneg);                                                            \
+    q0 = _mm_subs_##x##u8(q0, delta);                                                               \
+    p0 = _mm_adds_##x##u8(p0, delta);                                                               \
+    q0 = _mm_adds_##x##u8(q0, deltaneg);                                                            \
+}
+
+#define LOOP_FILTER_LUMA { \
+    __m128i alphavec, betavec, m0, m1, m2, m3, m4, newp1, newq1; \
+    betavec = _mm_set1_epi8(beta - 128); \
+    alphavec = _mm_set1_epi8(alpha - 128); \
+    m0 = deblock_mask_si128(p0, p1, q0, q1, alphavec, betavec); \
+    \
+    m4 = _mm_cvtsi32_si128(*(uint32_t*)tc0); \
+    m4 = _mm_unpacklo_epi8(m4, m4); \
+    m4 = _mm_unpacklo_epi8(m4, m4); \
+    m0 = _mm_blendv_epi8(m0, _mm_setzero_si128(), m4); \
+    m3 = _mm_and_si128(m4, m0); \
+    \
+    m1 = diff_lt_si128(p2, p0, betavec); \
+    m2 = diff_lt_si128(q2, q0, betavec); \
+    m1 = _mm_and_si128(m1, m0); \
+    m2 = _mm_and_si128(m2, m0); \
+    m3 = _mm_sub_epi8(m3, m1); \
+    m1 = _mm_and_si128(m1, m4); \
+    m3 = _mm_sub_epi8(m3, m2); \
+    m2 = _mm_and_si128(m2, m4); \
+    m0 = _mm_avg_epu8(p0, q0); \
+    newp1 = h264_deblock_q1(m0, p1, p2, m1); \
+    newq1 = h264_deblock_q1(m0, q1, q2, m2); \
+    h264_deblock_p0_q0(p0, p1, q0, q1, m3, __m128i, ep, si128); \
+    p1 = newp1; \
+    q1 = newq1; \
+}
+
+static void h264_v_loop_filter_luma_e2k(uint8_t *pix, ptrdiff_t stride, int alpha, int beta, int8_t *tc0) {
+    if ((tc0[0] & tc0[1] & tc0[2] & tc0[3]) >= 0) {
+        __m128i p2, p1, p0, q0, q1, q2;
+        p2 = VEC_LD(pix - 3 * stride);
+        p1 = VEC_LD(pix - 2 * stride);
+        p0 = VEC_LD(pix - stride);
+        q0 = VEC_LD(pix);
+        q1 = VEC_LD(pix + stride);
+        q2 = VEC_LD(pix + 2 * stride);
+        LOOP_FILTER_LUMA
+        VEC_ST(pix - 2 * stride, p1);
+        VEC_ST(pix - stride, p0);
+        VEC_ST(pix, q0);
+        VEC_ST(pix + stride, q1);
+    }
+}
+
+#define WRITE4(i, j) *(uint32_t*)&pix[(i * 4 + j) * stride] = _mm_extract_epi32(a##i, j);
+#define TR16X4_WR(i0, i1, i2, i3) { \
+    __m128i a0 = i0, a1 = i1, a2 = i2, a3 = i3; \
+    __m128i b0, b1, b2, b3; \
+    \
+    b0 = _mm_unpacklo_epi8(a0, a2); \
+    b1 = _mm_unpackhi_epi8(a0, a2); \
+    b2 = _mm_unpacklo_epi8(a1, a3); \
+    b3 = _mm_unpackhi_epi8(a1, a3); \
+    \
+    a0 = _mm_unpacklo_epi8(b0, b2); \
+    a1 = _mm_unpackhi_epi8(b0, b2); \
+    a2 = _mm_unpacklo_epi8(b1, b3); \
+    a3 = _mm_unpackhi_epi8(b1, b3); \
+    \
+    pix -= 2; \
+    WRITE4(0, 0) WRITE4(0, 1) WRITE4(0, 2) WRITE4(0, 3) \
+    WRITE4(1, 0) WRITE4(1, 1) WRITE4(1, 2) WRITE4(1, 3) \
+    WRITE4(2, 0) WRITE4(2, 1) WRITE4(2, 2) WRITE4(2, 3) \
+    WRITE4(3, 0) WRITE4(3, 1) WRITE4(3, 2) WRITE4(3, 3) \
+}
+
+static void h264_h_loop_filter_luma_e2k(uint8_t *pix, ptrdiff_t stride, int alpha, int beta, int8_t *tc0) {
+    __m128i p2, p1, p0, q0, q1, q2;
+    if ((tc0[0] & tc0[1] & tc0[2] & tc0[3]) < 0) return;
+    read_and_transpose16x6(pix - 3, stride, p2, p1, p0, q0, q1, q2);
+    LOOP_FILTER_LUMA
+    TR16X4_WR(p1, p0, q0, q1)
+}
+
+#define LOOP_FILTER_CHROMA(vec, x, si, n) { \
+    vec alphavec, betavec, m0, m1; \
+    betavec = _mm_set1_##x##i8(beta - 128); \
+    alphavec = _mm_set1_##x##i8(alpha - 128); \
+    m0 = deblock_mask_##si(p0, p1, q0, q1, alphavec, betavec); \
+    \
+    m1 = _mm_cvtsi32_##si(*(uint32_t*)tc0); \
+    m1 = _mm_unpacklo_##x##i8(m1, m1); \
+    if (n == 4) m1 = _mm_unpacklo_##x##i8(m1, m1); \
+    m0 = _mm_blendv_##x##i8(m0, _mm_setzero_##si(), m1); \
+    m0 = _mm_and_##si(m0, m1); \
+    h264_deblock_p0_q0(p0, p1, q0, q1, m0, vec, x, si) \
+}
+
+static void h264_v_loop_filter_chroma_e2k(uint8_t *pix, ptrdiff_t stride, int alpha, int beta, int8_t *tc0) {
+    if ((tc0[0] & tc0[1] & tc0[2] & tc0[3]) >= 0) {
+        __m64 p1, p0, q0, q1;
+        p1 = *(__m64*)(pix - 2 * stride);
+        p0 = *(__m64*)(pix - stride);
+        q0 = *(__m64*)pix;
+        q1 = *(__m64*)(pix + stride);
+        LOOP_FILTER_CHROMA(__m64, p, si64, 2)
+        *(__m64*)(pix - stride) = p0;
+        *(__m64*)pix = q0;
+    }
+}
+
+static void h264_h_loop_filter_chroma_e2k(uint8_t *pix, ptrdiff_t stride, int alpha, int beta, int8_t *tc0) {
+    __m64 p1, p0, q0, q1;
+    if ((tc0[0] & tc0[1] & tc0[2] & tc0[3]) < 0) return;
+    read_and_transpose8x4(pix - 2, stride, p1, p0, q0, q1);
+    LOOP_FILTER_CHROMA(__m64, p, si64, 2)
+    p1 = _mm_unpacklo_pi8(p0, q0);
+    q1 = _mm_unpackhi_pi8(p0, q0);
+#define WRITE2(v, i) *(uint16_t*)(pix + i * stride) = _mm_extract_pi16(v, i);
+    pix--;
+    WRITE2(p1, 0) WRITE2(p1, 1) WRITE2(p1, 2) WRITE2(p1, 3)
+    pix += stride * 4;
+    WRITE2(q1, 0) WRITE2(q1, 1) WRITE2(q1, 2) WRITE2(q1, 3)
+#undef WRITE2
+}
+
+static void h264_h_loop_filter_chroma422_e2k(uint8_t *pix, ptrdiff_t stride, int alpha, int beta, int8_t *tc0) {
+    __m128i p1, p0, q0, q1;
+    __m64 p1l, p0l, q0l, q1l, p1h, p0h, q0h, q1h;
+    if ((tc0[0] & tc0[1] & tc0[2] & tc0[3]) < 0) return;
+    read_and_transpose8x4(pix - 2, stride, p1l, p0l, q0l, q1l);
+    read_and_transpose8x4(pix - 2 + stride * 8, stride, p1h, p0h, q0h, q1h);
+    p1 = _mm_unpacklo_epi64(_mm_movpi64_epi64(p1l), _mm_movpi64_epi64(p1h));
+    p0 = _mm_unpacklo_epi64(_mm_movpi64_epi64(p0l), _mm_movpi64_epi64(p0h));
+    q0 = _mm_unpacklo_epi64(_mm_movpi64_epi64(q0l), _mm_movpi64_epi64(q0h));
+    q1 = _mm_unpacklo_epi64(_mm_movpi64_epi64(q1l), _mm_movpi64_epi64(q1h));
+    LOOP_FILTER_CHROMA(__m128i, ep, si128, 4)
+    p1 = _mm_unpacklo_epi8(p0, q0);
+    q1 = _mm_unpackhi_epi8(p0, q0);
+#define WRITE2(v, i) *(uint16_t*)(pix + i * stride) = _mm_extract_epi16(v, i);
+    pix--;
+    WRITE2(p1, 0) WRITE2(p1, 1) WRITE2(p1, 2) WRITE2(p1, 3)
+    WRITE2(p1, 4) WRITE2(p1, 5) WRITE2(p1, 6) WRITE2(p1, 7)
+    pix += stride * 8;
+    WRITE2(q1, 0) WRITE2(q1, 1) WRITE2(q1, 2) WRITE2(q1, 3)
+    WRITE2(q1, 4) WRITE2(q1, 5) WRITE2(q1, 6) WRITE2(q1, 7)
+#undef WRITE2
+}
+
+#define LOOP_FILTER_LUMA_INTRA_HALF(lo, a0, a1, a2, a3, b0, b1, b2, b3) \
+    a0 = _mm_unpack##lo##_epi8(p0, c0); \
+    a1 = _mm_unpack##lo##_epi8(p1, c0); \
+    a2 = _mm_unpack##lo##_epi8(p2, c0); \
+    a3 = _mm_unpack##lo##_epi8(p3, c0); \
+    b0 = _mm_unpack##lo##_epi8(q0, c0); \
+    b1 = _mm_unpack##lo##_epi8(q1, c0); \
+    b2 = _mm_unpack##lo##_epi8(q2, c0); \
+    b3 = _mm_unpack##lo##_epi8(q3, c0); \
+    t0 = _mm_add_epi16(a0, b1); \
+    t1 = _mm_add_epi16(a1, b0); \
+    t2 = _mm_add_epi16(t1, _mm_add_epi16(a0, a2)); \
+    t3 = _mm_add_epi16(t0, _mm_add_epi16(b0, b2)); \
+    t4 = _mm_add_epi16(_mm_add_epi16(t0, t1), c4); \
+    a3 = _mm_add_epi16(_mm_add_epi16(a2, a3), c2); \
+    b3 = _mm_add_epi16(_mm_add_epi16(b2, b3), c2); \
+    a0 = _mm_avg_epu16(a1, _mm_srai_epi16(t0, 1)); /* p0 if m3 */ \
+    b0 = _mm_avg_epu16(b1, _mm_srai_epi16(t1, 1)); /* q0 if m3 */ \
+    a1 = _mm_srai_epi16(_mm_add_epi16(t2, t4), 3); /* p0 if m1 */ \
+    b1 = _mm_srai_epi16(_mm_add_epi16(t3, t4), 3); /* q0 if m2 */ \
+    t2 = _mm_srai_epi16(t2, 1); a2 = _mm_avg_epu16(t2, c0); /* p1 if m1 */ \
+    t3 = _mm_srai_epi16(t3, 1); b2 = _mm_avg_epu16(t3, c0); /* q1 if m2 */ \
+    a3 = _mm_srai_epi16(_mm_add_epi16(a3, t2), 2); /* p2 if m1 */ \
+    b3 = _mm_srai_epi16(_mm_add_epi16(b3, t3), 2); /* q2 if m2 */
+
+#define LOOP_FILTER_LUMA_INTRA { \
+    __m128i alphavec, betavec, m0, m1, m2, m3, c0 = _mm_setzero_si128(); \
+    __m128i c2 = _mm_set1_epi16(2), c4 = _mm_set1_epi16(4); \
+    __m128i a0, a1, a2, a3, b0, b1, b2, b3, t0, t1, t2, t3, t4; \
+    __m128i x0, x1, x2, x3, y0, y1, y2, y3; \
+    \
+    betavec = _mm_set1_epi8(beta - 128); \
+    alphavec = _mm_set1_epi8(alpha - 128); \
+    m3 = deblock_mask_si128(p0, p1, q0, q1, alphavec, betavec); \
+    alphavec = _mm_set1_epi8((alpha >> 2) + 2 - 128); \
+    m0 = diff_lt_si128(p0, q0, alphavec); m0 = _mm_and_si128(m0, m3); \
+    m1 = diff_lt_si128(p2, p0, betavec); m1 = _mm_and_si128(m1, m0); \
+    m2 = diff_lt_si128(q2, q0, betavec); m2 = _mm_and_si128(m2, m0); \
+    LOOP_FILTER_LUMA_INTRA_HALF(lo, a0, a1, a2, a3, b0, b1, b2, b3) \
+    LOOP_FILTER_LUMA_INTRA_HALF(hi, x0, x1, x2, x3, y0, y1, y2, y3) \
+    p0 = _mm_blendv_epi8(p0, _mm_packus_epi16(a0, x0), m3); \
+    q0 = _mm_blendv_epi8(q0, _mm_packus_epi16(b0, y0), m3); \
+    p0 = _mm_blendv_epi8(p0, _mm_packus_epi16(a1, x1), m1); \
+    q0 = _mm_blendv_epi8(q0, _mm_packus_epi16(b1, y1), m2); \
+    p1 = _mm_blendv_epi8(p1, _mm_packus_epi16(a2, x2), m1); \
+    q1 = _mm_blendv_epi8(q1, _mm_packus_epi16(b2, y2), m2); \
+    p2 = _mm_blendv_epi8(p2, _mm_packus_epi16(a3, x3), m1); \
+    q2 = _mm_blendv_epi8(q2, _mm_packus_epi16(b3, y3), m2); \
+}
+
+static void h264_v_loop_filter_luma_intra_e2k(uint8_t *pix, ptrdiff_t stride, int alpha, int beta) {
+    __m128i p3, p2, p1, p0, q0, q1, q2, q3;
+    p3 = VEC_LD(pix - 4 * stride);
+    p2 = VEC_LD(pix - 3 * stride);
+    p1 = VEC_LD(pix - 2 * stride);
+    p0 = VEC_LD(pix - stride);
+    q0 = VEC_LD(pix);
+    q1 = VEC_LD(pix + stride);
+    q2 = VEC_LD(pix + 2 * stride);
+    q3 = VEC_LD(pix + 3 * stride);
+
+    LOOP_FILTER_LUMA_INTRA
+
+    VEC_ST(pix - 3 * stride, p2);
+    VEC_ST(pix - 2 * stride, p1);
+    VEC_ST(pix - stride, p0);
+    VEC_ST(pix, q0);
+    VEC_ST(pix + stride, q1);
+    VEC_ST(pix + stride * 2, q2);
+}
+
+#define TR8_ROUND(a, b) \
+    b##0 = _mm_unpacklo_epi8(a##0, a##4); \
+    b##1 = _mm_unpackhi_epi8(a##0, a##4); \
+    b##2 = _mm_unpacklo_epi8(a##1, a##5); \
+    b##3 = _mm_unpackhi_epi8(a##1, a##5); \
+    b##4 = _mm_unpacklo_epi8(a##2, a##6); \
+    b##5 = _mm_unpackhi_epi8(a##2, a##6); \
+    b##6 = _mm_unpacklo_epi8(a##3, a##7); \
+    b##7 = _mm_unpackhi_epi8(a##3, a##7);
+
+#define WRITE8X2(v0, i) VEC_STL(pix + i * stride, v0); VEC_STH(pix + (i + 1) * stride, v0);
+#define TR16X8_WR(i0, i1, i2, i3, i4, i5, i6, i7) { \
+    __m128i a0 = i0, a1 = i1, a2 = i2, a3 = i3; \
+    __m128i a4 = i4, a5 = i5, a6 = i6, a7 = i7; \
+    __m128i b0, b1, b2, b3, b4, b5, b6, b7; \
+    TR8_ROUND(a, b) TR8_ROUND(b, a) TR8_ROUND(a, b) \
+    pix -= 4; \
+    WRITE8X2(b0, 0) WRITE8X2(b1, 2) WRITE8X2(b2, 4) WRITE8X2(b3, 6) \
+    WRITE8X2(b4, 8) WRITE8X2(b5, 10) WRITE8X2(b6, 12) WRITE8X2(b7, 14) \
+}
+
+static void h264_h_loop_filter_luma_intra_e2k(uint8_t *pix, ptrdiff_t stride, int alpha, int beta) {
+    __m128i p3, p2, p1, p0, q0, q1, q2, q3;
+    read_and_transpose16x8(pix - 4, stride, p3, p2, p1, p0, q0, q1, q2, q3);
+    LOOP_FILTER_LUMA_INTRA
+    TR16X8_WR(p3, p2, p1, p0, q0, q1, q2, q3)
+}
+
+#define LOOP_FILTER_CHROMA_INTRA(vec, x, si) { \
+    vec alphavec, betavec, m0, t0, t1; \
+    betavec = _mm_set1_##x##i8(beta - 128); \
+    alphavec = _mm_set1_##x##i8(alpha - 128); \
+    m0 = deblock_mask_##si(p0, p1, q0, q1, alphavec, betavec); \
+    \
+    t1 = _mm_set1_##x##i8(-2); \
+    t0 = _mm_and_##si(_mm_xor_##si(p0, q1), t1); \
+    t1 = _mm_and_##si(_mm_xor_##si(q0, p1), t1); \
+    t0 = _mm_srli_##x##i16(t0, 1); \
+    t1 = _mm_srli_##x##i16(t1, 1); \
+    t0 = _mm_add_##x##i8(t0, _mm_and_##si(p0, q1)); \
+    t1 = _mm_add_##x##i8(t1, _mm_and_##si(q0, p1)); \
+    p0 = _mm_blendv_##x##i8(p0, _mm_avg_##x##u8(t0, p1), m0); \
+    q0 = _mm_blendv_##x##i8(q0, _mm_avg_##x##u8(t1, q1), m0); \
+}
+
+static void h264_v_loop_filter_chroma_intra_e2k(uint8_t *pix, ptrdiff_t stride, int alpha, int beta) {
+    __m64 p1, p0, q0, q1;
+    p1 = *(__m64*)(pix - 2 * stride);
+    p0 = *(__m64*)(pix - stride);
+    q0 = *(__m64*)pix;
+    q1 = *(__m64*)(pix + stride);
+
+    LOOP_FILTER_CHROMA_INTRA(__m64, p, si64)
+
+    *(__m64*)(pix - stride) = p0;
+    *(__m64*)pix = q0;
+}
+
+static void h264_h_loop_filter_chroma_intra_e2k(uint8_t *pix, ptrdiff_t stride, int alpha, int beta) {
+    __m64 p1, p0, q0, q1;
+    read_and_transpose8x4(pix - 2, stride, p1, p0, q0, q1);
+    LOOP_FILTER_CHROMA_INTRA(__m64, p, si64)
+    p1 = _mm_unpacklo_pi8(p0, q0);
+    q1 = _mm_unpackhi_pi8(p0, q0);
+#define WRITE2(v, i) *(uint16_t*)(pix + i * stride) = _mm_extract_pi16(v, i);
+    pix--;
+    WRITE2(p1, 0) WRITE2(p1, 1) WRITE2(p1, 2) WRITE2(p1, 3)
+    pix += stride * 4;
+    WRITE2(q1, 0) WRITE2(q1, 1) WRITE2(q1, 2) WRITE2(q1, 3)
+#undef WRITE2
+}
+
+static void h264_h_loop_filter_chroma422_intra_e2k(uint8_t *pix, ptrdiff_t stride, int alpha, int beta) {
+    __m128i p1, p0, q0, q1;
+    __m64 p1l, p0l, q0l, q1l, p1h, p0h, q0h, q1h;
+    read_and_transpose8x4(pix - 2, stride, p1l, p0l, q0l, q1l);
+    read_and_transpose8x4(pix - 2 + stride * 8, stride, p1h, p0h, q0h, q1h);
+    p1 = _mm_unpacklo_epi64(_mm_movpi64_epi64(p1l), _mm_movpi64_epi64(p1h));
+    p0 = _mm_unpacklo_epi64(_mm_movpi64_epi64(p0l), _mm_movpi64_epi64(p0h));
+    q0 = _mm_unpacklo_epi64(_mm_movpi64_epi64(q0l), _mm_movpi64_epi64(q0h));
+    q1 = _mm_unpacklo_epi64(_mm_movpi64_epi64(q1l), _mm_movpi64_epi64(q1h));
+    LOOP_FILTER_CHROMA_INTRA(__m128i, ep, si128)
+    p1 = _mm_unpacklo_epi8(p0, q0);
+    q1 = _mm_unpackhi_epi8(p0, q0);
+#define WRITE2(v, i) *(uint16_t*)(pix + i * stride) = _mm_extract_epi16(v, i);
+    pix--;
+    WRITE2(p1, 0) WRITE2(p1, 1) WRITE2(p1, 2) WRITE2(p1, 3)
+    WRITE2(p1, 4) WRITE2(p1, 5) WRITE2(p1, 6) WRITE2(p1, 7)
+    pix += stride * 8;
+    WRITE2(q1, 0) WRITE2(q1, 1) WRITE2(q1, 2) WRITE2(q1, 3)
+    WRITE2(q1, 4) WRITE2(q1, 5) WRITE2(q1, 6) WRITE2(q1, 7)
+#undef WRITE2
+}
+
+static void weight_h264_pixels16_e2k(uint8_t *block, ptrdiff_t stride, int height,
+                                     int log2_denom, int weight, int offset)
+{
+    int y; __m128i vweight, voffset, v0, v1; LOAD_ZERO;
+
+    offset <<= log2_denom;
+    offset += (1 << log2_denom) >> 1;
+
+    vweight = _mm_set1_epi16(weight);
+    voffset = _mm_set1_epi16(offset);
+
+    PRAGMA_E2K("ivdep")
+    PRAGMA_E2K("unroll(2)")
+    for (y = 0; y < height; y++) {
+        v1 = VEC_LD(block);
+        v0 = _mm_unpacklo_epi8(v1, zerov);
+        v1 = _mm_unpackhi_epi8(v1, zerov);
+
+        v0 = _mm_mullo_epi16(v0, vweight);
+        v1 = _mm_mullo_epi16(v1, vweight);
+        v0 = _mm_adds_epi16(v0, voffset);
+        v1 = _mm_adds_epi16(v1, voffset);
+        v0 = _mm_srai_epi16(v0, log2_denom);
+        v1 = _mm_srai_epi16(v1, log2_denom);
+
+        v0 = _mm_packus_epi16(v0, v1);
+        VEC_ST(block, v0);
+        block += stride;
+    }
+}
+
+static void weight_h264_pixels8_e2k(uint8_t *block, ptrdiff_t stride, int height,
+                                    int log2_denom, int weight, int offset)
+{
+    int y; __m128i vweight, voffset, v0; LOAD_ZERO;
+
+    offset <<= log2_denom;
+    offset += (1 << log2_denom) >> 1;
+
+    vweight = _mm_set1_epi16(weight);
+    voffset = _mm_set1_epi16(offset);
+
+    PRAGMA_E2K("ivdep")
+    PRAGMA_E2K("unroll(2)")
+    for (y = 0; y < height; y++) {
+        v0 = VEC_LD8(block);
+        v0 = _mm_unpacklo_epi8(v0, zerov);
+
+        v0 = _mm_mullo_epi16(v0, vweight);
+        v0 = _mm_adds_epi16(v0, voffset);
+        v0 = _mm_srai_epi16(v0, log2_denom);
+
+        v0 = _mm_packus_epi16(v0, v0);
+        VEC_STL(block, v0);
+        block += stride;
+    }
+}
+
+#define BIWEIGHT_INIT(vec, x) \
+    int y; vec vweight, voffset, v0, v1, v2, v3; \
+    \
+    offset = (offset + 1) | 1; \
+    log2_denom++; \
+    if (weights == 128 || weightd == 128) { \
+        weightd >>= 1; weights >>= 1; \
+        offset >>= 1; log2_denom -= 1; \
+    } \
+    offset = (offset << log2_denom) >> 1; \
+    vweight = _mm_set1_##x##i16(weightd << 8 | (weights & 255)); \
+    voffset = _mm_set1_##x##i16(offset);
+
+static void biweight_h264_pixels16_e2k(uint8_t *dst, uint8_t *src, ptrdiff_t stride, int height,
+                                       int log2_denom, int weightd, int weights, int offset)
+{
+    BIWEIGHT_INIT(__m128i, ep)
+
+    PRAGMA_E2K("ivdep")
+    PRAGMA_E2K("unroll(2)")
+    for (y = 0; y < height; y++) {
+        v2 = VEC_LD(src);
+        v3 = VEC_LD(dst);
+        v0 = _mm_unpacklo_epi8(v2, v3);
+        v1 = _mm_unpackhi_epi8(v2, v3);
+
+        v0 = _mm_maddubs_epi16(v0, vweight);
+        v1 = _mm_maddubs_epi16(v1, vweight);
+        v0 = _mm_adds_epi16(v0, voffset);
+        v1 = _mm_adds_epi16(v1, voffset);
+        v0 = _mm_srai_epi16(v0, log2_denom);
+        v1 = _mm_srai_epi16(v1, log2_denom);
+
+        v0 = _mm_packus_epi16(v0, v1);
+        VEC_ST(dst, v0);
+        dst += stride;
+        src += stride;
+    }
+}
+
+static void biweight_h264_pixels8_e2k(uint8_t *dst, uint8_t *src, ptrdiff_t stride, int height,
+                                      int log2_denom, int weightd, int weights, int offset)
+{
+    BIWEIGHT_INIT(__m64, p)
+
+    PRAGMA_E2K("ivdep")
+    PRAGMA_E2K("unroll(2)")
+    for (y = 0; y < height; y++) {
+        v2 = *(__m64*)src;
+        v3 = *(__m64*)dst;
+        v0 = _mm_unpacklo_pi8(v2, v3);
+        v1 = _mm_unpackhi_pi8(v2, v3);
+
+        v0 = _mm_maddubs_pi16(v0, vweight);
+        v1 = _mm_maddubs_pi16(v1, vweight);
+        v0 = _mm_adds_pi16(v0, voffset);
+        v1 = _mm_adds_pi16(v1, voffset);
+        v0 = _mm_srai_pi16(v0, log2_denom);
+        v1 = _mm_srai_pi16(v1, log2_denom);
+
+        v0 = _mm_packs_pu16(v0, v1);
+        *(__m64*)dst = v0;
+        dst += stride;
+        src += stride;
+    }
+}
+
+av_cold void ff_h264dsp_init_e2k(H264DSPContext *c, const int bit_depth,
+                                 const int chroma_format_idc)
+{
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    if (bit_depth == 8) {
+        c->h264_idct_add = h264_idct_add_e2k;
+        if (chroma_format_idc <= 1)
+            c->h264_idct_add8 = h264_idct_add8_e2k; // !checkasm
+
+        c->h264_idct_add16      = h264_idct_add16_e2k;
+        c->h264_idct_add16intra = h264_idct_add16intra_e2k;
+        c->h264_idct_dc_add     = h264_idct_dc_add_e2k;
+        c->h264_idct8_dc_add    = h264_idct8_dc_add_e2k;
+        c->h264_idct8_add       = h264_idct8_add_e2k;
+        c->h264_idct8_add4      = h264_idct8_add4_e2k;
+        c->h264_v_loop_filter_luma = h264_v_loop_filter_luma_e2k;
+        c->h264_h_loop_filter_luma = h264_h_loop_filter_luma_e2k;
+        c->h264_v_loop_filter_luma_intra = h264_v_loop_filter_luma_intra_e2k;
+        c->h264_h_loop_filter_luma_intra = h264_h_loop_filter_luma_intra_e2k;
+
+        c->h264_v_loop_filter_chroma = h264_v_loop_filter_chroma_e2k;
+        c->h264_v_loop_filter_chroma_intra = h264_v_loop_filter_chroma_intra_e2k;
+        if (chroma_format_idc <= 1) {
+            c->h264_h_loop_filter_chroma = h264_h_loop_filter_chroma_e2k;
+            c->h264_h_loop_filter_chroma_intra = h264_h_loop_filter_chroma_intra_e2k;
+        } else {
+            c->h264_h_loop_filter_chroma = h264_h_loop_filter_chroma422_e2k;
+            c->h264_h_loop_filter_chroma_intra = h264_h_loop_filter_chroma422_intra_e2k;
+        }
+        c->weight_h264_pixels_tab[0]   = weight_h264_pixels16_e2k;   // !checkasm
+        c->weight_h264_pixels_tab[1]   = weight_h264_pixels8_e2k;    // !checkasm
+        c->biweight_h264_pixels_tab[0] = biweight_h264_pixels16_e2k; // !checkasm
+        c->biweight_h264_pixels_tab[1] = biweight_h264_pixels8_e2k;  // !checkasm
+    }
+}
diff --git a/libavcodec/e2k/h264qpel.c b/libavcodec/e2k/h264qpel.c
new file mode 100644
index 0000000..eda9d0c
--- /dev/null
+++ b/libavcodec/e2k/h264qpel.c
@@ -0,0 +1,82 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavutil/mem.h"
+#include "libavutil/attributes.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/h264qpel.h"
+
+#define PREF_NAME1(p, name) p##_h264_qpel##name##_e2k
+#define PREF_NAME(p, name) PREF_NAME1(p, name)
+#define FUNC(p, name) \
+static void PREF_NAME(p, name)(uint8_t *dst, const uint8_t *src, ptrdiff_t stride)
+
+#define OP8_E2K(v0) *(__m64*)dst = v0; dst += stride;
+#define OP16_E2K(v0) VEC_ST(dst, v0); dst += stride;
+#define PREFIX put
+#include "h264qpel_template.c"
+
+#define OP8_E2K(v0) v0 = _mm_avg_pu8(v0, *(__m64*)dst); \
+    *(__m64*)dst = v0; dst += stride;
+#define OP16_E2K(v0) v0 = _mm_avg_epu8(v0, VEC_LD(dst)); \
+    VEC_ST(dst, v0); dst += stride;
+#define PREFIX avg
+#include "h264qpel_template.c"
+
+#undef FUNC
+
+av_cold void ff_h264qpel_init_e2k(H264QpelContext *c, int bit_depth)
+{
+    const int high_bit_depth = bit_depth > 8;
+
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    if (!high_bit_depth) {
+
+#define dspfunc(PFX, IDX, NUM) \
+        c->PFX##_pixels_tab[IDX][ 0] = PFX##NUM##_mc00_e2k; \
+        c->PFX##_pixels_tab[IDX][ 1] = PFX##NUM##_mc10_e2k; \
+        c->PFX##_pixels_tab[IDX][ 2] = PFX##NUM##_mc20_e2k; \
+        c->PFX##_pixels_tab[IDX][ 3] = PFX##NUM##_mc30_e2k; \
+        c->PFX##_pixels_tab[IDX][ 4] = PFX##NUM##_mc01_e2k; \
+        c->PFX##_pixels_tab[IDX][ 5] = PFX##NUM##_mc11_e2k; \
+        c->PFX##_pixels_tab[IDX][ 6] = PFX##NUM##_mc21_e2k; \
+        c->PFX##_pixels_tab[IDX][ 7] = PFX##NUM##_mc31_e2k; \
+        c->PFX##_pixels_tab[IDX][ 8] = PFX##NUM##_mc02_e2k; \
+        c->PFX##_pixels_tab[IDX][ 9] = PFX##NUM##_mc12_e2k; \
+        c->PFX##_pixels_tab[IDX][10] = PFX##NUM##_mc22_e2k; \
+        c->PFX##_pixels_tab[IDX][11] = PFX##NUM##_mc32_e2k; \
+        c->PFX##_pixels_tab[IDX][12] = PFX##NUM##_mc03_e2k; \
+        c->PFX##_pixels_tab[IDX][13] = PFX##NUM##_mc13_e2k; \
+        c->PFX##_pixels_tab[IDX][14] = PFX##NUM##_mc23_e2k; \
+        c->PFX##_pixels_tab[IDX][15] = PFX##NUM##_mc33_e2k
+
+        dspfunc(put_h264_qpel, 0, 16);
+        dspfunc(avg_h264_qpel, 0, 16);
+        dspfunc(put_h264_qpel, 1, 8);
+        dspfunc(avg_h264_qpel, 1, 8);
+#undef dspfunc
+    }
+}
diff --git a/libavcodec/e2k/h264qpel_template.c b/libavcodec/e2k/h264qpel_template.c
new file mode 100644
index 0000000..42989f3
--- /dev/null
+++ b/libavcodec/e2k/h264qpel_template.c
@@ -0,0 +1,504 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#define QPEL16_INIT \
+    int i; LOAD_ZERO; \
+    __m128i c20m5 = _mm_set1_epi16(20 | -5 << 8); \
+    __m128i srcM2, srcM1, srcP0, srcP1, srcP2, srcP3; \
+    __m128i v0, v1, v2, v3, v4, v5;
+
+#define QPEL16_LOAD_H \
+    srcM2 = VEC_LD(src - 2); \
+    srcM1 = VEC_LD(src - 1); \
+    srcP0 = VEC_LD(src); \
+    srcP1 = VEC_LD(src + 1); \
+    srcP2 = VEC_LD(src + 2); \
+    srcP3 = VEC_LD(src + 3);
+
+#define QPEL16_1(src) \
+    v0 = _mm_unpacklo_epi8(src##P0, src##M1); \
+    v1 = _mm_unpackhi_epi8(src##P0, src##M1); \
+    v2 = _mm_unpacklo_epi8(src##P1, src##P2); \
+    v3 = _mm_unpackhi_epi8(src##P1, src##P2); \
+    v4 = _mm_unpacklo_epi8(src##M2, zerov); \
+    v5 = _mm_unpackhi_epi8(src##M2, zerov); \
+    v4 = _mm_add_epi16(v4, _mm_unpacklo_epi8(src##P3, zerov)); \
+    v5 = _mm_add_epi16(v5, _mm_unpackhi_epi8(src##P3, zerov)); \
+    v0 = _mm_maddubs_epi16(v0, c20m5); \
+    v1 = _mm_maddubs_epi16(v1, c20m5); \
+    v2 = _mm_maddubs_epi16(v2, c20m5); \
+    v3 = _mm_maddubs_epi16(v3, c20m5);
+
+#define QPEL16_2 \
+    v4 = _mm_add_epi16(v4, c16); \
+    v5 = _mm_add_epi16(v5, c16); \
+    v0 = _mm_add_epi16(v4, _mm_add_epi16(v0, v2)); \
+    v1 = _mm_add_epi16(v5, _mm_add_epi16(v1, v3)); \
+    v0 = _mm_srai_epi16(v0, 5); \
+    v1 = _mm_srai_epi16(v1, 5); \
+    v0 = _mm_packus_epi16(v0, v1);
+
+FUNC(PREFIX, 16_mc00)
+{
+    int i; __m128i v0, v1;
+
+    PRAGMA_E2K("ivdep")
+    PRAGMA_E2K("unroll(8)")
+    for (i = 0; i < 16; i += 2) {
+        v0 = VEC_LD(src);
+        v1 = VEC_LD(src + stride);
+        OP16_E2K(v0)
+        OP16_E2K(v1)
+        src += stride << 1;
+    }
+}
+
+#define QPEL16_AVG_PX
+#define QPEL16_AVG_P0 v0 = _mm_avg_epu8(v0, srcP0);
+#define QPEL16_AVG_P1 v0 = _mm_avg_epu8(v0, srcP1);
+
+#define QPEL16_H_FUNC(name, PX) \
+FUNC(PREFIX, name) \
+{ \
+    __m128i c16 = _mm_set1_epi16(16); \
+    QPEL16_INIT \
+    \
+    PRAGMA_E2K("ivdep") \
+    for (i = 0; i < 16; i++) { \
+        QPEL16_LOAD_H \
+        QPEL16_1(src) \
+        QPEL16_2 \
+        QPEL16_AVG_##PX \
+        \
+        OP16_E2K(v0) \
+        src += stride; \
+    } \
+}
+
+#define QPEL16_V_FUNC(name, PX) \
+FUNC(PREFIX, name) \
+{ \
+    __m128i c16 = _mm_set1_epi16(16); \
+    QPEL16_INIT \
+    \
+    srcM2 = VEC_LD(src - stride * 2); \
+    srcM1 = VEC_LD(src - stride); \
+    srcP0 = VEC_LD(src); \
+    srcP1 = VEC_LD(src + stride); \
+    srcP2 = VEC_LD(src + stride * 2); \
+    src += stride * 3; \
+    \
+    PRAGMA_E2K("ivdep") \
+    for (i = 0; i < 16; i++) { \
+        srcP3 = VEC_LD(src); \
+        QPEL16_1(src) \
+        QPEL16_2 \
+        QPEL16_AVG_##PX \
+        \
+        OP16_E2K(v0) \
+        src += stride; \
+        \
+        srcM2 = srcM1; srcM1 = srcP0; srcP0 = srcP1; \
+        srcP1 = srcP2; srcP2 = srcP3; \
+    } \
+}
+
+static void PREF_NAME(PREFIX, 16_h_v)(uint8_t *dst, const uint8_t *src, ptrdiff_t stride, const uint8_t *srcv)
+{
+    __m128i c16 = _mm_set1_epi16(16);
+    __m128i tM1, tM2, tP0, tP1, tP2, tP3;
+    QPEL16_INIT
+
+    tM2 = VEC_LD(srcv - stride * 2);
+    tM1 = VEC_LD(srcv - stride);
+    tP0 = VEC_LD(srcv);
+    tP1 = VEC_LD(srcv + stride);
+    tP2 = VEC_LD(srcv + stride * 2);
+    srcv += stride * 3;
+
+    // PRAGMA_E2K("ivdep")
+    for (i = 0; i < 16; i++) {
+        QPEL16_LOAD_H
+        QPEL16_1(src)
+        QPEL16_2
+        srcP0 = v0;
+        src += stride;
+
+        tP3 = VEC_LD(srcv);
+        QPEL16_1(t)
+        QPEL16_2
+        srcv += stride;
+
+        v0 = _mm_avg_epu8(v0, srcP0);
+        OP16_E2K(v0)
+
+        tM2 = tM1; tM1 = tP0; tP0 = tP1; tP1 = tP2; tP2 = tP3;
+    }
+}
+
+#define QPEL16_H_V_FUNC(name, vx, hy) \
+FUNC(PREFIX, name) \
+{ \
+    PREF_NAME(PREFIX, 16_h_v)(dst, src + stride * hy, stride, src + vx); \
+}
+
+QPEL16_H_FUNC(16_mc10, P0)
+QPEL16_H_FUNC(16_mc20, PX)
+QPEL16_H_FUNC(16_mc30, P1)
+QPEL16_V_FUNC(16_mc01, P0)
+QPEL16_V_FUNC(16_mc02, PX)
+QPEL16_V_FUNC(16_mc03, P1)
+QPEL16_H_V_FUNC(16_mc11, 0, 0)
+QPEL16_H_V_FUNC(16_mc31, 1, 0)
+QPEL16_H_V_FUNC(16_mc13, 0, 1)
+QPEL16_H_V_FUNC(16_mc33, 1, 1)
+
+#define QPEL16_HV_INIT0
+#define QPEL16_HV_INIT1 __m128i c16 = _mm_set1_epi16(16);
+#define QPEL16_HV_INIT2 QPEL16_HV_INIT1 \
+    __m128i tM1, tM2, tP0, tP1, tP2, tP3;
+
+#define QPEL16_HVV1(PX) tM2 = tM1; tM1 = tP0; tP0 = tP1; tP1 = tP2; tP2 = src##PX;
+#define QPEL16_HVV2(PX) \
+    tP3A = v0; \
+    tP3 = src##PX; \
+    QPEL16_1(t) \
+    QPEL16_2 \
+    tM2 = tM1; tM1 = tP0; tP0 = tP1; tP1 = tP2; tP2 = tP3; \
+    v0 = _mm_avg_epu8(v0, tP3A);
+
+#define QPEL16_HHV(PX) \
+    v2 = _mm_srai_epi16(_mm_add_epi16(t##PX##A, c16), 5); \
+    v3 = _mm_srai_epi16(_mm_add_epi16(t##PX##B, c16), 5); \
+    v2 = _mm_packus_epi16(v2, v3); \
+    v0 = _mm_avg_epu8(v0, v2);
+
+#define QPEL16_HVV1_PX
+#define QPEL16_HVV1_P0 QPEL16_HVV1(P0)
+#define QPEL16_HVV1_P1 QPEL16_HVV1(P1)
+
+#define QPEL16_HVV2_PX
+#define QPEL16_HVV2_P0 QPEL16_HVV2(P0)
+#define QPEL16_HVV2_P1 QPEL16_HVV2(P1)
+
+#define QPEL16_HHV_PX
+#define QPEL16_HHV_P0 QPEL16_HHV(M1)
+#define QPEL16_HHV_P1 QPEL16_HHV(P0)
+
+#define QPEL16_HV_FUNC(name, init, PX, PY) \
+FUNC(PREFIX, name) \
+{ \
+    QPEL16_HV_INIT##init \
+    __m128i c32 = _mm_set1_epi16(32); \
+    QPEL16_INIT \
+    __m128i tM1A, tM2A, tP0A, tP1A, tP2A, tP3A; \
+    __m128i tM1B, tM2B, tP0B, tP1B, tP2B, tP3B; \
+    src -= 2 * stride; \
+    for (i = 0; i < 5; i++) { \
+        QPEL16_LOAD_H \
+        QPEL16_1(src) \
+        src += stride; \
+        tM2A = tM1A; tM2B = tM1B; \
+        tM1A = tP0A; tM1B = tP0B; \
+        tP0A = tP1A; tP0B = tP1B; \
+        tP1A = tP2A; tP1B = tP2B; \
+        tP2A = _mm_add_epi16(v4, _mm_add_epi16(v0, v2)); \
+        tP2B = _mm_add_epi16(v5, _mm_add_epi16(v1, v3)); \
+        QPEL16_HVV1_##PY \
+    } \
+    PRAGMA_E2K("ivdep") \
+    for (i = 0; i < 16; i++) { \
+        QPEL16_LOAD_H \
+        QPEL16_1(src) \
+        src += stride; \
+        tP3A = _mm_add_epi16(v4, _mm_add_epi16(v0, v2)); \
+        tP3B = _mm_add_epi16(v5, _mm_add_epi16(v1, v3)); \
+        \
+        v0 = _mm_add_epi16(tP0A, tP1A); \
+        v1 = _mm_add_epi16(tP0B, tP1B); \
+        v2 = _mm_add_epi16(tM1A, tP2A); \
+        v3 = _mm_add_epi16(tM1B, tP2B); \
+        v4 = _mm_add_epi16(tM2A, tP3A); \
+        v5 = _mm_add_epi16(tM2B, tP3B); \
+        \
+        tM2A = tM1A; tM2B = tM1B; \
+        tM1A = tP0A; tM1B = tP0B; \
+        tP0A = tP1A; tP0B = tP1B; \
+        tP1A = tP2A; tP1B = tP2B; \
+        tP2A = tP3A; tP2B = tP3B; \
+        \
+        v4 = _mm_srai_epi16(_mm_sub_epi16(v4, v2), 2); \
+        v5 = _mm_srai_epi16(_mm_sub_epi16(v5, v3), 2); \
+        v2 = _mm_sub_epi16(v0, v2); \
+        v3 = _mm_sub_epi16(v1, v3); \
+        v4 = _mm_srai_epi16(_mm_add_epi16(v4, v2), 2); \
+        v5 = _mm_srai_epi16(_mm_add_epi16(v5, v3), 2); \
+        v4 = _mm_add_epi16(v4, _mm_add_epi16(v0, c32)); \
+        v5 = _mm_add_epi16(v5, _mm_add_epi16(v1, c32)); \
+        v4 = _mm_srai_epi16(v4, 6); \
+        v5 = _mm_srai_epi16(v5, 6); \
+        v0 = _mm_packus_epi16(v4, v5); \
+        \
+        QPEL16_HHV_##PX \
+        QPEL16_HVV2_##PY \
+        OP16_E2K(v0) \
+    } \
+}
+
+QPEL16_HV_FUNC(16_mc21, 1, P0, PX)
+QPEL16_HV_FUNC(16_mc22, 0, PX, PX)
+QPEL16_HV_FUNC(16_mc23, 1, P1, PX)
+QPEL16_HV_FUNC(16_mc12, 2, PX, P0)
+QPEL16_HV_FUNC(16_mc32, 2, PX, P1)
+
+#define QPEL8_INIT \
+    int i; LOAD_ZERO; \
+    __m128i c20m5 = _mm_set1_epi16(20 | -5 << 8); \
+    __m128i srcM2, srcM1, srcP0, srcP1, srcP2, srcP3; \
+    __m128i v0, v2, v4; __m64 h0;
+
+#define QPEL8_LOAD_H \
+    srcM2 = VEC_LD8(src - 2); \
+    srcM1 = VEC_LD8(src - 1); \
+    srcP0 = VEC_LD8(src); \
+    srcP1 = VEC_LD8(src + 1); \
+    srcP2 = VEC_LD8(src + 2); \
+    srcP3 = VEC_LD8(src + 3);
+
+#define QPEL8_1(src) \
+    v0 = _mm_unpacklo_epi8(src##P0, src##M1); \
+    v2 = _mm_unpacklo_epi8(src##P1, src##P2); \
+    v4 = _mm_unpacklo_epi8(src##M2, zerov); \
+    v4 = _mm_add_epi16(v4, _mm_unpacklo_epi8(src##P3, zerov)); \
+    v0 = _mm_maddubs_epi16(v0, c20m5); \
+    v2 = _mm_maddubs_epi16(v2, c20m5);
+
+#define QPEL8_2 \
+    v4 = _mm_add_epi16(v4, c16); \
+    v0 = _mm_add_epi16(v4, _mm_add_epi16(v0, v2)); \
+    v0 = _mm_srai_epi16(v0, 5); \
+    h0 = _mm_movepi64_pi64(_mm_packus_epi16(v0, v0));
+
+FUNC(PREFIX, 8_mc00)
+{
+    int i; __m64 h0, h1;
+
+    PRAGMA_E2K("ivdep")
+    PRAGMA_E2K("unroll(4)")
+    for (i = 0; i < 8; i += 2) {
+        h0 = *(__m64*)src;
+        h1 = *(__m64*)(src + stride);
+        OP8_E2K(h0)
+        OP8_E2K(h1)
+        src += stride << 1;
+    }
+}
+
+#define QPEL8_AVG_PX
+#define QPEL8_AVG_P0 h0 = _mm_avg_pu8(h0, _mm_movepi64_pi64(srcP0));
+#define QPEL8_AVG_P1 h0 = _mm_avg_pu8(h0, _mm_movepi64_pi64(srcP1));
+
+#define QPEL8_H_FUNC(name, PX) \
+FUNC(PREFIX, name) \
+{ \
+    __m128i c16 = _mm_set1_epi16(16); \
+    QPEL8_INIT \
+    \
+    PRAGMA_E2K("ivdep") \
+    for (i = 0; i < 8; i++) { \
+        QPEL8_LOAD_H \
+        QPEL8_1(src) \
+        QPEL8_2 \
+        QPEL8_AVG_##PX \
+        \
+        OP8_E2K(h0) \
+        src += stride; \
+    } \
+}
+
+#define QPEL8_V_FUNC(name, PX) \
+FUNC(PREFIX, name) \
+{ \
+    __m128i c16 = _mm_set1_epi16(16); \
+    QPEL8_INIT \
+    \
+    srcM2 = VEC_LD8(src - stride * 2); \
+    srcM1 = VEC_LD8(src - stride); \
+    srcP0 = VEC_LD8(src); \
+    srcP1 = VEC_LD8(src + stride); \
+    srcP2 = VEC_LD8(src + stride * 2); \
+    src += stride * 3; \
+    \
+    PRAGMA_E2K("ivdep") \
+    for (i = 0; i < 8; i++) { \
+        srcP3 = VEC_LD8(src); \
+        QPEL8_1(src) \
+        QPEL8_2 \
+        QPEL8_AVG_##PX \
+        \
+        OP8_E2K(h0) \
+        src += stride; \
+        \
+        srcM2 = srcM1; srcM1 = srcP0; srcP0 = srcP1; \
+        srcP1 = srcP2; srcP2 = srcP3; \
+    } \
+}
+
+static void PREF_NAME(PREFIX, 8_h_v)(uint8_t *dst, const uint8_t *src, ptrdiff_t stride, const uint8_t *srcv)
+{
+    __m128i c16 = _mm_set1_epi16(16);
+    __m128i tM1, tM2, tP0, tP1, tP2, tP3;
+    __m64 h1;
+    QPEL8_INIT
+
+    tM2 = VEC_LD8(srcv - stride * 2);
+    tM1 = VEC_LD8(srcv - stride);
+    tP0 = VEC_LD8(srcv);
+    tP1 = VEC_LD8(srcv + stride);
+    tP2 = VEC_LD8(srcv + stride * 2);
+    srcv += stride * 3;
+
+    // PRAGMA_E2K("ivdep")
+    for (i = 0; i < 8; i++) {
+        QPEL8_LOAD_H
+        QPEL8_1(src)
+        QPEL8_2
+        h1 = h0;
+        src += stride;
+
+        tP3 = VEC_LD8(srcv);
+        QPEL8_1(t)
+        QPEL8_2
+        srcv += stride;
+
+        h0 = _mm_avg_pu8(h0, h1);
+        OP8_E2K(h0)
+
+        tM2 = tM1; tM1 = tP0; tP0 = tP1; tP1 = tP2; tP2 = tP3;
+    }
+}
+
+#define QPEL8_H_V_FUNC(name, vx, hy) \
+FUNC(PREFIX, name) \
+{ \
+    PREF_NAME(PREFIX, 8_h_v)(dst, src + stride * hy, stride, src + vx); \
+}
+
+QPEL8_H_FUNC(8_mc10, P0)
+QPEL8_H_FUNC(8_mc20, PX)
+QPEL8_H_FUNC(8_mc30, P1)
+QPEL8_V_FUNC(8_mc01, P0)
+QPEL8_V_FUNC(8_mc02, PX)
+QPEL8_V_FUNC(8_mc03, P1)
+QPEL8_H_V_FUNC(8_mc11, 0, 0)
+QPEL8_H_V_FUNC(8_mc31, 1, 0)
+QPEL8_H_V_FUNC(8_mc13, 0, 1)
+QPEL8_H_V_FUNC(8_mc33, 1, 1)
+
+#define QPEL8_HV_INIT0
+#define QPEL8_HV_INIT1 __m128i c16 = _mm_set1_epi16(16);
+#define QPEL8_HV_INIT2 QPEL8_HV_INIT1 \
+    __m128i tM1, tM2, tP0, tP1, tP2, tP3; __m64 h1;
+
+#define QPEL8_HVV1(PX) tM2 = tM1; tM1 = tP0; tP0 = tP1; tP1 = tP2; tP2 = src##PX;
+#define QPEL8_HVV2(PX) \
+    h1 = h0; \
+    tP3 = src##PX; \
+    QPEL8_1(t) \
+    QPEL8_2 \
+    tM2 = tM1; tM1 = tP0; tP0 = tP1; tP1 = tP2; tP2 = tP3; \
+    h0 = _mm_avg_pu8(h0, h1);
+
+#define QPEL8_HHV(PX) \
+    v2 = _mm_srai_epi16(_mm_add_epi16(t##PX##A, c16), 5); \
+    v2 = _mm_packus_epi16(v2, v2); \
+    h0 = _mm_avg_pu8(h0, _mm_movepi64_pi64(v2));
+
+#define QPEL8_HVV1_PX
+#define QPEL8_HVV1_P0 QPEL8_HVV1(P0)
+#define QPEL8_HVV1_P1 QPEL8_HVV1(P1)
+
+#define QPEL8_HVV2_PX
+#define QPEL8_HVV2_P0 QPEL8_HVV2(P0)
+#define QPEL8_HVV2_P1 QPEL8_HVV2(P1)
+
+#define QPEL8_HHV_PX
+#define QPEL8_HHV_P0 QPEL8_HHV(M1)
+#define QPEL8_HHV_P1 QPEL8_HHV(P0)
+
+#define QPEL8_HV_FUNC(name, init, PX, PY) \
+FUNC(PREFIX, name) \
+{ \
+    QPEL8_HV_INIT##init \
+    __m128i c32 = _mm_set1_epi16(32); \
+    QPEL8_INIT \
+    __m128i tM1A, tM2A, tP0A, tP1A, tP2A, tP3A; \
+    src -= 2 * stride; \
+    for (i = 0; i < 5; i++) { \
+        QPEL8_LOAD_H \
+        QPEL8_1(src) \
+        src += stride; \
+        tM2A = tM1A; \
+        tM1A = tP0A; \
+        tP0A = tP1A; \
+        tP1A = tP2A; \
+        tP2A = _mm_add_epi16(v4, _mm_add_epi16(v0, v2)); \
+        QPEL8_HVV1_##PY \
+    } \
+    PRAGMA_E2K("ivdep") \
+    for (i = 0; i < 8; i++) { \
+        QPEL8_LOAD_H \
+        QPEL8_1(src) \
+        src += stride; \
+        tP3A = _mm_add_epi16(v4, _mm_add_epi16(v0, v2)); \
+        \
+        v0 = _mm_add_epi16(tP0A, tP1A); \
+        v2 = _mm_add_epi16(tM1A, tP2A); \
+        v4 = _mm_add_epi16(tM2A, tP3A); \
+        \
+        tM2A = tM1A; \
+        tM1A = tP0A; \
+        tP0A = tP1A; \
+        tP1A = tP2A; \
+        tP2A = tP3A; \
+        \
+        v4 = _mm_srai_epi16(_mm_sub_epi16(v4, v2), 2); \
+        v2 = _mm_sub_epi16(v0, v2); \
+        v4 = _mm_srai_epi16(_mm_add_epi16(v4, v2), 2); \
+        v4 = _mm_add_epi16(v4, _mm_add_epi16(v0, c32)); \
+        v4 = _mm_srai_epi16(v4, 6); \
+        h0 = _mm_movepi64_pi64(_mm_packus_epi16(v4, v4)); \
+        \
+        QPEL8_HHV_##PX \
+        QPEL8_HVV2_##PY \
+        OP8_E2K(h0) \
+    } \
+}
+
+QPEL8_HV_FUNC(8_mc21, 1, P0, PX)
+QPEL8_HV_FUNC(8_mc22, 0, PX, PX)
+QPEL8_HV_FUNC(8_mc23, 1, P1, PX)
+QPEL8_HV_FUNC(8_mc12, 2, PX, P0)
+QPEL8_HV_FUNC(8_mc32, 2, PX, P1)
+
+#undef OP8_E2K
+#undef OP16_E2K
+#undef PREFIX
+
diff --git a/libavcodec/e2k/hevcdsp.c b/libavcodec/e2k/hevcdsp.c
new file mode 100644
index 0000000..e19296d
--- /dev/null
+++ b/libavcodec/e2k/hevcdsp.c
@@ -0,0 +1,2893 @@
+/*
+ * SIMD-optimized functions for HEVC decoding
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *
+ * SSE2-optimized HEVC deblocking code:
+ * Copyright (C) 2013 VTT
+ * Authors: Seppo Tomperi <seppo.tomperi@vtt.fi>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/hevcdsp.h"
+
+#define BIT_DEPTH 8
+
+/* add residual ----------------------------------------------------*/
+
+static void hevc_add_residual4x4_8_e2k(uint8_t *dst, const int16_t *res,
+                                       ptrdiff_t stride)
+{
+    __m64 v0, v1, v2, v3, vzero = _mm_setzero_si64();
+    int y;
+
+    PRAGMA_E2K("ivdep")
+    PRAGMA_E2K("unroll(2)")
+    for (y = 0; y < 4; y += 2) {
+        v0 = *(__m64*)res;
+        v1 = *(__m64*)(res + 4);
+        v2 = _mm_cvtsi32_si64(*(uint32_t*)dst);
+        v3 = _mm_cvtsi32_si64(*(uint32_t*)(dst + stride));
+        v2 = _mm_unpacklo_pi8(v2, vzero);
+        v3 = _mm_unpacklo_pi8(v3, vzero);
+        v0 = _mm_adds_pi16(v0, v2);
+        v1 = _mm_adds_pi16(v1, v3);
+        v0 = _mm_packs_pu16(v0, v1);
+        *(uint32_t*)dst = _mm_cvtsi64_si32(v0);
+        *(uint32_t*)(dst + stride) = _mm_extract_pi32(v0, 1);
+        res += 4 * 2;
+        dst += stride << 1;
+    }
+}
+
+static void hevc_add_residual8x8_8_e2k(uint8_t *dst, const int16_t *res,
+                                       ptrdiff_t stride)
+{
+    __m128i v0, v1, v2, v3, vzero = _mm_setzero_si128();
+    int y;
+
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < 8; y += 2) {
+        v0 = _mm_load_si128((const __m128i*)res);
+        v1 = _mm_load_si128((const __m128i*)(res + 8));
+        v2 = VEC_LD8(dst);
+        v3 = VEC_LD8(dst + stride);
+        v2 = _mm_unpacklo_epi8(v2, vzero);
+        v3 = _mm_unpacklo_epi8(v3, vzero);
+        v0 = _mm_adds_epi16(v0, v2);
+        v1 = _mm_adds_epi16(v1, v3);
+        v0 = _mm_packus_epi16(v0, v1);
+        VEC_STL(dst, v0);
+        VEC_STH(dst + stride, v0);
+        res += 8 * 2;
+        dst += stride << 1;
+    }
+}
+
+static void hevc_add_residual16x16_8_e2k(uint8_t *dst, const int16_t *res,
+                                         ptrdiff_t stride)
+{
+    __m128i v0, v1, v2, v3, vzero = _mm_setzero_si128();
+    int y;
+
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < 16; y++) {
+        v0 = _mm_load_si128((const __m128i*)res);
+        v1 = _mm_load_si128((const __m128i*)(res + 8));
+        v3 = _mm_load_si128((const __m128i*)dst);
+        v2 = _mm_unpacklo_epi8(v3, vzero);
+        v3 = _mm_unpackhi_epi8(v3, vzero);
+        v0 = _mm_adds_epi16(v0, v2);
+        v1 = _mm_adds_epi16(v1, v3);
+        v0 = _mm_packus_epi16(v0, v1);
+        _mm_store_si128((__m128i*)dst, v0);
+        res += 16;
+        dst += stride;
+    }
+}
+
+static void hevc_add_residual32x32_8_e2k(uint8_t *dst, const int16_t *res,
+                                         ptrdiff_t stride)
+{
+    __m128i v0, v1, v2, v3, v4, v5, v6, v7, vzero = _mm_setzero_si128();
+    int y;
+
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < 32; y++) {
+        v0 = _mm_load_si128((const __m128i*)res);
+        v1 = _mm_load_si128((const __m128i*)(res + 8));
+        v2 = _mm_load_si128((const __m128i*)(res + 16));
+        v3 = _mm_load_si128((const __m128i*)(res + 24));
+        v5 = _mm_load_si128((const __m128i*)dst);
+        v7 = _mm_load_si128((const __m128i*)(dst + 16));
+        v4 = _mm_unpacklo_epi8(v5, vzero);
+        v5 = _mm_unpackhi_epi8(v5, vzero);
+        v6 = _mm_unpacklo_epi8(v7, vzero);
+        v7 = _mm_unpackhi_epi8(v7, vzero);
+        v0 = _mm_adds_epi16(v0, v4);
+        v1 = _mm_adds_epi16(v1, v5);
+        v2 = _mm_adds_epi16(v2, v6);
+        v3 = _mm_adds_epi16(v3, v7);
+        v0 = _mm_packus_epi16(v0, v1);
+        v2 = _mm_packus_epi16(v2, v3);
+        _mm_store_si128((__m128i*)dst, v0);
+        _mm_store_si128((__m128i*)(dst + 16), v2);
+        res += 32;
+        dst += stride;
+    }
+}
+
+/* sao band filter -------------------------------------------------*/
+
+static void hevc_sao_band_filter_8_e2k(uint8_t *dst, const uint8_t *src,
+                                       ptrdiff_t stride_dst, ptrdiff_t stride_src,
+                                       const int16_t *sao_offset_val, int sao_left_class,
+                                       int width, int height)
+{
+    int x, y, shift = BIT_DEPTH - 5;
+    __m64 v0, v1, v2, vadd, vsub, c1, c2, c7 = _mm_set1_pi8(7);
+
+    v0 = *(__m64*)(sao_offset_val + 1);
+    v1 = _mm_sub_pi16(_mm_setzero_si64(), v0);
+    vadd = _mm_packs_pu16(v0, v0);
+    vsub = _mm_packs_pu16(v1, v1);
+
+    c1 = _mm_set1_pi8(((sao_left_class + 4) << shift) + 128);
+    c2 = _mm_set1_pi8(128 - (4 << shift));
+
+    for (y = 0; y < height; y++) {
+        PRAGMA_E2K("ivdep")
+        for (x = 0; x < width; x += 8) {
+            v0 = *(__m64*)(src + x);
+            v1 = _mm_sub_pi8(v0, c1);
+            v2 = _mm_and_si64(_mm_srai_pi16(v1, shift), c7);
+            v1 = _mm_or_si64(v2, _mm_cmpgt_pi8(c2, v1));
+            v0 = _mm_adds_pu8(v0, _mm_shuffle_pi8(vadd, v1));
+            v0 = _mm_subs_pu8(v0, _mm_shuffle_pi8(vsub, v1));
+            *(__m64*)(dst + x) = v0;
+        }
+        dst += stride_dst;
+        src += stride_src;
+    }
+}
+
+/* sao edge filter -------------------------------------------------*/
+
+#define SAO_EDGE_FUNC(w) \
+static void hevc_sao_edge_filter_##w##_8_e2k(uint8_t *dst, const uint8_t *src, ptrdiff_t stride_dst, \
+                                             const int16_t *sao_offset_val, int eo, int width, int height)
+#define SAO_EDGE_INIT \
+    int y; \
+    __m64 htab; \
+    ptrdiff_t a_stride, stride_src = 2*MAX_PB_SIZE + AV_INPUT_BUFFER_PADDING_SIZE; \
+    a_stride = (eo >> 1 | ((eo & 1) - 1)) - (eo ? stride_src : 0); \
+    htab = *(__m64*)sao_offset_val; \
+    htab = _mm_shuffle_pi16(htab, 0xc9); /* [1, 2, 0, 3], 4 */ \
+    htab = _mm_packs_pi16(htab, _mm_cvtsi32_si64(*(uint16_t*)&sao_offset_val[4]));
+
+SAO_EDGE_FUNC(8) {
+    __m64 v0, v2, v3, v4, v5;
+    __m64 c1 = _mm_set1_pi8(128), c2 = _mm_set1_pi8(2);
+    SAO_EDGE_INIT
+
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < height; y++) {
+        v0 = *(__m64*)src;
+        v2 = *(__m64*)(src + a_stride);
+        v3 = *(__m64*)(src - a_stride);
+        v0 = _mm_xor_si64(v0, c1);
+        v2 = _mm_xor_si64(v2, c1);
+        v3 = _mm_xor_si64(v3, c1);
+
+        v4 = _mm_cmpgt_pi8(v0, v2);
+        v5 = _mm_cmpgt_pi8(v0, v3);
+        v2 = _mm_cmpgt_pi8(v2, v0);
+        v3 = _mm_cmpgt_pi8(v3, v0);
+        v2 = _mm_sub_pi8(v2, v4);
+        v3 = _mm_sub_pi8(v3, v5);
+        v2 = _mm_add_pi8(v2, c2);
+        v2 = _mm_add_pi8(v2, v3);
+
+        v2 = _mm_shuffle_pi8(htab, v2);
+        v0 = _mm_adds_pi8(v0, v2);
+        v0 = _mm_xor_si64(v0, c1);
+
+        *(__m64*)dst = v0;
+        src += stride_src;
+        dst += stride_dst;
+    }
+}
+
+#define SAO_EDGE_V16_INIT \
+    __m128i v0, v2, v3, v4, v5, vtab; \
+    __m128i c1 = _mm_set1_epi8(128), c2 = _mm_set1_epi8(2); \
+    SAO_EDGE_INIT \
+    vtab = _mm_movpi64_epi64(htab); \
+    PRAGMA_E2K("ivdep") \
+    for (y = 0; y < height; y++) {
+
+#define SAO_EDGE_V16_FILTER(i, out) \
+    v0 = _mm_load_si128((const __m128i*)(src + i)); \
+    v2 = VEC_LD(src + i + a_stride); \
+    v3 = VEC_LD(src + i - a_stride); \
+    v0 = _mm_xor_si128(v0, c1); \
+    v2 = _mm_xor_si128(v2, c1); \
+    v3 = _mm_xor_si128(v3, c1); \
+    \
+    v4 = _mm_cmpgt_epi8(v0, v2); \
+    v5 = _mm_cmpgt_epi8(v0, v3); \
+    v2 = _mm_cmpgt_epi8(v2, v0); \
+    v3 = _mm_cmpgt_epi8(v3, v0); \
+    v2 = _mm_sub_epi8(v2, v4); \
+    v3 = _mm_sub_epi8(v3, v5); \
+    v2 = _mm_add_epi8(v2, c2); \
+    v2 = _mm_add_epi8(v2, v3); \
+    \
+    v2 = _mm_shuffle_epi8(vtab, v2); \
+    v0 = _mm_adds_epi8(v0, v2); \
+    out = _mm_xor_si128(v0, c1);
+
+SAO_EDGE_FUNC(16) {
+    SAO_EDGE_V16_INIT
+        SAO_EDGE_V16_FILTER(0, v3)
+        _mm_store_si128((__m128i*)dst, v3);
+        src += stride_src;
+        dst += stride_dst;
+    }
+}
+
+SAO_EDGE_FUNC(32) {
+    __m128i o0, o1;
+    SAO_EDGE_V16_INIT
+        SAO_EDGE_V16_FILTER(0, o0)
+        SAO_EDGE_V16_FILTER(16, o1)
+        _mm_store_si128((__m128i*)dst, o0);
+        _mm_store_si128((__m128i*)(dst + 16), o1);
+        src += stride_src;
+        dst += stride_dst;
+    }
+}
+
+SAO_EDGE_FUNC(48) {
+    __m128i o0, o1, o2;
+    SAO_EDGE_V16_INIT
+        SAO_EDGE_V16_FILTER(0, o0)
+        SAO_EDGE_V16_FILTER(16, o1)
+        SAO_EDGE_V16_FILTER(32, o2)
+        _mm_store_si128((__m128i*)dst, o0);
+        _mm_store_si128((__m128i*)(dst + 16), o1);
+        _mm_store_si128((__m128i*)(dst + 32), o2);
+        src += stride_src;
+        dst += stride_dst;
+    }
+}
+
+SAO_EDGE_FUNC(64) {
+    __m128i o0, o1, o2, o3;
+    SAO_EDGE_V16_INIT
+        SAO_EDGE_V16_FILTER(0, o0)
+        SAO_EDGE_V16_FILTER(16, o1)
+        SAO_EDGE_V16_FILTER(32, o2)
+        SAO_EDGE_V16_FILTER(48, o3)
+        _mm_store_si128((__m128i*)dst, o0);
+        _mm_store_si128((__m128i*)(dst + 16), o1);
+        _mm_store_si128((__m128i*)(dst + 32), o2);
+        _mm_store_si128((__m128i*)(dst + 48), o3);
+        src += stride_src;
+        dst += stride_dst;
+    }
+}
+
+/* epel/qpel -------------------------------------------------------*/
+
+#define PEL_FIN_UNI8(v6, v7, out) \
+    v6 = _mm_srai_pi16(_mm_add_pi16(v6, vbias), shift); \
+    v7 = _mm_srai_pi16(_mm_add_pi16(v7, vbias), shift); \
+    out = _mm_packs_pu16(v6, v7);
+
+#define PEL_FIN_UNI(v6, v7, out) \
+    v6 = _mm_srai_epi16(_mm_add_epi16(v6, vbias), shift); \
+    v7 = _mm_srai_epi16(_mm_add_epi16(v7, vbias), shift); \
+    out = _mm_packus_epi16(v6, v7);
+
+#define PEL_FIN_BI8(src2, off, v6, v7, v2, v3, out) \
+    v2 = *(__m64*)(src2); \
+    v3 = *(__m64*)(src2 + off); \
+    v6 = _mm_adds_pi16(_mm_adds_pi16(v2, vbias), v6); \
+    v7 = _mm_adds_pi16(_mm_adds_pi16(v3, vbias), v7); \
+    v6 = _mm_srai_pi16(v6, shift); \
+    v7 = _mm_srai_pi16(v7, shift); \
+    out = _mm_packs_pu16(v6, v7);
+
+#define PEL_FIN_BI(src2, off, v6, v7, v2, v3, out) \
+    v2 = VEC_LD(src2); \
+    v3 = VEC_LD(src2 + off); \
+    v6 = _mm_adds_epi16(_mm_adds_epi16(v2, vbias), v6); \
+    v7 = _mm_adds_epi16(_mm_adds_epi16(v3, vbias), v7); \
+    v6 = _mm_srai_epi16(v6, shift); \
+    v7 = _mm_srai_epi16(v7, shift); \
+    out = _mm_packus_epi16(v6, v7);
+
+#define PEL_FIN_UNI_W8(v6, v7, v2, v3, v4, v5, out) \
+    v2 = _mm_mullo_pi16(v6, vwx); \
+    v3 = _mm_mulhi_pi16(v6, vwx); \
+    v4 = _mm_unpacklo_pi16(v2, v3); \
+    v5 = _mm_unpackhi_pi16(v2, v3); \
+    v2 = _mm_mullo_pi16(v7, vwx); \
+    v3 = _mm_mulhi_pi16(v7, vwx); \
+    v6 = _mm_unpacklo_pi16(v2, v3); \
+    v7 = _mm_unpackhi_pi16(v2, v3); \
+    v4 = _mm_srai_pi32(_mm_add_pi32(v4, vbias), shift); \
+    v5 = _mm_srai_pi32(_mm_add_pi32(v5, vbias), shift); \
+    v6 = _mm_srai_pi32(_mm_add_pi32(v6, vbias), shift); \
+    v7 = _mm_srai_pi32(_mm_add_pi32(v7, vbias), shift); \
+    v4 = _mm_adds_pi16(_mm_packs_pi32(v4, v5), vox); \
+    v6 = _mm_adds_pi16(_mm_packs_pi32(v6, v7), vox); \
+    out = _mm_packs_pu16(v4, v6);
+
+#define PEL_FIN_UNI_W(v6, v7, v2, v3, v4, v5, out) \
+    v2 = _mm_mullo_epi16(v6, vwx); \
+    v3 = _mm_mulhi_epi16(v6, vwx); \
+    v4 = _mm_unpacklo_epi16(v2, v3); \
+    v5 = _mm_unpackhi_epi16(v2, v3); \
+    v2 = _mm_mullo_epi16(v7, vwx); \
+    v3 = _mm_mulhi_epi16(v7, vwx); \
+    v6 = _mm_unpacklo_epi16(v2, v3); \
+    v7 = _mm_unpackhi_epi16(v2, v3); \
+    v4 = _mm_srai_epi32(_mm_add_epi32(v4, vbias), shift); \
+    v5 = _mm_srai_epi32(_mm_add_epi32(v5, vbias), shift); \
+    v6 = _mm_srai_epi32(_mm_add_epi32(v6, vbias), shift); \
+    v7 = _mm_srai_epi32(_mm_add_epi32(v7, vbias), shift); \
+    v4 = _mm_adds_epi16(_mm_packs_epi32(v4, v5), vox); \
+    v6 = _mm_adds_epi16(_mm_packs_epi32(v6, v7), vox); \
+    out = _mm_packus_epi16(v4, v6);
+
+#define PEL_FUNC_TMP(name) \
+static void put_hevc_##name##_8_e2k(int16_t *dst, const uint8_t *src, ptrdiff_t srcstride, \
+                                          int height, intptr_t mx, intptr_t my, int width)
+#define PEL_FUNC_UNI(name) \
+static void put_hevc_##name##_8_e2k(uint8_t *dst, ptrdiff_t dststride, \
+                                          const uint8_t *src, ptrdiff_t srcstride, \
+                                          int height, intptr_t mx, intptr_t my, int width)
+#define PEL_FUNC_BI(name) \
+static void put_hevc_##name##_8_e2k(uint8_t *dst, ptrdiff_t dststride, \
+                                          const uint8_t *src, ptrdiff_t srcstride, \
+                                          const int16_t *src2, int height, \
+                                          intptr_t mx, intptr_t my, int width)
+#define PEL_FUNC_UNI_W(name) \
+static void put_hevc_##name##_8_e2k(uint8_t *dst, ptrdiff_t dststride, \
+                                          const uint8_t *src, ptrdiff_t srcstride, \
+                                          int height, int denom, int wx, int ox, \
+                                          intptr_t mx, intptr_t my, int width)
+
+#define PEL_FUNC(w, vec, n) PEL_FUNC_TMP(pel_pixels##w) { \
+    int y, shift = 14 - BIT_DEPTH; \
+    vec v0, v1, vzero = _mm_setzero_si##n();
+
+#define PEL_LD16(i) \
+    v1 = VEC_LD(src + i * 16); \
+    v0 = _mm_unpacklo_epi8(v1, vzero); \
+    v1 = _mm_unpackhi_epi8(v1, vzero); \
+    l##i = _mm_slli_epi16(v0, shift); \
+    h##i = _mm_slli_epi16(v1, shift);
+
+PEL_FUNC(8, __m64, 64)
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < height; y++) {
+        v1 = *(__m64*)src;
+        v0 = _mm_unpacklo_pi8(v1, vzero);
+        v1 = _mm_unpackhi_pi8(v1, vzero);
+        v0 = _mm_slli_pi16(v0, shift);
+        v1 = _mm_slli_pi16(v1, shift);
+        *(__m64*)dst = v0;
+        *(__m64*)(dst + 4) = v1;
+        src += srcstride;
+        dst += MAX_PB_SIZE;
+    }
+}
+
+PEL_FUNC(16, __m128i, 128)
+    __m128i l0, h0;
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < height; y++) {
+        PEL_LD16(0)
+        VEC_ST(dst, l0); VEC_ST(dst + 8, h0);
+        src += srcstride;
+        dst += MAX_PB_SIZE;
+    }
+}
+
+PEL_FUNC(32, __m128i, 128)
+    __m128i l0, h0, l1, h1;
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < height; y++) {
+        PEL_LD16(0) PEL_LD16(1)
+        VEC_ST(dst, l0); VEC_ST(dst + 8, h0);
+        VEC_ST(dst + 16, l1); VEC_ST(dst + 24, h1);
+        src += srcstride;
+        dst += MAX_PB_SIZE;
+    }
+}
+
+PEL_FUNC(64, __m128i, 128)
+    __m128i l0, h0, l1, h1, l2, h2, l3, h3;
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < height; y++) {
+        PEL_LD16(0) PEL_LD16(1) PEL_LD16(2) PEL_LD16(3)
+        VEC_ST(dst, l0); VEC_ST(dst + 8, h0);
+        VEC_ST(dst + 16, l1); VEC_ST(dst + 24, h1);
+        VEC_ST(dst + 32, l2); VEC_ST(dst + 40, h2);
+        VEC_ST(dst + 48, l3); VEC_ST(dst + 56, h3);
+        src += srcstride;
+        dst += MAX_PB_SIZE;
+    }
+}
+
+#undef PEL_FUNC
+#undef PEL_LD16
+
+#define PEL_FUNC(w) PEL_FUNC_UNI(pel_uni_pixels##w) { \
+    int y;
+
+PEL_FUNC(8)
+    __m64 v0;
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < height; y++) {
+        v0 = *(__m64*)src;
+        *(__m64*)dst = v0;
+        src += srcstride;
+        dst += dststride;
+    }
+}
+
+PEL_FUNC(16)
+    __m128i v0;
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < height; y++) {
+        v0 = VEC_LD(src);
+        VEC_ST(dst, v0);
+        src += srcstride;
+        dst += dststride;
+    }
+}
+
+PEL_FUNC(32)
+    __m128i v0, v1;
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < height; y++) {
+        v0 = VEC_LD(src);
+        v1 = VEC_LD(src + 16);
+        VEC_ST(dst, v0);
+        VEC_ST(dst + 16, v1);
+        src += srcstride;
+        dst += dststride;
+    }
+}
+
+PEL_FUNC(64)
+    __m128i v0, v1, v2, v3;
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < height; y++) {
+        v0 = VEC_LD(src);
+        v1 = VEC_LD(src + 16);
+        v2 = VEC_LD(src + 32);
+        v3 = VEC_LD(src + 48);
+        VEC_ST(dst, v0);
+        VEC_ST(dst + 16, v1);
+        VEC_ST(dst + 32, v2);
+        VEC_ST(dst + 48, v3);
+        src += srcstride;
+        dst += dststride;
+    }
+}
+
+#undef PEL_FUNC
+
+#define PEL_FUNC(w, vec, pi, n) PEL_FUNC_BI(pel_bi_pixels##w) { \
+    int y, shift = 14 + 1 - BIT_DEPTH; \
+    vec v0, v1, v2, v3, vzero = _mm_setzero_si##n(), vbias = _mm_set1_##pi##16(1 << (shift - 1));
+
+#define PEL_LD16(i) \
+    v1 = VEC_LD(src + i * 16); \
+    v0 = _mm_slli_epi16(_mm_unpacklo_epi8(v1, vzero), shift - 1); \
+    v1 = _mm_slli_epi16(_mm_unpackhi_epi8(v1, vzero), shift - 1); \
+    PEL_FIN_BI(src2 + i * 16, 8, v0, v1, v2, v3, o##i)
+
+PEL_FUNC(8, __m64, pi, 64)
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < height; y++) {
+        v1 = *(__m64*)src;
+        v0 = _mm_slli_pi16(_mm_unpacklo_pi8(v1, vzero), shift - 1);
+        v1 = _mm_slli_pi16(_mm_unpackhi_pi8(v1, vzero), shift - 1);
+        PEL_FIN_BI8(src2, 4, v0, v1, v2, v3, v0)
+        *(__m64*)dst = v0;
+        src += srcstride;
+        dst += dststride;
+        src2 += MAX_PB_SIZE;
+    }
+}
+
+PEL_FUNC(16, __m128i, epi, 128)
+    __m128i o0;
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < height; y++) {
+        PEL_LD16(0)
+        VEC_ST(dst, o0);
+        src += srcstride;
+        dst += dststride;
+        src2 += MAX_PB_SIZE;
+    }
+}
+
+PEL_FUNC(32, __m128i, epi, 128)
+    __m128i o0, o1;
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < height; y++) {
+        PEL_LD16(0) PEL_LD16(1)
+        VEC_ST(dst, o0);
+        VEC_ST(dst + 16, o1);
+        src += srcstride;
+        dst += dststride;
+        src2 += MAX_PB_SIZE;
+    }
+}
+
+PEL_FUNC(64, __m128i, epi, 128)
+    __m128i o0, o1, o2, o3;
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < height; y++) {
+        PEL_LD16(0) PEL_LD16(1) PEL_LD16(2) PEL_LD16(3)
+        VEC_ST(dst, o0);
+        VEC_ST(dst + 16, o1);
+        VEC_ST(dst + 32, o2);
+        VEC_ST(dst + 48, o3);
+        src += srcstride;
+        dst += dststride;
+        src2 += MAX_PB_SIZE;
+    }
+}
+
+#undef PEL_FUNC
+#undef PEL_LD16
+
+#define PEL_FUNC(w, vec, pi, n) PEL_FUNC_UNI_W(pel_uni_w_pixels##w) { \
+    int y, shift = denom + 14 - BIT_DEPTH; \
+    vec v0, v1, v2, v3, v4, v5, vzero = _mm_setzero_si##n(); \
+    vec vbias = _mm_set1_##pi##32(1 << (shift - 1)); \
+    vec vwx = _mm_set1_##pi##16(wx), vox = _mm_set1_##pi##16(ox);
+
+#define PEL_LD16(i) \
+    v1 = VEC_LD(src + i * 16); \
+    v0 = _mm_slli_epi16(_mm_unpacklo_epi8(v1, vzero), 14 - BIT_DEPTH); \
+    v1 = _mm_slli_epi16(_mm_unpackhi_epi8(v1, vzero), 14 - BIT_DEPTH); \
+    PEL_FIN_UNI_W(v0, v1, v2, v3, v4, v5, o##i)
+
+PEL_FUNC(8, __m64, pi, 64)
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < height; y++) {
+        v1 = *(__m64*)src;
+        v0 = _mm_slli_pi16(_mm_unpacklo_pi8(v1, vzero), 14 - BIT_DEPTH);
+        v1 = _mm_slli_pi16(_mm_unpackhi_pi8(v1, vzero), 14 - BIT_DEPTH);
+        PEL_FIN_UNI_W8(v0, v1, v2, v3, v4, v5, v0)
+        *(__m64*)dst = v0;
+        src += srcstride;
+        dst += dststride;
+    }
+}
+
+PEL_FUNC(16, __m128i, epi, 128)
+    __m128i o0;
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < height; y++) {
+        PEL_LD16(0)
+        VEC_ST(dst, o0);
+        src += srcstride;
+        dst += dststride;
+    }
+}
+
+PEL_FUNC(32, __m128i, epi, 128)
+    __m128i o0, o1;
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < height; y++) {
+        PEL_LD16(0) PEL_LD16(1)
+        VEC_ST(dst, o0);
+        VEC_ST(dst + 16, o1);
+        src += srcstride;
+        dst += dststride;
+    }
+}
+
+PEL_FUNC(64, __m128i, epi, 128)
+    __m128i o0, o1, o2, o3;
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < height; y++) {
+        PEL_LD16(0) PEL_LD16(1) PEL_LD16(2) PEL_LD16(3)
+        VEC_ST(dst, o0);
+        VEC_ST(dst + 16, o1);
+        VEC_ST(dst + 32, o2);
+        VEC_ST(dst + 48, o3);
+        src += srcstride;
+        dst += dststride;
+    }
+}
+
+#undef PEL_FUNC
+#undef PEL_LD16
+
+#define EPEL_INIT_FILTER(m) \
+    filter = ff_hevc_epel_filters[m - 1]; \
+    f0 = _mm_set1_epi16(*(int16_t*)&filter[0]); \
+    f1 = _mm_set1_epi16(*(int16_t*)&filter[2]);
+
+#define QPEL_INIT_FILTER(m) \
+    filter = ff_hevc_qpel_filters[m - 1]; \
+    f0 = _mm_set1_epi16(*(int16_t*)&filter[0]); \
+    f1 = _mm_set1_epi16(*(int16_t*)&filter[2]); \
+    f2 = _mm_set1_epi16(*(int16_t*)&filter[4]); \
+    f3 = _mm_set1_epi16(*(int16_t*)&filter[6]);
+
+#define PEL_A4_0 \
+    v4 = _mm_alignr_pi8(v1, v0, 1); \
+    v6 = _mm_maddubs_pi16(v4, f0);
+
+#define PEL_A4(i, o0) \
+    v4 = _mm_alignr_pi8(v1, v0, (i * 4 + 1) & 7); \
+    o0 = _mm_add_pi16(v6, _mm_maddubs_pi16(v4, f##i));
+
+#define PEL_A8_0 \
+    v4 = _mm_alignr_pi8(v1, v0, 1); \
+    v5 = _mm_alignr_pi8(v2, v1, 1); \
+    v6 = _mm_maddubs_pi16(v4, f0); \
+    v7 = _mm_maddubs_pi16(v5, f0);
+
+#define PEL_A8(i, o0, o1) \
+    v4 = _mm_alignr_pi8(v1, v0, (i * 4 + 1) & 7); \
+    v5 = _mm_alignr_pi8(v2, v1, (i * 4 + 1) & 7); \
+    o0 = _mm_add_pi16(v6, _mm_maddubs_pi16(v4, f##i)); \
+    o1 = _mm_add_pi16(v7, _mm_maddubs_pi16(v5, f##i));
+
+#define PEL_A16_0 \
+    v4 = _mm_alignr_epi8(v3, v2, 1); \
+    v5 = _mm_alignr_epi8(v0, v3, 1); \
+    v6 = _mm_maddubs_epi16(v4, f0); \
+    v7 = _mm_maddubs_epi16(v5, f0);
+
+#define PEL_A16(i, o0, o1) \
+    v4 = _mm_alignr_epi8(v3, v2, i * 4 + 1); \
+    v5 = _mm_alignr_epi8(v0, v3, i * 4 + 1); \
+    o0 = _mm_add_epi16(v6, _mm_maddubs_epi16(v4, f##i)); \
+    o1 = _mm_add_epi16(v7, _mm_maddubs_epi16(v5, f##i));
+
+#define EPEL_LD8 \
+    filter = ff_hevc_epel_filters[mx - 1]; \
+    v0 = _mm_cvtsi32_si64(*(uint32_t*)filter); \
+    f0 = _mm_shuffle_pi16(v0, 0x00); \
+    f1 = _mm_shuffle_pi16(v0, 0x55); \
+    PRAGMA_E2K("ivdep") \
+    for (y = 0; y < height; y++) { \
+        v1 = *(__m64*)(src - 1); \
+        v3 = _mm_cvtsi32_si64(*(uint32_t*)(src + 6)); \
+        v3 = _mm_srli_si64(v3, 8); \
+        v0 = _mm_unpacklo_pi8(v1, v1); \
+        v1 = _mm_unpackhi_pi8(v1, v1); \
+        v2 = _mm_unpacklo_pi8(v3, v3); \
+        PEL_A8_0 \
+        PEL_A8(1, v6, v7)
+
+#define EPEL_LD16 \
+    EPEL_INIT_FILTER(mx) \
+    PRAGMA_E2K("ivdep") \
+    for (y = 0; y < height; y++) { \
+        v3 = VEC_LD(src - 1); \
+        v1 = _mm_cvtsi32_si128(*(uint32_t*)(src - 1 + 1 * 16 - 1)); \
+        v1 = _mm_bsrli_si128(v1, 1); \
+        v2 = _mm_unpacklo_epi8(v3, v3); \
+        v3 = _mm_unpackhi_epi8(v3, v3); \
+        v0 = _mm_unpacklo_epi8(v1, v1); \
+        PEL_X16(0)
+
+#define EPEL_LD32 \
+    EPEL_INIT_FILTER(mx) \
+    PRAGMA_E2K("ivdep") \
+    for (y = 0; y < height; y++) { \
+        v3 = VEC_LD(src - 1); \
+        v1 = VEC_LD(src - 1 + 1 * 16); \
+        v2 = _mm_unpacklo_epi8(v3, v3); \
+        v3 = _mm_unpackhi_epi8(v3, v3); \
+        v0 = _mm_unpacklo_epi8(v1, v1); \
+        v1 = _mm_unpackhi_epi8(v1, v1); \
+        PEL_X16(0) \
+        v2 = v0; v3 = v1; \
+        v1 = _mm_cvtsi32_si128(*(uint32_t*)(src - 1 + 2 * 16 - 1)); \
+        v1 = _mm_bsrli_si128(v1, 1); \
+        v0 = _mm_unpacklo_epi8(v1, v1); \
+        PEL_X16(1)
+
+#define EPEL_LD64 \
+    EPEL_INIT_FILTER(mx) \
+    PRAGMA_E2K("ivdep") \
+    for (y = 0; y < height; y++) { \
+        v3 = VEC_LD(src - 1); \
+        v1 = VEC_LD(src - 1 + 1 * 16); \
+        v2 = _mm_unpacklo_epi8(v3, v3); \
+        v3 = _mm_unpackhi_epi8(v3, v3); \
+        v0 = _mm_unpacklo_epi8(v1, v1); \
+        v1 = _mm_unpackhi_epi8(v1, v1); \
+        PEL_X16(0) \
+        v2 = v0; v3 = v1; \
+        v1 = VEC_LD(src - 1 + 2 * 16); \
+        v0 = _mm_unpacklo_epi8(v1, v1); \
+        v1 = _mm_unpackhi_epi8(v1, v1); \
+        PEL_X16(1) \
+        v2 = v0; v3 = v1; \
+        v1 = VEC_LD(src - 1 + 3 * 16); \
+        v0 = _mm_unpacklo_epi8(v1, v1); \
+        v1 = _mm_unpackhi_epi8(v1, v1); \
+        PEL_X16(2) \
+        v2 = v0; v3 = v1; \
+        v1 = _mm_cvtsi32_si128(*(uint32_t*)(src - 1 + 4 * 16 - 1)); \
+        v1 = _mm_bsrli_si128(v1, 1); \
+        v0 = _mm_unpacklo_epi8(v1, v1); \
+        PEL_X16(3)
+
+#define QPEL_LD8 \
+    filter = ff_hevc_qpel_filters[mx - 1]; \
+    v0 = *(__m64*)filter; \
+    f0 = _mm_shuffle_pi16(v0, 0x00); \
+    f1 = _mm_shuffle_pi16(v0, 0x55); \
+    f2 = _mm_shuffle_pi16(v0, 0xaa); \
+    f3 = _mm_shuffle_pi16(v0, 0xff); \
+    PRAGMA_E2K("ivdep") \
+    for (y = 0; y < height; y++) { \
+        v1 = *(__m64*)(src - 3); \
+        v3 = *(__m64*)(src + 4); \
+        v3 = _mm_srli_si64(v3, 8); \
+        v0 = _mm_unpacklo_pi8(v1, v1); \
+        v1 = _mm_unpackhi_pi8(v1, v1); \
+        v2 = _mm_unpacklo_pi8(v3, v3); \
+        v3 = _mm_unpackhi_pi8(v3, v3); \
+        PEL_A8_0 \
+        PEL_A8(1, v6, v7) \
+        v0 = v1; v1 = v2; v2 = v3; \
+        PEL_A8(2, v6, v7) \
+        PEL_A8(3, v6, v7)
+
+#define QPEL_LD16 \
+    QPEL_INIT_FILTER(mx) \
+    PRAGMA_E2K("ivdep") \
+    for (y = 0; y < height; y++) { \
+        v3 = VEC_LD(src - 3); \
+        v1 = VEC_LD8(src - 3 + 1 * 16 - 1); \
+        v1 = _mm_bsrli_si128(v1, 1); \
+        v2 = _mm_unpacklo_epi8(v3, v3); \
+        v3 = _mm_unpackhi_epi8(v3, v3); \
+        v0 = _mm_unpacklo_epi8(v1, v1); \
+        PEL_X16(0)
+
+#define QPEL_LD32 \
+    QPEL_INIT_FILTER(mx) \
+    PRAGMA_E2K("ivdep") \
+    for (y = 0; y < height; y++) { \
+        v3 = VEC_LD(src - 3); \
+        v1 = VEC_LD(src - 3 + 1 * 16); \
+        v2 = _mm_unpacklo_epi8(v3, v3); \
+        v3 = _mm_unpackhi_epi8(v3, v3); \
+        v0 = _mm_unpacklo_epi8(v1, v1); \
+        v1 = _mm_unpackhi_epi8(v1, v1); \
+        PEL_X16(0) \
+        v2 = v0; v3 = v1; \
+        v1 = VEC_LD8(src - 3 + 2 * 16 - 1); \
+        v1 = _mm_bsrli_si128(v1, 1); \
+        v0 = _mm_unpacklo_epi8(v1, v1); \
+        PEL_X16(1)
+
+#define QPEL_LD64 \
+    QPEL_INIT_FILTER(mx) \
+    PRAGMA_E2K("ivdep") \
+    for (y = 0; y < height; y++) { \
+        v3 = VEC_LD(src - 3); \
+        v1 = VEC_LD(src - 3 + 1 * 16); \
+        v2 = _mm_unpacklo_epi8(v3, v3); \
+        v3 = _mm_unpackhi_epi8(v3, v3); \
+        v0 = _mm_unpacklo_epi8(v1, v1); \
+        v1 = _mm_unpackhi_epi8(v1, v1); \
+        PEL_X16(0) \
+        v2 = v0; v3 = v1; \
+        v1 = VEC_LD(src - 3 + 2 * 16); \
+        v0 = _mm_unpacklo_epi8(v1, v1); \
+        v1 = _mm_unpackhi_epi8(v1, v1); \
+        PEL_X16(1) \
+        v2 = v0; v3 = v1; \
+        v1 = VEC_LD(src - 3 + 3 * 16); \
+        v0 = _mm_unpacklo_epi8(v1, v1); \
+        v1 = _mm_unpackhi_epi8(v1, v1); \
+        PEL_X16(2) \
+        v2 = v0; v3 = v1; \
+        v1 = VEC_LD8(src - 3 + 4 * 16 - 1); \
+        v1 = _mm_bsrli_si128(v1, 1); \
+        v0 = _mm_unpacklo_epi8(v1, v1); \
+        PEL_X16(3)
+
+#define PEL_FUNC(w, vec) PEL_FUNC_TMP(epel_h##w) { \
+    int y; const int8_t *filter; \
+    vec v0, v1, v2, v3, v4, v5, v6, v7; \
+    vec f0, f1;
+
+#define PEL_X16(i) \
+    PEL_A16_0 \
+    PEL_A16(1, l##i, h##i)
+
+PEL_FUNC(4, __m64)
+    filter = ff_hevc_epel_filters[mx - 1];
+    v0 = _mm_cvtsi32_si64(*(uint32_t*)filter);
+    f0 = _mm_shuffle_pi16(v0, 0x00);
+    f1 = _mm_shuffle_pi16(v0, 0x55);
+    (void)v2; (void)v3; (void)v5; (void)v7;
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < height; y++) {
+        v0 = _mm_cvtsi32_si64(*(uint32_t*)(src - 1));
+        v1 = _mm_cvtsi32_si64(*(uint32_t*)(src + 2));
+        v1 = _mm_srli_si64(v1, 8);
+        v0 = _mm_unpacklo_pi8(v0, v0);
+        v1 = _mm_unpacklo_pi8(v1, v1);
+        PEL_A4_0
+        PEL_A4(1, v6)
+        *(__m64*)dst = v6;
+        src += srcstride;
+        dst += MAX_PB_SIZE;
+    }
+}
+
+PEL_FUNC(8, __m64)
+    EPEL_LD8
+        *(__m64*)dst = v6; *(__m64*)(dst + 4) = v7;
+        src += srcstride;
+        dst += MAX_PB_SIZE;
+    }
+}
+
+PEL_FUNC(16, __m128i)
+    __m128i l0, h0;
+    EPEL_LD16
+        VEC_ST(dst, l0); VEC_ST(dst + 8, h0);
+        src += srcstride;
+        dst += MAX_PB_SIZE;
+    }
+}
+
+PEL_FUNC(32, __m128i)
+    __m128i l0, h0, l1, h1;
+    EPEL_LD32
+        VEC_ST(dst, l0); VEC_ST(dst + 8, h0);
+        VEC_ST(dst + 16, l1); VEC_ST(dst + 24, h1);
+        src += srcstride;
+        dst += MAX_PB_SIZE;
+    }
+}
+
+PEL_FUNC(64, __m128i)
+    __m128i l0, h0, l1, h1, l2, h2, l3, h3;
+    EPEL_LD64
+        VEC_ST(dst, l0); VEC_ST(dst + 8, h0);
+        VEC_ST(dst + 16, l1); VEC_ST(dst + 24, h1);
+        VEC_ST(dst + 32, l2); VEC_ST(dst + 40, h2);
+        VEC_ST(dst + 48, l3); VEC_ST(dst + 56, h3);
+        src += srcstride;
+        dst += MAX_PB_SIZE;
+    }
+}
+
+#undef PEL_FUNC
+#undef PEL_X16
+
+#define PEL_FUNC(w, vec) PEL_FUNC_TMP(qpel_h##w) { \
+    int y; const int8_t *filter; \
+    vec v0, v1, v2, v3, v4, v5, v6, v7; \
+    vec f0, f1, f2, f3;
+
+#define PEL_X16(i) \
+    PEL_A16_0 \
+    PEL_A16(1, v6, v7) \
+    PEL_A16(2, v6, v7) \
+    PEL_A16(3, l##i, h##i)
+
+PEL_FUNC(8, __m64)
+    QPEL_LD8
+        *(__m64*)dst = v6; *(__m64*)(dst + 4) = v7;
+        src += srcstride;
+        dst += MAX_PB_SIZE;
+    }
+}
+
+PEL_FUNC(16, __m128i)
+    __m128i l0, h0;
+    QPEL_LD16
+        VEC_ST(dst, l0); VEC_ST(dst + 8, h0);
+        src += srcstride;
+        dst += MAX_PB_SIZE;
+    }
+}
+
+PEL_FUNC(32, __m128i)
+    __m128i l0, h0, l1, h1;
+    QPEL_LD32
+        VEC_ST(dst, l0); VEC_ST(dst + 8, h0);
+        VEC_ST(dst + 16, l1); VEC_ST(dst + 24, h1);
+        src += srcstride;
+        dst += MAX_PB_SIZE;
+    }
+}
+
+PEL_FUNC(64, __m128i)
+    __m128i l0, h0, l1, h1, l2, h2, l3, h3;
+    QPEL_LD64
+        VEC_ST(dst, l0); VEC_ST(dst + 8, h0);
+        VEC_ST(dst + 16, l1); VEC_ST(dst + 24, h1);
+        VEC_ST(dst + 32, l2); VEC_ST(dst + 40, h2);
+        VEC_ST(dst + 48, l3); VEC_ST(dst + 56, h3);
+        src += srcstride;
+        dst += MAX_PB_SIZE;
+    }
+}
+
+#undef PEL_FUNC
+#undef PEL_X16
+
+#define PEL_FUNC(w, vec, pi) PEL_FUNC_UNI(epel_uni_h##w) { \
+    int y, shift = 14 - BIT_DEPTH; \
+    const int8_t *filter; \
+    vec v0, v1, v2, v3, v4, v5, v6, v7; \
+    vec f0, f1, vbias = _mm_set1_##pi##16(1 << (shift - 1));
+
+#define PEL_X16(i) \
+    PEL_A16_0 \
+    PEL_A16(1, v6, v7) \
+    PEL_FIN_UNI(v6, v7, o##i)
+
+PEL_FUNC(8, __m64, pi)
+    EPEL_LD8
+        PEL_FIN_UNI8(v6, v7, v6)
+        *(__m64*)dst = v6;
+        src += srcstride;
+        dst += dststride;
+    }
+}
+
+PEL_FUNC(16, __m128i, epi)
+    __m128i o0;
+    EPEL_LD16
+        VEC_ST(dst, o0);
+        src += srcstride;
+        dst += dststride;
+    }
+}
+
+PEL_FUNC(32, __m128i, epi)
+    __m128i o0, o1;
+    EPEL_LD32
+        VEC_ST(dst, o0);
+        VEC_ST(dst + 16, o1);
+        src += srcstride;
+        dst += dststride;
+    }
+}
+
+PEL_FUNC(64, __m128i, epi)
+    __m128i o0, o1, o2, o3;
+    EPEL_LD64
+        VEC_ST(dst, o0);
+        VEC_ST(dst + 16, o1);
+        VEC_ST(dst + 32, o2);
+        VEC_ST(dst + 48, o3);
+        src += srcstride;
+        dst += dststride;
+    }
+}
+
+#undef PEL_FUNC
+#undef PEL_X16
+
+#define PEL_FUNC(w, vec, pi) PEL_FUNC_UNI(qpel_uni_h##w) { \
+    int y, shift = 14 - BIT_DEPTH; \
+    const int8_t *filter; \
+    vec v0, v1, v2, v3, v4, v5, v6, v7; \
+    vec f0, f1, f2, f3, vbias = _mm_set1_##pi##16(1 << (shift - 1));
+
+#define PEL_X16(i) \
+    PEL_A16_0 \
+    PEL_A16(1, v6, v7) \
+    PEL_A16(2, v6, v7) \
+    PEL_A16(3, v6, v7) \
+    PEL_FIN_UNI(v6, v7, o##i)
+
+PEL_FUNC(8, __m64, pi)
+    QPEL_LD8
+        PEL_FIN_UNI8(v6, v7, v6)
+        *(__m64*)dst = v6;
+        src += srcstride;
+        dst += dststride;
+    }
+}
+
+PEL_FUNC(16, __m128i, epi)
+    __m128i o0;
+    QPEL_LD16
+        VEC_ST(dst, o0);
+        src += srcstride;
+        dst += dststride;
+    }
+}
+
+PEL_FUNC(32, __m128i, epi)
+    __m128i o0, o1;
+    QPEL_LD32
+        VEC_ST(dst, o0);
+        VEC_ST(dst + 16, o1);
+        src += srcstride;
+        dst += dststride;
+    }
+}
+
+PEL_FUNC(64, __m128i, epi)
+    __m128i o0, o1, o2, o3;
+    QPEL_LD64
+        VEC_ST(dst, o0);
+        VEC_ST(dst + 16, o1);
+        VEC_ST(dst + 32, o2);
+        VEC_ST(dst + 48, o3);
+        src += srcstride;
+        dst += dststride;
+    }
+}
+
+#undef PEL_FUNC
+#undef PEL_X16
+
+#define PEL_FUNC(w) PEL_FUNC_BI(epel_bi_h##w) { \
+    int y, shift = 14 + 1 - BIT_DEPTH; \
+    const int8_t *filter; \
+    __m128i v0, v1, v2, v3, v4, v5, v6, v7; \
+    __m128i f0, f1, vbias = _mm_set1_epi16(1 << (shift - 1));
+
+#define PEL_X16(i) \
+    PEL_A16_0 \
+    PEL_A16(1, v6, v7) \
+    PEL_FIN_BI(src2 + i * 16, 8, v6, v7, v2, v3, o##i)
+
+PEL_FUNC(16)
+    __m128i o0;
+    EPEL_LD16
+        VEC_ST(dst, o0);
+        src += srcstride;
+        dst += dststride;
+        src2 += MAX_PB_SIZE;
+    }
+}
+
+PEL_FUNC(32)
+    __m128i o0, o1;
+    EPEL_LD32
+        VEC_ST(dst, o0);
+        VEC_ST(dst + 16, o1);
+        src += srcstride;
+        dst += dststride;
+        src2 += MAX_PB_SIZE;
+    }
+}
+
+PEL_FUNC(64)
+    __m128i o0, o1, o2, o3;
+    EPEL_LD64
+        VEC_ST(dst, o0);
+        VEC_ST(dst + 16, o1);
+        VEC_ST(dst + 32, o2);
+        VEC_ST(dst + 48, o3);
+        src += srcstride;
+        dst += dststride;
+        src2 += MAX_PB_SIZE;
+    }
+}
+
+#undef PEL_FUNC
+#undef PEL_X16
+
+#define PEL_FUNC(w) PEL_FUNC_BI(qpel_bi_h##w) { \
+    int y, shift = 14 + 1 - BIT_DEPTH; \
+    const int8_t *filter; \
+    __m128i v0, v1, v2, v3, v4, v5, v6, v7; \
+    __m128i f0, f1, f2, f3, vbias = _mm_set1_epi16(1 << (shift - 1));
+
+#define PEL_X16(i) \
+    PEL_A16_0 \
+    PEL_A16(1, v6, v7) \
+    PEL_A16(2, v6, v7) \
+    PEL_A16(3, v6, v7) \
+    PEL_FIN_BI(src2 + i * 16, 8, v6, v7, v2, v3, o##i)
+
+PEL_FUNC(16)
+    __m128i o0;
+    QPEL_LD16
+        VEC_ST(dst, o0);
+        src += srcstride;
+        dst += dststride;
+        src2 += MAX_PB_SIZE;
+    }
+}
+
+PEL_FUNC(32)
+    __m128i o0, o1;
+    QPEL_LD32
+        VEC_ST(dst, o0);
+        VEC_ST(dst + 16, o1);
+        src += srcstride;
+        dst += dststride;
+        src2 += MAX_PB_SIZE;
+    }
+}
+
+PEL_FUNC(64)
+    __m128i o0, o1, o2, o3;
+    QPEL_LD64
+        VEC_ST(dst, o0);
+        VEC_ST(dst + 16, o1);
+        VEC_ST(dst + 32, o2);
+        VEC_ST(dst + 48, o3);
+        src += srcstride;
+        dst += dststride;
+        src2 += MAX_PB_SIZE;
+    }
+}
+
+#undef PEL_FUNC
+#undef PEL_X16
+
+#define PEL_FUNC(w, vec, pi) PEL_FUNC_UNI_W(epel_uni_w_h##w) { \
+    int y, shift = denom + 14 - BIT_DEPTH; \
+    const int8_t *filter; \
+    vec v0, v1, v2, v3, v4, v5, v6, v7; \
+    vec f0, f1, vbias = _mm_set1_##pi##32(1 << (shift - 1)); \
+    vec vwx = _mm_set1_##pi##16(wx), vox = _mm_set1_##pi##16(ox);
+
+#define PEL_X16(i) \
+    PEL_A16_0 \
+    PEL_A16(1, v6, v7) \
+    PEL_FIN_UNI_W(v6, v7, v2, v3, v4, v5, o##i)
+
+PEL_FUNC(8, __m64, pi)
+    EPEL_LD8
+        PEL_FIN_UNI_W8(v6, v7, v2, v3, v4, v5, v6)
+        *(__m64*)dst = v6;
+        src += srcstride;
+        dst += dststride;
+    }
+}
+
+PEL_FUNC(16, __m128i, epi)
+    __m128i o0;
+    EPEL_LD16
+        VEC_ST(dst, o0);
+        src += srcstride;
+        dst += dststride;
+    }
+}
+
+PEL_FUNC(32, __m128i, epi)
+    __m128i o0, o1;
+    EPEL_LD32
+        VEC_ST(dst, o0);
+        VEC_ST(dst + 16, o1);
+        src += srcstride;
+        dst += dststride;
+    }
+}
+
+PEL_FUNC(64, __m128i, epi)
+    __m128i o0, o1, o2, o3;
+    EPEL_LD64
+        VEC_ST(dst, o0);
+        VEC_ST(dst + 16, o1);
+        VEC_ST(dst + 32, o2);
+        VEC_ST(dst + 48, o3);
+        src += srcstride;
+        dst += dststride;
+    }
+}
+
+#undef PEL_FUNC
+#undef PEL_X16
+
+#define PEL_FUNC(w, vec, pi) PEL_FUNC_UNI_W(qpel_uni_w_h##w) { \
+    int y, shift = denom + 14 - BIT_DEPTH; \
+    const int8_t *filter; \
+    vec v0, v1, v2, v3, v4, v5, v6, v7; \
+    vec f0, f1, f2, f3, vbias = _mm_set1_##pi##32(1 << (shift - 1)); \
+    vec vwx = _mm_set1_##pi##16(wx), vox = _mm_set1_##pi##16(ox);
+
+#define PEL_X16(i) \
+    PEL_A16_0 \
+    PEL_A16(1, v6, v7) \
+    PEL_A16(2, v6, v7) \
+    PEL_A16(3, v6, v7) \
+    PEL_FIN_UNI_W(v6, v7, v2, v3, v4, v5, o##i)
+
+PEL_FUNC(8, __m64, pi)
+    QPEL_LD8
+        PEL_FIN_UNI_W8(v6, v7, v2, v3, v4, v5, v6)
+        *(__m64*)dst = v6;
+        src += srcstride;
+        dst += dststride;
+    }
+}
+
+PEL_FUNC(16, __m128i, epi)
+    __m128i o0;
+    QPEL_LD16
+        VEC_ST(dst, o0);
+        src += srcstride;
+        dst += dststride;
+    }
+}
+
+PEL_FUNC(32, __m128i, epi)
+    __m128i o0, o1;
+    QPEL_LD32
+        VEC_ST(dst, o0);
+        VEC_ST(dst + 16, o1);
+        src += srcstride;
+        dst += dststride;
+    }
+}
+
+PEL_FUNC(64, __m128i, epi)
+    __m128i o0, o1, o2, o3;
+    QPEL_LD64
+        VEC_ST(dst, o0);
+        VEC_ST(dst + 16, o1);
+        VEC_ST(dst + 32, o2);
+        VEC_ST(dst + 48, o3);
+        src += srcstride;
+        dst += dststride;
+    }
+}
+
+#undef PEL_FUNC
+#undef PEL_X16
+
+#undef PEL_A4_0
+#undef PEL_A4
+#undef PEL_A8_0
+#undef PEL_A8
+#undef PEL_A16_0
+#undef PEL_A16
+#undef EPEL_LD8
+#undef EPEL_LD16
+#undef EPEL_LD32
+#undef EPEL_LD64
+#undef QPEL_LD8
+#undef QPEL_LD16
+#undef QPEL_LD32
+#undef QPEL_LD64
+
+#define EPEL_V_INIT \
+    int x, y; const int8_t *filter; \
+    __m128i a0, a1, a2, a3; \
+    __m128i b0, b1, b2, b3; \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    __m128i f0, f1; \
+    EPEL_INIT_FILTER(my)
+
+#define EPEL_V \
+    v1 = VEC_LD(src1 - srcstride); \
+    v2 = VEC_LD(src1); \
+    v0 = VEC_LD(src1 + srcstride); \
+    a0 = _mm_unpacklo_epi8(v1, v2); \
+    a1 = _mm_unpackhi_epi8(v1, v2); \
+    b0 = _mm_unpacklo_epi8(v2, v0); \
+    b1 = _mm_unpackhi_epi8(v2, v0); \
+    PRAGMA_E2K("ivdep") \
+    for (y = 0; y < height; y += 2) { \
+        v1 = VEC_LD(src1 + 2 * srcstride); \
+        v2 = VEC_LD(src1 + 3 * srcstride); \
+        src1 += srcstride << 1; \
+        a2 = _mm_unpacklo_epi8(v0, v1); \
+        a3 = _mm_unpackhi_epi8(v0, v1); \
+        b2 = _mm_unpacklo_epi8(v1, v2); \
+        b3 = _mm_unpackhi_epi8(v1, v2); \
+        v0 = v2; \
+        v2 = _mm_maddubs_epi16(a0, f0); \
+        v3 = _mm_maddubs_epi16(a1, f0); \
+        v4 = _mm_maddubs_epi16(b0, f0); \
+        v5 = _mm_maddubs_epi16(b1, f0); \
+        v2 = _mm_add_epi16(v2, _mm_maddubs_epi16(a2, f1)); \
+        v3 = _mm_add_epi16(v3, _mm_maddubs_epi16(a3, f1)); \
+        v4 = _mm_add_epi16(v4, _mm_maddubs_epi16(b2, f1)); \
+        v5 = _mm_add_epi16(v5, _mm_maddubs_epi16(b3, f1)); \
+        a0 = a2; a1 = a3; \
+        b0 = b2; b1 = b3;
+
+#define QPEL_V_INIT \
+    int x, y; const int8_t *filter; \
+    __m128i a0, a1, a2, a3, a4, a5, a6, a7; \
+    __m128i b0, b1, b2, b3, b4, b5, b6, b7; \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    __m128i f0, f1, f2, f3; \
+    QPEL_INIT_FILTER(my)
+
+#define QPEL_V \
+    v0 = VEC_LD(src1 - 3 * srcstride); \
+    v1 = VEC_LD(src1 - 2 * srcstride); \
+    v2 = VEC_LD(src1 - srcstride); \
+    a0 = _mm_unpacklo_epi8(v0, v1); \
+    a1 = _mm_unpackhi_epi8(v0, v1); \
+    v0 = VEC_LD(src1); \
+    b0 = _mm_unpacklo_epi8(v1, v2); \
+    b1 = _mm_unpackhi_epi8(v1, v2); \
+    v1 = VEC_LD(src1 + srcstride); \
+    a2 = _mm_unpacklo_epi8(v2, v0); \
+    a3 = _mm_unpackhi_epi8(v2, v0); \
+    v2 = VEC_LD(src1 + 2 * srcstride); \
+    b2 = _mm_unpacklo_epi8(v0, v1); \
+    b3 = _mm_unpackhi_epi8(v0, v1); \
+    v0 = VEC_LD(src1 + 3 * srcstride); \
+    a4 = _mm_unpacklo_epi8(v1, v2); \
+    a5 = _mm_unpackhi_epi8(v1, v2); \
+    b4 = _mm_unpacklo_epi8(v2, v0); \
+    b5 = _mm_unpackhi_epi8(v2, v0); \
+    PRAGMA_E2K("ivdep") \
+    for (y = 0; y < height; y += 2) { \
+        v1 = VEC_LD(src1 + 4 * srcstride); \
+        v2 = VEC_LD(src1 + 5 * srcstride); \
+        src1 += srcstride << 1; \
+        a6 = _mm_unpacklo_epi8(v0, v1); \
+        a7 = _mm_unpackhi_epi8(v0, v1); \
+        b6 = _mm_unpacklo_epi8(v1, v2); \
+        b7 = _mm_unpackhi_epi8(v1, v2); \
+        v0 = v2; \
+        v2 = _mm_maddubs_epi16(a0, f0); \
+        v3 = _mm_maddubs_epi16(a1, f0); \
+        v4 = _mm_maddubs_epi16(b0, f0); \
+        v5 = _mm_maddubs_epi16(b1, f0); \
+        v2 = _mm_add_epi16(v2, _mm_maddubs_epi16(a2, f1)); \
+        v3 = _mm_add_epi16(v3, _mm_maddubs_epi16(a3, f1)); \
+        v4 = _mm_add_epi16(v4, _mm_maddubs_epi16(b2, f1)); \
+        v5 = _mm_add_epi16(v5, _mm_maddubs_epi16(b3, f1)); \
+        v2 = _mm_add_epi16(v2, _mm_maddubs_epi16(a4, f2)); \
+        v3 = _mm_add_epi16(v3, _mm_maddubs_epi16(a5, f2)); \
+        v4 = _mm_add_epi16(v4, _mm_maddubs_epi16(b4, f2)); \
+        v5 = _mm_add_epi16(v5, _mm_maddubs_epi16(b5, f2)); \
+        v2 = _mm_add_epi16(v2, _mm_maddubs_epi16(a6, f3)); \
+        v3 = _mm_add_epi16(v3, _mm_maddubs_epi16(a7, f3)); \
+        v4 = _mm_add_epi16(v4, _mm_maddubs_epi16(b6, f3)); \
+        v5 = _mm_add_epi16(v5, _mm_maddubs_epi16(b7, f3)); \
+        a0 = a2; a1 = a3; a2 = a4; a3 = a5; a4 = a6; a5 = a7; \
+        b0 = b2; b1 = b3; b2 = b4; b3 = b5; b4 = b6; b5 = b7;
+
+PEL_FUNC_TMP(epel_v) {
+    const uint8_t *src1; int16_t *dst1;
+    EPEL_V_INIT
+    for (x = 0; x < (width & -16); x += 16, src += 16, dst += 16) {
+        src1 = src; dst1 = dst;
+        EPEL_V
+            VEC_ST(dst1, v2); VEC_ST(dst1 + 8, v3);
+            VEC_ST(dst1 + MAX_PB_SIZE, v4);
+            VEC_ST(dst1 + MAX_PB_SIZE + 8, v5);
+            dst1 += MAX_PB_SIZE << 1;
+        }
+    }
+}
+
+PEL_FUNC_TMP(qpel_v) {
+    const uint8_t *src1; int16_t *dst1;
+    QPEL_V_INIT
+    for (x = 0; x < (width & -16); x += 16, src += 16, dst += 16) {
+        src1 = src; dst1 = dst;
+        QPEL_V
+            VEC_ST(dst1, v2); VEC_ST(dst1 + 8, v3);
+            VEC_ST(dst1 + MAX_PB_SIZE, v4);
+            VEC_ST(dst1 + MAX_PB_SIZE + 8, v5);
+            dst1 += MAX_PB_SIZE << 1;
+        }
+    }
+}
+
+PEL_FUNC_UNI(epel_uni_v) {
+    int shift = 14 - BIT_DEPTH;
+    __m128i vbias = _mm_set1_epi16(1 << (shift - 1));
+    const uint8_t *src1, *dst1;
+    EPEL_V_INIT
+    for (x = 0; x < (width & -16); x += 16, src += 16, dst += 16) {
+        src1 = src; dst1 = dst;
+        EPEL_V
+            PEL_FIN_UNI(v2, v3, v2)
+            PEL_FIN_UNI(v4, v5, v4)
+            VEC_ST(dst1, v2);
+            VEC_ST(dst1 + dststride, v4);
+            dst1 += dststride << 1;
+        }
+    }
+}
+
+PEL_FUNC_UNI(qpel_uni_v) {
+    int shift = 14 - BIT_DEPTH;
+    __m128i vbias = _mm_set1_epi16(1 << (shift - 1));
+    const uint8_t *src1, *dst1;
+    QPEL_V_INIT
+    for (x = 0; x < (width & -16); x += 16, src += 16, dst += 16) {
+        src1 = src; dst1 = dst;
+        QPEL_V
+            PEL_FIN_UNI(v2, v3, v2)
+            PEL_FIN_UNI(v4, v5, v4)
+            VEC_ST(dst1, v2);
+            VEC_ST(dst1 + dststride, v4);
+            dst1 += dststride << 1;
+        }
+    }
+}
+
+PEL_FUNC_BI(epel_bi_v) {
+    int shift = 14 + 1 - BIT_DEPTH;
+    __m128i vbias = _mm_set1_epi16(1 << (shift - 1));
+    const uint8_t *src1; uint8_t *dst1; const int16_t *src3;
+    EPEL_V_INIT
+    for (x = 0; x < (width & -16); x += 16, src += 16, dst += 16, src2 += 16) {
+        src1 = src; dst1 = dst; src3 = src2;
+        EPEL_V
+            PEL_FIN_BI(src3, 8, v2, v3, a2, a3, v2)
+            PEL_FIN_BI(src3 + MAX_PB_SIZE, 8, v4, v5, b2, b3, v4)
+            VEC_ST(dst1, v2);
+            VEC_ST(dst1 + dststride, v4);
+            dst1 += dststride << 1;
+            src3 += MAX_PB_SIZE << 1;
+        }
+    }
+}
+
+PEL_FUNC_BI(qpel_bi_v) {
+    int shift = 14 + 1 - BIT_DEPTH;
+    __m128i vbias = _mm_set1_epi16(1 << (shift - 1));
+    const uint8_t *src1; uint8_t *dst1; const int16_t *src3;
+    QPEL_V_INIT
+    for (x = 0; x < (width & -16); x += 16, src += 16, dst += 16, src2 += 16) {
+        src1 = src; dst1 = dst; src3 = src2;
+        QPEL_V
+            PEL_FIN_BI(src3, 8, v2, v3, a6, a7, v2)
+            PEL_FIN_BI(src3 + MAX_PB_SIZE, 8, v4, v5, b6, b7, v4)
+            VEC_ST(dst1, v2);
+            VEC_ST(dst1 + dststride, v4);
+            dst1 += dststride << 1;
+            src3 += MAX_PB_SIZE << 1;
+        }
+    }
+}
+
+PEL_FUNC_UNI_W(epel_uni_w_v) {
+    int shift = denom + 14 - BIT_DEPTH;
+    __m128i vbias = _mm_set1_epi32(1 << (shift - 1));
+    __m128i vwx = _mm_set1_epi16(wx), vox = _mm_set1_epi16(ox);
+    const uint8_t *src1; uint8_t *dst1;
+    EPEL_V_INIT
+    for (x = 0; x < (width & -16); x += 16, src += 16, dst += 16) {
+        src1 = src; dst1 = dst;
+        EPEL_V
+            PEL_FIN_UNI_W(v2, v3, a2, a3, b2, b3, v2)
+            PEL_FIN_UNI_W(v4, v5, a2, a3, b2, b3, v4)
+            VEC_ST(dst1, v2);
+            VEC_ST(dst1 + dststride, v4);
+            dst1 += dststride << 1;
+        }
+    }
+}
+
+PEL_FUNC_UNI_W(qpel_uni_w_v) {
+    int shift = denom + 14 - BIT_DEPTH;
+    __m128i vbias = _mm_set1_epi32(1 << (shift - 1));
+    __m128i vwx = _mm_set1_epi16(wx), vox = _mm_set1_epi16(ox);
+    const uint8_t *src1; uint8_t *dst1;
+    QPEL_V_INIT
+    for (x = 0; x < (width & -16); x += 16, src += 16, dst += 16) {
+        src1 = src; dst1 = dst;
+        QPEL_V
+            PEL_FIN_UNI_W(v2, v3, a6, a7, b6, b7, v2)
+            PEL_FIN_UNI_W(v4, v5, a6, a7, b6, b7, v4)
+            VEC_ST(dst1, v2);
+            VEC_ST(dst1 + dststride, v4);
+            dst1 += dststride << 1;
+        }
+    }
+}
+
+#undef EPEL_V_INIT
+#undef EPEL_V
+#undef QPEL_V_INIT
+#undef QPEL_V
+
+#define EPEL_HV4_INIT \
+    int y; const int8_t *filter; \
+    DECLARE_ALIGNED(16, int16_t, tmp_array)[(MAX_PB_SIZE + 3) * MAX_PB_SIZE]; \
+    int16_t *tmp = tmp_array; \
+    __m64 a0, a1, a2, a3; \
+    __m64 b0, b1, b2, b3; \
+    __m64 v0, v1, v2, v3, v4, v5; \
+    __m64 f0, f1; \
+    \
+    src -= srcstride; \
+    put_hevc_epel_h4_8_e2k(tmp, src, srcstride, height + 3, mx, my, width);\
+    \
+    filter = ff_hevc_epel_filters[my - 1]; \
+    v0 = _mm_cvtsi32_si64(*(uint32_t*)filter); \
+    v0 = _mm_srai_pi16(_mm_unpacklo_pi8(v0, v0), 8); \
+    f0 = _mm_unpacklo_pi32(v0, v0); \
+    f1 = _mm_unpackhi_pi32(v0, v0); \
+    \
+    tmp = tmp_array + MAX_PB_SIZE;
+
+#define EPEL_HV4 \
+    v1 = *(__m64*)(tmp - MAX_PB_SIZE); \
+    v2 = *(__m64*)(tmp); \
+    v0 = *(__m64*)(tmp + MAX_PB_SIZE); \
+    a0 = _mm_unpacklo_pi16(v1, v2); \
+    a1 = _mm_unpackhi_pi16(v1, v2); \
+    b0 = _mm_unpacklo_pi16(v2, v0); \
+    b1 = _mm_unpackhi_pi16(v2, v0); \
+    PRAGMA_E2K("ivdep") \
+    for (y = 0; y < height; y += 2) { \
+        v1 = *(__m64*)(tmp + 2 * MAX_PB_SIZE); \
+        v2 = *(__m64*)(tmp + 3 * MAX_PB_SIZE); \
+        tmp += MAX_PB_SIZE << 1; \
+        a2 = _mm_unpacklo_pi16(v0, v1); \
+        a3 = _mm_unpackhi_pi16(v0, v1); \
+        b2 = _mm_unpacklo_pi16(v1, v2); \
+        b3 = _mm_unpackhi_pi16(v1, v2); \
+        v0 = v2; \
+        v2 = _mm_madd_pi16(a0, f0); \
+        v3 = _mm_madd_pi16(a1, f0); \
+        v4 = _mm_madd_pi16(b0, f0); \
+        v5 = _mm_madd_pi16(b1, f0); \
+        v2 = _mm_add_pi32(v2, _mm_madd_pi16(a2, f1)); \
+        v3 = _mm_add_pi32(v3, _mm_madd_pi16(a3, f1)); \
+        v4 = _mm_add_pi32(v4, _mm_madd_pi16(b2, f1)); \
+        v5 = _mm_add_pi32(v5, _mm_madd_pi16(b3, f1)); \
+        a0 = a2; a1 = a3; \
+        b0 = b2; b1 = b3; \
+        v2 = _mm_packs_pi32(_mm_srai_pi32(v2, 6), _mm_srai_pi32(v3, 6)); \
+        v4 = _mm_packs_pi32(_mm_srai_pi32(v4, 6), _mm_srai_pi32(v5, 6));
+
+#define EPEL_HV_INIT \
+    int x, y; const int8_t *filter; \
+    DECLARE_ALIGNED(16, int16_t, tmp_array)[(MAX_PB_SIZE + 3) * MAX_PB_SIZE]; \
+    int16_t *tmp1, *tmp = tmp_array; \
+    __m128i a0, a1, a2, a3; \
+    __m128i b0, b1, b2, b3; \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    __m128i f0, f1; \
+    \
+    src -= srcstride; \
+    switch (width) { \
+        case 8: put_hevc_epel_h8_8_e2k(tmp, src, srcstride, height + 3, mx, my, width); break; \
+        case 16: put_hevc_epel_h16_8_e2k(tmp, src, srcstride, height + 3, mx, my, width); break; \
+        case 32: put_hevc_epel_h32_8_e2k(tmp, src, srcstride, height + 3, mx, my, width); break; \
+        default: put_hevc_epel_h64_8_e2k(tmp, src, srcstride, height + 3, mx, my, width);    \
+    } \
+    \
+    filter = ff_hevc_epel_filters[my - 1]; \
+    v0 = _mm_cvtsi32_si128(*(uint32_t*)filter); \
+    v0 = _mm_srai_epi16(_mm_unpacklo_epi8(v0, v0), 8); \
+    f0 = _mm_shuffle_epi32(v0, 0x00); \
+    f1 = _mm_shuffle_epi32(v0, 0x55); \
+    \
+    tmp = tmp_array + MAX_PB_SIZE;
+
+#define EPEL_HV \
+    v1 = VEC_LD(tmp1 - MAX_PB_SIZE); \
+    v2 = VEC_LD(tmp1); \
+    v0 = VEC_LD(tmp1 + MAX_PB_SIZE); \
+    a0 = _mm_unpacklo_epi16(v1, v2); \
+    a1 = _mm_unpackhi_epi16(v1, v2); \
+    b0 = _mm_unpacklo_epi16(v2, v0); \
+    b1 = _mm_unpackhi_epi16(v2, v0); \
+    PRAGMA_E2K("ivdep") \
+    for (y = 0; y < height; y += 2) { \
+        v1 = VEC_LD(tmp1 + 2 * MAX_PB_SIZE); \
+        v2 = VEC_LD(tmp1 + 3 * MAX_PB_SIZE); \
+        tmp1 += MAX_PB_SIZE << 1; \
+        a2 = _mm_unpacklo_epi16(v0, v1); \
+        a3 = _mm_unpackhi_epi16(v0, v1); \
+        b2 = _mm_unpacklo_epi16(v1, v2); \
+        b3 = _mm_unpackhi_epi16(v1, v2); \
+        v0 = v2; \
+        v2 = _mm_madd_epi16(a0, f0); \
+        v3 = _mm_madd_epi16(a1, f0); \
+        v4 = _mm_madd_epi16(b0, f0); \
+        v5 = _mm_madd_epi16(b1, f0); \
+        v2 = _mm_add_epi32(v2, _mm_madd_epi16(a2, f1)); \
+        v3 = _mm_add_epi32(v3, _mm_madd_epi16(a3, f1)); \
+        v4 = _mm_add_epi32(v4, _mm_madd_epi16(b2, f1)); \
+        v5 = _mm_add_epi32(v5, _mm_madd_epi16(b3, f1)); \
+        a0 = a2; a1 = a3; \
+        b0 = b2; b1 = b3; \
+        v2 = _mm_packs_epi32(_mm_srai_epi32(v2, 6), _mm_srai_epi32(v3, 6)); \
+        v4 = _mm_packs_epi32(_mm_srai_epi32(v4, 6), _mm_srai_epi32(v5, 6));
+
+#define QPEL_HV_INIT \
+    int x, y; const int8_t *filter; \
+    DECLARE_ALIGNED(16, int16_t, tmp_array)[(MAX_PB_SIZE + 7) * MAX_PB_SIZE]; \
+    int16_t *tmp1, *tmp = tmp_array; \
+    __m128i a0, a1, a2, a3, a4, a5, a6, a7; \
+    __m128i b0, b1, b2, b3, b4, b5, b6, b7; \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    __m128i f0, f1, f2, f3; \
+    \
+    src -= 3 * srcstride; \
+    switch (width) { \
+        case 8: put_hevc_qpel_h8_8_e2k(tmp, src, srcstride, height + 7, mx, my, width); break; \
+        case 16: put_hevc_qpel_h16_8_e2k(tmp, src, srcstride, height + 7, mx, my, width); break; \
+        case 32: put_hevc_qpel_h32_8_e2k(tmp, src, srcstride, height + 7, mx, my, width); break; \
+        default: put_hevc_qpel_h64_8_e2k(tmp, src, srcstride, height + 7, mx, my, width);    \
+    } \
+    \
+    filter = ff_hevc_qpel_filters[my - 1]; \
+    v0 = VEC_LD8(filter); \
+    v0 = _mm_srai_epi16(_mm_unpacklo_epi8(v0, v0), 8); \
+    f0 = _mm_shuffle_epi32(v0, 0x00); \
+    f1 = _mm_shuffle_epi32(v0, 0x55); \
+    f2 = _mm_shuffle_epi32(v0, 0xaa); \
+    f3 = _mm_shuffle_epi32(v0, 0xff); \
+    \
+    tmp = tmp_array + 3 * MAX_PB_SIZE;
+
+#define QPEL_HV \
+    v0 = VEC_LD(tmp1 - 3 * MAX_PB_SIZE); \
+    v1 = VEC_LD(tmp1 - 2 * MAX_PB_SIZE); \
+    v2 = VEC_LD(tmp1 - MAX_PB_SIZE); \
+    a0 = _mm_unpacklo_epi16(v0, v1); \
+    a1 = _mm_unpackhi_epi16(v0, v1); \
+    v0 = VEC_LD(tmp1); \
+    b0 = _mm_unpacklo_epi16(v1, v2); \
+    b1 = _mm_unpackhi_epi16(v1, v2); \
+    v1 = VEC_LD(tmp1 + MAX_PB_SIZE); \
+    a2 = _mm_unpacklo_epi16(v2, v0); \
+    a3 = _mm_unpackhi_epi16(v2, v0); \
+    v2 = VEC_LD(tmp1 + 2 * MAX_PB_SIZE); \
+    b2 = _mm_unpacklo_epi16(v0, v1); \
+    b3 = _mm_unpackhi_epi16(v0, v1); \
+    v0 = VEC_LD(tmp1 + 3 * MAX_PB_SIZE); \
+    a4 = _mm_unpacklo_epi16(v1, v2); \
+    a5 = _mm_unpackhi_epi16(v1, v2); \
+    b4 = _mm_unpacklo_epi16(v2, v0); \
+    b5 = _mm_unpackhi_epi16(v2, v0); \
+    PRAGMA_E2K("ivdep") \
+    for (y = 0; y < height; y += 2) { \
+        v1 = VEC_LD(tmp1 + 4 * MAX_PB_SIZE); \
+        v2 = VEC_LD(tmp1 + 5 * MAX_PB_SIZE); \
+        tmp1 += MAX_PB_SIZE << 1; \
+        a6 = _mm_unpacklo_epi16(v0, v1); \
+        a7 = _mm_unpackhi_epi16(v0, v1); \
+        b6 = _mm_unpacklo_epi16(v1, v2); \
+        b7 = _mm_unpackhi_epi16(v1, v2); \
+        v0 = v2; \
+        v2 = _mm_madd_epi16(a0, f0); \
+        v3 = _mm_madd_epi16(a1, f0); \
+        v4 = _mm_madd_epi16(b0, f0); \
+        v5 = _mm_madd_epi16(b1, f0); \
+        v2 = _mm_add_epi32(v2, _mm_madd_epi16(a2, f1)); \
+        v3 = _mm_add_epi32(v3, _mm_madd_epi16(a3, f1)); \
+        v4 = _mm_add_epi32(v4, _mm_madd_epi16(b2, f1)); \
+        v5 = _mm_add_epi32(v5, _mm_madd_epi16(b3, f1)); \
+        v2 = _mm_add_epi32(v2, _mm_madd_epi16(a4, f2)); \
+        v3 = _mm_add_epi32(v3, _mm_madd_epi16(a5, f2)); \
+        v4 = _mm_add_epi32(v4, _mm_madd_epi16(b4, f2)); \
+        v5 = _mm_add_epi32(v5, _mm_madd_epi16(b5, f2)); \
+        v2 = _mm_add_epi32(v2, _mm_madd_epi16(a6, f3)); \
+        v3 = _mm_add_epi32(v3, _mm_madd_epi16(a7, f3)); \
+        v4 = _mm_add_epi32(v4, _mm_madd_epi16(b6, f3)); \
+        v5 = _mm_add_epi32(v5, _mm_madd_epi16(b7, f3)); \
+        a0 = a2; a1 = a3; a2 = a4; a3 = a5; a4 = a6; a5 = a7; \
+        b0 = b2; b1 = b3; b2 = b4; b3 = b5; b4 = b6; b5 = b7; \
+        v2 = _mm_packs_epi32(_mm_srai_epi32(v2, 6), _mm_srai_epi32(v3, 6)); \
+        v4 = _mm_packs_epi32(_mm_srai_epi32(v4, 6), _mm_srai_epi32(v5, 6));
+
+PEL_FUNC_TMP(epel_hv) {
+    int16_t *dst1;
+    EPEL_HV_INIT
+    for (x = 0; x < (width & -8); x += 8, tmp += 8, dst += 8) {
+        tmp1 = tmp; dst1 = dst;
+        EPEL_HV
+            VEC_ST(dst1, v2);
+            VEC_ST(dst1 + MAX_PB_SIZE, v4);
+            dst1 += MAX_PB_SIZE << 1;
+        }
+    }
+}
+
+PEL_FUNC_TMP(qpel_hv) {
+    int16_t *dst1;
+    QPEL_HV_INIT
+    for (x = 0; x < (width & -8); x += 8, tmp += 8, dst += 8) {
+        tmp1 = tmp; dst1 = dst;
+        QPEL_HV
+            VEC_ST(dst1, v2);
+            VEC_ST(dst1 + MAX_PB_SIZE, v4);
+            dst1 += MAX_PB_SIZE << 1;
+        }
+    }
+}
+
+PEL_FUNC_UNI(epel_uni_hv4) {
+    int shift = 14 - BIT_DEPTH;
+    __m64 vbias = _mm_set1_pi16(1 << (shift - 1));
+    EPEL_HV4_INIT
+    EPEL_HV4
+        PEL_FIN_UNI8(v2, v4, v2)
+        *(uint32_t*)dst = _mm_cvtsi64_si32(v2);
+        *(uint32_t*)(dst + dststride) = _mm_extract_pi32(v2, 1);
+        dst += dststride << 1;
+    }
+}
+
+PEL_FUNC_UNI(epel_uni_hv) {
+    int shift = 14 - BIT_DEPTH; uint8_t *dst1;
+    __m128i vbias = _mm_set1_epi16(1 << (shift - 1));
+    EPEL_HV_INIT
+    for (x = 0; x < (width & -8); x += 8, tmp += 8, dst += 8) {
+        tmp1 = tmp; dst1 = dst;
+        EPEL_HV
+            PEL_FIN_UNI(v2, v4, v2)
+            VEC_STL(dst1, v2);
+            VEC_STH(dst1 + dststride, v2);
+            dst1 += dststride << 1;
+        }
+    }
+}
+
+PEL_FUNC_UNI(qpel_uni_hv) {
+    int shift = 14 - BIT_DEPTH; uint8_t *dst1;
+    __m128i vbias = _mm_set1_epi16(1 << (shift - 1));
+    QPEL_HV_INIT
+    for (x = 0; x < (width & -8); x += 8, tmp += 8, dst += 8) {
+        tmp1 = tmp; dst1 = dst;
+        QPEL_HV
+            PEL_FIN_UNI(v2, v4, v2)
+            VEC_STL(dst1, v2);
+            VEC_STH(dst1 + dststride, v2);
+            dst1 += dststride << 1;
+        }
+    }
+}
+
+PEL_FUNC_BI(epel_bi_hv) {
+    int shift = 14 + 1 - BIT_DEPTH; uint8_t *dst1; const int16_t *src3;
+    __m128i vbias = _mm_set1_epi16(1 << (shift - 1));
+    EPEL_HV_INIT
+    for (x = 0; x < (width & -8); x += 8, tmp += 8, dst += 8, src2 += 8) {
+        tmp1 = tmp; dst1 = dst; src3 = src2;
+        EPEL_HV
+            PEL_FIN_BI(src3, MAX_PB_SIZE, v2, v4, a2, a3, v2)
+            VEC_STL(dst1, v2);
+            VEC_STH(dst1 + dststride, v2);
+            dst1 += dststride << 1;
+            src3 += MAX_PB_SIZE << 1;
+        }
+    }
+}
+
+PEL_FUNC_BI(qpel_bi_hv) {
+    int shift = 14 + 1 - BIT_DEPTH; uint8_t *dst1; const int16_t *src3;
+    __m128i vbias = _mm_set1_epi16(1 << (shift - 1));
+    QPEL_HV_INIT
+    for (x = 0; x < (width & -8); x += 8, tmp += 8, dst += 8, src2 += 8) {
+        tmp1 = tmp; dst1 = dst; src3 = src2;
+        QPEL_HV
+            PEL_FIN_BI(src3, MAX_PB_SIZE, v2, v4, a6, a7, v2)
+            VEC_STL(dst1, v2);
+            VEC_STH(dst1 + dststride, v2);
+            dst1 += dststride << 1;
+            src3 += MAX_PB_SIZE << 1;
+        }
+    }
+}
+
+PEL_FUNC_UNI_W(epel_uni_w_hv4) {
+    int shift = denom + 14 - BIT_DEPTH;
+    __m64 vbias = _mm_set1_pi32(1 << (shift - 1));
+    __m64 vwx = _mm_set1_pi16(wx), vox = _mm_set1_pi16(ox);
+    EPEL_HV4_INIT
+    EPEL_HV4
+        PEL_FIN_UNI_W8(v2, v4, a2, a3, b2, b3, v2)
+        *(uint32_t*)dst = _mm_cvtsi64_si32(v2);
+        *(uint32_t*)(dst + dststride) = _mm_extract_pi32(v2, 1);
+        dst += dststride << 1;
+    }
+}
+
+PEL_FUNC_UNI_W(epel_uni_w_hv) {
+    int shift = denom + 14 - BIT_DEPTH; uint8_t *dst1;
+    __m128i vbias = _mm_set1_epi32(1 << (shift - 1));
+    __m128i vwx = _mm_set1_epi16(wx), vox = _mm_set1_epi16(ox);
+    EPEL_HV_INIT
+    for (x = 0; x < (width & -8); x += 8, tmp += 8, dst += 8) {
+        tmp1 = tmp; dst1 = dst;
+        EPEL_HV
+            PEL_FIN_UNI_W(v2, v4, a2, a3, b2, b3, v2)
+            VEC_STL(dst1, v2);
+            VEC_STH(dst1 + dststride, v2);
+            dst1 += dststride << 1;
+        }
+    }
+}
+
+PEL_FUNC_UNI_W(qpel_uni_w_hv) {
+    int shift = denom + 14 - BIT_DEPTH; uint8_t *dst1;
+    __m128i vbias = _mm_set1_epi32(1 << (shift - 1));
+    __m128i vwx = _mm_set1_epi16(wx), vox = _mm_set1_epi16(ox);
+    QPEL_HV_INIT
+    for (x = 0; x < (width & -8); x += 8, tmp += 8, dst += 8) {
+        tmp1 = tmp; dst1 = dst;
+        QPEL_HV
+            PEL_FIN_UNI_W(v2, v4, a6, a7, b6, b7, v2)
+            VEC_STL(dst1, v2);
+            VEC_STH(dst1 + dststride, v2);
+            dst1 += dststride << 1;
+        }
+    }
+}
+
+#undef EPEL_HV4_INIT
+#undef EPEL_HV4
+#undef EPEL_HV_INIT
+#undef EPEL_HV
+#undef QPEL_HV_INIT
+#undef QPEL_HV
+
+#undef PEL_FUNC_TMP
+#undef PEL_FUNC_UNI
+#undef PEL_FUNC_BI
+#undef PEL_FUNC_UNI_W
+
+#undef PEL_FIN_UNI8
+#undef PEL_FIN_UNI
+#undef PEL_FIN_BI8
+#undef PEL_FIN_BI
+#undef PEL_FIN_UNI_W8
+#undef PEL_FIN_UNI_W
+
+#undef EPEL_INIT_FILTER
+#undef QPEL_INIT_FILTER
+
+/* idct ------------------------------------------------------------*/
+
+#define SET_BIAS(shr) bias = _mm_set1_epi32(1 << (shr - 1));
+#define LD_CONST2(a, b) _mm_set1_epi32((a & 0xffff) | b << 16)
+#define CONST2(name, a, b) const __m128i name = LD_CONST2(a, b);
+#define IDCT_CONST4 \
+    CONST2(c4_e0, 64,  64) \
+    CONST2(c4_o0, 83,  36) \
+    CONST2(c4_e1, 64, -64) \
+    CONST2(c4_o1, 36, -83)
+
+#define TR_4X4(shr, fin) \
+    m2 = _mm_madd_epi16(m0, c4_e0); /* e0 */ \
+    t0 = _mm_madd_epi16(m1, c4_o0); /* o0 */ \
+    m0 = _mm_madd_epi16(m0, c4_e1); /* e1 */ \
+    t1 = _mm_madd_epi16(m1, c4_o1); /* o1 */ \
+    m2 = _mm_add_epi32(m2, bias); \
+    m0 = _mm_add_epi32(m0, bias); \
+    \
+    m3 = _mm_add_epi32(m2, t0); \
+    m2 = _mm_sub_epi32(m2, t0); \
+    m1 = _mm_add_epi32(m0, t1); \
+    m0 = _mm_sub_epi32(m0, t1); \
+    \
+    /* m3, m1, m0, m2 */ \
+    if (fin) { \
+        m3 = _mm_srai_epi32(m3, shr); /* e0 + o0 */ \
+        m1 = _mm_srai_epi32(m1, shr); /* e1 + o1 */ \
+        m2 = _mm_srai_epi32(m2, shr); /* e0 - o0 */ \
+        m0 = _mm_srai_epi32(m0, shr); /* e1 - o1 */ \
+        m3 = _mm_packs_epi32(m3, m1); \
+        m0 = _mm_packs_epi32(m0, m2); \
+        m1 = _mm_unpacklo_epi16(m3, m0); \
+        m3 = _mm_unpackhi_epi16(m3, m0); \
+        m2 = _mm_unpacklo_epi16(m1, m3); \
+        m1 = _mm_unpackhi_epi16(m1, m3); \
+    }
+
+#define IDCT_4X4(depth) \
+static void hevc_idct_4x4_##depth##_e2k(int16_t *coeffs, int col_limit) { \
+    __m128i m0, m1, m2, m3, t0, t1, bias; \
+    int shift = 20 - depth; \
+    IDCT_CONST4 \
+    \
+    m2 = _mm_load_si128((const __m128i*)coeffs); \
+    m1 = _mm_load_si128((const __m128i*)(coeffs + 8)); \
+    m0 = _mm_unpacklo_epi16(m2, m1); \
+    m1 = _mm_unpackhi_epi16(m2, m1); \
+    SET_BIAS(7) TR_4X4(7, 1); \
+    m0 = _mm_unpacklo_epi16(m2, m1); \
+    m1 = _mm_unpackhi_epi16(m2, m1); \
+    SET_BIAS(shift) TR_4X4(shift, 1); \
+    \
+    _mm_store_si128((__m128i*)coeffs, m2); \
+    _mm_store_si128((__m128i*)(coeffs + 8), m1); \
+}
+
+IDCT_4X4(8)
+IDCT_4X4(10)
+
+#define IDCT_CONST8 IDCT_CONST4 \
+    CONST2(c8_30,  89,  75) \
+    CONST2(c8_31,  50,  18) \
+    CONST2(c8_10,  75, -18) \
+    CONST2(c8_11, -89, -50) \
+    CONST2(c8_00,  50, -89) \
+    CONST2(c8_01,  18,  75) \
+    CONST2(c8_20,  18, -50) \
+    CONST2(c8_21,  75, -89)
+
+#define E8_O8(i, a, b) \
+    b = _mm_add_epi32(_mm_madd_epi16(m4, c8_##i##0), _mm_madd_epi16(m5, c8_##i##1)); \
+    a = _mm_add_epi32(b, m##i); /* o8 + e8 */ \
+    b = _mm_sub_epi32(m##i, b); /* e8 - o8 */
+
+#define TRANSPOSE_8X4(shr, t0, t1, t2, t3, t4, t5, t6, t7, o0, o1, o2, o3) \
+    m0 = _mm_packs_epi32(_mm_srai_epi32(t0, shr), _mm_srai_epi32(t4, shr)); \
+    m1 = _mm_packs_epi32(_mm_srai_epi32(t1, shr), _mm_srai_epi32(t5, shr)); \
+    m2 = _mm_packs_epi32(_mm_srai_epi32(t2, shr), _mm_srai_epi32(t6, shr)); \
+    m3 = _mm_packs_epi32(_mm_srai_epi32(t3, shr), _mm_srai_epi32(t7, shr)); \
+    t0 = _mm_unpacklo_epi16(m0, m2); \
+    t1 = _mm_unpackhi_epi16(m0, m2); \
+    t2 = _mm_unpacklo_epi16(m1, m3); \
+    t3 = _mm_unpackhi_epi16(m1, m3); \
+    o0 = _mm_unpacklo_epi16(t0, t2); \
+    o1 = _mm_unpackhi_epi16(t0, t2); \
+    o2 = _mm_unpacklo_epi16(t1, t3); \
+    o3 = _mm_unpackhi_epi16(t1, t3);
+
+#define TR_8X4(shr, fin, o) \
+    TR_4X4(shr, 0) \
+    E8_O8(3, t0, t7) \
+    E8_O8(1, t1, t6) \
+    E8_O8(0, t2, t5) \
+    E8_O8(2, t3, t4) \
+    if (fin) { \
+        TRANSPOSE_8X4(shr, t0, t1, t2, t3, t4, t5, t6, t7, o##0, o##1, o##2, o##3) \
+    }
+
+#define IDCT_8X8(depth) \
+static void hevc_idct_8x8_##depth##_e2k(int16_t *coeffs, int col_limit) { \
+    __m128i m0, m1, m2, m3, m4, m5, bias; \
+    __m128i t0, t1, t2, t3, t4, t5, t6, t7; \
+    __m128i l0, l1, l2, l3, h0, h1, h2, h3; \
+    int shift = 20 - depth; \
+    IDCT_CONST8 \
+    __m128i i0 = _mm_load_si128((const __m128i*)coeffs); \
+    __m128i i1 = _mm_load_si128((const __m128i*)(coeffs + 8)); \
+    __m128i i2 = _mm_load_si128((const __m128i*)(coeffs + 8 * 2)); \
+    __m128i i3 = _mm_load_si128((const __m128i*)(coeffs + 8 * 3)); \
+    __m128i i4 = _mm_load_si128((const __m128i*)(coeffs + 8 * 4)); \
+    __m128i i5 = _mm_load_si128((const __m128i*)(coeffs + 8 * 5)); \
+    __m128i i6 = _mm_load_si128((const __m128i*)(coeffs + 8 * 6)); \
+    __m128i i7 = _mm_load_si128((const __m128i*)(coeffs + 8 * 7)); \
+    \
+    m0 = _mm_unpacklo_epi16(i0, i4); \
+    m1 = _mm_unpacklo_epi16(i2, i6); \
+    m4 = _mm_unpacklo_epi16(i1, i3); \
+    m5 = _mm_unpacklo_epi16(i5, i7); \
+    SET_BIAS(7) \
+    TR_8X4(7, 1, l) \
+    m0 = _mm_unpackhi_epi16(i0, i4); \
+    m1 = _mm_unpackhi_epi16(i2, i6); \
+    m4 = _mm_unpackhi_epi16(i1, i3); \
+    m5 = _mm_unpackhi_epi16(i5, i7); \
+    TR_8X4(7, 1, h) \
+    m0 = _mm_unpacklo_epi16(l0, h0); \
+    m1 = _mm_unpacklo_epi16(l1, h1); \
+    m4 = _mm_unpackhi_epi16(l0, l1); \
+    m5 = _mm_unpackhi_epi16(h0, h1); \
+    SET_BIAS(shift) \
+    TR_8X4(shift, 1, i) \
+    m0 = _mm_unpacklo_epi16(l2, h2); \
+    m1 = _mm_unpacklo_epi16(l3, h3); \
+    m4 = _mm_unpackhi_epi16(l2, l3); \
+    m5 = _mm_unpackhi_epi16(h2, h3); \
+    TR_8X4(shift, 1, m) \
+    VEC_STL(coeffs + 8 * 0, i0); VEC_STL(coeffs + 8 * 0 + 4, i2); \
+    VEC_STH(coeffs + 8 * 1, i0); VEC_STH(coeffs + 8 * 1 + 4, i2); \
+    VEC_STL(coeffs + 8 * 2, i1); VEC_STL(coeffs + 8 * 2 + 4, i3); \
+    VEC_STH(coeffs + 8 * 3, i1); VEC_STH(coeffs + 8 * 3 + 4, i3); \
+    VEC_STL(coeffs + 8 * 4, m0); VEC_STL(coeffs + 8 * 4 + 4, m2); \
+    VEC_STH(coeffs + 8 * 5, m0); VEC_STH(coeffs + 8 * 5 + 4, m2); \
+    VEC_STL(coeffs + 8 * 6, m1); VEC_STL(coeffs + 8 * 6 + 4, m3); \
+    VEC_STH(coeffs + 8 * 7, m1); VEC_STH(coeffs + 8 * 7 + 4, m3); \
+}
+
+IDCT_8X8(8)
+IDCT_8X8(10)
+
+#define C16_00 LD_CONST2( 90,  87)
+#define C16_01 LD_CONST2( 80,  70)
+#define C16_02 LD_CONST2( 57,  43)
+#define C16_03 LD_CONST2( 25,   9)
+#define C16_10 LD_CONST2( 87,  57)
+#define C16_11 LD_CONST2(  9, -43)
+#define C16_12 LD_CONST2(-80, -90)
+#define C16_13 LD_CONST2(-70, -25)
+#define C16_20 LD_CONST2( 80,   9)
+#define C16_21 LD_CONST2(-70, -87)
+#define C16_22 LD_CONST2(-25,  57)
+#define C16_23 LD_CONST2( 90,  43)
+#define C16_30 LD_CONST2( 70, -43)
+#define C16_31 LD_CONST2(-87,   9)
+#define C16_32 LD_CONST2( 90,  25)
+#define C16_33 LD_CONST2(-80, -57)
+#define C16_40 LD_CONST2( 57, -80)
+#define C16_41 LD_CONST2(-25,  90)
+#define C16_42 LD_CONST2( -9, -87)
+#define C16_43 LD_CONST2( 43,  70)
+#define C16_50 LD_CONST2( 43, -90)
+#define C16_51 LD_CONST2( 57,  25)
+#define C16_52 LD_CONST2(-87,  70)
+#define C16_53 LD_CONST2(  9, -80)
+#define C16_60 LD_CONST2( 25, -70)
+#define C16_61 LD_CONST2( 90, -80)
+#define C16_62 LD_CONST2( 43,   9)
+#define C16_63 LD_CONST2(-57,  87)
+#define C16_70 LD_CONST2( 9,  -25)
+#define C16_71 LD_CONST2( 43, -57)
+#define C16_72 LD_CONST2( 70, -80)
+#define C16_73 LD_CONST2( 87, -90)
+
+#define E16_O16(i, a, b) \
+    a = _mm_add_epi32(_mm_madd_epi16(m0, C16_##i##0), _mm_madd_epi16(m1, C16_##i##1)); \
+    b = _mm_add_epi32(_mm_madd_epi16(m2, C16_##i##2), _mm_madd_epi16(m3, C16_##i##3)); \
+    b = _mm_add_epi32(a, b); \
+    a = _mm_add_epi32(b, t##i); /* o16 + e16 */ \
+    b = _mm_sub_epi32(t##i, b); /* e16 - o16 */
+
+#define LDPAIR_V(j, n, a, b) \
+    m##j = VEC_LD8(coeffs + i + n * (j * 4 + 1)); \
+    m##j = _mm_unpacklo_epi16(m##j, VEC_LD8(coeffs + i + n * (j * 4 + 3)));
+
+#define LDPAIR_H(j, n, a, b) \
+    m##j = VEC_LD8(tmp + i * 4 + n * 4 * j + a); \
+    m##j = _mm_unpacklo_epi16(m##j, VEC_LD8(tmp + i * 4 + n * 4 * j + b));
+
+#define TR_16X4(shift, V) \
+    TR_8X4(shift, 0, m) \
+    LDPAIR_##V(0, 16, 4, 12) \
+    LDPAIR_##V(1, 16, 4, 12) \
+    LDPAIR_##V(2, 16, 4, 12) \
+    LDPAIR_##V(3, 16, 4, 12) \
+    E16_O16(0, x0, x15) \
+    E16_O16(1, x1, x14) \
+    E16_O16(2, x2, x13) \
+    E16_O16(3, x3, x12) \
+    E16_O16(4, x4, x11) \
+    E16_O16(5, x5, x10) \
+    E16_O16(6, x6, x9) \
+    E16_O16(7, x7, x8) \
+    TRANSPOSE_8X4(shift, x0, x1, x2, x3, x12, x13, x14, x15, t0, t1, t6, t7) \
+    TRANSPOSE_8X4(shift, x4, x5, x6, x7, x8, x9, x10, x11, t2, t3, t4, t5)
+
+#define IDCT_16X16(depth) \
+static void hevc_idct_16x16_##depth##_e2k(int16_t *coeffs, int col_limit) { \
+    DECLARE_ALIGNED(16, int16_t, tmp)[16 * 16]; \
+    __m128i m0, m1, m2, m3, m4, m5, bias; \
+    __m128i t0, t1, t2, t3, t4, t5, t6, t7; \
+    __m128i x0, x1, x2, x3, x4, x5, x6, x7; \
+    __m128i x8, x9, x10, x11, x12, x13, x14, x15; \
+    __m128i *ptr = (__m128i*)tmp; \
+    int i, shift = 20 - depth; \
+    IDCT_CONST8 \
+    SET_BIAS(7) \
+    PRAGMA_E2K("ivdep") \
+    for (i = 0; i < 16; i += 4) { \
+        t0 = VEC_LD8(coeffs + i + 32 * 0); \
+        t1 = VEC_LD8(coeffs + i + 32 * 1); \
+        t2 = VEC_LD8(coeffs + i + 32 * 2); \
+        t3 = VEC_LD8(coeffs + i + 32 * 3); \
+        t4 = VEC_LD8(coeffs + i + 32 * 4); \
+        t5 = VEC_LD8(coeffs + i + 32 * 5); \
+        t6 = VEC_LD8(coeffs + i + 32 * 6); \
+        t7 = VEC_LD8(coeffs + i + 32 * 7); \
+        m0 = _mm_unpacklo_epi16(t0, t4); \
+        m1 = _mm_unpacklo_epi16(t2, t6); \
+        m4 = _mm_unpacklo_epi16(t1, t3); \
+        m5 = _mm_unpacklo_epi16(t5, t7); \
+        TR_16X4(7, V) \
+        \
+        ptr[0] = t0; \
+        ptr[1] = t1; \
+        ptr[2] = t2; \
+        ptr[3] = t3; \
+        ptr[4] = t4; \
+        ptr[5] = t5; \
+        ptr[6] = t6; \
+        ptr[7] = t7; \
+        ptr += 8; \
+    } \
+    SET_BIAS(shift) \
+    PRAGMA_E2K("ivdep") \
+    for (i = 0; i < 16; i += 4) { \
+        t0 = VEC_LD8(tmp + i * 4 + 8 * 0); \
+        t1 = VEC_LD8(tmp + i * 4 + 8 * 1); \
+        t2 = VEC_LD8(tmp + i * 4 + 8 * 8); \
+        t3 = VEC_LD8(tmp + i * 4 + 8 * 9); \
+        t4 = VEC_LD8(tmp + i * 4 + 8 * 16); \
+        t5 = VEC_LD8(tmp + i * 4 + 8 * 17); \
+        t6 = VEC_LD8(tmp + i * 4 + 8 * 24); \
+        t7 = VEC_LD8(tmp + i * 4 + 8 * 25); \
+        m0 = _mm_unpacklo_epi16(t0, t4); \
+        m1 = _mm_unpacklo_epi16(t2, t6); \
+        m4 = _mm_unpacklo_epi16(t1, t3); \
+        m5 = _mm_unpacklo_epi16(t5, t7); \
+        TR_16X4(shift, H) \
+        \
+        VEC_STL(coeffs + i * 16 + 0, t0); \
+        VEC_STL(coeffs + i * 16 + 4, t2); \
+        VEC_STL(coeffs + i * 16 + 8, t4); \
+        VEC_STL(coeffs + i * 16 + 12, t6); \
+        VEC_STH(coeffs + i * 16 + 16, t0); \
+        VEC_STH(coeffs + i * 16 + 20, t2); \
+        VEC_STH(coeffs + i * 16 + 24, t4); \
+        VEC_STH(coeffs + i * 16 + 28, t6); \
+        VEC_STL(coeffs + i * 16 + 32, t1); \
+        VEC_STL(coeffs + i * 16 + 36, t3); \
+        VEC_STL(coeffs + i * 16 + 40, t5); \
+        VEC_STL(coeffs + i * 16 + 44, t7); \
+        VEC_STH(coeffs + i * 16 + 48, t1); \
+        VEC_STH(coeffs + i * 16 + 52, t3); \
+        VEC_STH(coeffs + i * 16 + 56, t5); \
+        VEC_STH(coeffs + i * 16 + 60, t7); \
+    } \
+}
+
+IDCT_16X16(8)
+IDCT_16X16(10)
+
+#define C32_00 LD_CONST2( 90,  90)
+#define C32_01 LD_CONST2( 88,  85)
+#define C32_02 LD_CONST2( 82,  78)
+#define C32_03 LD_CONST2( 73,  67)
+#define C32_04 LD_CONST2( 61,  54)
+#define C32_05 LD_CONST2( 46,  38)
+#define C32_06 LD_CONST2( 31,  22)
+#define C32_07 LD_CONST2( 13,   4)
+#define C32_10 LD_CONST2( 90,  82)
+#define C32_11 LD_CONST2( 67,  46)
+#define C32_12 LD_CONST2( 22,  -4)
+#define C32_13 LD_CONST2(-31, -54)
+#define C32_14 LD_CONST2(-73, -85)
+#define C32_15 LD_CONST2(-90, -88)
+#define C32_16 LD_CONST2(-78, -61)
+#define C32_17 LD_CONST2(-38, -13)
+#define C32_20 LD_CONST2( 88,  67)
+#define C32_21 LD_CONST2( 31, -13)
+#define C32_22 LD_CONST2(-54, -82)
+#define C32_23 LD_CONST2(-90, -78)
+#define C32_24 LD_CONST2(-46,  -4)
+#define C32_25 LD_CONST2( 38,  73)
+#define C32_26 LD_CONST2( 90,  85)
+#define C32_27 LD_CONST2( 61,  22)
+#define C32_30 LD_CONST2( 85,  46)
+#define C32_31 LD_CONST2(-13, -67)
+#define C32_32 LD_CONST2(-90, -73)
+#define C32_33 LD_CONST2(-22,  38)
+#define C32_34 LD_CONST2( 82,  88)
+#define C32_35 LD_CONST2( 54,  -4)
+#define C32_36 LD_CONST2(-61, -90)
+#define C32_37 LD_CONST2(-78, -31)
+#define C32_40 LD_CONST2( 82,  22)
+#define C32_41 LD_CONST2(-54, -90)
+#define C32_42 LD_CONST2(-61,  13)
+#define C32_43 LD_CONST2( 78,  85)
+#define C32_44 LD_CONST2( 31, -46)
+#define C32_45 LD_CONST2(-90, -67)
+#define C32_46 LD_CONST2(  4,  73)
+#define C32_47 LD_CONST2( 88,  38)
+#define C32_50 LD_CONST2( 78,  -4)
+#define C32_51 LD_CONST2(-82, -73)
+#define C32_52 LD_CONST2( 13,  85)
+#define C32_53 LD_CONST2( 67, -22)
+#define C32_54 LD_CONST2(-88, -61)
+#define C32_55 LD_CONST2( 31,  90)
+#define C32_56 LD_CONST2( 54, -38)
+#define C32_57 LD_CONST2(-90, -46)
+#define C32_60 LD_CONST2( 73, -31)
+#define C32_61 LD_CONST2(-90, -22)
+#define C32_62 LD_CONST2( 78,  67)
+#define C32_63 LD_CONST2(-38, -90)
+#define C32_64 LD_CONST2(-13,  82)
+#define C32_65 LD_CONST2( 61, -46)
+#define C32_66 LD_CONST2(-88,  -4)
+#define C32_67 LD_CONST2( 85,  54)
+#define C32_70 LD_CONST2( 67, -54)
+#define C32_71 LD_CONST2(-78,  38)
+#define C32_72 LD_CONST2( 85, -22)
+#define C32_73 LD_CONST2(-90,   4)
+#define C32_74 LD_CONST2( 90,  13)
+#define C32_75 LD_CONST2(-88, -31)
+#define C32_76 LD_CONST2( 82,  46)
+#define C32_77 LD_CONST2(-73, -61)
+#define C32_80 LD_CONST2( 61, -73)
+#define C32_81 LD_CONST2(-46,  82)
+#define C32_82 LD_CONST2( 31, -88)
+#define C32_83 LD_CONST2(-13,  90)
+#define C32_84 LD_CONST2( -4, -90)
+#define C32_85 LD_CONST2( 22,  85)
+#define C32_86 LD_CONST2(-38, -78)
+#define C32_87 LD_CONST2( 54,  67)
+#define C32_90 LD_CONST2( 54, -85)
+#define C32_91 LD_CONST2(-4,   88)
+#define C32_92 LD_CONST2(-46, -61)
+#define C32_93 LD_CONST2( 82,  13)
+#define C32_94 LD_CONST2(-90,  38)
+#define C32_95 LD_CONST2( 67, -78)
+#define C32_96 LD_CONST2(-22,  90)
+#define C32_97 LD_CONST2(-31, -73)
+#define C32_100 LD_CONST2( 46, -90)
+#define C32_101 LD_CONST2( 38,  54)
+#define C32_102 LD_CONST2(-90,  31)
+#define C32_103 LD_CONST2( 61, -88)
+#define C32_104 LD_CONST2( 22,  67)
+#define C32_105 LD_CONST2(-85,  13)
+#define C32_106 LD_CONST2( 73, -82)
+#define C32_107 LD_CONST2(  4,  78)
+#define C32_110 LD_CONST2( 38, -88)
+#define C32_111 LD_CONST2( 73,  -4)
+#define C32_112 LD_CONST2(-67,  90)
+#define C32_113 LD_CONST2(-46, -31)
+#define C32_114 LD_CONST2( 85, -78)
+#define C32_115 LD_CONST2( 13,  61)
+#define C32_116 LD_CONST2(-90,  54)
+#define C32_117 LD_CONST2( 22, -82)
+#define C32_120 LD_CONST2( 31, -78)
+#define C32_121 LD_CONST2( 90, -61)
+#define C32_122 LD_CONST2(  4,  54)
+#define C32_123 LD_CONST2(-88,  82)
+#define C32_124 LD_CONST2(-38, -22)
+#define C32_125 LD_CONST2( 73, -90)
+#define C32_126 LD_CONST2( 67, -13)
+#define C32_127 LD_CONST2(-46,  85)
+#define C32_130 LD_CONST2( 22, -61)
+#define C32_131 LD_CONST2( 85, -90)
+#define C32_132 LD_CONST2( 73, -38)
+#define C32_133 LD_CONST2( -4,  46)
+#define C32_134 LD_CONST2(-78,  90)
+#define C32_135 LD_CONST2(-82,  54)
+#define C32_136 LD_CONST2(-13, -31)
+#define C32_137 LD_CONST2( 67, -88)
+#define C32_140 LD_CONST2( 13, -38)
+#define C32_141 LD_CONST2( 61, -78)
+#define C32_142 LD_CONST2( 88, -90)
+#define C32_143 LD_CONST2( 85, -73)
+#define C32_144 LD_CONST2( 54, -31)
+#define C32_145 LD_CONST2(  4,  22)
+#define C32_146 LD_CONST2(-46,  67)
+#define C32_147 LD_CONST2(-82,  90)
+#define C32_150 LD_CONST2(  4, -13)
+#define C32_151 LD_CONST2( 22, -31)
+#define C32_152 LD_CONST2( 38, -46)
+#define C32_153 LD_CONST2( 54, -61)
+#define C32_154 LD_CONST2( 67, -73)
+#define C32_155 LD_CONST2( 78, -82)
+#define C32_156 LD_CONST2( 85, -88)
+#define C32_157 LD_CONST2( 90, -90)
+
+#define E32_O32(i, a, b) \
+    a = _mm_add_epi32(_mm_madd_epi16(m0, C32_##i##0), _mm_madd_epi16(m1, C32_##i##1)); \
+    b = _mm_add_epi32(_mm_madd_epi16(m2, C32_##i##2), _mm_madd_epi16(m3, C32_##i##3)); \
+    m8 = _mm_add_epi32(_mm_madd_epi16(m4, C32_##i##4), _mm_madd_epi16(m5, C32_##i##5)); \
+    m9 = _mm_add_epi32(_mm_madd_epi16(m6, C32_##i##6), _mm_madd_epi16(m7, C32_##i##7)); \
+    b = _mm_add_epi32(a, b); \
+    b = _mm_add_epi32(b, _mm_add_epi32(m8, m9)); \
+    a = _mm_add_epi32(b, x##i); /* o32 + e32 */ \
+    b = _mm_sub_epi32(x##i, b); /* e32 - o32 */
+
+#define TR_32X4(shift, V) \
+    TR_8X4(shift, 0, m) \
+    LDPAIR_##V(0, 64, 8, 136) \
+    LDPAIR_##V(1, 64, 8, 136) \
+    LDPAIR_##V(2, 64, 8, 136) \
+    LDPAIR_##V(3, 64, 8, 136) \
+    E16_O16(0, x0, x15) \
+    E16_O16(1, x1, x14) \
+    E16_O16(2, x2, x13) \
+    E16_O16(3, x3, x12) \
+    E16_O16(4, x4, x11) \
+    E16_O16(5, x5, x10) \
+    E16_O16(6, x6, x9) \
+    E16_O16(7, x7, x8) \
+    LDPAIR_##V(0, 32, 4, 12) \
+    LDPAIR_##V(1, 32, 4, 12) \
+    LDPAIR_##V(2, 32, 4, 12) \
+    LDPAIR_##V(3, 32, 4, 12) \
+    LDPAIR_##V(4, 32, 4, 12) \
+    LDPAIR_##V(5, 32, 4, 12) \
+    LDPAIR_##V(6, 32, 4, 12) \
+    LDPAIR_##V(7, 32, 4, 12) \
+    E32_O32(0, t0, t31) \
+    E32_O32(1, t1, t30) \
+    E32_O32(2, t2, t29) \
+    E32_O32(3, t3, t28) \
+    E32_O32(4, t4, t27) \
+    E32_O32(5, t5, t26) \
+    E32_O32(6, t6, t25) \
+    E32_O32(7, t7, t24) \
+    E32_O32(8, t8, t23) \
+    E32_O32(9, t9, t22) \
+    E32_O32(10, t10, t21) \
+    E32_O32(11, t11, t20) \
+    E32_O32(12, t12, t19) \
+    E32_O32(13, t13, t18) \
+    E32_O32(14, t14, t17) \
+    E32_O32(15, t15, t16) \
+    TRANSPOSE_8X4(shift, t0, t1, t2, t3, t28, t29, t30, t31, x0, x1, x14, x15) \
+    TRANSPOSE_8X4(shift, t4, t5, t6, t7, t24, t25, t26, t27, x2, x3, x12, x13) \
+    TRANSPOSE_8X4(shift, t8, t9, t10, t11, t20, t21, t22, t23, x4, x5, x10, x11) \
+    TRANSPOSE_8X4(shift, t12, t13, t14, t15, t16, t17, t18, t19, x6, x7, x8, x9)
+
+#define IDCT_32X32(depth) \
+static void hevc_idct_32x32_##depth##_e2k(int16_t *coeffs, int col_limit) { \
+    DECLARE_ALIGNED(16, int16_t, tmp)[32 * 32]; \
+    __m128i m0, m1, m2, m3, m4, m5, m6, m7, m8, m9, bias; \
+    __m128i t0, t1, t2, t3, t4, t5, t6, t7; \
+    __m128i t8, t9, t10, t11, t12, t13, t14, t15; \
+    __m128i t16, t17, t18, t19, t20, t21, t22, t23; \
+    __m128i t24, t25, t26, t27, t28, t29, t30, t31; \
+    __m128i x0, x1, x2, x3, x4, x5, x6, x7; \
+    __m128i x8, x9, x10, x11, x12, x13, x14, x15; \
+    __m128i *ptr = (__m128i*)tmp; \
+    int i, shift = 20 - depth; \
+    IDCT_CONST8 \
+    SET_BIAS(7) \
+    PRAGMA_E2K("ivdep") \
+    for (i = 0; i < 32; i += 4) { \
+        t0 = VEC_LD8(coeffs + i + 128 * 0); \
+        t1 = VEC_LD8(coeffs + i + 128 * 1); \
+        t2 = VEC_LD8(coeffs + i + 128 * 2); \
+        t3 = VEC_LD8(coeffs + i + 128 * 3); \
+        t4 = VEC_LD8(coeffs + i + 128 * 4); \
+        t5 = VEC_LD8(coeffs + i + 128 * 5); \
+        t6 = VEC_LD8(coeffs + i + 128 * 6); \
+        t7 = VEC_LD8(coeffs + i + 128 * 7); \
+        m0 = _mm_unpacklo_epi16(t0, t4); \
+        m1 = _mm_unpacklo_epi16(t2, t6); \
+        m4 = _mm_unpacklo_epi16(t1, t3); \
+        m5 = _mm_unpacklo_epi16(t5, t7); \
+        TR_32X4(7, V) \
+        \
+        ptr[0] = x0; \
+        ptr[1] = x1; \
+        ptr[2] = x2; \
+        ptr[3] = x3; \
+        ptr[4] = x4; \
+        ptr[5] = x5; \
+        ptr[6] = x6; \
+        ptr[7] = x7; \
+        ptr[8] = x8; \
+        ptr[9] = x9; \
+        ptr[10] = x10; \
+        ptr[11] = x11; \
+        ptr[12] = x12; \
+        ptr[13] = x13; \
+        ptr[14] = x14; \
+        ptr[15] = x15; \
+        ptr += 16; \
+    } \
+    SET_BIAS(shift) \
+    PRAGMA_E2K("ivdep") \
+    for (i = 0; i < 32; i += 4) { \
+        t0 = VEC_LD8(tmp + i * 4 + 128 * 0); \
+        t1 = VEC_LD8(tmp + i * 4 + 128 * 1); \
+        t2 = VEC_LD8(tmp + i * 4 + 128 * 2); \
+        t3 = VEC_LD8(tmp + i * 4 + 128 * 3); \
+        t4 = VEC_LD8(tmp + i * 4 + 128 * 4); \
+        t5 = VEC_LD8(tmp + i * 4 + 128 * 5); \
+        t6 = VEC_LD8(tmp + i * 4 + 128 * 6); \
+        t7 = VEC_LD8(tmp + i * 4 + 128 * 7); \
+        m0 = _mm_unpacklo_epi16(t0, t4); \
+        m1 = _mm_unpacklo_epi16(t2, t6); \
+        m4 = _mm_unpacklo_epi16(t1, t3); \
+        m5 = _mm_unpacklo_epi16(t5, t7); \
+        TR_32X4(shift, H) \
+        \
+        VEC_STL(coeffs + i * 32 + 0, x0); \
+        VEC_STL(coeffs + i * 32 + 4, x2); \
+        VEC_STL(coeffs + i * 32 + 8, x4); \
+        VEC_STL(coeffs + i * 32 + 12, x6); \
+        VEC_STL(coeffs + i * 32 + 16, x8); \
+        VEC_STL(coeffs + i * 32 + 20, x10); \
+        VEC_STL(coeffs + i * 32 + 24, x12); \
+        VEC_STL(coeffs + i * 32 + 28, x14); \
+        VEC_STH(coeffs + i * 32 + 32, x0); \
+        VEC_STH(coeffs + i * 32 + 36, x2); \
+        VEC_STH(coeffs + i * 32 + 40, x4); \
+        VEC_STH(coeffs + i * 32 + 44, x6); \
+        VEC_STH(coeffs + i * 32 + 48, x8); \
+        VEC_STH(coeffs + i * 32 + 52, x10); \
+        VEC_STH(coeffs + i * 32 + 56, x12); \
+        VEC_STH(coeffs + i * 32 + 60, x14); \
+        VEC_STL(coeffs + i * 32 + 64, x1); \
+        VEC_STL(coeffs + i * 32 + 68, x3); \
+        VEC_STL(coeffs + i * 32 + 72, x5); \
+        VEC_STL(coeffs + i * 32 + 76, x7); \
+        VEC_STL(coeffs + i * 32 + 80, x9); \
+        VEC_STL(coeffs + i * 32 + 84, x11); \
+        VEC_STL(coeffs + i * 32 + 88, x13); \
+        VEC_STL(coeffs + i * 32 + 92, x15); \
+        VEC_STH(coeffs + i * 32 + 96, x1); \
+        VEC_STH(coeffs + i * 32 + 100, x3); \
+        VEC_STH(coeffs + i * 32 + 104, x5); \
+        VEC_STH(coeffs + i * 32 + 108, x7); \
+        VEC_STH(coeffs + i * 32 + 112, x9); \
+        VEC_STH(coeffs + i * 32 + 116, x11); \
+        VEC_STH(coeffs + i * 32 + 120, x13); \
+        VEC_STH(coeffs + i * 32 + 124, x15); \
+    } \
+}
+
+IDCT_32X32(8)
+IDCT_32X32(10)
+
+/* loop_filter -----------------------------------------------------*/
+
+#define _mm_shuffle2_epi32(a, b, c) _mm_castps_si128(_mm_shuffle_ps(_mm_castsi128_ps(a), _mm_castsi128_ps(b), c))
+
+#define LUMA_DEBLOCK_STRONG(q1, p0, p1, p2, p3, m12) \
+    m8 = _mm_add_epi16(m22, p1);   /* p1 + p0 + q0 + 2 */ \
+    m10 = _mm_add_epi16(m8, p2);   /* p2 + p1 + p0 + q0 + 2 */ \
+    m8 = _mm_add_epi16(m8, q1);    /* p1 + p0 + q0 + q1 + 2 */ \
+    m8 = _mm_add_epi16(m8, m10);   /* p2 + 2*p1 + 2*p0 + 2*q0 + q1 + 4 */ \
+    m8 = _mm_srai_epi16(m8, 3);    /* ((p2 + 2*p1 + 2*p0 + 2*q0 + q1 + 4) >> 3) */ \
+    m8 = _mm_sub_epi16(m8, p0);    /* ((p2 + 2*p1 + 2*p0 + 2*q0 + q1 + 4) >> 3) - p0 */ \
+    m8 = _mm_max_epi16(m8, m14); \
+    m8 = _mm_min_epi16(m8, m9);    /* av_clip(X, -2 * tc, 2 * tc) */ \
+    m8 = _mm_add_epi16(m8, p0);    /* p0' */ \
+    p0 = _mm_blendv_epi8(p0, m8, m11); \
+    \
+    m12 = _mm_srai_epi16(m10, 2);  /* (p2 + p1 + p0 + q0 + 2) >> 2 */ \
+    m12 = _mm_sub_epi16(m12, p1);  /* ((p2 + p1 + p0 + q0 + 2) >> 2) - p1 */ \
+    m12 = _mm_max_epi16(m12, m14); \
+    m12 = _mm_min_epi16(m12, m9);  /* av_clip(X, -2 * tc, 2 * tc) */ \
+    m12 = _mm_add_epi16(m12, p1);  /* p1' */ \
+    \
+    m10 = _mm_add_epi16(m10, m13); /* p2 + p1 + p0 + q0 + 4 */ \
+    m8 = _mm_add_epi16(p2, p3);    /* p3 + p2 */ \
+    m8 = _mm_add_epi16(m8, m8);    /* 2*p3 + 2*p2 */ \
+    m8 = _mm_add_epi16(m8, m10);   /* 2*p3 + 3*p2 + p1 + p0 + q0 + 4 */ \
+    m8 = _mm_srai_epi16(m8, 3);    /* (2*p3 + 3*p2 + p1 + p0 + q0 + 4) >> 3 */ \
+    m8 = _mm_sub_epi16(m8, p2);    /* ((2*p3 + 3*p2 + p1 + p0 + q0 + 4) >> 3) - p2 */ \
+    m8 = _mm_max_epi16(m8, m14); \
+    m8 = _mm_min_epi16(m8, m9);    /* av_clip(X, -2 * tc, 2 * tc) */ \
+    m8 = _mm_add_epi16(m8, p2);    /* p2' */ \
+    p2 = _mm_blendv_epi8(p2, m8, m11);
+
+#define LUMA_DEBLOCK_WEAK(p0, p1, p2, add, m20) \
+    m9 = _mm_avg_epu16(p2, p0);  /* (p2 + p0 + 1) >> 1 */ \
+    m9 = _mm_sub_epi16(m9, p1);  /* ((p2 + p0 + 1) >> 1) - p1 */ \
+    m9 = _mm_##add##_epi16(m9, m12); /* ((p2 + p0 + 1) >> 1) - p1 + delta0 */ \
+    m9 = _mm_srai_epi16(m9, 1);  /* (((p2 + p0 + 1) >> 1) - p1 + delta0) >> 1 */ \
+    m9 = _mm_max_epi16(m9, m14); \
+    m9 = _mm_min_epi16(m9, m15); /* av_clip(deltap1, -tc/2, tc/2) */ \
+    m9 = _mm_add_epi16(m9, p1);  /* p1' */ \
+    \
+    m20 = _mm_add_epi16(m20, _mm_srli_epi64(m20, 48)); /* nd_p */ \
+    m13 = _mm_unpacklo_epi16(m20, m20); \
+    m8 = _mm_unpackhi_epi16(m20, m20); \
+    m13 = _mm_shuffle2_epi32(m13, m8, 0); /* dp0 + dp3 */ \
+    m8 = _mm_cmpgt_epi16(m10, m13); \
+    m8 = _mm_and_si128(m8, m11); \
+    p1 = _mm_blendv_epi8(p1, m9, m8); /* write p1' */ \
+    m9 = _mm_##add##_epi16(p0, m12); /* p0 + delta0 */ \
+    p0 = _mm_blendv_epi8(p0, m9, m11);
+
+#define LUMA_DEBLOCK_BODY do { \
+    __m128i m8, m9, m10, m11, m12, m13, m14, m15, m20, m21, m22; \
+    int32_t r3, r6, r11, r13; \
+    __m128i vshufm = _mm_setr_epi8(0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1); \
+    \
+    m20 = _mm_sub_epi16(m1, _mm_slli_epi16(m2, 1)); \
+    m21 = _mm_sub_epi16(m6, _mm_slli_epi16(m5, 1)); \
+    m20 = _mm_abs_epi16(_mm_add_epi16(m20, m3)); /* 0dp0, 0dp3, 1dp0, 1dp3 */ \
+    m21 = _mm_abs_epi16(_mm_add_epi16(m21, m4)); /* 0dq0, 0dq3, 1dq0, 1dq3 */ \
+    \
+    m9 = _mm_add_epi16(m20, m21); /* 0d0, 0d3, 1d0, 1d3 */ \
+    m14 = _mm_add_epi16(m9, _mm_slli_epi64(m9, 48)); /* 0d0+0d3, 1d0+1d3 */ \
+    \
+    /* compare */ \
+    m13 = _mm_set1_epi16(beta); \
+    m15 = _mm_cmpgt_epi16(m13, m14); \
+    r13 = _mm_movemask_epi8(m15); /* filtering mask 0d0 + 0d3 < beta0 (bit 2 or 3), 1d0 + 1d3 < beta1 (bit 0 or 1) */ \
+    if (!(r13 & 0x8080)) return; \
+    \
+    /* weak / strong decision compare to beta_2 */ \
+    m15 = _mm_srai_epi16(m13, 2); /* beta >> 2 */ \
+    m8 = _mm_slli_epi16(m9, 1); \
+    m15 = _mm_cmpgt_epi16(m15, m8); /* (d0 << 1) < beta_2, (d3 << 1) < beta_2 */ \
+    r6 = _mm_movemask_epi8(m15); \
+    \
+    /* filtering mask */ \
+    m11 = _mm_cvtsi32_si128(r13); \
+    m11 = _mm_shuffle_epi8(m11, vshufm); \
+    \
+    /* decide between strong and weak filtering */ \
+    /* tc25 calculations */ \
+    r11 = _tc[0]; r3 = _tc[1]; \
+    if (!(r11 + r3)) return; \
+    \
+    m8 = _mm_cvtsi32_si128(r11); \
+    m9 = _mm_cvtsi32_si128(r3); \
+    m8 = _mm_unpacklo_epi16(m8, m8); \
+    m9 = _mm_unpacklo_epi16(m9, m9); \
+    m9 = _mm_shuffle2_epi32(m8, m9, 0); /* tc0, tc1 */ \
+    m8 = _mm_slli_epi16(m9, 2); /* tc << 2 */ \
+    m8 = _mm_avg_epu16(m8, m9); /* tc25 = ((tc * 5 + 1) >> 1) */ \
+    \
+    /* --- beta_3 comparison --- */ \
+    m12 = _mm_abs_epi16(_mm_sub_epi16(m0, m3)); /* abs(p3 - p0) */ \
+    m15 = _mm_abs_epi16(_mm_sub_epi16(m7, m4)); /* abs(q3 - q0) */ \
+    m12 = _mm_add_epi16(m12, m15); \
+    \
+    m13 = _mm_srai_epi16(m13, 3); /* beta >> 3 */ \
+    m13 = _mm_cmpgt_epi16(m13, m12); \
+    r6 &= _mm_movemask_epi8(m13); /* strong mask, beta_2 and beta_3 comparisons */ \
+    /* --- tc25 comparison --- */ \
+    m12 = _mm_abs_epi16(_mm_sub_epi16(m3, m4)); /* abs(p0 - q0) */ \
+    \
+    m8 = _mm_cmpgt_epi16(m8, m12); /* tc25 comparisons */ \
+    r6 &= _mm_movemask_epi8(m8); /* strong mask, beta_2, beta_3 and tc25 comparisons */ \
+    r6 &= r6 << 6; \
+    \
+    m14 = _mm_mullo_epi16(m9, _mm_set1_epi16(-2)); /* -tc * 2 */ \
+    m9 = _mm_add_epi16(m9, m9); \
+    \
+    r6 &= 0x8080; /* strong mask, bits 15 and 7 */ \
+    if (r6) { \
+        m10 = _mm_cvtsi32_si128(r6); \
+        m10 = _mm_shuffle_epi8(m10, vshufm); /* strong mask */ \
+        m13 = _mm_set1_epi16(2); \
+        m11 = _mm_and_si128(m11, m10); /* combine filtering mask and strong mask */ \
+        \
+        m22 = _mm_add_epi16(_mm_add_epi16(m3, m4), m13); /* p0 + q0 + 2 */ \
+        LUMA_DEBLOCK_STRONG(m5, m3, m2, m1, m0, m12) \
+        LUMA_DEBLOCK_STRONG(m2, m4, m5, m6, m7, m15) \
+        m2 = _mm_blendv_epi8(m2, m12, m11); \
+        m5 = _mm_blendv_epi8(m5, m15, m11); \
+    } \
+    r6 = (~r6 & r13) & 0x8080; /* final weak filtering mask, bits 0 and 1 */ \
+    if (!r6) break; \
+    \
+    /* weak filtering mask */ \
+    m11 = _mm_cvtsi32_si128(r6); \
+    m11 = _mm_shuffle_epi8(m11, vshufm); /* filtering mask */ \
+    \
+    beta = (beta + (beta >> 1)) >> 3; \
+    \
+    m12 = _mm_sub_epi16(m4, m3);   /* q0 - p0 */ \
+    m10 = _mm_slli_epi16(m12, 3);  /* 8 * (q0 - p0) */ \
+    m12 = _mm_add_epi16(m12, m10); /* 9 * (q0 - p0) */ \
+    \
+    m10 = _mm_sub_epi16(m5, m2);   /* q1 - p1 */ \
+    m8 = _mm_slli_epi16(m10, 1);   /* 2 * (q1 - p1) */ \
+    m10 = _mm_add_epi16(m10, m8);  /* 3 * (q1 - p1) */ \
+    m12 = _mm_sub_epi16(m12, m10); /* 9 * (q0 - p0) - 3 * (q1 - p1) */ \
+    m12 = _mm_add_epi16(m12, _mm_set1_epi16(8)); \
+    m12 = _mm_srai_epi16(m12, 4);  /* >> 4, delta0 */ \
+    m13 = _mm_abs_epi16(m12);      /* abs(delta0) */ \
+    \
+    m10 = _mm_slli_epi16(m9, 2);   /* 8 * tc */ \
+    m10 = _mm_add_epi16(m10, m9);  /* 10 * tc */ \
+    m10 = _mm_cmpgt_epi16(m10, m13); \
+    m11 = _mm_and_si128(m11, m10); \
+    \
+    m9 = _mm_srai_epi16(m9, 1);    /* tc * 2 -> tc */ \
+    m14 = _mm_srai_epi16(m14, 1);  /*-tc * 2 -> -tc */ \
+    \
+    m12 = _mm_max_epi16(m12, m14); \
+    m12 = _mm_min_epi16(m12, m9);  /* av_clip(delta0, -tc, tc) */ \
+    \
+    m15 = _mm_srai_epi16(m9, 1);     /* tc -> tc / 2 */ \
+    m14 = _mm_sub_epi16(vzero, m15); /* -tc / 2 */ \
+    \
+    m10 = _mm_set1_epi16(beta); \
+    LUMA_DEBLOCK_WEAK(m3, m2, m1, add, m20) /* p1' */ \
+    LUMA_DEBLOCK_WEAK(m4, m5, m6, sub, m21) /* q1' */ \
+} while (0);
+
+static void hevc_h_loop_filter_luma_8_e2k(uint8_t *pix, ptrdiff_t stride, int beta,
+                                          const int32_t *_tc, const uint8_t *_no_p, const uint8_t *_no_q)
+{
+    __m128i m0, m1, m2, m3, m4, m5, m6, m7, vzero = _mm_setzero_si128();
+    m0 = VEC_LD8(pix - 4 * stride);
+    m1 = VEC_LD8(pix - 3 * stride);
+    m2 = VEC_LD8(pix - 2 * stride);
+    m3 = VEC_LD8(pix - stride);
+    m4 = VEC_LD8(pix);
+    m5 = VEC_LD8(pix + stride);
+    m6 = VEC_LD8(pix + 2 * stride);
+    m7 = VEC_LD8(pix + 3 * stride);
+    m0 = _mm_unpacklo_epi8(m0, vzero);
+    m1 = _mm_unpacklo_epi8(m1, vzero);
+    m2 = _mm_unpacklo_epi8(m2, vzero);
+    m3 = _mm_unpacklo_epi8(m3, vzero);
+    m4 = _mm_unpacklo_epi8(m4, vzero);
+    m5 = _mm_unpacklo_epi8(m5, vzero);
+    m6 = _mm_unpacklo_epi8(m6, vzero);
+    m7 = _mm_unpacklo_epi8(m7, vzero);
+    LUMA_DEBLOCK_BODY
+    m1 = _mm_packus_epi16(m1, m2);
+    m3 = _mm_packus_epi16(m3, m4);
+    m5 = _mm_packus_epi16(m5, m6);
+    VEC_STL(pix - 3 * stride, m1);
+    VEC_STH(pix - 2 * stride, m1);
+    VEC_STL(pix - stride,     m3);
+    VEC_STH(pix,              m3);
+    VEC_STL(pix + stride,     m5);
+    VEC_STH(pix + 2 * stride, m5);
+}
+
+#define TR_STEP(a, b) \
+    a##0 = _mm_unpacklo_epi8(b##0, b##2); \
+    a##1 = _mm_unpackhi_epi8(b##0, b##2); \
+    a##2 = _mm_unpacklo_epi8(b##1, b##3); \
+    a##3 = _mm_unpackhi_epi8(b##1, b##3);
+
+static void hevc_v_loop_filter_luma_8_e2k(uint8_t *pix, ptrdiff_t stride, int beta,
+                                          const int32_t *_tc, const uint8_t *_no_p, const uint8_t *_no_q)
+{
+    __m128i m0, m1, m2, m3, m4, m5, m6, m7, vzero = _mm_setzero_si128();
+    __m128i t0, t1, t2, t3;
+    pix -= 4;
+    m0 = VEC_LD8(pix);
+    m1 = VEC_LD8(pix + stride);
+    m2 = VEC_LD8(pix + stride * 2);
+    m3 = VEC_LD8(pix + stride * 3);
+    m4 = VEC_LD8(pix + stride * 4);
+    m5 = VEC_LD8(pix + stride * 5);
+    m6 = VEC_LD8(pix + stride * 6);
+    m7 = VEC_LD8(pix + stride * 7);
+
+    t0 = _mm_unpacklo_epi8(m0, m4);
+    t1 = _mm_unpacklo_epi8(m1, m5);
+    t2 = _mm_unpacklo_epi8(m2, m6);
+    t3 = _mm_unpacklo_epi8(m3, m7);
+    TR_STEP(m, t) TR_STEP(t, m)
+
+    m0 = _mm_unpacklo_epi8(t0, vzero);
+    m1 = _mm_unpackhi_epi8(t0, vzero);
+    m2 = _mm_unpacklo_epi8(t1, vzero);
+    m3 = _mm_unpackhi_epi8(t1, vzero);
+    m4 = _mm_unpacklo_epi8(t2, vzero);
+    m5 = _mm_unpackhi_epi8(t2, vzero);
+    m6 = _mm_unpacklo_epi8(t3, vzero);
+    m7 = _mm_unpackhi_epi8(t3, vzero);
+
+    LUMA_DEBLOCK_BODY
+    t0 = _mm_packus_epi16(m0, m1);
+    t1 = _mm_packus_epi16(m2, m3);
+    t2 = _mm_packus_epi16(m4, m5);
+    t3 = _mm_packus_epi16(m6, m7);
+    TR_STEP(m, t) TR_STEP(t, m) TR_STEP(m, t)
+
+    VEC_STL(pix,              m0);
+    VEC_STH(pix +     stride, m0);
+    VEC_STL(pix + 2 * stride, m1);
+    VEC_STH(pix + 3 * stride, m1);
+    VEC_STL(pix + 4 * stride, m2);
+    VEC_STH(pix + 5 * stride, m2);
+    VEC_STL(pix + 6 * stride, m3);
+    VEC_STH(pix + 7 * stride, m3);
+}
+
+#define CHROMA_DEBLOCK_BODY \
+    v0 = VEC_LD8(_tc); \
+    v0 = _mm_unpacklo_epi16(v0, v0); \
+    v0 = _mm_shuffle_epi32(v0, 0xa0); \
+    \
+    v1 = _mm_slli_epi16(_mm_sub_epi16(q0, p0), 2); \
+    v2 = _mm_add_epi16(_mm_sub_epi16(p1, q1), _mm_set1_epi16(4)); \
+    v1 = _mm_srai_epi16(_mm_add_epi16(v2, v1), 3); \
+    \
+    v1 = _mm_min_epi16(v1, v0); \
+    v1 = _mm_max_epi16(v1, _mm_sub_epi16(vzero, v0)); \
+    p0 = _mm_add_epi16(p0, v1); \
+    q0 = _mm_sub_epi16(q0, v1);
+
+static void hevc_h_loop_filter_chroma_8_e2k(uint8_t *pix, ptrdiff_t stride,
+                                            const int32_t *_tc, const uint8_t *_no_p, const uint8_t *_no_q)
+{
+    __m128i p1, p0, q0, q1, vzero = _mm_setzero_si128();
+    __m128i v0, v1, v2;
+    p1 = VEC_LD8(pix - 2 * stride);
+    p0 = VEC_LD8(pix - stride);
+    q0 = VEC_LD8(pix);
+    q1 = VEC_LD8(pix + stride);
+    p1 = _mm_unpacklo_epi8(p1, vzero);
+    p0 = _mm_unpacklo_epi8(p0, vzero);
+    q0 = _mm_unpacklo_epi8(q0, vzero);
+    q1 = _mm_unpacklo_epi8(q1, vzero);
+    CHROMA_DEBLOCK_BODY
+    p0 = _mm_packus_epi16(p0, q0);
+    VEC_STL(pix - stride, p0);
+    VEC_STH(pix, p0);
+}
+
+#define READ_TR8X4(src, st, r8, r9, r10, r11) {            \
+    __m64 r0, r1, r2, r3, r4, r5, r6, r7;                  \
+    r0 = _mm_cvtsi32_si64(*(uint32_t*)(src));              \
+    r1 = _mm_cvtsi32_si64(*(uint32_t*)(src + st));         \
+    r2 = _mm_cvtsi32_si64(*(uint32_t*)(src + st * 2));     \
+    r3 = _mm_cvtsi32_si64(*(uint32_t*)(src + st * 3));     \
+    r4 = _mm_cvtsi32_si64(*(uint32_t*)(src + st * 4));     \
+    r5 = _mm_cvtsi32_si64(*(uint32_t*)(src + st * 5));     \
+    r6 = _mm_cvtsi32_si64(*(uint32_t*)(src + st * 6));     \
+    r7 = _mm_cvtsi32_si64(*(uint32_t*)(src + st * 7));     \
+                                                           \
+    r0 = _mm_unpacklo_pi8(r0, r4);                         \
+    r1 = _mm_unpacklo_pi8(r1, r5);                         \
+    r2 = _mm_unpacklo_pi8(r2, r6);                         \
+    r3 = _mm_unpacklo_pi8(r3, r7);                         \
+                                                           \
+    r4 = _mm_unpacklo_pi8(r0, r2);                         \
+    r5 = _mm_unpackhi_pi8(r0, r2);                         \
+    r6 = _mm_unpacklo_pi8(r1, r3);                         \
+    r7 = _mm_unpackhi_pi8(r1, r3);                         \
+                                                           \
+    r8 = _mm_unpacklo_pi8(r4, r6);                         \
+    r9 = _mm_unpackhi_pi8(r4, r6);                         \
+    r10 = _mm_unpacklo_pi8(r5, r7);                        \
+    r11 = _mm_unpackhi_pi8(r5, r7);                        \
+}
+
+static void hevc_v_loop_filter_chroma_8_e2k(uint8_t *pix, ptrdiff_t stride,
+                                            const int32_t *_tc, const uint8_t *_no_p, const uint8_t *_no_q)
+{
+    __m128i p1, p0, q0, q1, vzero = _mm_setzero_si128();
+    __m128i v0, v1, v2;
+    __m64 h0, h1, h2, h3;
+    READ_TR8X4(pix - 2, stride, h0, h1, h2, h3);
+    p1 = _mm_unpacklo_epi8(_mm_movpi64_epi64(h0), vzero);
+    p0 = _mm_unpacklo_epi8(_mm_movpi64_epi64(h1), vzero);
+    q0 = _mm_unpacklo_epi8(_mm_movpi64_epi64(h2), vzero);
+    q1 = _mm_unpacklo_epi8(_mm_movpi64_epi64(h3), vzero);
+    CHROMA_DEBLOCK_BODY
+    p0 = _mm_packus_epi16(p0, p0);
+    q0 = _mm_packus_epi16(q0, q0);
+    p0 = _mm_unpacklo_epi8(p0, q0);
+#define WRITE2(v, i) *(uint16_t*)(pix - 1 + i * stride) = _mm_extract_epi16(v, i);
+    WRITE2(p0, 0) WRITE2(p0, 1) WRITE2(p0, 2) WRITE2(p0, 3)
+    WRITE2(p0, 4) WRITE2(p0, 5) WRITE2(p0, 6) WRITE2(p0, 7)
+#undef WRITE2
+}
+
+/* init ------------------------------------------------------------*/
+
+av_cold void ff_hevc_dsp_init_e2k(HEVCDSPContext *c, const int bit_depth)
+{
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    if (bit_depth == 8) {
+        c->idct[0] = hevc_idct_4x4_8_e2k;
+        c->idct[1] = hevc_idct_8x8_8_e2k;
+        c->idct[2] = hevc_idct_16x16_8_e2k;
+        c->idct[3] = hevc_idct_32x32_8_e2k;
+
+        c->add_residual[0] = hevc_add_residual4x4_8_e2k;
+        c->add_residual[1] = hevc_add_residual8x8_8_e2k;
+        c->add_residual[2] = hevc_add_residual16x16_8_e2k;
+        c->add_residual[3] = hevc_add_residual32x32_8_e2k;
+
+        c->sao_band_filter[0] =
+        c->sao_band_filter[1] =
+        c->sao_band_filter[2] =
+        c->sao_band_filter[3] =
+        c->sao_band_filter[4] = hevc_sao_band_filter_8_e2k;
+
+        c->sao_edge_filter[0] = hevc_sao_edge_filter_8_8_e2k;
+        c->sao_edge_filter[1] = hevc_sao_edge_filter_16_8_e2k;
+        c->sao_edge_filter[2] = hevc_sao_edge_filter_32_8_e2k;
+        c->sao_edge_filter[3] = hevc_sao_edge_filter_48_8_e2k;
+        c->sao_edge_filter[4] = hevc_sao_edge_filter_64_8_e2k;
+
+        c->hevc_h_loop_filter_luma = hevc_h_loop_filter_luma_8_e2k;
+        c->hevc_v_loop_filter_luma = hevc_v_loop_filter_luma_8_e2k;
+        c->hevc_h_loop_filter_chroma = hevc_h_loop_filter_chroma_8_e2k;
+        c->hevc_v_loop_filter_chroma = hevc_v_loop_filter_chroma_8_e2k;
+
+#define PEL_LINK(d, v, h, s) \
+    c->d[5][v][h] = s##16_8_e2k; \
+    c->d[7][v][h] = s##32_8_e2k; \
+    c->d[9][v][h] = s##64_8_e2k;
+
+#define PEL_LINK1(d, v, h, s) \
+    c->d[5][v][h] = \
+    c->d[7][v][h] = \
+    c->d[9][v][h] = s##_8_e2k;
+
+#define PEL_LINK2(d, v, h, s) \
+    c->d[3][v][h] = PEL_LINK1(d, v, h, s)
+
+        // !checkasm {
+        c->put_hevc_epel[3][0][0] = put_hevc_pel_pixels8_8_e2k;
+        c->put_hevc_qpel[3][0][0] = put_hevc_pel_pixels8_8_e2k;
+        PEL_LINK(put_hevc_epel, 0, 0, put_hevc_pel_pixels)
+        PEL_LINK(put_hevc_qpel, 0, 0, put_hevc_pel_pixels)
+        c->put_hevc_epel[1][0][1] = put_hevc_epel_h4_8_e2k;
+        c->put_hevc_epel[3][0][1] = put_hevc_epel_h8_8_e2k;
+        c->put_hevc_qpel[3][0][1] = put_hevc_qpel_h8_8_e2k;
+        PEL_LINK(put_hevc_epel, 0, 1, put_hevc_epel_h)
+        PEL_LINK(put_hevc_qpel, 0, 1, put_hevc_qpel_h)
+
+        c->put_hevc_epel_uni[3][0][0] = put_hevc_pel_uni_pixels8_8_e2k;
+        c->put_hevc_qpel_uni[3][0][0] = put_hevc_pel_uni_pixels8_8_e2k;
+        PEL_LINK(put_hevc_epel_uni, 0, 0, put_hevc_pel_uni_pixels)
+        PEL_LINK(put_hevc_qpel_uni, 0, 0, put_hevc_pel_uni_pixels)
+        c->put_hevc_epel_uni[3][0][1] = put_hevc_epel_uni_h8_8_e2k;
+        c->put_hevc_qpel_uni[3][0][1] = put_hevc_qpel_uni_h8_8_e2k;
+        PEL_LINK(put_hevc_epel_uni, 0, 1, put_hevc_epel_uni_h)
+        PEL_LINK(put_hevc_qpel_uni, 0, 1, put_hevc_qpel_uni_h)
+
+        c->put_hevc_epel_bi[3][0][0] = put_hevc_pel_bi_pixels8_8_e2k;
+        c->put_hevc_qpel_bi[3][0][0] = put_hevc_pel_bi_pixels8_8_e2k;
+        PEL_LINK(put_hevc_epel_bi, 0, 0, put_hevc_pel_bi_pixels)
+        PEL_LINK(put_hevc_qpel_bi, 0, 0, put_hevc_pel_bi_pixels)
+        PEL_LINK(put_hevc_epel_bi, 0, 1, put_hevc_epel_bi_h)
+        PEL_LINK(put_hevc_qpel_bi, 0, 1, put_hevc_qpel_bi_h)
+
+        c->put_hevc_epel_uni_w[3][0][0] = put_hevc_pel_uni_w_pixels8_8_e2k;
+        c->put_hevc_qpel_uni_w[3][0][0] = put_hevc_pel_uni_w_pixels8_8_e2k;
+        PEL_LINK(put_hevc_epel_uni_w, 0, 0, put_hevc_pel_uni_w_pixels)
+        PEL_LINK(put_hevc_qpel_uni_w, 0, 0, put_hevc_pel_uni_w_pixels)
+        c->put_hevc_epel_uni_w[3][0][1] = put_hevc_epel_uni_w_h8_8_e2k;
+        c->put_hevc_qpel_uni_w[3][0][1] = put_hevc_qpel_uni_w_h8_8_e2k;
+        PEL_LINK(put_hevc_epel_uni_w, 0, 1, put_hevc_epel_uni_w_h)
+        PEL_LINK(put_hevc_qpel_uni_w, 0, 1, put_hevc_qpel_uni_w_h)
+
+        PEL_LINK1(put_hevc_epel, 1, 0, put_hevc_epel_v)
+        PEL_LINK1(put_hevc_qpel, 1, 0, put_hevc_qpel_v)
+        PEL_LINK2(put_hevc_epel, 1, 1, put_hevc_epel_hv)
+        PEL_LINK2(put_hevc_qpel, 1, 1, put_hevc_qpel_hv)
+        PEL_LINK1(put_hevc_epel_uni, 1, 0, put_hevc_epel_uni_v)
+        PEL_LINK1(put_hevc_qpel_uni, 1, 0, put_hevc_qpel_uni_v)
+        c->put_hevc_epel_uni[1][1][1] = put_hevc_epel_uni_hv4_8_e2k;
+        PEL_LINK2(put_hevc_epel_uni, 1, 1, put_hevc_epel_uni_hv)
+        PEL_LINK2(put_hevc_qpel_uni, 1, 1, put_hevc_qpel_uni_hv)
+        PEL_LINK1(put_hevc_epel_bi, 1, 0, put_hevc_epel_bi_v)
+        PEL_LINK1(put_hevc_qpel_bi, 1, 0, put_hevc_qpel_bi_v)
+        PEL_LINK2(put_hevc_epel_bi, 1, 1, put_hevc_epel_bi_hv)
+        PEL_LINK2(put_hevc_qpel_bi, 1, 1, put_hevc_qpel_bi_hv)
+        PEL_LINK1(put_hevc_epel_uni_w, 1, 0, put_hevc_epel_uni_w_v)
+        PEL_LINK1(put_hevc_qpel_uni_w, 1, 0, put_hevc_qpel_uni_w_v)
+        c->put_hevc_epel_uni_w[1][1][1] = put_hevc_epel_uni_w_hv4_8_e2k;
+        PEL_LINK2(put_hevc_epel_uni_w, 1, 1, put_hevc_epel_uni_w_hv)
+        PEL_LINK2(put_hevc_qpel_uni_w, 1, 1, put_hevc_qpel_uni_w_hv)
+        // }
+    } else if (bit_depth == 10) {
+        c->idct[0] = hevc_idct_4x4_10_e2k;
+        c->idct[1] = hevc_idct_8x8_10_e2k;
+        c->idct[2] = hevc_idct_16x16_10_e2k;
+        c->idct[3] = hevc_idct_32x32_10_e2k;
+    }
+}
diff --git a/libavcodec/e2k/hpeldsp.c b/libavcodec/e2k/hpeldsp.c
new file mode 100644
index 0000000..7ec8832
--- /dev/null
+++ b/libavcodec/e2k/hpeldsp.c
@@ -0,0 +1,489 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/hpeldsp.h"
+
+#include "hpeldsp.h"
+
+#define FUNC(name) void name##_e2k(uint8_t *block, const uint8_t *pixels, \
+                                   ptrdiff_t line_size, int h)
+
+static FUNC(put_pixels8)
+{
+    int y; __m64 v0, v1, v2, v3;
+
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < h; y += 4) {
+        v0 = *(__m64*)(pixels);
+        v1 = *(__m64*)(pixels + line_size);
+        v2 = *(__m64*)(pixels + line_size * 2);
+        v3 = *(__m64*)(pixels + line_size * 3);
+        *(__m64*)block = v0;
+        *(__m64*)(block + line_size) = v1;
+        *(__m64*)(block + line_size * 2) = v2;
+        *(__m64*)(block + line_size * 3) = v3;
+        pixels += line_size * 4;
+        block += line_size * 4;
+    }
+}
+
+FUNC(ff_put_pixels16)
+{
+    int y; __m128i v0, v1, v2, v3;
+
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < h; y += 4) {
+        v0 = VEC_LD(pixels);
+        v1 = VEC_LD(pixels + line_size);
+        v2 = VEC_LD(pixels + line_size * 2);
+        v3 = VEC_LD(pixels + line_size * 3);
+        VEC_ST(block, v0);
+        VEC_ST(block + line_size, v1);
+        VEC_ST(block + line_size * 2, v2);
+        VEC_ST(block + line_size * 3, v3);
+        pixels += line_size * 4;
+        block += line_size * 4;
+    }
+}
+
+static FUNC(avg_pixels8)
+{
+    int y; __m64 v0, v1;
+
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < h; y++) {
+        v0 = *(__m64*)pixels;
+        v1 = *(__m64*)block;
+        v0 = _mm_avg_pu8(v0, v1);
+        *(__m64*)block = v0;
+        pixels += line_size;
+        block += line_size;
+    }
+}
+
+FUNC(ff_avg_pixels16)
+{
+    int y; __m128i v0, v1;
+
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < h; y++) {
+        v0 = VEC_LD(pixels);
+        v1 = VEC_LD(block);
+        v0 = _mm_avg_epu8(v0, v1);
+        VEC_ST(block, v0);
+        pixels += line_size;
+        block += line_size;
+    }
+}
+
+static FUNC(put_pixels8_x2)
+{
+    int y; __m64 v0, v1;
+
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < h; y++) {
+        v0 = *(__m64*)pixels;
+        v1 = *(__m64*)(pixels + 1);
+        v0 = _mm_avg_pu8(v0, v1);
+        *(__m64*)block = v0;
+        pixels += line_size;
+        block += line_size;
+    }
+}
+
+static FUNC(avg_pixels8_x2)
+{
+    int y; __m64 v0, v1;
+
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < h; y++) {
+        v0 = *(__m64*)pixels;
+        v1 = *(__m64*)(pixels + 1);
+        v0 = _mm_avg_pu8(v0, v1);
+        v1 = *(__m64*)block;
+        v0 = _mm_avg_pu8(v0, v1);
+        *(__m64*)block = v0;
+        pixels += line_size;
+        block += line_size;
+    }
+}
+
+static FUNC(put_no_rnd_pixels8_x2)
+{
+    int y; __m64 v0, v1, c1 = _mm_set1_pi8(-1);
+
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < h; y++) {
+        v0 = *(__m64*)pixels;
+        v1 = *(__m64*)(pixels + 1);
+        v0 = _mm_xor_si64(v0, c1);
+        v1 = _mm_xor_si64(v1, c1);
+        v0 = _mm_avg_pu8(v0, v1);
+        v0 = _mm_xor_si64(v0, c1);
+        *(__m64*)block = v0;
+        pixels += line_size;
+        block += line_size;
+    }
+}
+
+static FUNC(put_pixels8_y2)
+{
+    int y; __m64 v0, v1, v2;
+
+    v1 = *(__m64*)pixels;
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < h; y++) {
+        pixels += line_size;
+        v0 = *(__m64*)pixels;
+        v2 = _mm_avg_pu8(v0, v1);
+        v1 = v0;
+        *(__m64*)block = v2;
+        block += line_size;
+    }
+}
+
+static FUNC(avg_pixels8_y2)
+{
+    int y; __m64 v0, v1, v2;
+
+    v1 = *(__m64*)pixels;
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < h; y++) {
+        pixels += line_size;
+        v0 = *(__m64*)pixels;
+        v2 = _mm_avg_pu8(v0, v1);
+        v1 = v0;
+        v0 = *(__m64*)block;
+        v2 = _mm_avg_pu8(v2, v0);
+        *(__m64*)block = v2;
+        block += line_size;
+    }
+}
+
+static FUNC(put_no_rnd_pixels8_y2)
+{
+    int y; __m64 v0, v1, v2, c1 = _mm_set1_pi8(-1);
+
+    v1 = *(__m64*)pixels;
+    v1 = _mm_xor_si64(v1, c1);
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < h; y++) {
+        pixels += line_size;
+        v0 = *(__m64*)pixels;
+        v0 = _mm_xor_si64(v0, c1);
+        v2 = _mm_avg_pu8(v0, v1);
+        v2 = _mm_xor_si64(v2, c1);
+        v1 = v0;
+        *(__m64*)block = v2;
+        block += line_size;
+    }
+}
+
+static FUNC(put_pixels16_x2)
+{
+    int y; __m128i v0, v1;
+
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < h; y++) {
+        v0 = VEC_LD(pixels);
+        v1 = VEC_LD(pixels + 1);
+        v0 = _mm_avg_epu8(v0, v1);
+        VEC_ST(block, v0);
+        pixels += line_size;
+        block += line_size;
+    }
+}
+
+static FUNC(avg_pixels16_x2)
+{
+    int y; __m128i v0, v1;
+
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < h; y++) {
+        v0 = VEC_LD(pixels);
+        v1 = VEC_LD(pixels + 1);
+        v0 = _mm_avg_epu8(v0, v1);
+        v1 = VEC_LD(block);
+        v0 = _mm_avg_epu8(v0, v1);
+        VEC_ST(block, v0);
+        pixels += line_size;
+        block += line_size;
+    }
+}
+
+static FUNC(put_no_rnd_pixels16_x2)
+{
+    int y; __m128i v0, v1, c1 = _mm_set1_epi8(-1);
+
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < h; y++) {
+        v0 = VEC_LD(pixels);
+        v1 = VEC_LD(pixels + 1);
+        v0 = _mm_xor_si128(v0, c1);
+        v1 = _mm_xor_si128(v1, c1);
+        v0 = _mm_avg_epu8(v0, v1);
+        v0 = _mm_xor_si128(v0, c1);
+        VEC_ST(block, v0);
+        pixels += line_size;
+        block += line_size;
+    }
+}
+
+static FUNC(put_pixels16_y2)
+{
+    int y; __m128i v0, v1, v2;
+
+    v1 = VEC_LD(pixels);
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < h; y++) {
+        pixels += line_size;
+        v0 = VEC_LD(pixels);
+        v2 = _mm_avg_epu8(v0, v1);
+        v1 = v0;
+        VEC_ST(block, v2);
+        block += line_size;
+    }
+}
+
+static FUNC(avg_pixels16_y2)
+{
+    int y; __m128i v0, v1, v2;
+
+    v1 = VEC_LD(pixels);
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < h; y++) {
+        pixels += line_size;
+        v0 = VEC_LD(pixels);
+        v2 = _mm_avg_epu8(v0, v1);
+        v1 = v0;
+        v0 = VEC_LD(block);
+        v2 = _mm_avg_epu8(v2, v0);
+        VEC_ST(block, v2);
+        block += line_size;
+    }
+}
+
+static FUNC(put_no_rnd_pixels16_y2)
+{
+    int y; __m128i v0, v1, v2, c1 = _mm_set1_epi8(-1);
+
+    v1 = VEC_LD(pixels);
+    v1 = _mm_xor_si128(v1, c1);
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < h; y++) {
+        pixels += line_size;
+        v0 = VEC_LD(pixels);
+        v0 = _mm_xor_si128(v0, c1);
+        v2 = _mm_avg_epu8(v0, v1);
+        v2 = _mm_xor_si128(v2, c1);
+        v1 = v0;
+        VEC_ST(block, v2);
+        block += line_size;
+    }
+}
+
+#define PIXELS_X2(v2, v3) \
+    v1 = *(__m64*)pixels; \
+    v5 = *(__m64*)(pixels + 1); \
+    v0 = _mm_unpacklo_pi8(v1, vzero); \
+    v1 = _mm_unpackhi_pi8(v1, vzero); \
+    v4 = _mm_unpacklo_pi8(v5, vzero); \
+    v5 = _mm_unpackhi_pi8(v5, vzero); \
+    v2 = _mm_add_pi16(v0, v4); \
+    v3 = _mm_add_pi16(v1, v5);
+
+static FUNC(put_pixels8_xy2)
+{
+    int y; __m64 v0, v1, v2, v3, v4, v5, vzero = _mm_setzero_si64();
+    __m64 vbias = _mm_set1_pi16(2);
+    PIXELS_X2(v2, v3)
+
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < h; y++) {
+        pixels += line_size;
+        PIXELS_X2(v0, v1)
+        v2 = _mm_add_pi16(v2, vbias);
+        v3 = _mm_add_pi16(v3, vbias);
+        v4 = _mm_srai_pi16(_mm_add_pi16(v0, v2), 2);
+        v5 = _mm_srai_pi16(_mm_add_pi16(v1, v3), 2);
+        v2 = v0; v3 = v1;
+        v4 = _mm_packs_pu16(v4, v5);
+        *(__m64*)block = v4;
+        block += line_size;
+    }
+}
+
+static FUNC(avg_pixels8_xy2)
+{
+    int y; __m64 v0, v1, v2, v3, v4, v5, vzero = _mm_setzero_si64();
+    __m64 vbias = _mm_set1_pi16(2);
+    PIXELS_X2(v2, v3)
+
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < h; y++) {
+        pixels += line_size;
+        PIXELS_X2(v0, v1)
+        v2 = _mm_add_pi16(v2, vbias);
+        v3 = _mm_add_pi16(v3, vbias);
+        v4 = _mm_srai_pi16(_mm_add_pi16(v0, v2), 2);
+        v5 = _mm_srai_pi16(_mm_add_pi16(v1, v3), 2);
+        v2 = v0; v3 = v1;
+        v4 = _mm_packs_pu16(v4, v5);
+        v5 = *(__m64*)block;
+        v4 = _mm_avg_pu8(v4, v5);
+        *(__m64*)block = v4;
+        block += line_size;
+    }
+}
+
+static FUNC(put_no_rnd_pixels8_xy2)
+{
+    int y; __m64 v0, v1, v2, v3, v4, v5, vzero = _mm_setzero_si64();
+    PIXELS_X2(v2, v3)
+
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < h; y++) {
+        pixels += line_size;
+        PIXELS_X2(v0, v1)
+        v4 = _mm_srai_pi16(_mm_avg_pu16(v0, v2), 1);
+        v5 = _mm_srai_pi16(_mm_avg_pu16(v1, v3), 1);
+        v2 = v0; v3 = v1;
+        v4 = _mm_packs_pu16(v4, v5);
+        *(__m64*)block = v4;
+        block += line_size;
+    }
+}
+
+#undef PIXELS_X2
+
+#define PIXELS_X2(v2, v3) \
+    v1 = VEC_LD(pixels); \
+    v5 = VEC_LD(pixels + 1); \
+    v0 = _mm_unpacklo_epi8(v1, vzero); \
+    v1 = _mm_unpackhi_epi8(v1, vzero); \
+    v4 = _mm_unpacklo_epi8(v5, vzero); \
+    v5 = _mm_unpackhi_epi8(v5, vzero); \
+    v2 = _mm_add_epi16(v0, v4); \
+    v3 = _mm_add_epi16(v1, v5);
+
+static FUNC(put_pixels16_xy2)
+{
+    int y; __m128i v0, v1, v2, v3, v4, v5, vzero = _mm_setzero_si128();
+    __m128i vbias = _mm_set1_epi16(2);
+    PIXELS_X2(v2, v3)
+
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < h; y++) {
+        pixels += line_size;
+        PIXELS_X2(v0, v1)
+        v2 = _mm_add_epi16(v2, vbias);
+        v3 = _mm_add_epi16(v3, vbias);
+        v4 = _mm_srai_epi16(_mm_add_epi16(v0, v2), 2);
+        v5 = _mm_srai_epi16(_mm_add_epi16(v1, v3), 2);
+        v2 = v0; v3 = v1;
+        v4 = _mm_packus_epi16(v4, v5);
+        VEC_ST(block, v4);
+        block += line_size;
+    }
+}
+
+static FUNC(avg_pixels16_xy2)
+{
+    int y; __m128i v0, v1, v2, v3, v4, v5, vzero = _mm_setzero_si128();
+    __m128i vbias = _mm_set1_epi16(2);
+    PIXELS_X2(v2, v3)
+
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < h; y++) {
+        pixels += line_size;
+        PIXELS_X2(v0, v1)
+        v2 = _mm_add_epi16(v2, vbias);
+        v3 = _mm_add_epi16(v3, vbias);
+        v4 = _mm_srai_epi16(_mm_add_epi16(v0, v2), 2);
+        v5 = _mm_srai_epi16(_mm_add_epi16(v1, v3), 2);
+        v2 = v0; v3 = v1;
+        v4 = _mm_packus_epi16(v4, v5);
+        v5 = VEC_LD(block);
+        v4 = _mm_avg_epu8(v4, v5);
+        VEC_ST(block, v4);
+        block += line_size;
+    }
+}
+
+static FUNC(put_no_rnd_pixels16_xy2)
+{
+    int y; __m128i v0, v1, v2, v3, v4, v5, vzero = _mm_setzero_si128();
+    PIXELS_X2(v2, v3)
+
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < h; y++) {
+        pixels += line_size;
+        PIXELS_X2(v0, v1)
+        v4 = _mm_srai_epi16(_mm_avg_epu16(v0, v2), 1);
+        v5 = _mm_srai_epi16(_mm_avg_epu16(v1, v3), 1);
+        v2 = v0; v3 = v1;
+        v4 = _mm_packus_epi16(v4, v5);
+        VEC_ST(block, v4);
+        block += line_size;
+    }
+}
+
+#undef PIXELS_X2
+
+av_cold void ff_hpeldsp_init_e2k(HpelDSPContext *c, int flags)
+{
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+
+    c->avg_pixels_tab[0][0] = ff_avg_pixels16_e2k;
+    c->avg_pixels_tab[0][1] = avg_pixels16_x2_e2k; 
+    c->avg_pixels_tab[0][2] = avg_pixels16_y2_e2k; 
+    c->avg_pixels_tab[0][3] = avg_pixels16_xy2_e2k; 
+    c->avg_pixels_tab[1][0] = avg_pixels8_e2k;
+    c->avg_pixels_tab[1][1] = avg_pixels8_x2_e2k;
+    c->avg_pixels_tab[1][2] = avg_pixels8_y2_e2k;
+    c->avg_pixels_tab[1][3] = avg_pixels8_xy2_e2k; // fate vsynth1-mpeg2-422
+
+    c->put_pixels_tab[0][0] = ff_put_pixels16_e2k;
+    c->put_pixels_tab[0][1] = put_pixels16_x2_e2k;
+    c->put_pixels_tab[0][2] = put_pixels16_y2_e2k;
+    c->put_pixels_tab[0][3] = put_pixels16_xy2_e2k;
+    c->put_pixels_tab[1][0] = put_pixels8_e2k;
+    c->put_pixels_tab[1][1] = put_pixels8_x2_e2k;
+    c->put_pixels_tab[1][2] = put_pixels8_y2_e2k;
+    c->put_pixels_tab[1][3] = put_pixels8_xy2_e2k;
+
+    c->put_no_rnd_pixels_tab[0][0] = ff_put_pixels16_e2k;
+    c->put_no_rnd_pixels_tab[0][1] = put_no_rnd_pixels16_x2_e2k;
+    c->put_no_rnd_pixels_tab[0][2] = put_no_rnd_pixels16_y2_e2k;
+    c->put_no_rnd_pixels_tab[0][3] = put_no_rnd_pixels16_xy2_e2k;
+    c->put_no_rnd_pixels_tab[1][0] = put_pixels8_e2k;
+    c->put_no_rnd_pixels_tab[1][1] = put_no_rnd_pixels8_x2_e2k;
+    c->put_no_rnd_pixels_tab[1][2] = put_no_rnd_pixels8_y2_e2k;
+    c->put_no_rnd_pixels_tab[1][3] = put_no_rnd_pixels8_xy2_e2k;
+}
diff --git a/libavcodec/e2k/hpeldsp.h b/libavcodec/e2k/hpeldsp.h
new file mode 100644
index 0000000..0ade264
--- /dev/null
+++ b/libavcodec/e2k/hpeldsp.h
@@ -0,0 +1,30 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_E2K_HPELDSP_H
+#define AVCODEC_E2K_HPELDSP_H
+
+#include <stddef.h>
+#include <stdint.h>
+
+void ff_avg_pixels16_e2k(uint8_t *block, const uint8_t *pixels,
+                         ptrdiff_t line_size, int h);
+void ff_put_pixels16_e2k(uint8_t *block, const uint8_t *pixels,
+                         ptrdiff_t line_size, int h);
+
+#endif /* AVCODEC_E2K_HPELDSP_H */
diff --git a/libavcodec/e2k/idctdsp.c b/libavcodec/e2k/idctdsp.c
new file mode 100644
index 0000000..82a13c8
--- /dev/null
+++ b/libavcodec/e2k/idctdsp.c
@@ -0,0 +1,324 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2001 Michel Lespinasse
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include <stdlib.h>
+#include <string.h>
+
+#include "libavutil/attributes.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/avcodec.h"
+#include "libavcodec/idctdsp.h"
+#include "dctdsp.h"
+
+#define W1  22725  // cos(i*M_PI/16)*sqrt(2)*(1<<14) + 0.5
+#define W2  21407  // cos(i*M_PI/16)*sqrt(2)*(1<<14) + 0.5
+#define W3  19266  // cos(i*M_PI/16)*sqrt(2)*(1<<14) + 0.5
+#define W4  16383  // cos(i*M_PI/16)*sqrt(2)*(1<<14) + 0.5
+#define W5  12873  // cos(i*M_PI/16)*sqrt(2)*(1<<14) + 0.5
+#define W6  8867   // cos(i*M_PI/16)*sqrt(2)*(1<<14) + 0.5
+#define W7  4520   // cos(i*M_PI/16)*sqrt(2)*(1<<14) + 0.5
+
+#define ROW_SHIFT  11
+#define COL_SHIFT  20
+
+#define IDCT_ROW(i) \
+    v3 = _mm_load_si128((__m128i*)(block + i * 8)); \
+    v4 = v6; \
+    if ((_mm_movemask_epi8(_mm_cmpeq_epi16(v3, v5)) & 0xfffc) == 0xfffc) \
+        v4 = _mm_set1_epi32((int16_t)_mm_extract_epi16(v3, 0) + (1 << (ROW_SHIFT - 1))); \
+    v0 = _mm_unpacklo_epi64(v3, v3);  /* 0246 */ \
+    v1 = _mm_shuffle_epi32(v3, 0x11); /* 4602 */ \
+    v2 = _mm_shuffle_epi32(v3, 0xbb); /* 5713 */ \
+    v3 = _mm_unpackhi_epi64(v3, v3);  /* 1357 */ \
+    v0 = _mm_madd_epi16(v0, a0); \
+    v1 = _mm_madd_epi16(v1, a1); \
+    v2 = _mm_madd_epi16(v2, a2); \
+    v3 = _mm_madd_epi16(v3, a3); \
+    v0 = _mm_add_epi32(v0, v1); \
+    v1 = _mm_add_epi32(v2, v3); \
+    v0 = _mm_add_epi32(v0, v4); \
+    v2 = _mm_srai_epi32(_mm_add_epi32(v0, v1), ROW_SHIFT); \
+    v3 = _mm_srai_epi32(_mm_sub_epi32(v0, v1), ROW_SHIFT); \
+    x##i = _mm_packs_epi32(v2, _mm_shuffle_epi32(v3, 0x1b));
+
+#define CONST_PAIR(a, b) _mm_set1_epi32(b << 16 | (a & 0xffff))
+#define IDCT_COL_FIN(x0, x1, a0, b0, a4, b4) \
+    x0 = _mm_srai_epi32(_mm_add_epi32(a0, b0), COL_SHIFT); \
+    t0 = _mm_srai_epi32(_mm_add_epi32(a4, b4), COL_SHIFT); \
+    x1 = _mm_srai_epi32(_mm_sub_epi32(a0, b0), COL_SHIFT); \
+    t1 = _mm_srai_epi32(_mm_sub_epi32(a4, b4), COL_SHIFT); \
+    x0 = _mm_packs_epi32(x0, t0); \
+    x1 = _mm_packs_epi32(x1, t1);
+
+#define SIMPLE_IDCT \
+    __m128i x0, x1, x2, x3, x4, x5, x6, x7, t0, t1, t2, t3; \
+    __m128i a0, a1, a2, a3, a4, a5, a6, a7; \
+    __m128i b0, b1, b2, b3, b4, b5, b6, b7; \
+    __m128i v0, v1, v2, v3, v4, v5, v6, v7; \
+    \
+    a0 = _mm_setr_epi16(W4, W2, -W4, -W2,  W4, -W6, W4, -W6); \
+    a1 = _mm_setr_epi16(W4, W6,  W4,  W6, -W4,  W2, W4, -W2); \
+    a2 = _mm_setr_epi16(W5, W7,  W3, -W7,  W7,  W3, W7, -W5); \
+    a3 = _mm_setr_epi16(W1, W3, -W1, -W5,  W5, -W1, W3, -W1); \
+    v5 = _mm_setzero_si128(); \
+    v6 = _mm_set1_epi32(1 << (ROW_SHIFT - 1)); \
+    IDCT_ROW(0) IDCT_ROW(1) IDCT_ROW(2) IDCT_ROW(3) \
+    IDCT_ROW(4) IDCT_ROW(5) IDCT_ROW(6) IDCT_ROW(7) \
+    \
+    t0 = _mm_set1_epi16((1 << (COL_SHIFT - 1)) / W4); \
+    x0 = _mm_add_epi16(x0, t0); \
+    t0 = CONST_PAIR(W4,  W4); v4 = _mm_unpacklo_epi16(x0, x4); \
+    t1 = CONST_PAIR(W4, -W4); x4 = _mm_unpackhi_epi16(x0, x4); \
+    t2 = CONST_PAIR(W2,  W6); v6 = _mm_unpacklo_epi16(x2, x6); \
+    t3 = CONST_PAIR(W6, -W2); x6 = _mm_unpackhi_epi16(x2, x6); \
+    v0 = _mm_madd_epi16(v4, t0); x0 = _mm_madd_epi16(x4, t0); /* a0, a3 */ \
+    v4 = _mm_madd_epi16(v4, t1); x4 = _mm_madd_epi16(x4, t1); /* a1, a2 */ \
+    v2 = _mm_madd_epi16(v6, t2); x2 = _mm_madd_epi16(x6, t2); /* a0, -a3 */ \
+    v6 = _mm_madd_epi16(v6, t3); x6 = _mm_madd_epi16(x6, t3); /* a1, -a2 */ \
+    a0 = _mm_add_epi32(v0, v2); a4 = _mm_add_epi32(x0, x2); \
+    a1 = _mm_add_epi32(v4, v6); a5 = _mm_add_epi32(x4, x6); \
+    a2 = _mm_sub_epi32(v4, v6); a6 = _mm_sub_epi32(x4, x6); \
+    a3 = _mm_sub_epi32(v0, v2); a7 = _mm_sub_epi32(x0, x2); \
+    \
+    t0 = CONST_PAIR(W1,  W7); v7 = _mm_unpacklo_epi16(x1, x7); \
+    t1 = CONST_PAIR(W7, -W1); x7 = _mm_unpackhi_epi16(x1, x7); \
+    t2 = CONST_PAIR(W5,  W3); v3 = _mm_unpacklo_epi16(x5, x3); \
+    t3 = CONST_PAIR(W3, -W5); x3 = _mm_unpackhi_epi16(x5, x3); \
+    b0 = _mm_add_epi32(_mm_madd_epi16(v7, t0), _mm_madd_epi16(v3, t2)); \
+    b1 = _mm_sub_epi32(_mm_madd_epi16(v7, t3), _mm_madd_epi16(v3, t0)); \
+    b2 = _mm_add_epi32(_mm_madd_epi16(v7, t2), _mm_madd_epi16(v3, t1)); \
+    b3 = _mm_add_epi32(_mm_madd_epi16(v7, t1), _mm_madd_epi16(v3, t3)); \
+    b4 = _mm_add_epi32(_mm_madd_epi16(x7, t0), _mm_madd_epi16(x3, t2)); \
+    b5 = _mm_sub_epi32(_mm_madd_epi16(x7, t3), _mm_madd_epi16(x3, t0)); \
+    b6 = _mm_add_epi32(_mm_madd_epi16(x7, t2), _mm_madd_epi16(x3, t1)); \
+    b7 = _mm_add_epi32(_mm_madd_epi16(x7, t1), _mm_madd_epi16(x3, t3)); \
+    \
+    IDCT_COL_FIN(x0, x7, a0, b0, a4, b4) \
+    IDCT_COL_FIN(x1, x6, a1, b1, a5, b5) \
+    IDCT_COL_FIN(x2, x5, a2, b2, a6, b6) \
+    IDCT_COL_FIN(x3, x4, a3, b3, a7, b7)
+
+#define SAVE_ROW(i) \
+    _mm_store_si128((__m128i*)(block + 8 * i), x##i);
+
+#define PUT(a, b)                             \
+    t0 = _mm_packus_epi16(x##a, x##b);        \
+    VEC_STL(pixels, t0); pixels += line_size; \
+    VEC_STH(pixels, t0); pixels += line_size;
+
+#define ADD(a, b)                             \
+    t0 = VEC_LD8(pixels);                     \
+    t1 = VEC_LD8(pixels + line_size);         \
+    t0 = _mm_unpacklo_epi8(t0, t2);           \
+    t1 = _mm_unpacklo_epi8(t1, t2);           \
+    t0 = _mm_adds_epi16(t0, x##a);            \
+    t1 = _mm_adds_epi16(t1, x##b);            \
+    t0 = _mm_packus_epi16(t0, t1);            \
+    VEC_STL(pixels, t0); pixels += line_size; \
+    VEC_STH(pixels, t0); pixels += line_size;
+
+void ff_simple_idct_e2k(int16_t *block) {
+    SIMPLE_IDCT
+    SAVE_ROW(0) SAVE_ROW(1) SAVE_ROW(2) SAVE_ROW(3)
+    SAVE_ROW(4) SAVE_ROW(5) SAVE_ROW(6) SAVE_ROW(7)
+}
+
+static void simple_idct_put_e2k(uint8_t *pixels, ptrdiff_t line_size, int16_t *block)
+{
+    SIMPLE_IDCT
+    PUT(0, 1) PUT(2, 3) PUT(4, 5) PUT(6, 7)
+}
+
+static void simple_idct_add_e2k(uint8_t *pixels, ptrdiff_t line_size, int16_t *block)
+{
+    SIMPLE_IDCT
+    t2 = _mm_setzero_si128();
+    ADD(0, 1) ADD(2, 3) ADD(4, 5) ADD(6, 7)
+}
+
+/* NOTE: This code is based on GPL code from the libmpeg2 project.  The
+ * author, Michel Lespinasses, has given explicit permission to release
+ * under LGPL as part of FFmpeg.
+ *
+ * FFmpeg integration by Dieter Shirley
+ *
+ * This file is a direct copy of the AltiVec IDCT module from the libmpeg2
+ * project.  I've deleted all of the libmpeg2-specific code, renamed the
+ * functions and reordered the function parameters.  The only change to the
+ * IDCT function itself was to factor out the partial transposition, and to
+ * perform a full transpose at the end of the function. */
+
+#define FAST_IDCT_HALF                                     \
+    /* 1st stage */                                        \
+    t1 = _mm_adds_epi16(_mm_mulhrs_epi16(a1, x7), x1);     \
+    t6 = _mm_subs_epi16(_mm_mulhrs_epi16(a1, x1), x7);     \
+    t7 = _mm_adds_epi16(_mm_mulhrs_epi16(a2, x5), x3);     \
+    t3 = _mm_adds_epi16(_mm_mulhrs_epi16(ma2, x3), x5);    \
+                                                           \
+    /* 2nd stage */                                        \
+    t5 = _mm_adds_epi16(x0, x4);                           \
+    t0 = _mm_subs_epi16(x0, x4);                           \
+    t2 = _mm_adds_epi16(_mm_mulhrs_epi16(a0, x6), x2);     \
+    t4 = _mm_subs_epi16(_mm_mulhrs_epi16(a0, x2), x6);     \
+    x6 = _mm_adds_epi16(t6, t3);                           \
+    t3 = _mm_subs_epi16(t6, t3);                           \
+    t6 = _mm_subs_epi16(t1, t7);                           \
+    x1 = _mm_adds_epi16(t1, t7);                           \
+                                                           \
+    /* 3rd stage */                                        \
+    x7 = _mm_adds_epi16(t5, t2);                           \
+    x2 = _mm_subs_epi16(t5, t2);                           \
+    x5 = _mm_adds_epi16(t0, t4);                           \
+    x0 = _mm_subs_epi16(t0, t4);                           \
+    x4 = _mm_subs_epi16(t6, t3);                           \
+    x3 = _mm_adds_epi16(t6, t3);                           \
+                                                           \
+    /* 4th stage */                                        \
+    t0 = _mm_adds_epi16(x7, x1);                           \
+    t7 = _mm_subs_epi16(x7, x1);                           \
+    t1 = _mm_adds_epi16(_mm_mulhrs_epi16(c4, x3), x5);     \
+    t6 = _mm_adds_epi16(_mm_mulhrs_epi16(mc4, x3), x5);    \
+    t2 = _mm_adds_epi16(_mm_mulhrs_epi16(c4, x4), x0);     \
+    t5 = _mm_adds_epi16(_mm_mulhrs_epi16(mc4, x4), x0);    \
+    t3 = _mm_adds_epi16(x2, x6);                           \
+    t4 = _mm_subs_epi16(x2, x6);
+
+#define FAST_IDCT                                                    \
+    __m128i x0, x1, x2, x3, x4, x5, x6, x7, *p = (__m128i*)block;    \
+    __m128i t0, t1, t2, t3, t4, t5, t6, t7;                          \
+                                                                     \
+    __m128i c4  = _mm_set1_epi16(23170);                             \
+    __m128i a0  = _mm_set1_epi16(13573);                             \
+    __m128i a1  = _mm_set1_epi16(6518);                              \
+    __m128i a2  = _mm_set1_epi16(21895);                             \
+    __m128i mc4 = _mm_set1_epi16(-23170);                            \
+    __m128i ma2 = _mm_set1_epi16(-21895);                            \
+                                                                     \
+    t0 = _mm_setr_epi16(16384, 22725, 21407, 19266, 16384, 19266, 21407, 22725); \
+    t1 = _mm_setr_epi16(22725, 31521, 29692, 26722, 22725, 26722, 29692, 31521); \
+    t2 = _mm_setr_epi16(21407, 29692, 27969, 25172, 21407, 25172, 27969, 29692); \
+    t3 = _mm_setr_epi16(19266, 26722, 25172, 22654, 19266, 22654, 25172, 26722); \
+                                                                     \
+    x0 = _mm_mulhrs_epi16(_mm_slli_epi16(p[0], 4), t0);              \
+    x1 = _mm_mulhrs_epi16(_mm_slli_epi16(p[1], 4), t1);              \
+    x2 = _mm_mulhrs_epi16(_mm_slli_epi16(p[2], 4), t2);              \
+    x3 = _mm_mulhrs_epi16(_mm_slli_epi16(p[3], 4), t3);              \
+    x4 = _mm_mulhrs_epi16(_mm_slli_epi16(p[4], 4), t0);              \
+    x5 = _mm_mulhrs_epi16(_mm_slli_epi16(p[5], 4), t3);              \
+    x6 = _mm_mulhrs_epi16(_mm_slli_epi16(p[6], 4), t2);              \
+    x7 = _mm_mulhrs_epi16(_mm_slli_epi16(p[7], 4), t1);              \
+                                                                     \
+    FAST_IDCT_HALF                                                   \
+                                                                     \
+    x0 = _mm_unpacklo_epi16(t0, t2);                                 \
+    x1 = _mm_unpackhi_epi16(t0, t2);                                 \
+    x2 = _mm_unpacklo_epi16(t1, t3);                                 \
+    x3 = _mm_unpackhi_epi16(t1, t3);                                 \
+    x4 = _mm_unpacklo_epi16(t4, t6);                                 \
+    x5 = _mm_unpackhi_epi16(t4, t6);                                 \
+    x6 = _mm_unpacklo_epi16(t5, t7);                                 \
+    x7 = _mm_unpackhi_epi16(t5, t7);                                 \
+                                                                     \
+    t0 = _mm_unpacklo_epi16(x0, x2);                                 \
+    t1 = _mm_unpackhi_epi16(x0, x2);                                 \
+    t2 = _mm_unpacklo_epi16(x1, x3);                                 \
+    t3 = _mm_unpackhi_epi16(x1, x3);                                 \
+    t4 = _mm_unpacklo_epi16(x4, x6);                                 \
+    t5 = _mm_unpackhi_epi16(x4, x6);                                 \
+    t6 = _mm_unpacklo_epi16(x5, x7);                                 \
+    t7 = _mm_unpackhi_epi16(x5, x7);                                 \
+                                                                     \
+    x0 = _mm_unpacklo_epi64(t0, t4);                                 \
+    x1 = _mm_unpackhi_epi64(t0, t4);                                 \
+    x2 = _mm_unpacklo_epi64(t1, t5);                                 \
+    x3 = _mm_unpackhi_epi64(t1, t5);                                 \
+    x4 = _mm_unpacklo_epi64(t2, t6);                                 \
+    x5 = _mm_unpackhi_epi64(t2, t6);                                 \
+    x6 = _mm_unpacklo_epi64(t3, t7);                                 \
+    x7 = _mm_unpackhi_epi64(t3, t7);                                 \
+                                                                     \
+    x0 = _mm_adds_epi16(x0, _mm_set1_epi32(32 | 31 << 16));          \
+    FAST_IDCT_HALF                                                   \
+                                                                     \
+    x0 = _mm_srai_epi16(t0, 6);                                      \
+    x1 = _mm_srai_epi16(t1, 6);                                      \
+    x2 = _mm_srai_epi16(t2, 6);                                      \
+    x3 = _mm_srai_epi16(t3, 6);                                      \
+    x4 = _mm_srai_epi16(t4, 6);                                      \
+    x5 = _mm_srai_epi16(t5, 6);                                      \
+    x6 = _mm_srai_epi16(t6, 6);                                      \
+    x7 = _mm_srai_epi16(t7, 6);
+
+void ff_fast_idct_e2k(int16_t *block) {
+    FAST_IDCT
+    SAVE_ROW(0) SAVE_ROW(1) SAVE_ROW(2) SAVE_ROW(3)
+    SAVE_ROW(4) SAVE_ROW(5) SAVE_ROW(6) SAVE_ROW(7)
+}
+
+static void fast_idct_put_e2k(uint8_t *pixels, ptrdiff_t line_size, int16_t *block)
+{
+    FAST_IDCT
+    PUT(0, 1) PUT(2, 3) PUT(4, 5) PUT(6, 7)
+}
+
+static void fast_idct_add_e2k(uint8_t *pixels, ptrdiff_t line_size, int16_t *block)
+{
+    FAST_IDCT
+    t2 = _mm_setzero_si128();
+    ADD(0, 1) ADD(2, 3) ADD(4, 5) ADD(6, 7)
+}
+
+av_cold void ff_idctdsp_init_e2k(IDCTDSPContext *c, AVCodecContext *avctx,
+                                 unsigned high_bit_depth)
+{
+    int cpu_flags = av_get_cpu_flags();
+    if (!E2K_SIMD(cpu_flags))
+        return;
+
+    // !checkasm
+    // libavcodec/tests/dct -i
+
+    if (!high_bit_depth && avctx->lowres == 0) {
+        // FIXME: FATE tests fail because of a buggy "avidmeridianntsc.mov" test
+        // that incorrectly sets the -bitexact flag.
+        if (/*(avctx->idct_algo == FF_IDCT_AUTO && !(avctx->flags & AV_CODEC_FLAG_BITEXACT)) || */
+            avctx->idct_algo == FF_IDCT_ALTIVEC) {
+
+            c->idct      = ff_fast_idct_e2k;
+            c->idct_put  = fast_idct_put_e2k;
+            c->idct_add  = fast_idct_add_e2k;
+            c->perm_type = FF_IDCT_PERM_TRANSPOSE;
+        } else if (avctx->idct_algo == FF_IDCT_AUTO ||
+            avctx->idct_algo == FF_IDCT_SIMPLEAUTO ||
+            avctx->idct_algo == FF_IDCT_SIMPLE) {
+
+            c->idct      = ff_simple_idct_e2k;
+            c->idct_put  = simple_idct_put_e2k;
+            c->idct_add  = simple_idct_add_e2k;
+            // same as FF_IDCT_PERM_SSE2, but already initialized
+            c->perm_type = FF_IDCT_PERM_LIBMPEG2;
+        }
+    }
+}
diff --git a/libavcodec/e2k/lossless_audiodsp.c b/libavcodec/e2k/lossless_audiodsp.c
new file mode 100644
index 0000000..575f692
--- /dev/null
+++ b/libavcodec/e2k/lossless_audiodsp.c
@@ -0,0 +1,74 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2007 Luca Barbato <lu_zero@gentoo.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/lossless_audiodsp.h"
+
+#define GET_T(tt0, tt1, src, a, b) { \
+    tt0 = VEC_LD(src);               \
+    tt1 = VEC_LD(src + 8);           \
+}
+
+static int32_t scalarproduct_and_madd_int16_e2k(int16_t *v1,
+                                                const int16_t *v2,
+                                                const int16_t *v3,
+                                                int order, int mul)
+{
+    int i;
+    LOAD_ZERO;
+    vec_s16 *pv1 = (vec_s16*)v1;
+    vec_s16 muls = _mm_set1_epi16(mul);
+    vec_s16 t0, t1, i0, i1;
+    vec_s32 res = zerov;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < order; i += 16) {
+        GET_T(t0, t1, v2, i1, i2);
+        i0 = pv1[0];
+        i1 = pv1[1];
+        t0 = _mm_madd_epi16(t0, i0);
+        t1 = _mm_madd_epi16(t1, i1);
+        res = _mm_add_epi32(res, _mm_add_epi32(t0, t1));
+        GET_T(t0, t1, v3, i4, i3);
+        pv1[0] = _mm_add_epi16(_mm_mullo_epi16(t0, muls), i0);
+        pv1[1] = _mm_add_epi16(_mm_mullo_epi16(t1, muls), i1);
+        pv1 += 2;
+        v2 += 16;
+        v3 += 16;
+    }
+
+    res = _mm_hadd_epi32(res, res);
+    return _mm_extract_epi32(res, 0) + _mm_extract_epi32(res, 1);
+}
+
+av_cold void ff_llauddsp_init_e2k(LLAudDSPContext *c)
+{
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+    c->scalarproduct_and_madd_int16 = scalarproduct_and_madd_int16_e2k;
+}
diff --git a/libavcodec/e2k/lossless_videodsp.c b/libavcodec/e2k/lossless_videodsp.c
new file mode 100644
index 0000000..11cf1e1
--- /dev/null
+++ b/libavcodec/e2k/lossless_videodsp.c
@@ -0,0 +1,58 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2002 Brian Foley
+ * Copyright (c) 2002 Dieter Shirley
+ * Copyright (c) 2003-2004 Romain Dolbeau <romain@dolbeau.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/lossless_videodsp.h"
+
+static void add_bytes_e2k(uint8_t *dst, uint8_t *src, ptrdiff_t w)
+{
+    int i;
+    __m128i vdst, vsrc;
+
+    /* dst and src are 16 bytes-aligned (guaranteed). */
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i + 15 < w; i += 16) {
+        vdst = _mm_load_si128((const __m128i*)(dst + i));
+        vsrc = _mm_load_si128((const __m128i*)(src + i));
+        vdst = _mm_add_epi8(vsrc, vdst);
+        _mm_store_si128((__m128i*)(dst + i), vdst);
+    }
+    /* If w is not a multiple of 16. */
+    PRAGMA_E2K("ivdep")
+    for (; i < w; i++)
+        dst[i] = dst[i] + src[i];
+}
+
+av_cold void ff_llviddsp_init_e2k(LLVidDSPContext *c)
+{
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    // checkasm
+    c->add_bytes = add_bytes_e2k;
+}
diff --git a/libavcodec/e2k/me_cmp.c b/libavcodec/e2k/me_cmp.c
new file mode 100644
index 0000000..a1d8146
--- /dev/null
+++ b/libavcodec/e2k/me_cmp.c
@@ -0,0 +1,465 @@
+/*
+ * Copyright (C) 2021-2023 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2002 Brian Foley
+ * Copyright (c) 2002 Dieter Shirley
+ * Copyright (c) 2003-2004 Romain Dolbeau <romain@dolbeau.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/avcodec.h"
+#include "libavcodec/mpegvideo.h"
+#include "libavcodec/me_cmp.h"
+
+#define LOAD_PIX(v1, v2, pix) { \
+    v1 = VEC_LD(pix);           \
+    v2 = VEC_LD(pix + 1);       \
+}
+
+static int sad16_x2_e2k(MpegEncContext *v, const uint8_t *pix1, const uint8_t *pix2,
+                        ptrdiff_t stride, int h)
+{
+    int i;
+    __m128i v0, v1, v2, sum = _mm_setzero_si128();
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        LOAD_PIX(v1, v2, pix2);
+        v0 = VEC_LD(pix1);
+        v1 = _mm_avg_epu8(v1, v2);
+        sum = _mm_add_epi32(sum, _mm_sad_epu8(v0, v1));
+
+        pix1 += stride;
+        pix2 += stride;
+    }
+    return _mm_extract_epi32(sum, 0) + _mm_extract_epi32(sum, 2);
+}
+
+static int sad8_x2_e2k(MpegEncContext *v, const uint8_t *pix1, const uint8_t *pix2,
+                        ptrdiff_t stride, int h)
+{
+    int i;
+    __m64 v0, v1, v2, sum = _mm_setzero_si64();
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        v1 = *(__m64*)pix2;
+        v2 = *(__m64*)(pix2 + 1);
+        v0 = *(__m64*)pix1;
+        v1 = _mm_avg_pu8(v1, v2);
+        sum = _mm_add_pi32(sum, _mm_sad_pu8(v0, v1));
+
+        pix1 += stride;
+        pix2 += stride;
+    }
+    return _mm_extract_pi32(sum, 0);
+}
+
+static int sad16_y2_e2k(MpegEncContext *v, const uint8_t *pix1, const uint8_t *pix2,
+                        ptrdiff_t stride, int h)
+{
+    int i;
+    __m128i v0, v1, v2, sum = _mm_setzero_si128();
+
+    v2 = VEC_LD(pix2);
+    pix2 += stride;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        v1 = v2;
+        v2 = VEC_LD(pix2);
+        v0 = VEC_LD(pix1);
+        v1 = _mm_avg_epu8(v1, v2);
+        sum = _mm_add_epi32(sum, _mm_sad_epu8(v0, v1));
+        pix1 += stride;
+        pix2 += stride;
+    }
+    return _mm_extract_epi32(sum, 0) + _mm_extract_epi32(sum, 2);
+}
+
+static int sad8_y2_e2k(MpegEncContext *v, const uint8_t *pix1, const uint8_t *pix2,
+                        ptrdiff_t stride, int h)
+{
+    int i;
+    __m64 v0, v1, v2, sum = _mm_setzero_si64();
+
+    v2 = *(__m64*)pix2;
+    pix2 += stride;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        v1 = v2;
+        v2 = *(__m64*)pix2;
+        v0 = *(__m64*)pix1;
+        v1 = _mm_avg_pu8(v1, v2);
+        sum = _mm_add_pi32(sum, _mm_sad_pu8(v0, v1));
+        pix1 += stride;
+        pix2 += stride;
+    }
+    return _mm_extract_pi32(sum, 0);
+}
+
+static int sad16_xy2_e2k(MpegEncContext *v, const uint8_t *pix1, const uint8_t *pix2,
+                         ptrdiff_t stride, int h)
+{
+    int i;
+    LOAD_ZERO;
+    __m128i v0, v1, v2, sum = zerov;
+    __m128i t0, t1, t2, t3, t4, t5;
+    __m128i c2 = _mm_set1_epi16(2);
+
+    LOAD_PIX(v1, v2, pix2);
+    t2 = _mm_unpacklo_epi8(v1, zerov);
+    t3 = _mm_unpackhi_epi8(v1, zerov);
+    t4 = _mm_unpacklo_epi8(v2, zerov);
+    t5 = _mm_unpackhi_epi8(v2, zerov);
+    t2 = _mm_add_epi16(t2, t4);
+    t3 = _mm_add_epi16(t3, t5);
+    pix2 += stride;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        t0 = t2; t1 = t3;
+        LOAD_PIX(v1, v2, pix2);
+        v0 = VEC_LD(pix1);
+        t2 = _mm_unpacklo_epi8(v1, zerov);
+        t3 = _mm_unpackhi_epi8(v1, zerov);
+        t4 = _mm_unpacklo_epi8(v2, zerov);
+        t5 = _mm_unpackhi_epi8(v2, zerov);
+        t2 = _mm_add_epi16(t2, t4);
+        t3 = _mm_add_epi16(t3, t5);
+
+        v1 = _mm_srai_epi16(_mm_add_epi16(_mm_add_epi16(t0, t2), c2), 2);
+        v2 = _mm_srai_epi16(_mm_add_epi16(_mm_add_epi16(t1, t3), c2), 2);
+        v1 = _mm_packus_epi16(v1, v2);
+
+        sum = _mm_add_epi32(sum, _mm_sad_epu8(v0, v1));
+        pix1 += stride;
+        pix2 += stride;
+    }
+    return _mm_extract_epi32(sum, 0) + _mm_extract_epi32(sum, 2);
+}
+
+static int sad8_xy2_e2k(MpegEncContext *v, const uint8_t *pix1, const uint8_t *pix2,
+                         ptrdiff_t stride, int h)
+{
+    int i;
+    LOAD_ZERO;
+    __m64 v0, sum = _mm_movepi64_pi64(zerov);
+    __m128i v1, v2, t0, t1, t2, c2 = _mm_set1_epi16(2);
+
+    v1 = VEC_LD8(pix2);
+    v2 = VEC_LD8(pix2 + 1);
+    t1 = _mm_unpacklo_epi8(v1, zerov);
+    t2 = _mm_unpacklo_epi8(v2, zerov);
+    t1 = _mm_add_epi16(t1, t2);
+    pix2 += stride;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        t0 = t1;
+        v1 = VEC_LD8(pix2);
+        v2 = VEC_LD8(pix2 + 1);
+        v0 = *(__m64*)pix1;
+        t1 = _mm_unpacklo_epi8(v1, zerov);
+        t2 = _mm_unpacklo_epi8(v2, zerov);
+        t1 = _mm_add_epi16(t1, t2);
+
+        v1 = _mm_srai_epi16(_mm_add_epi16(_mm_add_epi16(t0, t1), c2), 2);
+        v1 = _mm_packus_epi16(v1, v1);
+
+        sum = _mm_add_pi32(sum, _mm_sad_pu8(v0, _mm_movepi64_pi64(v1)));
+        pix1 += stride;
+        pix2 += stride;
+    }
+    return _mm_extract_pi32(sum, 0);
+}
+
+static int sad16_e2k(MpegEncContext *v, const uint8_t *pix1, const uint8_t *pix2,
+                     ptrdiff_t stride, int h)
+{
+    int i;
+    __m128i v0, v1, sum = _mm_setzero_si128();
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        v0 = VEC_LD(pix1);
+        v1 = VEC_LD(pix2);
+        sum = _mm_add_epi32(sum, _mm_sad_epu8(v0, v1));
+        pix1 += stride;
+        pix2 += stride;
+    }
+    return _mm_extract_epi32(sum, 0) + _mm_extract_epi32(sum, 2);
+}
+
+static int sad8_e2k(MpegEncContext *v, const uint8_t *pix1, const uint8_t *pix2,
+                    ptrdiff_t stride, int h)
+{
+    int i;
+    __m64 v0, v1, sum = _mm_setzero_si64();
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        v0 = *(__m64*)pix1;
+        v1 = *(__m64*)pix2;
+        sum = _mm_add_pi32(sum, _mm_sad_pu8(v0, v1));
+        pix1 += stride;
+        pix2 += stride;
+    }
+    return _mm_extract_pi32(sum, 0);
+}
+
+/* Sum of Squared Errors for an 8x8 block. */
+static int sse8_e2k(MpegEncContext *v, const uint8_t *pix1, const uint8_t *pix2,
+                    ptrdiff_t stride, int h)
+{
+    int i;
+    LOAD_ZERO; 
+    __m128i v0, v1, sum = zerov;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        v0 = VEC_LD8(pix1);
+        v1 = VEC_LD8(pix2);
+        v0 = _mm_unpacklo_epi8(v0, zerov);
+        v1 = _mm_unpacklo_epi8(v1, zerov);
+        v0 = _mm_sub_epi16(v0, v1);
+        sum = _mm_add_epi32(sum, _mm_madd_epi16(v0, v0));
+        pix1 += stride;
+        pix2 += stride;
+    }
+    sum = _mm_hadd_epi32(sum, sum);
+    return _mm_extract_epi32(sum, 0) + _mm_extract_epi32(sum, 1);
+}
+
+/* Sum of Squared Errors for a 16x16 block. */
+static int sse16_e2k(MpegEncContext *v, const uint8_t *pix1, const uint8_t *pix2,
+                     ptrdiff_t stride, int h)
+{
+    int i;
+    LOAD_ZERO; 
+    __m128i v0, v1, v2, v3, sum = zerov;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        v2 = VEC_LD(pix1);
+        v3 = VEC_LD(pix2);
+        v0 = _mm_unpacklo_epi8(v2, zerov);
+        v1 = _mm_unpacklo_epi8(v3, zerov);
+        v2 = _mm_unpackhi_epi8(v2, zerov);
+        v3 = _mm_unpackhi_epi8(v3, zerov);
+        v0 = _mm_sub_epi16(v0, v1);
+        v2 = _mm_sub_epi16(v2, v3);
+        sum = _mm_add_epi32(sum, _mm_madd_epi16(v0, v0));
+        sum = _mm_add_epi32(sum, _mm_madd_epi16(v2, v2));
+        pix1 += stride;
+        pix2 += stride;
+    }
+    sum = _mm_hadd_epi32(sum, sum);
+    return _mm_extract_epi32(sum, 0) + _mm_extract_epi32(sum, 1);
+}
+
+#define HADAMARD8_FIN(t, sum) \
+    v0 = _mm_add_epi16(t##0, t##1); \
+    v1 = _mm_sub_epi16(t##0, t##1); \
+    v2 = _mm_add_epi16(t##2, t##3); \
+    v3 = _mm_sub_epi16(t##2, t##3); \
+    v4 = _mm_add_epi16(t##4, t##5); \
+    v5 = _mm_sub_epi16(t##4, t##5); \
+    v6 = _mm_add_epi16(t##6, t##7); \
+    v7 = _mm_sub_epi16(t##6, t##7); \
+    \
+    t0 = _mm_add_epi16(v0, v2); \
+    t2 = _mm_sub_epi16(v0, v2); \
+    t1 = _mm_add_epi16(v1, v3); \
+    t3 = _mm_sub_epi16(v1, v3); \
+    t4 = _mm_add_epi16(v4, v6); \
+    t6 = _mm_sub_epi16(v4, v6); \
+    t5 = _mm_add_epi16(v5, v7); \
+    t7 = _mm_sub_epi16(v5, v7); \
+    \
+    v0 = _mm_add_epi16(t0, t4); \
+    v4 = _mm_sub_epi16(t0, t4); \
+    v1 = _mm_add_epi16(t1, t5); \
+    v5 = _mm_sub_epi16(t1, t5); \
+    v2 = _mm_add_epi16(t2, t6); \
+    v6 = _mm_sub_epi16(t2, t6); \
+    v3 = _mm_add_epi16(t3, t7); \
+    v7 = _mm_sub_epi16(t3, t7); \
+    \
+    v0 = _mm_madd_epi16(_mm_abs_epi16(v0), onev); \
+    v1 = _mm_madd_epi16(_mm_abs_epi16(v1), onev); \
+    v2 = _mm_madd_epi16(_mm_abs_epi16(v2), onev); \
+    v3 = _mm_madd_epi16(_mm_abs_epi16(v3), onev); \
+    v4 = _mm_madd_epi16(_mm_abs_epi16(v4), onev); \
+    v5 = _mm_madd_epi16(_mm_abs_epi16(v5), onev); \
+    v6 = _mm_madd_epi16(_mm_abs_epi16(v6), onev); \
+    v7 = _mm_madd_epi16(_mm_abs_epi16(v7), onev); \
+    \
+    v0 = _mm_add_epi32(v0, v1); \
+    v2 = _mm_add_epi32(v2, v3); \
+    v4 = _mm_add_epi32(v4, v5); \
+    v6 = _mm_add_epi32(v6, v7); \
+    v0 = _mm_add_epi32(v0, v2); \
+    v4 = _mm_add_epi32(v4, v6); \
+    sum = _mm_add_epi32(v0, v4);
+
+static int hadamard8_diff8x8_e2k(MpegEncContext *s, const uint8_t *dst,
+                                 const uint8_t *src, ptrdiff_t stride, int h)
+{
+    LOAD_ZERO;
+    vec_s16 v0, v1, v2, v3, v4, v5, v6, v7;
+    vec_s16 t0, t1, t2, t3, t4, t5, t6, t7, sum;
+    const vec_s16 onev = _mm_set1_epi16(1);
+    const vec_s16 vprod1 = _mm_setr_epi16(1, -1, 1, -1, 1, -1, 1, -1);
+    const vec_s16 vprod2 = _mm_setr_epi16(1, 1, -1, -1, 1, 1, -1, -1);
+    const vec_s16 vprod3 = _mm_setr_epi16(1, 1, 1, 1, -1, -1, -1, -1);
+    const vec_u8 perm1 = _mm_setr_epi8(
+          0x02, 0x03, 0x00, 0x01, 0x06, 0x07, 0x04, 0x05,
+          0x0A, 0x0B, 0x08, 0x09, 0x0E, 0x0F, 0x0C, 0x0D);
+
+#define ITER(i) {                                         \
+    v0 = VEC_LD8(src + stride * i);                       \
+    v1 = VEC_LD8(dst + stride * i);                       \
+    v0 = _mm_unpacklo_epi8(v0, zerov);                    \
+    v1 = _mm_unpacklo_epi8(v1, zerov);                    \
+    v0 = _mm_sub_epi16(v0, v1);                           \
+    v1 = _mm_shuffle_epi8(v0, perm1);                     \
+    v0 = _mm_add_epi16(_mm_sign_epi16(v0, vprod1), v1);   \
+    v1 = _mm_shuffle_epi32(v0, 0xb1);                     \
+    v0 = _mm_add_epi16(_mm_sign_epi16(v0, vprod2), v1);   \
+    v1 = _mm_shuffle_epi32(v0, 0x4e);                     \
+    t##i = _mm_add_epi16(_mm_sign_epi16(v0, vprod3), v1); \
+}
+    ITER(0); ITER(1); ITER(2); ITER(3);
+    ITER(4); ITER(5); ITER(6); ITER(7);
+#undef ITER
+
+    HADAMARD8_FIN(t, sum)
+
+    sum = _mm_hadd_epi32(sum, sum);
+    return _mm_extract_epi32(sum, 0) + _mm_extract_epi32(sum, 1);
+}
+
+#if 1
+static int hadamard8_diff16_e2k(MpegEncContext *s, const uint8_t *dst,
+                                const uint8_t *src, ptrdiff_t stride, int h)
+{
+    LOAD_ZERO;
+    vec_s16 v0, v1, v2, v3, v4, v5, v6, v7;
+    vec_s16 x0, x1, x2, x3, x4, x5, x6, x7;
+    vec_s16 t0, t1, t2, t3, t4, t5, t6, t7, sum = zerov;
+    const vec_s16 onev = _mm_set1_epi16(1);
+    const vec_s16 vprod1 = _mm_setr_epi16(1, -1, 1, -1, 1, -1, 1, -1);
+    const vec_s16 vprod2 = _mm_setr_epi16(1, 1, -1, -1, 1, 1, -1, -1);
+    const vec_s16 vprod3 = _mm_setr_epi16(1, 1, 1, 1, -1, -1, -1, -1);
+    const vec_u8 perm1 = _mm_setr_epi8(
+          0x02, 0x03, 0x00, 0x01, 0x06, 0x07, 0x04, 0x05,
+          0x0A, 0x0B, 0x08, 0x09, 0x0E, 0x0F, 0x0C, 0x0D);
+
+#define ITER(i) {                                         \
+    v2 = VEC_LD(src + stride * i);                        \
+    v3 = VEC_LD(dst + stride * i);                        \
+    v0 = _mm_unpacklo_epi8(v2, zerov);                    \
+    v1 = _mm_unpacklo_epi8(v3, zerov);                    \
+    v2 = _mm_unpackhi_epi8(v2, zerov);                    \
+    v3 = _mm_unpackhi_epi8(v3, zerov);                    \
+    v0 = _mm_sub_epi16(v0, v1);                           \
+    v2 = _mm_sub_epi16(v2, v3);                           \
+    v1 = _mm_shuffle_epi8(v0, perm1);                     \
+    v3 = _mm_shuffle_epi8(v2, perm1);                     \
+    v0 = _mm_add_epi16(_mm_sign_epi16(v0, vprod1), v1);   \
+    v2 = _mm_add_epi16(_mm_sign_epi16(v2, vprod1), v3);   \
+    v1 = _mm_shuffle_epi32(v0, 0xb1);                     \
+    v3 = _mm_shuffle_epi32(v2, 0xb1);                     \
+    v0 = _mm_add_epi16(_mm_sign_epi16(v0, vprod2), v1);   \
+    v2 = _mm_add_epi16(_mm_sign_epi16(v2, vprod2), v3);   \
+    v1 = _mm_shuffle_epi32(v0, 0x4e);                     \
+    v3 = _mm_shuffle_epi32(v2, 0x4e);                     \
+    t##i = _mm_add_epi16(_mm_sign_epi16(v0, vprod3), v1); \
+    x##i = _mm_add_epi16(_mm_sign_epi16(v2, vprod3), v3); \
+}
+
+#define ITER2 \
+    ITER(0); ITER(1); ITER(2); ITER(3); \
+    ITER(4); ITER(5); ITER(6); ITER(7); \
+    HADAMARD8_FIN(t, v0) \
+    sum = _mm_add_epi32(sum, v0); \
+    HADAMARD8_FIN(x, v0) \
+    sum = _mm_add_epi32(sum, v0);
+
+    ITER2
+    if (h == 16) {
+        dst += 8 * stride;
+        src += 8 * stride;
+        ITER2
+    }
+#undef ITER2
+#undef ITER
+    sum = _mm_hadd_epi32(sum, sum);
+    return _mm_extract_epi32(sum, 0) + _mm_extract_epi32(sum, 1);
+}
+#else
+static int hadamard8_diff16_e2k(MpegEncContext *s, const uint8_t *dst,
+                                const uint8_t *src, ptrdiff_t stride, int h)
+{
+    int score = 0;
+
+    score += hadamard8_diff8x8_e2k(s, dst, src, stride, 8);
+    score += hadamard8_diff8x8_e2k(s, dst + 8, src + 8, stride, 8);
+    if (h == 16) {
+        dst += 8 * stride;
+        src += 8 * stride;
+        score += hadamard8_diff8x8_e2k(s, dst, src, stride, 8);
+        score += hadamard8_diff8x8_e2k(s, dst + 8, src + 8, stride, 8);
+    }
+    return score;
+}
+#endif
+
+av_cold void ff_me_cmp_init_e2k(MECmpContext *c, AVCodecContext *avctx)
+{
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+
+    // fate lavf-mxf
+    c->pix_abs[0][0] = sad16_e2k;
+    c->pix_abs[0][1] = sad16_x2_e2k;
+    c->pix_abs[0][2] = sad16_y2_e2k;
+    c->pix_abs[0][3] = sad16_xy2_e2k;
+    c->pix_abs[1][0] = sad8_e2k;
+    c->pix_abs[1][1] = sad8_x2_e2k;
+    c->pix_abs[1][2] = sad8_y2_e2k;
+    c->pix_abs[1][3] = sad8_xy2_e2k;
+
+    c->sad[0] = sad16_e2k;
+    c->sad[1] = sad8_e2k;
+    c->sse[0] = sse16_e2k;
+    c->sse[1] = sse8_e2k;
+
+    // fate vsynth1-mpeg4-qprd
+    c->hadamard8_diff[0] = hadamard8_diff16_e2k;
+    c->hadamard8_diff[1] = hadamard8_diff8x8_e2k;
+}
diff --git a/libavcodec/e2k/mpeg4videodsp.c b/libavcodec/e2k/mpeg4videodsp.c
new file mode 100644
index 0000000..3c233b2
--- /dev/null
+++ b/libavcodec/e2k/mpeg4videodsp.c
@@ -0,0 +1,85 @@
+/*
+ * GMC (Global Motion Compensation)
+ *
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2003 Romain Dolbeau <romain@dolbeau.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/mem.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/mpeg4videodsp.h"
+
+/* ATM this code assumes stride is a multiple of 8
+ * to preserve proper dst alignment. */
+static void gmc1_e2k(uint8_t *dst /* align 8 */, const uint8_t *src /* align1 */,
+                     int stride, int h, int x16, int y16, int rounder)
+{
+    int i;
+    LOAD_ZERO;
+    vec_u8 dstv, srcvA, srcvB;
+    vec_u16 t0, t1, t2, t3;
+
+    vec_u16 Av = _mm_set1_epi16((16 - x16) * (16 - y16));
+    vec_u16 Bv = _mm_set1_epi16(      x16  * (16 - y16));
+    vec_u16 Cv = _mm_set1_epi16((16 - x16) * y16);
+    vec_u16 Dv = _mm_set1_epi16(      x16  * y16);
+    vec_u16 rounderV = _mm_set1_epi16(rounder);
+
+    vec_u8 srcvC = VEC_LD8(src);
+    vec_u8 srcvD = VEC_LD8(src + 1);
+    srcvC = _mm_unpacklo_epi8(srcvC, zerov);
+    srcvD = _mm_unpacklo_epi8(srcvD, zerov);
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        src += stride;
+
+        srcvA = srcvC;
+        srcvB = srcvD;
+        srcvC = VEC_LD8(src);
+        srcvD = VEC_LD8(src + 1);
+        srcvC = _mm_unpacklo_epi8(srcvC, zerov);
+        srcvD = _mm_unpacklo_epi8(srcvD, zerov);
+
+        t0 = _mm_mullo_epi16(srcvA, Av);
+        t1 = _mm_mullo_epi16(srcvB, Bv);
+        t0 = _mm_add_epi16(t0, t1);
+        t2 = _mm_mullo_epi16(srcvC, Cv);
+        t3 = _mm_mullo_epi16(srcvD, Dv);
+        t0 = _mm_add_epi16(t0, rounderV);
+        t2 = _mm_add_epi16(t2, t3);
+        t0 = _mm_add_epi16(t0, t2);
+        t0 = _mm_srli_epi16(t0, 8);
+        dstv = _mm_packus_epi16(t0, t0);
+
+        VEC_STL(dst, dstv);
+        dst += stride;
+    }
+}
+
+av_cold void ff_mpeg4videodsp_init_e2k(Mpeg4VideoDSPContext *c)
+{
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+    c->gmc1 = gmc1_e2k;
+}
diff --git a/libavcodec/e2k/mpegaudiodsp.c b/libavcodec/e2k/mpegaudiodsp.c
new file mode 100644
index 0000000..d171bd2
--- /dev/null
+++ b/libavcodec/e2k/mpegaudiodsp.c
@@ -0,0 +1,143 @@
+/*
+ * Elbrus optimized MP3 decoding functions
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2010 Vitor Sessak
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include <string.h> // memcpy
+
+#include "config.h"
+#include "libavutil/attributes.h"
+#include "libavutil/internal.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+#include "libavcodec/mpegaudiodsp.h"
+#include "libavcodec/mpegaudio.h"
+
+#define MACS(rt, ra, rb) rt += (ra) * (rb)
+#define MLSS(rt, ra, rb) rt -= (ra) * (rb)
+
+#define SUM8(op, sum, w, p) {             \
+    op(sum, (w)[0 * 64], (p)[0 * 64]);    \
+    op(sum, (w)[1 * 64], (p)[1 * 64]);    \
+    op(sum, (w)[2 * 64], (p)[2 * 64]);    \
+    op(sum, (w)[3 * 64], (p)[3 * 64]);    \
+    op(sum, (w)[4 * 64], (p)[4 * 64]);    \
+    op(sum, (w)[5 * 64], (p)[5 * 64]);    \
+    op(sum, (w)[6 * 64], (p)[6 * 64]);    \
+    op(sum, (w)[7 * 64], (p)[7 * 64]);    \
+}
+
+static av_always_inline
+void apply_window(const float *buf, const float *win1,
+                  const float *win2, float *sum1, float *sum2)
+{
+    vec_f v0, v1, v2, v3, v4, v5;
+    int i;
+
+#define MULT(j)                                   \
+    v1 = _mm_load_ps(win1 + j * 64);              \
+    v2 = _mm_load_ps(win2 + j * 16);              \
+    v3 = _mm_load_ps(buf + j * 64);               \
+    v0 = _mm_sub_ps(v0, _mm_mul_ps(v3, v1));      \
+    v4 = _mm_sub_ps(v4, _mm_mul_ps(v2, v3))
+
+    v0 = v4 = _mm_setzero_ps();
+    MULT(0); MULT(1); MULT(2); MULT(3);
+    MULT(4); MULT(5); MULT(6); MULT(7);
+
+    PRAGMA_E2K("ivdep")
+    PRAGMA_E2K("unroll(3)")
+    for (i = 4; i < 16; i += 4) {
+        win1 += 4; win2 += 4; buf += 4;
+        _mm_store_ps(sum1, v0); v5 = v4;
+
+        v0 = v4 = _mm_setzero_ps();
+        MULT(0); MULT(1); MULT(2); MULT(3);
+        MULT(4); MULT(5); MULT(6); MULT(7);
+        _mm_store_ps(sum2, _mm_alignr_ps(v4, v5, 1));
+        sum1 += 4; sum2 += 4;
+    }
+    _mm_store_ps(sum1, v0);
+    _mm_store_ps(sum2, _mm_bsrli_ps(v4, 1));
+
+#undef MULT
+}
+
+static void apply_window_e2k(float *in, float *win, int *unused, float *out,
+                             ptrdiff_t incr)
+{
+    float ALIGNED(16) suma[16];
+    float ALIGNED(16) sumb[16];
+    float ALIGNED(16) sumc[16];
+    float ALIGNED(16) sumd[16];
+    float sum;
+
+    /* copy to avoid wrap */
+    memcpy(in + 512, in, 32 * sizeof(*in));
+
+    apply_window(in + 16, win     , win + 512, suma, sumc);
+    apply_window(in + 32, win + 48, win + 640, sumb, sumd);
+
+    sum = suma[0];
+    SUM8(MACS, sum, win + 32, in + 48);
+    suma[0] = sum;
+
+#define SUMS(a, b)                              \
+    v0 = _mm_load_ps(sumd + b);                 \
+    v1 = _mm_load_ps(sumc + a);                 \
+    v0 = _mm_shuffle_ps(v0, v0, 0x1b);          \
+    v1 = _mm_shuffle_ps(v1, v1, 0x1b);          \
+    v0 = _mm_sub_ps(v0, _mm_load_ps(suma + a)); \
+    v1 = _mm_add_ps(v1, _mm_load_ps(sumb + b)); \
+    _mm_storeu_ps(out + a, v0);                 \
+    _mm_storeu_ps(out + b + 16, v1)
+
+    if (incr == 1) {
+        vec_f v0, v1;
+        SUMS(0, 12); SUMS(4, 8); SUMS(8, 4); SUMS(12, 0);
+        out += 16 * incr;
+    } else {
+        int j;
+        float *out2 = out + 32 * incr;
+        out[0] = -suma[0];
+        out += incr;
+        out2 -= incr;
+        PRAGMA_E2K("ivdep")
+        for (j = 1; j < 16; j++) {
+            *out  = sumd[15 - j] - suma[j];
+            *out2 = sumb[16 - j] + sumc[j - 1];
+            out  += incr;
+            out2 -= incr;
+        }
+    }
+
+    sum = 0;
+    SUM8(MLSS, sum, win + 16 + 32, in + 32);
+    *out = sum;
+}
+
+av_cold void ff_mpadsp_init_e2k(MPADSPContext *s)
+{
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+    s->apply_window_float = apply_window_e2k; // fate audiomatch-square-mp3
+}
diff --git a/libavcodec/e2k/mpegvideo.c b/libavcodec/e2k/mpegvideo.c
new file mode 100644
index 0000000..8f9fdf9
--- /dev/null
+++ b/libavcodec/e2k/mpegvideo.c
@@ -0,0 +1,99 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2002 Dieter Shirley
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include <stdlib.h>
+#include <stdio.h>
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/mpegvideo.h"
+
+/* this code assumes `block' is 16 bytes-aligned */
+static void dct_unquantize_h263_intra_e2k(MpegEncContext *s,
+                                          int16_t *block, int n, int qscale)
+{
+    int level, qmul, qadd = 0, nCoeffs = 63, j;
+    __m128i qmulv, qaddv, v0, v1;
+
+    qmul = qscale << 1;
+    level = block[0];
+
+    if (!s->h263_aic) {
+        level *= n < 4 ? s->y_dc_scale : s->c_dc_scale;
+        qadd = (qscale - 1) | 1;
+    } else {
+        av_assert2(s->block_last_index[n] >= 0);
+    }
+    if (!s->ac_pred) {
+        nCoeffs = s->intra_scantable.raster_end[s->block_last_index[n]];
+    }
+
+    qmulv = _mm_set1_epi16(qmul);
+    qaddv = _mm_set1_epi16(qadd);
+    PRAGMA_E2K("ivdep")
+    for (j = 0; j <= nCoeffs; j += 8) {
+        v0 = _mm_load_si128((const __m128i*)(block + j));
+        v1 = _mm_mullo_epi16(v0, qmulv);
+        v1 = _mm_add_epi16(v1, _mm_sign_epi16(qaddv, v0));
+        _mm_store_si128((__m128i*)(block + j), v1);
+    }
+
+    block[0] = level;
+}
+
+static void dct_unquantize_h263_inter_e2k(MpegEncContext *s,
+                                          int16_t *block, int n, int qscale)
+{
+    int qmul, qadd, nCoeffs, j;
+    __m128i qmulv, qaddv, v0, v1;
+
+    qmul = qscale << 1;
+    qadd = (qscale - 1) | 1;
+
+    av_assert2(s->block_last_index[n] >= 0 || s->h263_aic);
+    nCoeffs = s->inter_scantable.raster_end[s->block_last_index[n]];
+
+    qmulv = _mm_set1_epi16(qmul);
+    qaddv = _mm_set1_epi16(qadd);
+    PRAGMA_E2K("ivdep")
+    for (j = 0; j <= nCoeffs; j += 8) {
+        v0 = _mm_load_si128((const __m128i*)(block + j));
+        v1 = _mm_mullo_epi16(v0, qmulv);
+        v1 = _mm_add_epi16(v1, _mm_sign_epi16(qaddv, v0));
+        _mm_store_si128((__m128i*)(block + j), v1);
+    }
+}
+
+av_cold void ff_mpv_common_init_e2k(MpegEncContext *s)
+{
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+    // fate flv-add_keyframe_index
+    s->dct_unquantize_h263_intra = dct_unquantize_h263_intra_e2k;
+    s->dct_unquantize_h263_inter = dct_unquantize_h263_inter_e2k;
+}
+
diff --git a/libavcodec/e2k/mpegvideoencdsp.c b/libavcodec/e2k/mpegvideoencdsp.c
new file mode 100644
index 0000000..ed31915
--- /dev/null
+++ b/libavcodec/e2k/mpegvideoencdsp.c
@@ -0,0 +1,74 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include <stdint.h>
+
+#include "libavutil/attributes.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/mpegvideoencdsp.h"
+
+static int pix_norm1_e2k(const uint8_t *pix, int line_size)
+{
+    int i;
+    LOAD_ZERO; 
+    __m128i v0, v1, sum = zerov;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < 16; i++) {
+        v1 = VEC_LD(pix);
+        v0 = _mm_unpacklo_epi8(v1, zerov);
+        v1 = _mm_unpackhi_epi8(v1, zerov);
+        sum = _mm_add_epi32(sum, _mm_madd_epi16(v0, v0));
+        sum = _mm_add_epi32(sum, _mm_madd_epi16(v1, v1));
+        pix += line_size;
+    }
+    sum = _mm_hadd_epi32(sum, sum);
+    return _mm_extract_epi32(sum, 0) + _mm_extract_epi32(sum, 1);
+}
+
+static int pix_sum_e2k(const uint8_t *pix, int line_size)
+{
+    int i;
+    LOAD_ZERO;
+    __m128i v0, sum = zerov;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < 16; i++) {
+        v0 = VEC_LD(pix);
+        sum = _mm_add_epi32(sum, _mm_sad_epu8(v0, zerov));
+        pix += line_size;
+    }
+    return _mm_extract_epi32(sum, 0) + _mm_extract_epi32(sum, 2);
+}
+
+av_cold void ff_mpegvideoencdsp_init_e2k(MpegvideoEncDSPContext *c,
+                                         AVCodecContext *avctx)
+{
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+    c->pix_norm1 = pix_norm1_e2k;
+    c->pix_sum   = pix_sum_e2k;
+}
diff --git a/libavcodec/e2k/pixblockdsp.c b/libavcodec/e2k/pixblockdsp.c
new file mode 100644
index 0000000..77e7d2d
--- /dev/null
+++ b/libavcodec/e2k/pixblockdsp.c
@@ -0,0 +1,82 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2002 Brian Foley
+ * Copyright (c) 2002 Dieter Shirley
+ * Copyright (c) 2003-2004 Romain Dolbeau <romain@dolbeau.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/avcodec.h"
+#include "libavcodec/pixblockdsp.h"
+
+static void get_pixels_e2k(int16_t * restrict block, const uint8_t *pixels,
+                           ptrdiff_t stride)
+{
+    LOAD_ZERO;
+    __m128i v0;
+    int i;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < 8; i++) {
+        v0 = VEC_LD8(pixels);
+        v0 = _mm_unpacklo_epi8(v0, zerov);
+        VEC_ST(block + i * 8, v0);
+        pixels += stride;
+    }
+}
+
+static void diff_pixels_e2k(int16_t * restrict block, const uint8_t *s1,
+                            const uint8_t *s2, ptrdiff_t stride)
+{
+    LOAD_ZERO;
+    __m128i v0, v1;
+    int i;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < 8; i++) {
+        v0 = VEC_LD8(s1);
+        v1 = VEC_LD8(s2);
+        v0 = _mm_unpacklo_epi8(v0, zerov);
+        v1 = _mm_unpacklo_epi8(v1, zerov);
+        v0 = _mm_sub_epi16(v0, v1);
+        VEC_ST(block + i * 8, v0);
+        s1 += stride;
+        s2 += stride;
+    }
+}
+
+av_cold void ff_pixblockdsp_init_e2k(PixblockDSPContext *c,
+                                     AVCodecContext *avctx,
+                                     unsigned high_bit_depth)
+{
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    // checkasm
+
+    c->diff_pixels = diff_pixels_e2k;
+
+    if (!high_bit_depth)
+        c->get_pixels = get_pixels_e2k;
+}
diff --git a/libavcodec/e2k/svq1enc.c b/libavcodec/e2k/svq1enc.c
new file mode 100644
index 0000000..6b0d8e6
--- /dev/null
+++ b/libavcodec/e2k/svq1enc.c
@@ -0,0 +1,67 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2007 Luca Barbato <lu_zero@gentoo.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include <stdint.h>
+
+#include "libavutil/attributes.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/svq1encdsp.h"
+
+static int ssd_int8_vs_int16_e2k(const int8_t *pix1, const int16_t *pix2,
+                                 intptr_t size)
+{
+    int i, res;
+    __m128i v0, v1, v2, v3, sum = _mm_setzero_si128();
+
+    for (i = 0; i + 15 < size; i += 16) {
+        v1 = VEC_LD(pix1);
+        v0 = _mm_srai_epi16(_mm_unpacklo_epi8(v1, v1), 8);
+        v1 = _mm_srai_epi16(_mm_unpackhi_epi8(v1, v1), 8);
+        v2 = VEC_LD(pix2);
+        v3 = VEC_LD(pix2 + 8);
+        v0 = _mm_sub_epi16(v0, v2);
+        v1 = _mm_sub_epi16(v1, v3);
+        sum = _mm_add_epi32(sum, _mm_madd_epi16(v0, v0));
+        sum = _mm_add_epi32(sum, _mm_madd_epi16(v1, v1));
+        pix1 += 16;
+        pix2 += 16;
+    }
+    sum = _mm_hadd_epi32(sum, sum);
+    res = _mm_extract_epi32(sum, 0) + _mm_extract_epi32(sum, 1);
+
+    for (; i < size; i++)
+        res += (pix1[i] - pix2[i]) * (pix1[i] - pix2[i]);
+
+    return res;
+}
+
+av_cold void ff_svq1enc_init_e2k(SVQ1EncDSPContext *c)
+{
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+    c->ssd_int8_vs_int16 = ssd_int8_vs_int16_e2k;
+}
diff --git a/libavcodec/e2k/vc1dsp.c b/libavcodec/e2k/vc1dsp.c
new file mode 100644
index 0000000..9ccc72a
--- /dev/null
+++ b/libavcodec/e2k/vc1dsp.c
@@ -0,0 +1,297 @@
+/*
+ * VC-1 and WMV3 decoder - DSP functions
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2006 Konstantin Shishkov
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/vc1dsp.h"
+
+// main steps of 8x8 transform
+#define STEP8(s0, s1, s2, s3, s4, s5, s6, s7, vec_rnd) do { \
+    t0 = _mm_slli_epi32(_mm_add_epi32(s0, s4), 2); \
+    t0 = _mm_add_epi32(_mm_slli_epi32(t0, 1), t0); \
+    t0 = _mm_add_epi32(t0, vec_rnd); \
+    t1 = _mm_slli_epi32(_mm_sub_epi32(s0, s4), 2); \
+    t1 = _mm_add_epi32(_mm_slli_epi32(t1, 1), t1); \
+    t1 = _mm_add_epi32(t1, vec_rnd); \
+    t2 = _mm_add_epi32(_mm_slli_epi32(s6, 2), _mm_slli_epi32(s6, 1)); \
+    t2 = _mm_add_epi32(t2, _mm_slli_epi32(s2, 4)); \
+    t3 = _mm_add_epi32(_mm_slli_epi32(s2, 2), _mm_slli_epi32(s2, 1)); \
+    t3 = _mm_sub_epi32(t3, _mm_slli_epi32(s6, 4)); \
+    t4 = _mm_add_epi32(t0, t2); \
+    t5 = _mm_add_epi32(t1, t3); \
+    t6 = _mm_sub_epi32(t1, t3); \
+    t7 = _mm_sub_epi32(t0, t2); \
+\
+    t0 = _mm_slli_epi32(_mm_add_epi32(s1, s3), 4); \
+    t0 = _mm_add_epi32(t0, _mm_slli_epi32(s5, 3)); \
+    t0 = _mm_add_epi32(t0, _mm_slli_epi32(s7, 2)); \
+    t0 = _mm_add_epi32(t0, _mm_sub_epi32(s5, s3)); \
+\
+    t1 = _mm_slli_epi32(_mm_sub_epi32(s1, s5), 4); \
+    t1 = _mm_sub_epi32(t1, _mm_slli_epi32(s7, 3)); \
+    t1 = _mm_sub_epi32(t1, _mm_slli_epi32(s3, 2)); \
+    t1 = _mm_sub_epi32(t1, _mm_add_epi32(s1, s7)); \
+\
+    t2 = _mm_slli_epi32(_mm_sub_epi32(s7, s3), 4); \
+    t2 = _mm_add_epi32(t2, _mm_slli_epi32(s1, 3)); \
+    t2 = _mm_add_epi32(t2, _mm_slli_epi32(s5, 2)); \
+    t2 = _mm_add_epi32(t2, _mm_sub_epi32(s1, s7)); \
+\
+    t3 = _mm_slli_epi32(_mm_sub_epi32(s5, s7), 4); \
+    t3 = _mm_sub_epi32(t3, _mm_slli_epi32(s3, 3)); \
+    t3 = _mm_add_epi32(t3, _mm_slli_epi32(s1, 2)); \
+    t3 = _mm_sub_epi32(t3, _mm_add_epi32(s3, s5)); \
+\
+    s0 = _mm_add_epi32(t4, t0); \
+    s1 = _mm_add_epi32(t5, t1); \
+    s2 = _mm_add_epi32(t6, t2); \
+    s3 = _mm_add_epi32(t7, t3); \
+    s4 = _mm_sub_epi32(t7, t3); \
+    s5 = _mm_sub_epi32(t6, t2); \
+    s6 = _mm_sub_epi32(t5, t1); \
+    s7 = _mm_sub_epi32(t4, t0); \
+}while(0)
+
+#define SHIFT_HOR8(s0, s1, s2, s3, s4, s5, s6, s7) do { \
+    s0 = _mm_srai_epi32(s0, 3); \
+    s1 = _mm_srai_epi32(s1, 3); \
+    s2 = _mm_srai_epi32(s2, 3); \
+    s3 = _mm_srai_epi32(s3, 3); \
+    s4 = _mm_srai_epi32(s4, 3); \
+    s5 = _mm_srai_epi32(s5, 3); \
+    s6 = _mm_srai_epi32(s6, 3); \
+    s7 = _mm_srai_epi32(s7, 3); \
+} while(0)
+
+#define SHIFT_VERT8(s0, s1, s2, s3, s4, s5, s6, s7) do { \
+    s0 = _mm_srai_epi32(s0, 7); \
+    s1 = _mm_srai_epi32(s1, 7); \
+    s2 = _mm_srai_epi32(s2, 7); \
+    s3 = _mm_srai_epi32(s3, 7); \
+    s4 = _mm_srai_epi32(_mm_add_epi32(s4, c1), 7); \
+    s5 = _mm_srai_epi32(_mm_add_epi32(s5, c1), 7); \
+    s6 = _mm_srai_epi32(_mm_add_epi32(s6, c1), 7); \
+    s7 = _mm_srai_epi32(_mm_add_epi32(s7, c1), 7); \
+} while(0)
+
+/* main steps of 4x4 transform */
+#define STEP4(s0, s1, s2, s3, vec_rnd) do { \
+    t1 = _mm_add_epi32(_mm_slli_epi32(s0, 4), s0); \
+    t1 = _mm_add_epi32(t1, vec_rnd); \
+    t2 = _mm_add_epi32(_mm_slli_epi32(s2, 4), s2); \
+    t0 = _mm_add_epi32(t1, t2); \
+    t1 = _mm_sub_epi32(t1, t2); \
+    t3 = _mm_slli_epi32(_mm_sub_epi32(s3, s1), 1); \
+    t3 = _mm_add_epi32(t3, _mm_slli_epi32(t3, 2)); \
+    t2 = _mm_add_epi32(t3, _mm_slli_epi32(s1, 5)); \
+    t3 = _mm_add_epi32(t3, _mm_slli_epi32(s3, 3)); \
+    t3 = _mm_add_epi32(t3, _mm_slli_epi32(s3, 2)); \
+    s0 = _mm_add_epi32(t0, t2); \
+    s1 = _mm_sub_epi32(t1, t3); \
+    s2 = _mm_add_epi32(t1, t3); \
+    s3 = _mm_sub_epi32(t0, t2); \
+} while (0)
+
+#define SHIFT_HOR4(s0, s1, s2, s3) \
+    s0 = _mm_srai_epi32(s0, 3); \
+    s1 = _mm_srai_epi32(s1, 3); \
+    s2 = _mm_srai_epi32(s2, 3); \
+    s3 = _mm_srai_epi32(s3, 3)
+
+#define SHIFT_VERT4(s0, s1, s2, s3) \
+    s0 = _mm_srai_epi32(s0, 7); \
+    s1 = _mm_srai_epi32(s1, 7); \
+    s2 = _mm_srai_epi32(s2, 7); \
+    s3 = _mm_srai_epi32(s3, 7)
+
+#define _mm_unpacklo1_epi16(v) _mm_srai_epi32(_mm_unpacklo_epi16(v, v), 16)
+#define _mm_unpackhi1_epi16(v) _mm_srai_epi32(_mm_unpackhi_epi16(v, v), 16)
+
+#define TRANSPOSE4_32(s0, s1, s2, s3) \
+    t0 = _mm_unpacklo_epi32(s0, s1); \
+    t1 = _mm_unpackhi_epi32(s0, s1); \
+    t2 = _mm_unpacklo_epi32(s2, s3); \
+    t3 = _mm_unpackhi_epi32(s2, s3); \
+    s0 = _mm_unpacklo_epi64(t0, t2); \
+    s1 = _mm_unpackhi_epi64(t0, t2); \
+    s2 = _mm_unpacklo_epi64(t1, t3); \
+    s3 = _mm_unpackhi_epi64(t1, t3);
+
+/* Do inverse transform on 8x8 block */
+static void vc1_inv_trans_8x8_e2k(int16_t block[64])
+{
+    vec_s16 src0, src1, src2, src3, src4, src5, src6, src7;
+    vec_s32 s0, s1, s2, s3, s4, s5, s6, s7;
+    vec_s32 s8, s9, sA, sB, sC, sD, sE, sF;
+    vec_s32 t0, t1, t2, t3, t4, t5, t6, t7;
+    const vec_s32 c64 = _mm_set1_epi32(64);
+    const vec_s32 c4 = _mm_set1_epi32(4);
+    const vec_s32 c1 = _mm_set1_epi32(1);
+
+    src0 = VEC_LD(block + 8 * 0);
+    src1 = VEC_LD(block + 8 * 1);
+    src2 = VEC_LD(block + 8 * 2);
+    src3 = VEC_LD(block + 8 * 3);
+    src4 = VEC_LD(block + 8 * 4);
+    src5 = VEC_LD(block + 8 * 5);
+    src6 = VEC_LD(block + 8 * 6);
+    src7 = VEC_LD(block + 8 * 7);
+
+    s0 = _mm_unpacklo1_epi16(src0);
+    s1 = _mm_unpacklo1_epi16(src1);
+    s2 = _mm_unpacklo1_epi16(src2);
+    s3 = _mm_unpacklo1_epi16(src3);
+    s4 = _mm_unpacklo1_epi16(src4);
+    s5 = _mm_unpacklo1_epi16(src5);
+    s6 = _mm_unpacklo1_epi16(src6);
+    s7 = _mm_unpacklo1_epi16(src7);
+    s8 = _mm_unpackhi1_epi16(src0);
+    s9 = _mm_unpackhi1_epi16(src1);
+    sA = _mm_unpackhi1_epi16(src2);
+    sB = _mm_unpackhi1_epi16(src3);
+    sC = _mm_unpackhi1_epi16(src4);
+    sD = _mm_unpackhi1_epi16(src5);
+    sE = _mm_unpackhi1_epi16(src6);
+    sF = _mm_unpackhi1_epi16(src7);
+    STEP8(s0, s1, s2, s3, s4, s5, s6, s7, c4);
+    SHIFT_HOR8(s0, s1, s2, s3, s4, s5, s6, s7);
+    STEP8(s8, s9, sA, sB, sC, sD, sE, sF, c4);
+    SHIFT_HOR8(s8, s9, sA, sB, sC, sD, sE, sF);
+
+    TRANSPOSE4_32(s0, s1, s2, s3)
+    TRANSPOSE4_32(s4, s5, s6, s7)
+    TRANSPOSE4_32(s8, s9, sA, sB)
+    TRANSPOSE4_32(sC, sD, sE, sF)
+
+    STEP8(s0, s1, s2, s3, s8, s9, sA, sB, c64);
+    SHIFT_VERT8(s0, s1, s2, s3, s8, s9, sA, sB);
+    STEP8(s4, s5, s6, s7, sC, sD, sE, sF, c64);
+    SHIFT_VERT8(s4, s5, s6, s7, sC, sD, sE, sF);
+    src0 = _mm_packs_epi32(s0, s4);
+    src1 = _mm_packs_epi32(s1, s5);
+    src2 = _mm_packs_epi32(s2, s6);
+    src3 = _mm_packs_epi32(s3, s7);
+    src4 = _mm_packs_epi32(s8, sC);
+    src5 = _mm_packs_epi32(s9, sD);
+    src6 = _mm_packs_epi32(sA, sE);
+    src7 = _mm_packs_epi32(sB, sF);
+
+    VEC_ST(block + 8 * 0, src0);
+    VEC_ST(block + 8 * 1, src1);
+    VEC_ST(block + 8 * 2, src2);
+    VEC_ST(block + 8 * 3, src3);
+    VEC_ST(block + 8 * 4, src4);
+    VEC_ST(block + 8 * 5, src5);
+    VEC_ST(block + 8 * 6, src6);
+    VEC_ST(block + 8 * 7, src7);
+}
+
+/* Do inverse transform on 8x4 part of block */
+static void vc1_inv_trans_8x4_e2k(uint8_t *dest, ptrdiff_t stride,
+                                  int16_t *block)
+{
+    LOAD_ZERO;
+    vec_s16 src0, src1, src2, src3;
+    vec_s32 s0, s1, s2, s3, s4, s5, s6, s7;
+    vec_s32 t0, t1, t2, t3, t4, t5, t6, t7;
+    const vec_s32 c64 = _mm_set1_epi32(64);
+    const vec_s32 c4 = _mm_set1_epi32(4);
+    __m128i tmp;
+
+    src0 = VEC_LD(block + 8 * 0);
+    src1 = VEC_LD(block + 8 * 1);
+    src2 = VEC_LD(block + 8 * 2);
+    src3 = VEC_LD(block + 8 * 3);
+
+    t0 = _mm_unpacklo_epi16(src0, src1);
+    t1 = _mm_unpackhi_epi16(src0, src1);
+    t2 = _mm_unpacklo_epi16(src2, src3);
+    t3 = _mm_unpackhi_epi16(src2, src3);
+
+    t4 = _mm_unpacklo_epi32(t0, t2);
+    t5 = _mm_unpackhi_epi32(t0, t2);
+    t6 = _mm_unpacklo_epi32(t1, t3);
+    t7 = _mm_unpackhi_epi32(t1, t3);
+
+    s0 = _mm_unpacklo1_epi16(t4);
+    s1 = _mm_unpackhi1_epi16(t4);
+    s2 = _mm_unpacklo1_epi16(t5);
+    s3 = _mm_unpackhi1_epi16(t5);
+    s4 = _mm_unpacklo1_epi16(t6);
+    s5 = _mm_unpackhi1_epi16(t6);
+    s6 = _mm_unpacklo1_epi16(t7);
+    s7 = _mm_unpackhi1_epi16(t7);
+
+    STEP8(s0, s1, s2, s3, s4, s5, s6, s7, c4);
+    SHIFT_HOR8(s0, s1, s2, s3, s4, s5, s6, s7);
+
+    TRANSPOSE4_32(s0, s1, s2, s3)
+    TRANSPOSE4_32(s4, s5, s6, s7)
+
+    STEP4(s0, s1, s2, s3, c64);
+    SHIFT_VERT4(s0, s1, s2, s3);
+    STEP4(s4, s5, s6, s7, c64);
+    SHIFT_VERT4(s4, s5, s6, s7);
+    src0 = _mm_packs_epi32(s0, s4);
+    src1 = _mm_packs_epi32(s1, s5);
+    src2 = _mm_packs_epi32(s2, s6);
+    src3 = _mm_packs_epi32(s3, s7);
+
+#define ADD(dest, src)                   \
+    tmp = VEC_LD8(dest);                 \
+    tmp = _mm_unpacklo_epi8(tmp, zerov); \
+    tmp = _mm_adds_epi16(tmp, src);      \
+    tmp = _mm_packus_epi16(tmp, tmp);    \
+    VEC_STL(dest, tmp)
+
+    ADD(dest, src0); dest += stride;
+    ADD(dest, src1); dest += stride;
+    ADD(dest, src2); dest += stride;
+    ADD(dest, src3);
+}
+
+#define OP8_E2K(d, s, dst) d = s
+#define OP16_E2K(d, s, dst) d = s
+#define PREFIX_no_rnd_vc1_chroma_mc8_e2k put_no_rnd_vc1_chroma_mc8_e2k
+#include "h264chroma_template.c"
+
+#define OP8_E2K(d, s, dst) d = _mm_avg_pu8(dst, s)
+#define OP16_E2K(d, s, dst) d = _mm_avg_epu8(dst, s)
+#define PREFIX_no_rnd_vc1_chroma_mc8_e2k avg_no_rnd_vc1_chroma_mc8_e2k
+#include "h264chroma_template.c"
+
+av_cold void ff_vc1dsp_init_e2k(VC1DSPContext *dsp)
+{
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+
+    dsp->vc1_inv_trans_8x8 = vc1_inv_trans_8x8_e2k; // fate mss2-wmv
+    dsp->vc1_inv_trans_8x4 = vc1_inv_trans_8x4_e2k; // fate wmv3-drm-dec
+    dsp->put_no_rnd_vc1_chroma_pixels_tab[0] = put_no_rnd_vc1_chroma_mc8_e2k;
+    dsp->avg_no_rnd_vc1_chroma_pixels_tab[0] = avg_no_rnd_vc1_chroma_mc8_e2k;
+}
diff --git a/libavcodec/e2k/videodsp.c b/libavcodec/e2k/videodsp.c
new file mode 100644
index 0000000..f0ab58e
--- /dev/null
+++ b/libavcodec/e2k/videodsp.c
@@ -0,0 +1,36 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2003-2004 Romain Dolbeau
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/attributes.h"
+#include "libavcodec/videodsp.h"
+
+static void prefetch_e2k(const uint8_t *mem, ptrdiff_t stride, int h)
+{
+    do {
+        __builtin_prefetch(mem);
+        mem += stride;
+    } while (--h);
+}
+
+av_cold void ff_videodsp_init_e2k(VideoDSPContext *ctx, int bpc)
+{
+    ctx->prefetch = prefetch_e2k;
+}
diff --git a/libavcodec/e2k/vorbisdsp.c b/libavcodec/e2k/vorbisdsp.c
new file mode 100644
index 0000000..4b26b16
--- /dev/null
+++ b/libavcodec/e2k/vorbisdsp.c
@@ -0,0 +1,61 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2006 Luca Barbato <lu_zero@gentoo.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/vorbisdsp.h"
+
+static void vorbis_inverse_coupling_e2k(float *mag, float *ang,
+                                        intptr_t blocksize)
+{
+    int i;
+    vec_f m, a, t0, t1, zerov = _mm_setzero_ps();
+    vec_f sign = _mm_castsi128_ps(_mm_set1_epi32(1 << 31));
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < blocksize; i += 4) {
+        m = _mm_load_ps(mag + i);
+        a = _mm_load_ps(ang + i);
+        t0 = _mm_cmple_ps(m, zerov);
+        t1 = _mm_cmple_ps(a, zerov);
+        a = _mm_xor_ps(a, _mm_and_ps(t0, sign));
+        t0 = _mm_andnot_ps(t1, a);
+        t1 = _mm_and_ps(t1, a);
+        a = _mm_sub_ps(m, t0);
+        m = _mm_add_ps(m, t1);
+        _mm_store_ps(ang + i, a);
+        _mm_store_ps(mag + i, m);
+    }
+}
+
+av_cold void ff_vorbisdsp_init_e2k(VorbisDSPContext *c)
+{
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+    // fate vorbis-encode
+    c->vorbis_inverse_coupling = vorbis_inverse_coupling_e2k;
+}
diff --git a/libavcodec/e2k/vp3dsp.c b/libavcodec/e2k/vp3dsp.c
new file mode 100644
index 0000000..7f8a521
--- /dev/null
+++ b/libavcodec/e2k/vp3dsp.c
@@ -0,0 +1,168 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (C) 2009 David Conrad
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include <string.h>
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/vp3dsp.h"
+
+#define IDCT_START(extra) \
+    vec_s16 A, B, C, D, Ad, Bd, Cd, Dd, E, F, G, H; \
+    vec_s16 Ed, Gd, Add, Bdd, Fd, Hd; \
+    vec_s16 addv = _mm_set1_epi16(extra + 8); \
+    \
+    vec_s16 C1 = _mm_set1_epi16(64277); \
+    vec_s16 C2 = _mm_set1_epi16(60547); \
+    vec_s16 C3 = _mm_set1_epi16(54491); \
+    vec_s16 C4 = _mm_set1_epi16(46341); \
+    vec_s16 C5 = _mm_set1_epi16(36410); \
+    vec_s16 C6 = _mm_set1_epi16(25080); \
+    vec_s16 C7 = _mm_set1_epi16(12785); \
+    \
+    vec_s16 b0 = VEC_LD(block + 8 * 0); \
+    vec_s16 b1 = VEC_LD(block + 8 * 1); \
+    vec_s16 b2 = VEC_LD(block + 8 * 2); \
+    vec_s16 b3 = VEC_LD(block + 8 * 3); \
+    vec_s16 b4 = VEC_LD(block + 8 * 4); \
+    vec_s16 b5 = VEC_LD(block + 8 * 5); \
+    vec_s16 b6 = VEC_LD(block + 8 * 6); \
+    vec_s16 b7 = VEC_LD(block + 8 * 7);
+
+// these functions do (a*C)>>16
+// things are tricky because a is signed, but C unsigned.
+// M15 is used if C fits in 15 bit unsigned (C6,C7)
+// M16 is used if C requires 16 bits unsigned
+#define M15(a, C) _mm_mulhi_epi16(a, C)
+#define M16(a, C) _mm_add_epi16(a, M15(a, C))
+
+#define IDCT_1D(ADD, SHIFT)\
+    A = _mm_add_epi16(M16(b1, C1), M15(b7, C7)); \
+    B = _mm_sub_epi16(M15(b1, C7), M16(b7, C1)); \
+    C = _mm_add_epi16(M16(b3, C3), M16(b5, C5)); \
+    D = _mm_sub_epi16(M16(b5, C3), M16(b3, C5)); \
+    \
+    Ad = M16(_mm_sub_epi16(A, C), C4); \
+    Bd = M16(_mm_sub_epi16(B, D), C4); \
+    \
+    Cd = _mm_add_epi16(A, C); \
+    Dd = _mm_add_epi16(B, D); \
+    \
+    E = ADD(M16(_mm_add_epi16(b0, b4), C4)); \
+    F = ADD(M16(_mm_sub_epi16(b0, b4), C4)); \
+    \
+    G = _mm_add_epi16(M16(b2, C2), M15(b6, C6)); \
+    H = _mm_sub_epi16(M15(b2, C6), M16(b6, C2)); \
+    \
+    Ed = _mm_sub_epi16(E, G); \
+    Gd = _mm_add_epi16(E, G); \
+    \
+    Add = _mm_add_epi16(F, Ad); \
+    Bdd = _mm_sub_epi16(Bd, H); \
+    \
+    Fd = _mm_sub_epi16(F, Ad); \
+    Hd = _mm_add_epi16(Bd, H); \
+    \
+    b0 = SHIFT(_mm_add_epi16(Gd, Cd)); \
+    b7 = SHIFT(_mm_sub_epi16(Gd, Cd)); \
+    \
+    b1 = SHIFT(_mm_add_epi16(Add, Hd)); \
+    b2 = SHIFT(_mm_sub_epi16(Add, Hd)); \
+    \
+    b3 = SHIFT(_mm_add_epi16(Ed, Dd)); \
+    b4 = SHIFT(_mm_sub_epi16(Ed, Dd)); \
+    \
+    b5 = SHIFT(_mm_add_epi16(Fd, Bdd)); \
+    b6 = SHIFT(_mm_sub_epi16(Fd, Bdd));
+
+#define NOP(a) a
+#define ADD8(a) _mm_add_epi16(a, addv)
+#define SHIFT4(a) _mm_srai_epi16(a, 4)
+
+static void vp3_idct_put_e2k(uint8_t *dst, ptrdiff_t stride, int16_t block[64])
+{
+    vec_u8 vdst;
+    IDCT_START(2048)
+
+    IDCT_1D(NOP, NOP)
+    TRANSPOSE8(b0, b1, b2, b3, b4, b5, b6, b7);
+    IDCT_1D(ADD8, SHIFT4)
+
+#define PUT(a) \
+    vdst = _mm_packus_epi16(a, a); \
+    VEC_STL(dst, vdst);
+
+    PUT(b0)     dst += stride;
+    PUT(b1)     dst += stride;
+    PUT(b2)     dst += stride;
+    PUT(b3)     dst += stride;
+    PUT(b4)     dst += stride;
+    PUT(b5)     dst += stride;
+    PUT(b6)     dst += stride;
+    PUT(b7)
+    memset(block, 0, sizeof(*block) * 64);
+}
+
+static void vp3_idct_add_e2k(uint8_t *dst, ptrdiff_t stride, int16_t block[64])
+{
+    LOAD_ZERO;
+    vec_u8 vdst;
+    vec_s16 vdst_16;
+
+    IDCT_START(0)
+
+    IDCT_1D(NOP, NOP)
+    TRANSPOSE8(b0, b1, b2, b3, b4, b5, b6, b7);
+    IDCT_1D(ADD8, SHIFT4)
+
+#define ADD(a) \
+    vdst = VEC_LD8(dst); \
+    vdst_16 = _mm_unpacklo_epi8(vdst, zerov); \
+    vdst_16 = _mm_adds_epi16(a, vdst_16); \
+    vdst = _mm_packus_epi16(vdst_16, vdst_16); \
+    VEC_STL(dst, vdst);
+
+    ADD(b0)     dst += stride;
+    ADD(b1)     dst += stride;
+    ADD(b2)     dst += stride;
+    ADD(b3)     dst += stride;
+    ADD(b4)     dst += stride;
+    ADD(b5)     dst += stride;
+    ADD(b6)     dst += stride;
+    ADD(b7)
+    memset(block, 0, sizeof(*block) * 64);
+}
+
+av_cold void ff_vp3dsp_init_e2k(VP3DSPContext *c, int flags)
+{
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+    // fate theora-coeff-level64
+
+    c->idct_put = vp3_idct_put_e2k;
+    c->idct_add = vp3_idct_add_e2k;
+}
diff --git a/libavcodec/e2k/vp8dsp.c b/libavcodec/e2k/vp8dsp.c
new file mode 100644
index 0000000..6e7d429
--- /dev/null
+++ b/libavcodec/e2k/vp8dsp.c
@@ -0,0 +1,429 @@
+/*
+ * VP8 compatible video decoder
+ *
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (C) 2010 David Conrad
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavutil/mem_internal.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/vp8dsp.h"
+
+#include "hpeldsp.h"
+
+#define REPT4(a, b, c, d) { a, b, c, d,  a, b, c, d,  a, b, c, d,  a, b, c, d }
+
+// h subpel filter uses msum to multiply+add 4 pixel taps at once
+static const uint8_t ALIGNED(16) h_subpel_filters_inner[7][16] =
+{
+    REPT4( -6, 123,  12,  -1),
+    REPT4(-11, 108,  36,  -8),
+    REPT4( -9,  93,  50,  -6),
+    REPT4(-16,  77,  77, -16),
+    REPT4( -6,  50,  93,  -9),
+    REPT4( -8,  36, 108, -11),
+    REPT4( -1,  12, 123,  -6)
+};
+
+// for 6tap filters, these are the outer two taps
+// The zeros mask off pixels 4-7 when filtering 0-3
+// and vice-versa
+static const uint8_t ALIGNED(16) h_subpel_filters_outer[3][16] =
+{
+    REPT4(2, 1, 2, 1),
+    REPT4(3, 3, 3, 3),
+    REPT4(1, 2, 1, 2)
+};
+
+#define INNER_PERM(x) x, x+1, x+2, x+3, x+1, x+2, x+3, x+4
+
+#define INIT_H_SUBPEL_FILTER(j, n, is6tap) \
+    vec_s8 filter_inner = *(__m128i*)h_subpel_filters_inner[j]; \
+    is6tap( \
+        vec_s8 filter_outer = *(__m128i*)h_subpel_filters_outer[(j) >> 1]; \
+        vec_u8 perm_outer = _mm_setr_epi8(0,5, 1,6, 2,7, 3,8, 4,9, 5,10, 6,11, 7,12); \
+    ) \
+    vec_s32 c64 = _mm_set1_epi16(64); \
+    vec_u8 perm_inner_l = _mm_setr_epi8(INNER_PERM(n), INNER_PERM(n + 2)); \
+    vec_u8 perm_inner_h = _mm_setr_epi8(INNER_PERM(n + 4), INNER_PERM(n + 6)); \
+    __m128i v0, v1; \
+    int i
+
+#define FILTER_H(a, is6tap) \
+    v0 = _mm_shuffle_epi8(a, perm_inner_l); \
+    v1 = _mm_shuffle_epi8(a, perm_inner_h); \
+    v0 = _mm_maddubs_epi16(v0, filter_inner); \
+    v1 = _mm_maddubs_epi16(v1, filter_inner); \
+    v0 = _mm_hadds_epi16(v0, v1); \
+    is6tap( \
+        a = _mm_shuffle_epi8(a, perm_outer); \
+        v0 = _mm_adds_epi16(v0, _mm_maddubs_epi16(a, filter_outer)); \
+    ) \
+    v0 = _mm_adds_epi16(v0, c64); \
+    a = _mm_srai_epi16(v0, 7)
+
+#define INIT_H_SUBPEL_FILTER4(j, n, is6tap) \
+    __m64 filter_inner = *(__m64*)h_subpel_filters_inner[j]; \
+    is6tap( \
+        __m64 filter_outer = *(__m64*)h_subpel_filters_outer[(j) >> 1]; \
+        __m64 perm_outer = _mm_setr_pi8(0,5, 1,6, 2,7, 3,8); \
+        __m64 a1; \
+    ) \
+    __m64 c64 = _mm_set1_pi16(64); \
+    __m64 perm_inner_l = _mm_setr_pi8(INNER_PERM(n)); \
+    __m64 perm_inner_h = _mm_setr_pi8(INNER_PERM(n + 2)); \
+    __m64 v0, v1, a0; \
+    int i
+
+#define FILTER_H4(is6tap) \
+    v0 = _mm_shuffle_pi8(a0, perm_inner_l); \
+    v1 = _mm_shuffle_pi8(a0, perm_inner_h); \
+    v0 = _mm_maddubs_pi16(v0, filter_inner); \
+    v1 = _mm_maddubs_pi16(v1, filter_inner); \
+    v0 = _mm_hadds_pi16(v0, v1); \
+    is6tap( \
+        a0 = _mm_shuffle2_pi8(a0, a1, perm_outer); \
+        v0 = _mm_adds_pi16(v0, _mm_maddubs_pi16(a0, filter_outer)); \
+    ) \
+    v0 = _mm_adds_pi16(v0, c64); \
+    a0 = _mm_srai_pi16(v0, 7); \
+    a0 = _mm_packs_pu16(a0, a0); \
+    *(uint32_t*)dst = _mm_cvtsi64_si32(a0)
+
+#define COPY(code) code
+#define NOP(code)
+#define IF6TAP(code) code
+
+static void put_vp8_epel16_h6_e2k(uint8_t *dst, ptrdiff_t dst_stride, const uint8_t *src,
+                                  ptrdiff_t src_stride, int h, int mx, int my)
+{
+    INIT_H_SUBPEL_FILTER(mx - 1, 1, IF6TAP);
+    __m128i a0, a1;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        a0 = VEC_LD(src - 2);
+        a1 = VEC_LD(src - 2 + 8);
+        FILTER_H(a0, IF6TAP);
+        FILTER_H(a1, IF6TAP);
+        a0 = _mm_packus_epi16(a0, a1);
+        VEC_ST(dst, a0);
+        src += src_stride;
+        dst += dst_stride;
+    }
+}
+
+static void put_vp8_epel8_h6_e2k(uint8_t *dst, ptrdiff_t dst_stride, const uint8_t *src,
+                                  ptrdiff_t src_stride, int h, int mx, int my)
+{
+    INIT_H_SUBPEL_FILTER(mx - 1, 1, IF6TAP);
+    __m128i a0;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        a0 = VEC_LD(src - 2);
+        FILTER_H(a0, IF6TAP);
+        a0 = _mm_packus_epi16(a0, a0);
+        VEC_STL(dst, a0);
+        src += src_stride;
+        dst += dst_stride;
+    }
+}
+
+static void put_vp8_epel8_h4_e2k(uint8_t *dst, ptrdiff_t dst_stride, const uint8_t *src,
+                                  ptrdiff_t src_stride, int h, int mx, int my)
+{
+    INIT_H_SUBPEL_FILTER(mx - 1, 0, NOP);
+    __m128i a0;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        a0 = VEC_LD(src - 1);
+        FILTER_H(a0, NOP);
+        a0 = _mm_packus_epi16(a0, a0);
+        VEC_STL(dst, a0);
+        src += src_stride;
+        dst += dst_stride;
+    }
+}
+
+static void put_vp8_epel4_h6_e2k(uint8_t *dst, ptrdiff_t dst_stride, const uint8_t *src,
+                                  ptrdiff_t src_stride, int h, int mx, int my)
+{
+    INIT_H_SUBPEL_FILTER4(mx - 1, 1, IF6TAP);
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        a0 = *(__m64*)(src - 2);
+        a1 = _mm_cvtsi32_si64(src[8 - 2]);
+        FILTER_H4(IF6TAP);
+        src += src_stride;
+        dst += dst_stride;
+    }
+}
+
+static void put_vp8_epel4_h4_e2k(uint8_t *dst, ptrdiff_t dst_stride, const uint8_t *src,
+                                  ptrdiff_t src_stride, int h, int mx, int my)
+{
+    INIT_H_SUBPEL_FILTER4(mx - 1, 0, NOP);
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        a0 = *(__m64*)(src - 1);
+        FILTER_H4(NOP);
+        src += src_stride;
+        dst += dst_stride;
+    }
+}
+
+#define PAIR_8X2(a, b) (a & 255) | b * 256
+static const int16_t v_subpel_filters[7][3] =
+{
+    { PAIR_8X2( -6, 123), PAIR_8X2( 12,  -1), PAIR_8X2(0, 0) },
+    { PAIR_8X2(-11, 108), PAIR_8X2( 36,  -8), PAIR_8X2(2, 1) },
+    { PAIR_8X2( -9,  93), PAIR_8X2( 50,  -6), PAIR_8X2(0, 0) },
+    { PAIR_8X2(-16,  77), PAIR_8X2( 77, -16), PAIR_8X2(3, 3) },
+    { PAIR_8X2( -6,  50), PAIR_8X2( 93,  -9), PAIR_8X2(0, 0) },
+    { PAIR_8X2( -8,  36), PAIR_8X2(108, -11), PAIR_8X2(1, 2) },
+    { PAIR_8X2( -1,  12), PAIR_8X2(123,  -6), PAIR_8X2(0, 0) }
+};
+
+#define INIT_V_SUBPEL_FILTER(p, type, j, is6tap) \
+    type v0, v1, r0; \
+    type c64 = _mm_set1_##p(64); \
+    type f0 = _mm_set1_##p(v_subpel_filters[j][0]); \
+    type f1 = _mm_set1_##p(v_subpel_filters[j][1]); \
+    is6tap(type f2 = _mm_set1_##p(v_subpel_filters[j][2]);) \
+    int i
+
+#define FILTER_V(p, dstv, lo, CVT, is6tap) \
+    v0 = _mm_maddubs_##p(_mm_unpack##lo(CVT(s1), CVT(s2)), f0); \
+    v1 = _mm_maddubs_##p(_mm_unpack##lo(CVT(s3), CVT(s4)), f1); \
+    v0 = _mm_adds_##p(v0, v1); \
+    is6tap( \
+        v1 = _mm_maddubs_##p(_mm_unpack##lo(CVT(s0), CVT(s5)), f2); \
+        v0 = _mm_adds_##p(v0, v1); \
+    ) \
+    v0 = _mm_adds_##p(v0, c64); \
+    dstv = _mm_srai_##p(v0, 7)
+
+static void put_vp8_epel16_v6_e2k(uint8_t *dst, ptrdiff_t dst_stride, const uint8_t *src, 
+                                  ptrdiff_t src_stride, int h, int mx, int my)
+{
+    INIT_V_SUBPEL_FILTER(epi16, __m128i, my - 1, IF6TAP);
+    __m128i s0, s1, s2, s3, s4, s5;
+
+    s0 = VEC_LD(src - 2 * src_stride);
+    s1 = VEC_LD(src - 1 * src_stride);
+    s2 = VEC_LD(src);
+    s3 = VEC_LD(src + 1 * src_stride);
+    s4 = VEC_LD(src + 2 * src_stride);
+    src += src_stride * 3;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        s5 = VEC_LD(src);
+        FILTER_V(epi16, r0, lo_epi8, COPY, IF6TAP);
+        FILTER_V(epi16, v0, hi_epi8, COPY, IF6TAP);
+        r0 = _mm_packus_epi16(r0, v0);
+        VEC_ST(dst, r0);
+        s0 = s1; s1 = s2; s2 = s3; s3 = s4; s4 = s5;
+        dst += dst_stride;
+        src += src_stride;
+    }
+}
+
+static void put_vp8_epel8_v6_e2k(uint8_t *dst, ptrdiff_t dst_stride, const uint8_t *src, 
+                                 ptrdiff_t src_stride, int h, int mx, int my)
+{
+    INIT_V_SUBPEL_FILTER(epi16, __m128i, my - 1, IF6TAP);
+    __m64 s0, s1, s2, s3, s4, s5;
+
+    s0 = *(__m64*)(src - 2 * src_stride);
+    s1 = *(__m64*)(src - 1 * src_stride);
+    s2 = *(__m64*)src;
+    s3 = *(__m64*)(src + 1 * src_stride);
+    s4 = *(__m64*)(src + 2 * src_stride);
+    src += src_stride * 3;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        s5 = *(__m64*)src;
+        FILTER_V(epi16, r0, lo_epi8, _mm_movpi64_epi64, IF6TAP);
+        r0 = _mm_packus_epi16(r0, r0);
+        VEC_STL(dst, r0);
+        s0 = s1; s1 = s2; s2 = s3; s3 = s4; s4 = s5;
+        dst += dst_stride;
+        src += src_stride;
+    }
+}
+
+static void put_vp8_epel4_v6_e2k(uint8_t *dst, ptrdiff_t dst_stride, const uint8_t *src, 
+                                 ptrdiff_t src_stride, int h, int mx, int my)
+{
+    INIT_V_SUBPEL_FILTER(pi16, __m64, my - 1, IF6TAP);
+    __m64 s0, s1, s2, s3, s4, s5;
+
+    s0 = _mm_cvtsi32_si64(*(uint32_t*)(src - 2 * src_stride));
+    s1 = _mm_cvtsi32_si64(*(uint32_t*)(src - 1 * src_stride));
+    s2 = _mm_cvtsi32_si64(*(uint32_t*)src);
+    s3 = _mm_cvtsi32_si64(*(uint32_t*)(src + 1 * src_stride));
+    s4 = _mm_cvtsi32_si64(*(uint32_t*)(src + 2 * src_stride));
+    src += src_stride * 3;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        s5 = _mm_cvtsi32_si64(*(uint32_t*)src);
+        FILTER_V(pi16, r0, lo_pi8, COPY, IF6TAP);
+        r0 = _mm_packs_pu16(r0, r0);
+        *(uint32_t*)dst = _mm_cvtsi64_si32(r0);
+        s0 = s1; s1 = s2; s2 = s3; s3 = s4; s4 = s5;
+        dst += dst_stride;
+        src += src_stride;
+    }
+}
+
+static void put_vp8_epel8_v4_e2k(uint8_t *dst, ptrdiff_t dst_stride, const uint8_t *src, 
+                                 ptrdiff_t src_stride, int h, int mx, int my)
+{
+    INIT_V_SUBPEL_FILTER(epi16, __m128i, my - 1, NOP);
+    __m64 s1, s2, s3, s4;
+
+    s1 = *(__m64*)(src - 1 * src_stride);
+    s2 = *(__m64*)src;
+    s3 = *(__m64*)(src + 1 * src_stride);
+    src += src_stride * 2;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        s4 = *(__m64*)src;
+        FILTER_V(epi16, r0, lo_epi8, _mm_movpi64_epi64, NOP);
+        r0 = _mm_packus_epi16(r0, r0);
+        VEC_STL(dst, r0);
+        s1 = s2; s2 = s3; s3 = s4;
+        dst += dst_stride;
+        src += src_stride;
+    }
+}
+
+static void put_vp8_epel4_v4_e2k(uint8_t *dst, ptrdiff_t dst_stride, const uint8_t *src, 
+                                 ptrdiff_t src_stride, int h, int mx, int my)
+{
+    INIT_V_SUBPEL_FILTER(pi16, __m64, my - 1, NOP);
+    __m64 s1, s2, s3, s4;
+
+    s1 = _mm_cvtsi32_si64(*(uint32_t*)(src - 1 * src_stride));
+    s2 = _mm_cvtsi32_si64(*(uint32_t*)src);
+    s3 = _mm_cvtsi32_si64(*(uint32_t*)(src + 1 * src_stride));
+    src += src_stride * 2;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        s4 = _mm_cvtsi32_si64(*(uint32_t*)src);
+        FILTER_V(pi16, r0, lo_pi8, COPY, NOP);
+        r0 = _mm_packs_pu16(r0, r0);
+        *(uint32_t*)dst = _mm_cvtsi64_si32(r0);
+        s1 = s2; s2 = s3; s3 = s4;
+        dst += dst_stride;
+        src += src_stride;
+    }
+}
+
+#define EPEL_HV(WIDTH, HTAPS, VTAPS) \
+static void put_vp8_epel##WIDTH##_h##HTAPS##v##VTAPS##_e2k(uint8_t *dst, ptrdiff_t dstride, \
+                                                           const uint8_t *src, ptrdiff_t sstride, \
+                                                           int h, int mx, int my) \
+{ \
+    DECLARE_ALIGNED(16, uint8_t, tmp)[(2*WIDTH+5)*16]; \
+    if (VTAPS == 6) { \
+        put_vp8_epel##WIDTH##_h##HTAPS##_e2k(tmp, 16,      src-2*sstride, sstride, h+5, mx, my); \
+        put_vp8_epel##WIDTH##_v##VTAPS##_e2k(dst, dstride, tmp+2*16,      16,      h,   mx, my); \
+    } else { \
+        put_vp8_epel##WIDTH##_h##HTAPS##_e2k(tmp, 16,      src-sstride, sstride, h+4, mx, my); \
+        put_vp8_epel##WIDTH##_v##VTAPS##_e2k(dst, dstride, tmp+16,      16,      h,   mx, my); \
+    } \
+}
+
+EPEL_HV(16, 6,6)
+EPEL_HV(8,  6,6)
+EPEL_HV(8,  4,6)
+EPEL_HV(8,  6,4)
+EPEL_HV(8,  4,4)
+EPEL_HV(4,  6,6)
+EPEL_HV(4,  4,6)
+EPEL_HV(4,  6,4)
+EPEL_HV(4,  4,4)
+
+static void put_vp8_pixels16_e2k(uint8_t *dst, ptrdiff_t dstride, const uint8_t *src,
+                                 ptrdiff_t sstride, int h, int mx, int my)
+{
+    __m128i v0, v1, v2, v3;
+    int i;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i += 4) {
+        v0 = VEC_LD(src);
+        v1 = VEC_LD(src + sstride);
+        v2 = VEC_LD(src + sstride * 2);
+        v3 = VEC_LD(src + sstride * 3);
+        VEC_ST(dst, v0);
+        VEC_ST(dst + dstride, v1);
+        VEC_ST(dst + dstride * 2, v2);
+        VEC_ST(dst + dstride * 3, v3);
+        src += sstride * 4;
+        dst += dstride * 4;
+    }
+}
+
+
+av_cold void ff_vp78dsp_init_e2k(VP8DSPContext *c)
+{
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    // checkasm
+    c->put_vp8_epel_pixels_tab[0][0][0] = put_vp8_pixels16_e2k;
+    c->put_vp8_epel_pixels_tab[0][0][2] = put_vp8_epel16_h6_e2k;
+    c->put_vp8_epel_pixels_tab[0][2][0] = put_vp8_epel16_v6_e2k;
+    c->put_vp8_epel_pixels_tab[0][2][2] = put_vp8_epel16_h6v6_e2k;
+
+    c->put_vp8_epel_pixels_tab[1][0][2] = put_vp8_epel8_h6_e2k;
+    c->put_vp8_epel_pixels_tab[1][2][0] = put_vp8_epel8_v6_e2k;
+    c->put_vp8_epel_pixels_tab[1][0][1] = put_vp8_epel8_h4_e2k;
+    c->put_vp8_epel_pixels_tab[1][1][0] = put_vp8_epel8_v4_e2k;
+
+    c->put_vp8_epel_pixels_tab[1][2][2] = put_vp8_epel8_h6v6_e2k;
+    c->put_vp8_epel_pixels_tab[1][1][1] = put_vp8_epel8_h4v4_e2k;
+    c->put_vp8_epel_pixels_tab[1][1][2] = put_vp8_epel8_h6v4_e2k;
+    c->put_vp8_epel_pixels_tab[1][2][1] = put_vp8_epel8_h4v6_e2k;
+
+    c->put_vp8_epel_pixels_tab[2][0][2] = put_vp8_epel4_h6_e2k;
+    c->put_vp8_epel_pixels_tab[2][2][0] = put_vp8_epel4_v6_e2k;
+    c->put_vp8_epel_pixels_tab[2][0][1] = put_vp8_epel4_h4_e2k;
+    c->put_vp8_epel_pixels_tab[2][1][0] = put_vp8_epel4_v4_e2k;
+
+    c->put_vp8_epel_pixels_tab[2][2][2] = put_vp8_epel4_h6v6_e2k;
+    c->put_vp8_epel_pixels_tab[2][1][1] = put_vp8_epel4_h4v4_e2k;
+    c->put_vp8_epel_pixels_tab[2][1][2] = put_vp8_epel4_h6v4_e2k;
+    c->put_vp8_epel_pixels_tab[2][2][1] = put_vp8_epel4_h4v6_e2k;
+}
diff --git a/libavcodec/e2k/vp9dsp.c b/libavcodec/e2k/vp9dsp.c
new file mode 100644
index 0000000..5f3909d
--- /dev/null
+++ b/libavcodec/e2k/vp9dsp.c
@@ -0,0 +1,1739 @@
+/*
+ * VP9 compatible video decoder
+ *
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (C) 2013 Ronald S. Bultje <rsbultje gmail com>
+ * Copyright (C) 2013 Clément Bœsch <u pkh me>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#define BIT_DEPTH 8
+
+#include "config.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavutil/common.h"
+#include "libavutil/intreadwrite.h"
+#include "libavcodec/vp9dsp.h"
+
+#define pixel   uint8_t
+
+#define itxfm_wrapper4(type_a, type_b, sz, bits, has_dconly) \
+static void type_a##_##type_b##_##sz##x##sz##_add_e2k(uint8_t *dst, \
+                                                      ptrdiff_t stride, \
+                                                      int16_t *block, int eob) \
+{ \
+    int j; \
+    int16_t tmp[sz * sz], out[sz * sz]; \
+    __m64 h0, h1, h2, round, zerov = _mm_setzero_si64(); \
+\
+    if (has_dconly && eob == 1) { \
+        int t = ((((int)block[0] * 11585 + (1 << 13)) >> 14) \
+                                 * 11585 + (1 << 13)) >> 14; \
+        block[0] = 0; \
+        t = bits ? (t + (1 << (bits - 1))) >> bits : t; \
+        h1 = _mm_set1_pi16(t); \
+        h2 = _mm_set1_pi16(-t); \
+        h1 = _mm_packs_pu16(h1, h1); \
+        h2 = _mm_packs_pu16(h2, h2); \
+        \
+        PRAGMA_E2K("ivdep") \
+        for (j = 0; j < sz; j++, dst += stride) { \
+            h0 = _mm_cvtsi32_si64(*(uint32_t*)dst); \
+            h0 = _mm_subs_pu8(_mm_adds_pu8(h0, h1), h2); \
+            *(uint32_t*)dst = _mm_cvtsi64_si32(h0); \
+        } \
+        return; \
+    } \
+    \
+    type_a##sz##_1d(block, tmp, 0); \
+    memset(block, 0, sz * sz * sizeof(*block)); \
+    type_b##sz##_1d(tmp, out, 1); \
+    round = _mm_set1_pi16((1 << bits) >> 1); \
+    PRAGMA_E2K("ivdep") \
+    for (j = 0; j < sz; j++, dst += stride) { \
+         h1 = *(__m64*)(out + j * sz); \
+         h1 = _mm_srai_pi16(_mm_add_pi16(h1, round), bits); \
+         h0 = _mm_cvtsi32_si64(*(uint32_t*)dst); \
+         h0 = _mm_unpacklo_pi8(h0, zerov); \
+         h0 = _mm_add_pi16(h0, h1); \
+         h0 = _mm_packs_pu16(h0, h0); \
+         *(uint32_t*)dst = _mm_cvtsi64_si32(h0); \
+    } \
+}
+
+#define itxfm_wrapper8(type_a, type_b, sz, bits, has_dconly) \
+static void type_a##_##type_b##_##sz##x##sz##_add_e2k(uint8_t *dst, \
+                                                      ptrdiff_t stride, \
+                                                      int16_t *block, int eob) \
+{ \
+    int j; \
+    int16_t tmp[sz * sz], out[sz * sz]; \
+    __m128i v0, v1, round; \
+    LOAD_ZERO; \
+\
+    if (has_dconly && eob == 1) { \
+        __m64 h0, h1, h2; \
+        int t = ((((int)block[0] * 11585 + (1 << 13)) >> 14) \
+                                 * 11585 + (1 << 13)) >> 14; \
+        block[0] = 0; \
+        t = bits ? (t + (1 << (bits - 1))) >> bits : t; \
+        h1 = _mm_set1_pi16(t); \
+        h2 = _mm_set1_pi16(-t); \
+        h1 = _mm_packs_pu16(h1, h1); \
+        h2 = _mm_packs_pu16(h2, h2); \
+        \
+        PRAGMA_E2K("ivdep") \
+        for (j = 0; j < sz; j++, dst += stride) { \
+            h0 = *(__m64*)dst; \
+            h0 = _mm_subs_pu8(_mm_adds_pu8(h0, h1), h2); \
+            *(__m64*)dst = h0; \
+        } \
+        return; \
+    } \
+    \
+    type_a##sz##_1d(block, tmp, 0); \
+    memset(block, 0, sz * sz * sizeof(*block)); \
+    type_b##sz##_1d(tmp, out, 1); \
+    round = _mm_set1_epi16((1 << bits) >> 1); \
+    PRAGMA_E2K("ivdep") \
+    for (j = 0; j < sz; j++, dst += stride) { \
+         v1 = VEC_LD(out + j * sz); \
+         v1 = _mm_srai_epi16(_mm_add_epi16(v1, round), bits); \
+         v0 = VEC_LD8(dst); \
+         v0 = _mm_unpacklo_epi8(v0, zerov); \
+         v0 = _mm_add_epi16(v0, v1); \
+         v0 = _mm_packus_epi16(v0, v0); \
+         VEC_STL(dst, v0); \
+    } \
+}
+
+#define itxfm_wrapper16(type_a, type_b, sz, bits, has_dconly) \
+static void type_a##_##type_b##_##sz##x##sz##_add_e2k(uint8_t *dst, \
+                                                      ptrdiff_t stride, \
+                                                      int16_t *block, int eob) \
+{ \
+    int i, j; \
+    int16_t tmp[sz * sz], out[sz * sz]; \
+    __m128i v0, v1, v2, v3, round; \
+    LOAD_ZERO; \
+\
+    if (has_dconly && eob == 1) { \
+        int t = ((((int)block[0] * 11585 + (1 << 13)) >> 14) \
+                                 * 11585 + (1 << 13)) >> 14; \
+        block[0] = 0; \
+        t = bits ? (t + (1 << (bits - 1))) >> bits : t; \
+        v1 = _mm_set1_epi16(t); \
+        v2 = _mm_set1_epi16(-t); \
+        v1 = _mm_packus_epi16(v1, v1); \
+        v2 = _mm_packus_epi16(v2, v2); \
+        \
+        for (j = 0; j < sz; j++, dst += stride) \
+        PRAGMA_E2K("ivdep") \
+        for (i = 0; i < sz; i += 16) { \
+            v0 = VEC_LD(dst + i); \
+            v0 = _mm_subs_epu8(_mm_adds_epu8(v0, v1), v2); \
+            VEC_ST(dst + i, v0); \
+        } \
+        return; \
+    } \
+    \
+    type_a##sz##_1d(block, tmp, 0); \
+    memset(block, 0, sz * sz * sizeof(*block)); \
+    type_b##sz##_1d(tmp, out, 1); \
+    round = _mm_set1_epi16((1 << bits) >> 1); \
+    for (j = 0; j < sz; j++, dst += stride) \
+    PRAGMA_E2K("ivdep") \
+    for (i = 0; i < sz; i += 16) { \
+         v2 = VEC_LD(out + j * sz + i); \
+         v3 = VEC_LD(out + j * sz + i + 8); \
+         v2 = _mm_srai_epi16(_mm_add_epi16(v2, round), bits); \
+         v3 = _mm_srai_epi16(_mm_add_epi16(v3, round), bits); \
+         v1 = VEC_LD(dst + i); \
+         v0 = _mm_unpacklo_epi8(v1, zerov); \
+         v1 = _mm_unpackhi_epi8(v1, zerov); \
+         v0 = _mm_add_epi16(v0, v2); \
+         v1 = _mm_add_epi16(v1, v3); \
+         v0 = _mm_packus_epi16(v0, v1); \
+         VEC_ST(dst + i, v0); \
+    } \
+}
+
+#define IN(x) VEC_LD8(in + (x) * sz)
+
+#define X1(x, a, b) \
+    __m128i x = _mm_set1_epi32((a & 0xffff) | b << 16);
+
+#define X2(x, y, i0, i1) \
+    v1 = _mm_unpacklo_epi16(IN(i0), IN(i1)); \
+    v0 = _mm_madd_epi16(v1, f##x); \
+    v1 = _mm_madd_epi16(v1, f##y); \
+    t##x##a = _mm_srai_epi32(_mm_add_epi32(v0, round), 14); \
+    t##y##a = _mm_srai_epi32(_mm_add_epi32(v1, round), 14);
+
+#define X3(x, y, i0, i1) \
+    v0 = _mm_mullo_epi32(_mm_sub_epi32(i0, i1), c11585); \
+    v1 = _mm_mullo_epi32(_mm_add_epi32(i0, i1), c11585); \
+    x = _mm_srai_epi32(_mm_add_epi32(v0, round), 14); \
+    y = _mm_srai_epi32(_mm_add_epi32(v1, round), 14);
+
+#define X4(x, y, i0, i1, m0, m1) \
+    v0 = _mm_add_epi32(_mm_mullo_epi32(i0, m0), _mm_mullo_epi32(i1, m1)); \
+    v1 = _mm_sub_epi32(_mm_mullo_epi32(i0, m1), _mm_mullo_epi32(i1, m0)); \
+    x = _mm_srai_epi32(_mm_add_epi32(v0, round), 14); \
+    y = _mm_srai_epi32(_mm_add_epi32(v1, round), 14);
+
+#define X5(d, add, a0, a1, a2, a3, b0, b1, b2, b3) \
+    v0 = _mm_##add##_epi32(a0, b0); \
+    v1 = _mm_##add##_epi32(a1, b1); \
+    v2 = _mm_##add##_epi32(a2, b2); \
+    v3 = _mm_##add##_epi32(a3, b3); \
+    v0 = _mm_packs_epi32(v0, v1); \
+    v1 = _mm_packs_epi32(v2, v3); \
+    v2 = _mm_unpacklo_epi16(v0, v1); \
+    v3 = _mm_unpackhi_epi16(v0, v1); \
+    v0 = _mm_unpacklo_epi16(v2, v3); \
+    v1 = _mm_unpackhi_epi16(v2, v3); \
+    VEC_STL(out + d + sz * 0, v0); \
+    VEC_STH(out + d + sz * 1, v0); \
+    VEC_STL(out + d + sz * 2, v1); \
+    VEC_STH(out + d + sz * 3, v1);
+
+#define X6(d, add, a0, a1, a2, a3, b0, b1, b2, b3) \
+    v0 = _mm_##add##_epi32(a0, b0); \
+    v1 = _mm_##add##_epi32(a1, b1); \
+    v2 = _mm_##add##_epi32(a2, b2); \
+    v3 = _mm_##add##_epi32(a3, b3); \
+    v0 = _mm_packs_epi32(v0, v1); \
+    v1 = _mm_packs_epi32(v2, v3); \
+    VEC_STL(out + sz * (d + 0), v0); \
+    VEC_STH(out + sz * (d + 1), v0); \
+    VEC_STL(out + sz * (d + 2), v1); \
+    VEC_STH(out + sz * (d + 3), v1);
+
+static av_always_inline void idct4_1d(const int16_t *in,
+                                      int16_t *out, int pass)
+{
+    __m128i v0, v1, v2, v3;
+    __m128i t0a, t1a, t2a, t3a;
+    __m128i round = _mm_set1_epi32(1 << 13);
+    int sz = 4;
+
+    X1(f0, 11585, 11585)
+    X1(f1, 11585, -11585)
+    X1(f2, 6270, -15137)
+    X1(f3, 15137, 6270)
+
+    X2(0, 1, 0, 2)
+    X2(2, 3, 1, 3)
+
+    v0 = _mm_add_epi32(t0a, t3a);
+    v1 = _mm_add_epi32(t1a, t2a);
+    v2 = _mm_sub_epi32(t1a, t2a);
+    v3 = _mm_sub_epi32(t0a, t3a);
+    v0 = _mm_packs_epi32(v0, v1);
+    v1 = _mm_packs_epi32(v2, v3);
+    if (!pass) {
+        v2 = _mm_unpacklo_epi16(v0, v1);
+        v3 = _mm_unpackhi_epi16(v0, v1);
+        v0 = _mm_unpacklo_epi16(v2, v3);
+        v1 = _mm_unpackhi_epi16(v2, v3);
+    }
+    VEC_STL(out + sz * 0, v0);
+    VEC_STH(out + sz * 1, v0);
+    VEC_STL(out + sz * 2, v1);
+    VEC_STH(out + sz * 3, v1);
+}
+
+static av_always_inline void idct8_1d(const int16_t *in,
+                                      int16_t *out, int pass)
+{
+    __m128i v0, v1, v2, v3;
+    __m128i t0a, t1a, t2a, t3a, t4a, t5a, t6a, t7a;
+    __m128i t0, t1, t2, t3, t4, t5, t6, t7;
+    __m128i round = _mm_set1_epi32(1 << 13);
+    __m128i c11585 = _mm_set1_epi32(11585);
+
+    X1(f0, 11585, 11585)
+    X1(f1, 11585, -11585)
+    X1(f2, 6270, -15137)
+    X1(f3, 15137, 6270)
+
+    X1(f4, 3196, -16069)
+    X1(f7, 16069, 3196)
+    X1(f5, 13623, -9102)
+    X1(f6, 9102, 13623)
+
+    int i, sz = 8;
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < sz; i += 4, in += 4) {
+        X2(0, 1, 0, 4)
+        X2(2, 3, 2, 6)
+        X2(4, 7, 1, 7)
+        X2(5, 6, 5, 3)
+
+        t0  = _mm_add_epi32(t0a, t3a);
+        t1  = _mm_add_epi32(t1a, t2a);
+        t2  = _mm_sub_epi32(t1a, t2a);
+        t3  = _mm_sub_epi32(t0a, t3a);
+        t4  = _mm_add_epi32(t4a, t5a);
+        t5a = _mm_sub_epi32(t4a, t5a);
+        t7  = _mm_add_epi32(t7a, t6a);
+        t6a = _mm_sub_epi32(t7a, t6a);
+
+        X3(t5, t6, t6a, t5a)
+
+        if (!pass) {
+            X5(0, add, t0, t1, t2, t3, t7, t6, t5, t4)
+            X5(4, sub, t3, t2, t1, t0, t4, t5, t6, t7)
+            out += 4 * sz;
+        } else {
+            X6(0, add, t0, t1, t2, t3, t7, t6, t5, t4)
+            X6(4, sub, t3, t2, t1, t0, t4, t5, t6, t7)
+            out += 4;
+        }
+    }
+}
+
+static av_always_inline void idct16_1d(const int16_t *in,
+                                       int16_t *out, int pass)
+{
+    __m128i v0, v1, v2, v3;
+    __m128i t0a, t1a, t2a, t3a, t4a, t5a, t6a, t7a;
+    __m128i t8a, t9a, t10a, t11a, t12a, t13a, t14a, t15a;
+    __m128i t0, t1, t2, t3, t4, t5, t6, t7;
+    __m128i t8, t9, t10, t11, t12, t13, t14, t15;
+    __m128i round = _mm_set1_epi32(1 << 13);
+    __m128i c11585 = _mm_set1_epi32(11585);
+    __m128i m15137 = _mm_set1_epi32(-15137), c6270 = _mm_set1_epi32(6270);
+
+    X1(f0, 11585, 11585)
+    X1(f1, 11585, -11585)
+    X1(f2, 6270, -15137)
+    X1(f3, 15137, 6270)
+
+    X1(f4, 3196, -16069)
+    X1(f7, 16069, 3196)
+    X1(f5, 13623, -9102)
+    X1(f6, 9102, 13623)
+
+    X1(f8, 1606, -16305)
+    X1(f15, 16305, 1606)
+    X1(f9, 12665, -10394)
+    X1(f14, 10394, 12665)
+    X1(f10, 7723, -14449)
+    X1(f13, 14449, 7723)
+    X1(f11, 15679, -4756)
+    X1(f12, 4756, 15679)
+
+    int i, sz = 16;
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < sz; i += 4, in += 4) {
+        X2( 0,  1,  0,  8)
+        X2( 2,  3,  4, 12)
+        X2( 4,  7,  2, 14)
+        X2( 5,  6, 10,  6)
+        X2( 8, 15,  1, 15)
+        X2( 9, 14,  9,  7)
+        X2(10, 13,  5, 11)
+        X2(11, 12, 13,  3)
+
+        t0  = _mm_add_epi32(t0a, t3a);
+        t1  = _mm_add_epi32(t1a, t2a);
+        t2  = _mm_sub_epi32(t1a, t2a);
+        t3  = _mm_sub_epi32(t0a, t3a);
+        t4  = _mm_add_epi32(t4a, t5a);
+        t5  = _mm_sub_epi32(t4a, t5a);
+        t6  = _mm_sub_epi32(t7a, t6a);
+        t7  = _mm_add_epi32(t7a, t6a);
+        t8  = _mm_add_epi32(t8a, t9a);
+        t9  = _mm_sub_epi32(t8a, t9a);
+        t10 = _mm_sub_epi32(t11a, t10a);
+        t11 = _mm_add_epi32(t11a, t10a);
+        t12 = _mm_add_epi32(t12a, t13a);
+        t13 = _mm_sub_epi32(t12a, t13a);
+        t14 = _mm_sub_epi32(t15a, t14a);
+        t15 = _mm_add_epi32(t15a, t14a);
+
+        X3( t5a,  t6a, t6,  t5)
+        X4( t9a, t14a, t9,  t14, m15137, c6270)
+        X4(t13a, t10a, t13, t10, c6270, m15137)
+
+        t0a  = _mm_add_epi32(t0, t7);
+        t1a  = _mm_add_epi32(t1, t6a);
+        t2a  = _mm_add_epi32(t2, t5a);
+        t3a  = _mm_add_epi32(t3, t4);
+        t4   = _mm_sub_epi32(t3, t4);
+        t5   = _mm_sub_epi32(t2, t5a);
+        t6   = _mm_sub_epi32(t1, t6a);
+        t7   = _mm_sub_epi32(t0, t7);
+        t8a  = _mm_add_epi32(t8, t11);
+        t9   = _mm_add_epi32(t9a, t10a);
+        t10  = _mm_sub_epi32(t9a, t10a);
+        t11a = _mm_sub_epi32(t8, t11);
+        t12a = _mm_sub_epi32(t15, t12);
+        t13  = _mm_sub_epi32(t14a, t13a);
+        t14  = _mm_add_epi32(t14a, t13a);
+        t15a = _mm_add_epi32(t15, t12);
+
+        X3(t10a, t13a, t13, t10)
+        X3(t11, t12, t12a, t11a)
+
+        if (!pass) {
+            X5( 0, add, t0a, t1a, t2a, t3a, t15a, t14, t13a, t12)
+            X5( 4, add, t4,  t5,  t6,  t7,  t11, t10a, t9, t8a)
+            X5( 8, sub, t7,  t6,  t5,  t4,  t8a, t9, t10a, t11)
+            X5(12, sub, t3a, t2a, t1a, t0a, t12, t13a, t14, t15a)
+            out += 4 * sz;
+        } else {
+            X6( 0, add, t0a, t1a, t2a, t3a, t15a, t14, t13a, t12)
+            X6( 4, add, t4,  t5,  t6,  t7,  t11, t10a, t9, t8a)
+            X6( 8, sub, t7,  t6,  t5,  t4,  t8a, t9, t10a, t11)
+            X6(12, sub, t3a, t2a, t1a, t0a, t12, t13a, t14, t15a)
+            out += 4;
+        }
+    }
+}
+
+static av_always_inline void idct32_1d(const int16_t *in,
+                                       int16_t *out, int pass)
+{
+    __m128i v0, v1, v2, v3;
+    __m128i t0a, t1a, t2a, t3a, t4a, t5a, t6a, t7a;
+    __m128i t8a, t9a, t10a, t11a, t12a, t13a, t14a, t15a;
+    __m128i t16a, t17a, t18a, t19a, t20a, t21a, t22a, t23a;
+    __m128i t24a, t25a, t26a, t27a, t28a, t29a, t30a, t31a;
+    __m128i t0, t1, t2, t3, t4, t5, t6, t7;
+    __m128i t8, t9, t10, t11, t12, t13, t14, t15;
+    __m128i t16, t17, t18, t19, t20, t21, t22, t23;
+    __m128i t24, t25, t26, t27, t28, t29, t30, t31;
+    __m128i round = _mm_set1_epi32(1 << 13);
+    __m128i c11585 = _mm_set1_epi32(11585);
+    __m128i m15137 = _mm_set1_epi32(-15137), c6270 = _mm_set1_epi32(6270);
+    __m128i m16069 = _mm_set1_epi32(-16069), c3196 = _mm_set1_epi32(3196);
+    __m128i m9102 = _mm_set1_epi32(-9102), c13623 = _mm_set1_epi32(13623);
+
+    X1(f0, 11585, 11585)
+    X1(f1, 11585, -11585)
+    X1(f2, 6270, -15137)
+    X1(f3, 15137, 6270)
+
+    X1(f4, 3196, -16069)
+    X1(f7, 16069, 3196)
+    X1(f5, 13623, -9102)
+    X1(f6, 9102, 13623)
+
+    X1(f8, 1606, -16305)
+    X1(f15, 16305, 1606)
+    X1(f9, 12665, -10394)
+    X1(f14, 10394, 12665)
+    X1(f10, 7723, -14449)
+    X1(f13, 14449, 7723)
+    X1(f11, 15679, -4756)
+    X1(f12, 4756, 15679)
+
+    X1(f16, 804, -16364)
+    X1(f31, 16364, 804)
+    X1(f17, 12140, -11003)
+    X1(f30, 11003, 12140)
+    X1(f18, 7005, -14811)
+    X1(f29, 14811, 7005)
+    X1(f19, 15426, -5520)
+    X1(f28, 5520, 15426)
+    X1(f20, 3981, -15893)
+    X1(f27, 15893, 3981)
+    X1(f21, 14053, -8423U)
+    X1(f26, 8423U, 14053)
+    X1(f22, 9760, -13160)
+    X1(f25, 13160, 9760)
+    X1(f23, 16207, -2404)
+    X1(f24, 2404, 16207)
+
+    int i, sz = 32;
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < sz; i += 4, in += 4) {
+        X2( 0,  1,  0, 16)
+        X2( 2,  3,  8, 24)
+        X2( 4,  7,  4, 28)
+        X2( 5,  6, 20, 12)
+        X2( 8, 15,  2, 30)
+        X2( 9, 14, 18, 14)
+        X2(10, 13, 10, 22)
+        X2(11, 12, 26,  6)
+        X2(16, 31,  1, 31)
+        X2(17, 30, 17, 15)
+        X2(18, 29,  9, 23)
+        X2(19, 28, 25,  7)
+        X2(20, 27,  5, 27)
+        X2(21, 26, 21, 11)
+        X2(22, 25, 13, 19)
+        X2(23, 24, 29,  3)
+
+        t0  = _mm_add_epi32(t0a, t3a);
+        t1  = _mm_add_epi32(t1a, t2a);
+        t2  = _mm_sub_epi32(t1a, t2a);
+        t3  = _mm_sub_epi32(t0a, t3a);
+        t4  = _mm_add_epi32(t4a, t5a);
+        t5  = _mm_sub_epi32(t4a, t5a);
+        t6  = _mm_sub_epi32(t7a, t6a);
+        t7  = _mm_add_epi32(t7a, t6a);
+        t8  = _mm_add_epi32(t8a, t9a);
+        t9  = _mm_sub_epi32(t8a, t9a);
+        t10 = _mm_sub_epi32(t11a, t10a);
+        t11 = _mm_add_epi32(t11a, t10a);
+        t12 = _mm_add_epi32(t12a, t13a);
+        t13 = _mm_sub_epi32(t12a, t13a);
+        t14 = _mm_sub_epi32(t15a, t14a);
+        t15 = _mm_add_epi32(t15a, t14a);
+        t16 = _mm_add_epi32(t16a, t17a);
+        t17 = _mm_sub_epi32(t16a, t17a);
+        t18 = _mm_sub_epi32(t19a, t18a);
+        t19 = _mm_add_epi32(t19a, t18a);
+        t20 = _mm_add_epi32(t20a, t21a);
+        t21 = _mm_sub_epi32(t20a, t21a);
+        t22 = _mm_sub_epi32(t23a, t22a);
+        t23 = _mm_add_epi32(t23a, t22a);
+        t24 = _mm_add_epi32(t24a, t25a);
+        t25 = _mm_sub_epi32(t24a, t25a);
+        t26 = _mm_sub_epi32(t27a, t26a);
+        t27 = _mm_add_epi32(t27a, t26a);
+        t28 = _mm_add_epi32(t28a, t29a);
+        t29 = _mm_sub_epi32(t28a, t29a);
+        t30 = _mm_sub_epi32(t31a, t30a);
+        t31 = _mm_add_epi32(t31a, t30a);
+
+        X3( t5a,  t6a, t6,  t5)
+        X4( t9a, t14a, t9,  t14, m15137, c6270)
+        X4(t13a, t10a, t13, t10, c6270, m15137)
+        X4(t17a, t30a, t17, t30, m16069, c3196)
+        X4(t29a, t18a, t29, t18, c3196, m16069)
+        X4(t21a, t26a, t21, t26, m9102, c13623)
+        X4(t25a, t22a, t25, t22, c13623, m9102)
+
+        t0a  = _mm_add_epi32(t0, t7);
+        t1a  = _mm_add_epi32(t1, t6a);
+        t2a  = _mm_add_epi32(t2, t5a);
+        t3a  = _mm_add_epi32(t3, t4);
+        t4a  = _mm_sub_epi32(t3, t4);
+        t5   = _mm_sub_epi32(t2, t5a);
+        t6   = _mm_sub_epi32(t1, t6a);
+        t7a  = _mm_sub_epi32(t0, t7);
+        t8a  = _mm_add_epi32(t8, t11);
+        t9   = _mm_add_epi32(t9a, t10a);
+        t10  = _mm_sub_epi32(t9a, t10a);
+        t11a = _mm_sub_epi32(t8, t11);
+        t12a = _mm_sub_epi32(t15, t12);
+        t13  = _mm_sub_epi32(t14a, t13a);
+        t14  = _mm_add_epi32(t14a, t13a);
+        t15a = _mm_add_epi32(t15, t12);
+        t16a = _mm_add_epi32(t16, t19);
+        t17  = _mm_add_epi32(t17a, t18a);
+        t18  = _mm_sub_epi32(t17a, t18a);
+        t19a = _mm_sub_epi32(t16, t19);
+        t20a = _mm_sub_epi32(t23, t20);
+        t21  = _mm_sub_epi32(t22a, t21a);
+        t22  = _mm_add_epi32(t22a, t21a);
+        t23a = _mm_add_epi32(t23, t20);
+        t24a = _mm_add_epi32(t24, t27);
+        t25  = _mm_add_epi32(t25a, t26a);
+        t26  = _mm_sub_epi32(t25a, t26a);
+        t27a = _mm_sub_epi32(t24, t27);
+        t28a = _mm_sub_epi32(t31, t28);
+        t29  = _mm_sub_epi32(t30a, t29a);
+        t30  = _mm_add_epi32(t30a, t29a);
+        t31a = _mm_add_epi32(t31, t28);
+
+        X3(t10a, t13a, t13, t10)
+        X3(t11, t12, t12a, t11a)
+        X4(t18a, t29a, t18, t29, m15137, c6270)
+        X4(t19, t28, t19a, t28a, m15137, c6270)
+        X4(t27, t20, t27a, t20a, c6270, m15137)
+        X4(t26a, t21a, t26, t21, c6270, m15137)
+
+        t0   = _mm_add_epi32(t0a, t15a);
+        t1   = _mm_add_epi32(t1a, t14);
+        t2   = _mm_add_epi32(t2a, t13a);
+        t3   = _mm_add_epi32(t3a, t12);
+        t4   = _mm_add_epi32(t4a, t11);
+        t5a  = _mm_add_epi32(t5, t10a);
+        t6a  = _mm_add_epi32(t6, t9);
+        t7   = _mm_add_epi32(t7a, t8a);
+        t8   = _mm_sub_epi32(t7a, t8a);
+        t9a  = _mm_sub_epi32(t6, t9);
+        t10  = _mm_sub_epi32(t5, t10a);
+        t11a = _mm_sub_epi32(t4a, t11);
+        t12a = _mm_sub_epi32(t3a, t12);
+        t13  = _mm_sub_epi32(t2a, t13a);
+        t14a = _mm_sub_epi32(t1a, t14);
+        t15  = _mm_sub_epi32(t0a, t15a);
+        t16  = _mm_add_epi32(t16a, t23a);
+        t17a = _mm_add_epi32(t17, t22);
+        t18  = _mm_add_epi32(t18a, t21a);
+        t19a = _mm_add_epi32(t19, t20);
+        t20a = _mm_sub_epi32(t19, t20);
+        t21  = _mm_sub_epi32(t18a, t21a);
+        t22a = _mm_sub_epi32(t17, t22);
+        t23  = _mm_sub_epi32(t16a, t23a);
+        t24  = _mm_sub_epi32(t31a, t24a);
+        t25a = _mm_sub_epi32(t30, t25);
+        t26  = _mm_sub_epi32(t29a, t26a);
+        t27a = _mm_sub_epi32(t28, t27);
+        t28a = _mm_add_epi32(t28, t27);
+        t29  = _mm_add_epi32(t29a, t26a);
+        t30a = _mm_add_epi32(t30, t25);
+        t31  = _mm_add_epi32(t31a, t24a);
+
+        X3(t20, t27, t27a, t20a)
+        X3(t21a, t26a, t26, t21)
+        X3(t22, t25, t25a, t22a)
+        X3(t23a, t24a, t24, t23)
+
+        if (!pass) {
+            X5( 0, add, t0,   t1,  t2,   t3,   t31, t30a, t29, t28a)
+            X5( 4, add, t4,   t5a, t6a,  t7,   t27, t26a, t25, t24a)
+            X5( 8, add, t8,   t9a, t10,  t11a, t23a, t22, t21a, t20)
+            X5(12, add, t12a, t13, t14a, t15,  t19a, t18, t17a, t16)
+            X5(16, sub, t15,  t14a, t13, t12a, t16, t17a, t18, t19a)
+            X5(20, sub, t11a, t10,  t9a, t8,   t20, t21a, t22, t23a)
+            X5(24, sub, t7,   t6a,  t5a, t4,   t24a, t25, t26a, t27)
+            X5(28, sub, t3,   t2,   t1,  t0,   t28a, t29, t30a, t31)
+            out += 4 * sz;
+        } else {
+            X6( 0, add, t0,   t1,  t2,   t3,   t31, t30a, t29, t28a)
+            X6( 4, add, t4,   t5a, t6a,  t7,   t27, t26a, t25, t24a)
+            X6( 8, add, t8,   t9a, t10,  t11a, t23a, t22, t21a, t20)
+            X6(12, add, t12a, t13, t14a, t15,  t19a, t18, t17a, t16)
+            X6(16, sub, t15,  t14a, t13, t12a, t16, t17a, t18, t19a)
+            X6(20, sub, t11a, t10,  t9a, t8,   t20, t21a, t22, t23a)
+            X6(24, sub, t7,   t6a,  t5a, t4,   t24a, t25, t26a, t27)
+            X6(28, sub, t3,   t2,   t1,  t0,   t28a, t29, t30a, t31)
+            out += 4;
+        }
+    }
+}
+
+#undef IN
+#undef X1
+#undef X2
+#undef X3
+#undef X4
+#undef X5
+#undef X6
+
+itxfm_wrapper4(idct, idct, 4, 4, 1)
+itxfm_wrapper8(idct, idct, 8, 5, 1)
+itxfm_wrapper16(idct, idct, 16, 6, 1)
+itxfm_wrapper16(idct, idct, 32, 6, 1)
+
+#undef itxfm_wrapper4
+#undef itxfm_wrapper8
+#undef itxfm_wrapper16
+
+static av_cold void ff_vp9dsp_itxfm_init_8_e2k(VP9DSPContext *dsp)
+{
+
+#define init_idct(tx, nm) \
+    dsp->itxfm_add[tx][DCT_DCT]   = \
+    dsp->itxfm_add[tx][ADST_DCT]  = \
+    dsp->itxfm_add[tx][DCT_ADST]  = \
+    dsp->itxfm_add[tx][ADST_ADST] = nm##_add_e2k
+
+    dsp->itxfm_add[TX_4X4][DCT_DCT] = idct_idct_4x4_add_e2k;
+    dsp->itxfm_add[TX_8X8][DCT_DCT] = idct_idct_8x8_add_e2k;
+    dsp->itxfm_add[TX_16X16][DCT_DCT] = idct_idct_16x16_add_e2k;
+
+    init_idct(TX_32X32, idct_idct_32x32);
+
+#undef init_idct
+}
+
+#define LOAD_TRANSPOSE8(dst, a0, a1, a2, a3, a4, a5, a6, a7) \
+    t0 = VEC_LD8(dst + stride * 0); \
+    t1 = VEC_LD8(dst + stride * 1); \
+    t2 = VEC_LD8(dst + stride * 2); \
+    t3 = VEC_LD8(dst + stride * 3); \
+    t4 = VEC_LD8(dst + stride * 4); \
+    t5 = VEC_LD8(dst + stride * 5); \
+    t6 = VEC_LD8(dst + stride * 6); \
+    t7 = VEC_LD8(dst + stride * 7); \
+    t0 = _mm_unpacklo_epi8(t0, t4); \
+    t1 = _mm_unpacklo_epi8(t1, t5); \
+    t2 = _mm_unpacklo_epi8(t2, t6); \
+    t3 = _mm_unpacklo_epi8(t3, t7); \
+    t4 = _mm_unpacklo_epi8(t0, t2); \
+    t5 = _mm_unpackhi_epi8(t0, t2); \
+    t6 = _mm_unpacklo_epi8(t1, t3); \
+    t7 = _mm_unpackhi_epi8(t1, t3); \
+    t0 = _mm_unpacklo_epi8(t4, t6); \
+    t1 = _mm_unpackhi_epi8(t4, t6); \
+    t2 = _mm_unpacklo_epi8(t5, t7); \
+    t3 = _mm_unpackhi_epi8(t5, t7); \
+    a0 = _mm_unpacklo_epi8(t0, zerov); \
+    a1 = _mm_unpackhi_epi8(t0, zerov); \
+    a2 = _mm_unpacklo_epi8(t1, zerov); \
+    a3 = _mm_unpackhi_epi8(t1, zerov); \
+    a4 = _mm_unpacklo_epi8(t2, zerov); \
+    a5 = _mm_unpackhi_epi8(t2, zerov); \
+    a6 = _mm_unpacklo_epi8(t3, zerov); \
+    a7 = _mm_unpackhi_epi8(t3, zerov)
+
+#define STORE_TRANSPOSE8(dst, a0, a1, a2, a3, a4, a5, a6, a7) \
+    t0 = _mm_packus_epi16(a0, a1); \
+    t1 = _mm_packus_epi16(a2, a3); \
+    t2 = _mm_packus_epi16(a4, a5); \
+    t3 = _mm_packus_epi16(a6, a7); \
+    t4 = _mm_unpacklo_epi8(t0, t2); \
+    t5 = _mm_unpackhi_epi8(t0, t2); \
+    t6 = _mm_unpacklo_epi8(t1, t3); \
+    t7 = _mm_unpackhi_epi8(t1, t3); \
+    t0 = _mm_unpacklo_epi8(t4, t6); \
+    t1 = _mm_unpackhi_epi8(t4, t6); \
+    t2 = _mm_unpacklo_epi8(t5, t7); \
+    t3 = _mm_unpackhi_epi8(t5, t7); \
+    t4 = _mm_unpacklo_epi8(t0, t2); \
+    t5 = _mm_unpackhi_epi8(t0, t2); \
+    t6 = _mm_unpacklo_epi8(t1, t3); \
+    t7 = _mm_unpackhi_epi8(t1, t3); \
+    VEC_STL(dst + stride * 0, t4); \
+    VEC_STH(dst + stride * 1, t4); \
+    VEC_STL(dst + stride * 2, t5); \
+    VEC_STH(dst + stride * 3, t5); \
+    VEC_STL(dst + stride * 4, t6); \
+    VEC_STH(dst + stride * 5, t6); \
+    VEC_STL(dst + stride * 6, t7); \
+    VEC_STH(dst + stride * 7, t7)
+
+
+static av_always_inline void loop_filter_h(uint8_t *dst, int E, int I, int H,
+                                           ptrdiff_t stride, int wd)
+{
+    int F = 1;
+    LOAD_ZERO;
+    __m128i t0, t1, t2, t3, t4, t5, t6, t7;
+    __m128i vfm, vflat8out, vflat8in, vhev;
+    __m128i p2a, p1a, p0a, q0a, q1a, q2a;
+    __m128i p6a, p5a, p4a, p3a, q3a, q4a, q5a, q6a;
+
+    __m128i vF = _mm_set1_epi16(F);
+    __m128i vI = _mm_set1_epi16(I);
+    __m128i vE = _mm_set1_epi16(E);
+    __m128i vH = _mm_set1_epi16(H);
+
+    __m128i p7, p6, p5, p4, p3, p2, p1, p0;
+    __m128i q0, q1, q2, q3, q4, q5, q6, q7;
+
+    if (wd >= 16) {
+        LOAD_TRANSPOSE8(dst - 8, p7, p6, p5, p4, p3, p2, p1, p0);
+        LOAD_TRANSPOSE8(dst    , q0, q1, q2, q3, q4, q5, q6, q7);
+        p6a = p6; p5a = p5;
+        p4a = p4; p3a = p3;
+        q3a = q3; q4a = q4;
+        q5a = q5; q6a = q6;
+    } else {
+        LOAD_TRANSPOSE8(dst - 4, p3, p2, p1, p0, q0, q1, q2, q3);
+    }
+
+    t0 = _mm_abs_epi16(_mm_sub_epi16(p3, p2));
+    t1 = _mm_abs_epi16(_mm_sub_epi16(p2, p1));
+    t2 = _mm_abs_epi16(_mm_sub_epi16(p1, p0));
+    t3 = _mm_abs_epi16(_mm_sub_epi16(q1, q0));
+    t4 = _mm_abs_epi16(_mm_sub_epi16(q2, q1));
+    t5 = _mm_abs_epi16(_mm_sub_epi16(q3, q2));
+    vhev = _mm_or_si128(_mm_cmpgt_epi16(t2, vH), _mm_cmpgt_epi16(t3, vH));
+    t0 = _mm_cmpgt_epi16(t0, vI);
+    t1 = _mm_cmpgt_epi16(t1, vI);
+    t2 = _mm_cmpgt_epi16(t2, vI);
+    t3 = _mm_cmpgt_epi16(t3, vI);
+    t4 = _mm_cmpgt_epi16(t4, vI);
+    t5 = _mm_cmpgt_epi16(t5, vI);
+
+    t0 = _mm_or_si128(_mm_or_si128(t0, t1), t2);
+    t3 = _mm_or_si128(_mm_or_si128(t3, t4), t5);
+    t0 = _mm_or_si128(t0, t3);
+
+    t6 = _mm_abs_epi16(_mm_sub_epi16(p0, q0));
+    t7 = _mm_abs_epi16(_mm_sub_epi16(p1, q1));
+    t6 = _mm_add_epi16(_mm_slli_epi16(t6, 1), _mm_srai_epi16(t7, 1));
+    t6 = _mm_cmpgt_epi16(t6, vE);
+    vfm = _mm_or_si128(t0, t6); // !fm
+
+    if (_mm_movemask_epi8(vfm) == 0xffff) return;
+
+    if (wd >= 8) {
+        t0 = _mm_abs_epi16(_mm_sub_epi16(p3, p0));
+        t1 = _mm_abs_epi16(_mm_sub_epi16(p2, p0));
+        t2 = _mm_abs_epi16(_mm_sub_epi16(p1, p0));
+        t3 = _mm_abs_epi16(_mm_sub_epi16(q1, q0));
+        t4 = _mm_abs_epi16(_mm_sub_epi16(q2, q0));
+        t5 = _mm_abs_epi16(_mm_sub_epi16(q3, q0));
+        t0 = _mm_cmpgt_epi16(t0, vF);
+        t1 = _mm_cmpgt_epi16(t1, vF);
+        t2 = _mm_cmpgt_epi16(t2, vF);
+        t3 = _mm_cmpgt_epi16(t3, vF);
+        t4 = _mm_cmpgt_epi16(t4, vF);
+        t5 = _mm_cmpgt_epi16(t5, vF);
+
+        t0 = _mm_or_si128(_mm_or_si128(t0, t1), t2);
+        t3 = _mm_or_si128(_mm_or_si128(t3, t4), t5);
+        vflat8in = _mm_or_si128(_mm_or_si128(t0, t3), vfm);
+    }
+
+    {
+        __m128i c1 = _mm_set1_epi16(1);
+        __m128i c127 = _mm_set1_epi16(127);
+        __m128i m128 = _mm_set1_epi16(-128);
+        __m128i c43 = _mm_setr_epi8(4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3);
+        t0 = _mm_and_si128(vhev, _mm_sub_epi16(p1, q1));
+        t1 = _mm_sub_epi16(q0, p0);
+        t0 = _mm_min_epi16(_mm_max_epi16(t0, m128), c127);
+        // f = av_clip_intp2(p1 - q1, 7) & hev;
+        t1 = _mm_add_epi16(_mm_add_epi16(t1, t0), _mm_add_epi16(t1, t1));
+        t1 = _mm_andnot_si128(vfm, t1);
+        t1 = _mm_packs_epi16(t1, t1);
+        // f = av_clip_intp2(3 * (q0 - p0) + f, 7);
+        t3 = _mm_adds_epi8(t1, c43);
+        t2 = _mm_srai_epi16(_mm_unpacklo_epi8(t3, t3), 8 + 3);
+        // f1 = FFMIN(f + 4, 0x7f) >> 3;
+        t3 = _mm_srai_epi16(_mm_unpackhi_epi8(t3, t3), 8 + 3);
+        // f2 = FFMIN(f + 3, 0x7f) >> 3;
+        t4 = _mm_srai_epi16(_mm_add_epi16(t2, c1), 1);
+        t4 = _mm_andnot_si128(vhev, t4); // f3 = ((f1 + 1) >> 1) & ~hev;
+        p1a = _mm_add_epi16(p1, t4); // av_clip_uint8(p1 + f3);
+        p0a = _mm_add_epi16(p0, t3); // av_clip_uint8(p0 + f2);
+        q0a = _mm_sub_epi16(q0, t2); // av_clip_uint8(q0 - f1);
+        q1a = _mm_sub_epi16(q1, t4); // av_clip_uint8(q1 - f3);
+    }
+
+    p2a = p2; q2a = q2;
+
+    if (wd >= 8 && (_mm_movemask_epi8(vflat8in) != 0xffff)) {
+        __m128i c4 = _mm_set1_epi16(4);
+        t0 = _mm_add_epi16(_mm_slli_epi16(p3, 2), c4);
+        t1 = _mm_add_epi16(_mm_add_epi16(q0, p0), _mm_add_epi16(p1, p2));
+        t0 = _mm_sub_epi16(t0, p3);
+        t0 = _mm_add_epi16(t0, t1);
+        t1 = _mm_srai_epi16(_mm_add_epi16(p2, t0), 3);
+        t0 = _mm_add_epi16(t0, _mm_sub_epi16(q1, p3));
+        t2 = _mm_srai_epi16(_mm_add_epi16(p1, t0), 3);
+        p2a = _mm_blendv_epi8(t1, p2, vflat8in);
+        p1a = _mm_blendv_epi8(t2, p1a, vflat8in);
+
+        t0 = _mm_add_epi16(t0, _mm_sub_epi16(q2, p3));
+        t1 = _mm_srai_epi16(_mm_add_epi16(p0, t0), 3);
+        t0 = _mm_add_epi16(t0, _mm_sub_epi16(q3, p3));
+        t2 = _mm_srai_epi16(_mm_add_epi16(q0, t0), 3);
+        p0a = _mm_blendv_epi8(t1, p0a, vflat8in);
+        q0a = _mm_blendv_epi8(t2, q0a, vflat8in);
+
+        t0 = _mm_add_epi16(t0, _mm_sub_epi16(q3, p2));
+        t1 = _mm_srai_epi16(_mm_add_epi16(q1, t0), 3);
+        t0 = _mm_add_epi16(t0, _mm_sub_epi16(q3, p1));
+        t2 = _mm_srai_epi16(_mm_add_epi16(q2, t0), 3);
+        q1a = _mm_blendv_epi8(t1, q1a, vflat8in);
+        q2a = _mm_blendv_epi8(t2, q2, vflat8in);
+
+        if (wd >= 16) {
+            t0 = _mm_abs_epi16(_mm_sub_epi16(p7, p0));
+            t1 = _mm_abs_epi16(_mm_sub_epi16(p6, p0));
+            t2 = _mm_abs_epi16(_mm_sub_epi16(p5, p0));
+            t3 = _mm_abs_epi16(_mm_sub_epi16(p4, p0));
+            t4 = _mm_abs_epi16(_mm_sub_epi16(q4, q0));
+            t5 = _mm_abs_epi16(_mm_sub_epi16(q5, q0));
+            t6 = _mm_abs_epi16(_mm_sub_epi16(q6, q0));
+            t7 = _mm_abs_epi16(_mm_sub_epi16(q7, q0));
+
+            t0 = _mm_cmpgt_epi16(t0, vF);
+            t1 = _mm_cmpgt_epi16(t1, vF);
+            t2 = _mm_cmpgt_epi16(t2, vF);
+            t3 = _mm_cmpgt_epi16(t3, vF);
+            t4 = _mm_cmpgt_epi16(t4, vF);
+            t5 = _mm_cmpgt_epi16(t5, vF);
+            t6 = _mm_cmpgt_epi16(t6, vF);
+            t7 = _mm_cmpgt_epi16(t7, vF);
+
+            t0 = _mm_or_si128(_mm_or_si128(t0, t1), _mm_or_si128(t2, t3));
+            t4 = _mm_or_si128(_mm_or_si128(t4, t5), _mm_or_si128(t6, t7));
+            vflat8out = _mm_or_si128(t0, t4);
+            vflat8out = _mm_or_si128(vflat8out, vflat8in);
+
+            if (_mm_movemask_epi8(vflat8out) != 0xffff) {
+                __m128i c8 = _mm_set1_epi16(8);
+                t0 = _mm_add_epi16(_mm_slli_epi16(p7, 3), c8);
+                t1 = _mm_add_epi16(_mm_add_epi16(q0, p0), _mm_add_epi16(p1, p2));
+                t2 = _mm_add_epi16(_mm_add_epi16(p3, p4), _mm_add_epi16(p5, p6));
+                t0 = _mm_sub_epi16(t0, p7);
+                t0 = _mm_add_epi16(t0, t1);
+                t0 = _mm_add_epi16(t0, t2);
+                t1 = _mm_srai_epi16(_mm_add_epi16(p6, t0), 4);
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q1, p7));
+                t2 = _mm_srai_epi16(_mm_add_epi16(p5, t0), 4);
+                p6a = _mm_blendv_epi8(t1, p6, vflat8out);
+                p5a = _mm_blendv_epi8(t2, p5, vflat8out);
+
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q2, p7));
+                t1 = _mm_srai_epi16(_mm_add_epi16(p4, t0), 4);
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q3, p7));
+                t2 = _mm_srai_epi16(_mm_add_epi16(p3, t0), 4);
+                p4a = _mm_blendv_epi8(t1, p4, vflat8out);
+                p3a = _mm_blendv_epi8(t2, p3, vflat8out);
+
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q4, p7));
+                t1 = _mm_srai_epi16(_mm_add_epi16(p2, t0), 4);
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q5, p7));
+                t2 = _mm_srai_epi16(_mm_add_epi16(p1, t0), 4);
+                p2a = _mm_blendv_epi8(t1, p2a, vflat8out);
+                p1a = _mm_blendv_epi8(t2, p1a, vflat8out);
+
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q6, p7));
+                t1 = _mm_srai_epi16(_mm_add_epi16(p0, t0), 4);
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q7, p7));
+                t2 = _mm_srai_epi16(_mm_add_epi16(q0, t0), 4);
+                p0a = _mm_blendv_epi8(t1, p0a, vflat8out);
+                q0a = _mm_blendv_epi8(t2, q0a, vflat8out);
+
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q7, p6));
+                t1 = _mm_srai_epi16(_mm_add_epi16(q1, t0), 4);
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q7, p5));
+                t2 = _mm_srai_epi16(_mm_add_epi16(q2, t0), 4);
+                q1a = _mm_blendv_epi8(t1, q1a, vflat8out);
+                q2a = _mm_blendv_epi8(t2, q2a, vflat8out);
+
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q7, p4));
+                t1 = _mm_srai_epi16(_mm_add_epi16(q3, t0), 4);
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q7, p3));
+                t2 = _mm_srai_epi16(_mm_add_epi16(q4, t0), 4);
+                q3a = _mm_blendv_epi8(t1, q3, vflat8out);
+                q4a = _mm_blendv_epi8(t2, q4, vflat8out);
+
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q7, p2));
+                t1 = _mm_srai_epi16(_mm_add_epi16(q5, t0), 4);
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q7, p1));
+                t2 = _mm_srai_epi16(_mm_add_epi16(q6, t0), 4);
+                q5a = _mm_blendv_epi8(t1, q5, vflat8out);
+                q6a = _mm_blendv_epi8(t2, q6, vflat8out);
+            }
+        }
+    }
+
+    if (wd >= 16) {
+        STORE_TRANSPOSE8(dst - 8, p7,  p6a, p5a, p4a, p3a, p2a, p1a, p0a);
+        STORE_TRANSPOSE8(dst    , q0a, q1a, q2a, q3a, q4a, q5a, q6a, q7 );
+    } else if (wd >= 8) {
+        STORE_TRANSPOSE8(dst - 4, p3,  p2a, p1a, p0a, q0a, q1a, q2a, q3 );
+    } else {
+        t0 = _mm_packus_epi16(p1a, p0a);
+        t1 = _mm_packus_epi16(q0a, q1a);
+        t2 = _mm_unpacklo_epi8(t0, t1);
+        t3 = _mm_unpackhi_epi8(t0, t1);
+        t0 = _mm_unpacklo_epi8(t2, t3);
+        t1 = _mm_unpackhi_epi8(t2, t3);
+        *(uint32_t*)(dst - 2 + stride * 0) = _mm_extract_epi32(t0, 0);
+        *(uint32_t*)(dst - 2 + stride * 1) = _mm_extract_epi32(t0, 1);
+        *(uint32_t*)(dst - 2 + stride * 2) = _mm_extract_epi32(t0, 2);
+        *(uint32_t*)(dst - 2 + stride * 3) = _mm_extract_epi32(t0, 3);
+        *(uint32_t*)(dst - 2 + stride * 4) = _mm_extract_epi32(t1, 0);
+        *(uint32_t*)(dst - 2 + stride * 5) = _mm_extract_epi32(t1, 1);
+        *(uint32_t*)(dst - 2 + stride * 6) = _mm_extract_epi32(t1, 2);
+        *(uint32_t*)(dst - 2 + stride * 7) = _mm_extract_epi32(t1, 3);
+    }
+}
+
+#undef LOAD_TRANSPOSE8
+#undef STORE_TRANSPOSE8
+
+static av_always_inline void loop_filter_v(uint8_t *dst, int E, int I, int H,
+                                           ptrdiff_t stride, int wd)
+{
+    int F = 1;
+    LOAD_ZERO;
+    __m128i t0, t1, t2, t3, t4, t5, t6, t7;
+    __m128i vfm, vflat8out, vflat8in, vhev;
+    __m128i p2a, p1a, p0a, q0a, q1a, q2a;
+
+    __m128i vF = _mm_set1_epi16(F);
+    __m128i vI = _mm_set1_epi16(I);
+    __m128i vE = _mm_set1_epi16(E);
+    __m128i vH = _mm_set1_epi16(H);
+
+    __m128i p7, p6, p5, p4;
+    __m128i p3 = VEC_LD8(dst - stride * 4), p2 = VEC_LD8(dst - stride * 3);
+    __m128i p1 = VEC_LD8(dst - stride * 2), p0 = VEC_LD8(dst - stride * 1);
+    __m128i q0 = VEC_LD8(dst + stride * 0), q1 = VEC_LD8(dst + stride * 1);
+    __m128i q2 = VEC_LD8(dst + stride * 2), q3 = VEC_LD8(dst + stride * 3);
+    __m128i q4, q5, q6, q7;
+
+    p3 = _mm_unpacklo_epi8(p3, zerov);
+    p2 = _mm_unpacklo_epi8(p2, zerov);
+    p1 = _mm_unpacklo_epi8(p1, zerov);
+    p0 = _mm_unpacklo_epi8(p0, zerov);
+    q0 = _mm_unpacklo_epi8(q0, zerov);
+    q1 = _mm_unpacklo_epi8(q1, zerov);
+    q2 = _mm_unpacklo_epi8(q2, zerov);
+    q3 = _mm_unpacklo_epi8(q3, zerov);
+
+    t0 = _mm_abs_epi16(_mm_sub_epi16(p3, p2));
+    t1 = _mm_abs_epi16(_mm_sub_epi16(p2, p1));
+    t2 = _mm_abs_epi16(_mm_sub_epi16(p1, p0));
+    t3 = _mm_abs_epi16(_mm_sub_epi16(q1, q0));
+    t4 = _mm_abs_epi16(_mm_sub_epi16(q2, q1));
+    t5 = _mm_abs_epi16(_mm_sub_epi16(q3, q2));
+    vhev = _mm_or_si128(_mm_cmpgt_epi16(t2, vH), _mm_cmpgt_epi16(t3, vH));
+    t0 = _mm_cmpgt_epi16(t0, vI);
+    t1 = _mm_cmpgt_epi16(t1, vI);
+    t2 = _mm_cmpgt_epi16(t2, vI);
+    t3 = _mm_cmpgt_epi16(t3, vI);
+    t4 = _mm_cmpgt_epi16(t4, vI);
+    t5 = _mm_cmpgt_epi16(t5, vI);
+
+    t0 = _mm_or_si128(_mm_or_si128(t0, t1), t2);
+    t3 = _mm_or_si128(_mm_or_si128(t3, t4), t5);
+    t0 = _mm_or_si128(t0, t3);
+
+    t6 = _mm_abs_epi16(_mm_sub_epi16(p0, q0));
+    t7 = _mm_abs_epi16(_mm_sub_epi16(p1, q1));
+    t6 = _mm_add_epi16(_mm_slli_epi16(t6, 1), _mm_srai_epi16(t7, 1));
+    t6 = _mm_cmpgt_epi16(t6, vE);
+    vfm = _mm_or_si128(t0, t6); // !fm
+
+    if (_mm_movemask_epi8(vfm) == 0xffff) return;
+
+    if (wd >= 8) {
+        t0 = _mm_abs_epi16(_mm_sub_epi16(p3, p0));
+        t1 = _mm_abs_epi16(_mm_sub_epi16(p2, p0));
+        t2 = _mm_abs_epi16(_mm_sub_epi16(p1, p0));
+        t3 = _mm_abs_epi16(_mm_sub_epi16(q1, q0));
+        t4 = _mm_abs_epi16(_mm_sub_epi16(q2, q0));
+        t5 = _mm_abs_epi16(_mm_sub_epi16(q3, q0));
+        t0 = _mm_cmpgt_epi16(t0, vF);
+        t1 = _mm_cmpgt_epi16(t1, vF);
+        t2 = _mm_cmpgt_epi16(t2, vF);
+        t3 = _mm_cmpgt_epi16(t3, vF);
+        t4 = _mm_cmpgt_epi16(t4, vF);
+        t5 = _mm_cmpgt_epi16(t5, vF);
+
+        t0 = _mm_or_si128(_mm_or_si128(t0, t1), t2);
+        t3 = _mm_or_si128(_mm_or_si128(t3, t4), t5);
+        vflat8in = _mm_or_si128(_mm_or_si128(t0, t3), vfm);
+    }
+
+    {
+        __m128i c1 = _mm_set1_epi16(1);
+        __m128i c127 = _mm_set1_epi16(127);
+        __m128i m128 = _mm_set1_epi16(-128);
+        __m128i c43 = _mm_setr_epi8(4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3);
+        t0 = _mm_and_si128(vhev, _mm_sub_epi16(p1, q1));
+        t1 = _mm_sub_epi16(q0, p0);
+        t0 = _mm_min_epi16(_mm_max_epi16(t0, m128), c127);
+        // f = av_clip_intp2(p1 - q1, 7) & hev;
+        t1 = _mm_add_epi16(_mm_add_epi16(t1, t0), _mm_add_epi16(t1, t1));
+        t1 = _mm_andnot_si128(vfm, t1);
+        t1 = _mm_packs_epi16(t1, t1);
+        // f = av_clip_intp2(3 * (q0 - p0) + f, 7);
+        t3 = _mm_adds_epi8(t1, c43);
+        t2 = _mm_srai_epi16(_mm_unpacklo_epi8(t3, t3), 8 + 3);
+        // f1 = FFMIN(f + 4, 0x7f) >> 3;
+        t3 = _mm_srai_epi16(_mm_unpackhi_epi8(t3, t3), 8 + 3);
+        // f2 = FFMIN(f + 3, 0x7f) >> 3;
+        t4 = _mm_srai_epi16(_mm_add_epi16(t2, c1), 1);
+        t4 = _mm_andnot_si128(vhev, t4); // f3 = ((f1 + 1) >> 1) & ~hev;
+        p1a = _mm_add_epi16(p1, t4); // av_clip_uint8(p1 + f3);
+        p0a = _mm_add_epi16(p0, t3); // av_clip_uint8(p0 + f2);
+        q0a = _mm_sub_epi16(q0, t2); // av_clip_uint8(q0 - f1);
+        q1a = _mm_sub_epi16(q1, t4); // av_clip_uint8(q1 - f3);
+    }
+
+    if (wd >= 8 && _mm_movemask_epi8(vflat8in) != 0xffff) {
+        __m128i c4 = _mm_set1_epi16(4);
+        t0 = _mm_add_epi16(_mm_slli_epi16(p3, 2), c4);
+        t1 = _mm_add_epi16(_mm_add_epi16(q0, p0), _mm_add_epi16(p1, p2));
+        t0 = _mm_sub_epi16(t0, p3);
+        t0 = _mm_add_epi16(t0, t1);
+        t1 = _mm_srai_epi16(_mm_add_epi16(p2, t0), 3);
+        t0 = _mm_add_epi16(t0, _mm_sub_epi16(q1, p3));
+        t2 = _mm_srai_epi16(_mm_add_epi16(p1, t0), 3);
+        p2a = _mm_blendv_epi8(t1, p2, vflat8in);
+        p1a = _mm_blendv_epi8(t2, p1a, vflat8in);
+
+        t0 = _mm_add_epi16(t0, _mm_sub_epi16(q2, p3));
+        t1 = _mm_srai_epi16(_mm_add_epi16(p0, t0), 3);
+        t0 = _mm_add_epi16(t0, _mm_sub_epi16(q3, p3));
+        t2 = _mm_srai_epi16(_mm_add_epi16(q0, t0), 3);
+        p0a = _mm_blendv_epi8(t1, p0a, vflat8in);
+        q0a = _mm_blendv_epi8(t2, q0a, vflat8in);
+
+        t0 = _mm_add_epi16(t0, _mm_sub_epi16(q3, p2));
+        t1 = _mm_srai_epi16(_mm_add_epi16(q1, t0), 3);
+        t0 = _mm_add_epi16(t0, _mm_sub_epi16(q3, p1));
+        t2 = _mm_srai_epi16(_mm_add_epi16(q2, t0), 3);
+        q1a = _mm_blendv_epi8(t1, q1a, vflat8in);
+        q2a = _mm_blendv_epi8(t2, q2, vflat8in);
+
+        if (wd >= 16) {
+            p7 = VEC_LD8(dst - stride * 8);
+            p6 = VEC_LD8(dst - stride * 7);
+            p5 = VEC_LD8(dst - stride * 6);
+            p4 = VEC_LD8(dst - stride * 5);
+            q4 = VEC_LD8(dst + stride * 4);
+            q5 = VEC_LD8(dst + stride * 5);
+            q6 = VEC_LD8(dst + stride * 6);
+            q7 = VEC_LD8(dst + stride * 7);
+
+            p7 = _mm_unpacklo_epi8(p7, zerov);
+            p6 = _mm_unpacklo_epi8(p6, zerov);
+            p5 = _mm_unpacklo_epi8(p5, zerov);
+            p4 = _mm_unpacklo_epi8(p4, zerov);
+            q4 = _mm_unpacklo_epi8(q4, zerov);
+            q5 = _mm_unpacklo_epi8(q5, zerov);
+            q6 = _mm_unpacklo_epi8(q6, zerov);
+            q7 = _mm_unpacklo_epi8(q7, zerov);
+
+            t0 = _mm_abs_epi16(_mm_sub_epi16(p7, p0));
+            t1 = _mm_abs_epi16(_mm_sub_epi16(p6, p0));
+            t2 = _mm_abs_epi16(_mm_sub_epi16(p5, p0));
+            t3 = _mm_abs_epi16(_mm_sub_epi16(p4, p0));
+            t4 = _mm_abs_epi16(_mm_sub_epi16(q4, q0));
+            t5 = _mm_abs_epi16(_mm_sub_epi16(q5, q0));
+            t6 = _mm_abs_epi16(_mm_sub_epi16(q6, q0));
+            t7 = _mm_abs_epi16(_mm_sub_epi16(q7, q0));
+
+            t0 = _mm_cmpgt_epi16(t0, vF);
+            t1 = _mm_cmpgt_epi16(t1, vF);
+            t2 = _mm_cmpgt_epi16(t2, vF);
+            t3 = _mm_cmpgt_epi16(t3, vF);
+            t4 = _mm_cmpgt_epi16(t4, vF);
+            t5 = _mm_cmpgt_epi16(t5, vF);
+            t6 = _mm_cmpgt_epi16(t6, vF);
+            t7 = _mm_cmpgt_epi16(t7, vF);
+
+            t0 = _mm_or_si128(_mm_or_si128(t0, t1), _mm_or_si128(t2, t3));
+            t4 = _mm_or_si128(_mm_or_si128(t4, t5), _mm_or_si128(t6, t7));
+            vflat8out = _mm_or_si128(t0, t4);
+
+            vflat8out = _mm_or_si128(vflat8out, vflat8in);
+            if (_mm_movemask_epi8(vflat8out) != 0xffff) {
+                __m128i c8 = _mm_set1_epi16(8);
+                t0 = _mm_add_epi16(_mm_slli_epi16(p7, 3), c8);
+                t1 = _mm_add_epi16(_mm_add_epi16(q0, p0), _mm_add_epi16(p1, p2));
+                t2 = _mm_add_epi16(_mm_add_epi16(p3, p4), _mm_add_epi16(p5, p6));
+                t0 = _mm_sub_epi16(t0, p7);
+                t0 = _mm_add_epi16(t0, t1);
+                t0 = _mm_add_epi16(t0, t2);
+                t1 = _mm_srai_epi16(_mm_add_epi16(p6, t0), 4);
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q1, p7));
+                t2 = _mm_srai_epi16(_mm_add_epi16(p5, t0), 4);
+                t1 = _mm_blendv_epi8(t1, p6, vflat8out);
+                t2 = _mm_blendv_epi8(t2, p5, vflat8out);
+                t1 = _mm_packus_epi16(t1, t2);
+                VEC_STL(dst - stride * 7, t1);
+                VEC_STH(dst - stride * 6, t1);
+
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q2, p7));
+                t1 = _mm_srai_epi16(_mm_add_epi16(p4, t0), 4);
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q3, p7));
+                t2 = _mm_srai_epi16(_mm_add_epi16(p3, t0), 4);
+                t1 = _mm_blendv_epi8(t1, p4, vflat8out);
+                t2 = _mm_blendv_epi8(t2, p3, vflat8out);
+                t1 = _mm_packus_epi16(t1, t2);
+                VEC_STL(dst - stride * 5, t1);
+                VEC_STH(dst - stride * 4, t1);
+
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q4, p7));
+                t1 = _mm_srai_epi16(_mm_add_epi16(p2, t0), 4);
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q5, p7));
+                t2 = _mm_srai_epi16(_mm_add_epi16(p1, t0), 4);
+                p2a = _mm_blendv_epi8(t1, p2a, vflat8out);
+                p1a = _mm_blendv_epi8(t2, p1a, vflat8out);
+
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q6, p7));
+                t1 = _mm_srai_epi16(_mm_add_epi16(p0, t0), 4);
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q7, p7));
+                t2 = _mm_srai_epi16(_mm_add_epi16(q0, t0), 4);
+                p0a = _mm_blendv_epi8(t1, p0a, vflat8out);
+                q0a = _mm_blendv_epi8(t2, q0a, vflat8out);
+
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q7, p6));
+                t1 = _mm_srai_epi16(_mm_add_epi16(q1, t0), 4);
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q7, p5));
+                t2 = _mm_srai_epi16(_mm_add_epi16(q2, t0), 4);
+                q1a = _mm_blendv_epi8(t1, q1a, vflat8out);
+                q2a = _mm_blendv_epi8(t2, q2a, vflat8out);
+
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q7, p4));
+                t1 = _mm_srai_epi16(_mm_add_epi16(q3, t0), 4);
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q7, p3));
+                t2 = _mm_srai_epi16(_mm_add_epi16(q4, t0), 4);
+                t1 = _mm_blendv_epi8(t1, q3, vflat8out);
+                t2 = _mm_blendv_epi8(t2, q4, vflat8out);
+                t1 = _mm_packus_epi16(t1, t2);
+                VEC_STL(dst + stride * 3, t1);
+                VEC_STH(dst + stride * 4, t1);
+
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q7, p2));
+                t1 = _mm_srai_epi16(_mm_add_epi16(q5, t0), 4);
+                t0 = _mm_add_epi16(t0, _mm_sub_epi16(q7, p1));
+                t2 = _mm_srai_epi16(_mm_add_epi16(q6, t0), 4);
+                t1 = _mm_blendv_epi8(t1, q5, vflat8out);
+                t2 = _mm_blendv_epi8(t2, q6, vflat8out);
+                t1 = _mm_packus_epi16(t1, t2);
+                VEC_STL(dst + stride * 5, t1);
+                VEC_STH(dst + stride * 6, t1);
+            }
+        }
+        t1 = _mm_packus_epi16(p2a, q2a);
+        VEC_STL(dst - stride * 3, t1);
+        VEC_STH(dst + stride * 2, t1);
+    }
+
+    t0 = _mm_packus_epi16(p1a, p0a);
+    t1 = _mm_packus_epi16(q0a, q1a);
+    VEC_STL(dst - stride * 2, t0);
+    VEC_STH(dst - stride * 1, t0);
+    VEC_STL(dst + stride * 0, t1);
+    VEC_STH(dst + stride * 1, t1);
+}
+
+#define lf_8_fns(wd) \
+static void loop_filter_h_##wd##_8_e2k(uint8_t *dst, \
+                                       ptrdiff_t stride, \
+                                       int E, int I, int H) \
+{ \
+    loop_filter_h(dst, E, I, H, stride, wd); \
+} \
+static void loop_filter_v_##wd##_8_e2k(uint8_t *dst, \
+                                       ptrdiff_t stride, \
+                                       int E, int I, int H) \
+{ \
+    loop_filter_v(dst, E, I, H, stride, wd); \
+}
+
+lf_8_fns(4)
+lf_8_fns(8)
+lf_8_fns(16)
+
+#undef lf_8_fn
+#undef lf_8_fns
+
+#define lf_16_fn(dir, stridea) \
+static void loop_filter_##dir##_16_16_e2k(uint8_t *dst, \
+                                          ptrdiff_t stride, \
+                                          int E, int I, int H) \
+{ \
+    loop_filter_##dir##_16_8_e2k(dst, stride, E, I, H); \
+    loop_filter_##dir##_16_8_e2k(dst + 8 * stridea, stride, E, I, H); \
+}
+
+lf_16_fn(h, stride)
+lf_16_fn(v, sizeof(pixel))
+
+#undef lf_16_fn
+
+#define lf_mix_fn(dir, wd1, wd2, stridea) \
+static void loop_filter_##dir##_##wd1##wd2##_16_e2k(uint8_t *dst, \
+                                                    ptrdiff_t stride, \
+                                                    int E, int I, int H) \
+{ \
+    loop_filter_##dir##_##wd1##_8_e2k(dst, stride, E & 0xff, I & 0xff, H & 0xff); \
+    loop_filter_##dir##_##wd2##_8_e2k(dst + 8 * stridea, stride, E >> 8, I >> 8, H >> 8); \
+}
+
+#define lf_mix_fns(wd1, wd2) \
+lf_mix_fn(h, wd1, wd2, stride) \
+lf_mix_fn(v, wd1, wd2, sizeof(pixel))
+
+lf_mix_fns(4, 4)
+lf_mix_fns(4, 8)
+lf_mix_fns(8, 4)
+lf_mix_fns(8, 8)
+
+#undef lf_mix_fn
+#undef lf_mix_fns
+
+static av_cold void ff_vp9dsp_loopfilter_init_8_e2k(VP9DSPContext *dsp)
+{
+    dsp->loop_filter_8[0][0] = loop_filter_h_4_8_e2k;
+    dsp->loop_filter_8[0][1] = loop_filter_v_4_8_e2k;
+    dsp->loop_filter_8[1][0] = loop_filter_h_8_8_e2k;
+    dsp->loop_filter_8[1][1] = loop_filter_v_8_8_e2k;
+    dsp->loop_filter_8[2][0] = loop_filter_h_16_8_e2k;
+    dsp->loop_filter_8[2][1] = loop_filter_v_16_8_e2k;
+
+    dsp->loop_filter_16[0] = loop_filter_h_16_16_e2k;
+    dsp->loop_filter_16[1] = loop_filter_v_16_16_e2k;
+
+    dsp->loop_filter_mix2[0][0][0] = loop_filter_h_44_16_e2k;
+    dsp->loop_filter_mix2[0][0][1] = loop_filter_v_44_16_e2k;
+    dsp->loop_filter_mix2[0][1][0] = loop_filter_h_48_16_e2k;
+    dsp->loop_filter_mix2[0][1][1] = loop_filter_v_48_16_e2k;
+    dsp->loop_filter_mix2[1][0][0] = loop_filter_h_84_16_e2k;
+    dsp->loop_filter_mix2[1][0][1] = loop_filter_v_84_16_e2k;
+    dsp->loop_filter_mix2[1][1][0] = loop_filter_h_88_16_e2k;
+    dsp->loop_filter_mix2[1][1][1] = loop_filter_v_88_16_e2k;
+}
+
+#if BIT_DEPTH != 12
+
+static av_always_inline void copy_e2k(uint8_t *dst, ptrdiff_t dst_stride,
+                                      const uint8_t *src, ptrdiff_t src_stride,
+                                      int w, int h)
+{
+    do {
+        memcpy(dst, src, w);
+        dst += dst_stride;
+        src += src_stride;
+    } while (--h);
+}
+
+static av_always_inline void avg_e2k(uint8_t *dst, ptrdiff_t dst_stride,
+                                     const uint8_t *src, ptrdiff_t src_stride,
+                                     int w, int h)
+{
+    int y;
+    __m128i v0, v1, v2, v3; __m64 h0, h1;
+    if (w >= 64) {
+        PRAGMA_E2K("ivdep")
+        for (y = 0; y < h; y++) {
+            v0 = VEC_LD(src);
+            v1 = VEC_LD(src + 16);
+            v2 = VEC_LD(src + 32);
+            v3 = VEC_LD(src + 48);
+            v0 = _mm_avg_epu8(v0, VEC_LD(dst));
+            v1 = _mm_avg_epu8(v1, VEC_LD(dst + 16));
+            v2 = _mm_avg_epu8(v2, VEC_LD(dst + 32));
+            v3 = _mm_avg_epu8(v3, VEC_LD(dst + 48));
+            VEC_ST(dst, v0);
+            VEC_ST(dst + 16, v1);
+            VEC_ST(dst + 32, v2);
+            VEC_ST(dst + 48, v3);
+            src += src_stride;
+            dst += dst_stride;
+        }
+    } else if (w >= 32) {
+        PRAGMA_E2K("ivdep")
+        for (y = 0; y < h; y++) {
+            v0 = VEC_LD(src);
+            v1 = VEC_LD(src + 16);
+            v0 = _mm_avg_epu8(v0, VEC_LD(dst));
+            v1 = _mm_avg_epu8(v1, VEC_LD(dst + 16));
+            VEC_ST(dst, v0);
+            VEC_ST(dst + 16, v1);
+            src += src_stride;
+            dst += dst_stride;
+        }
+    } else if (w >= 16) {
+        PRAGMA_E2K("ivdep")
+        for (y = 0; y < h; y++) {
+            v0 = VEC_LD(src);
+            v0 = _mm_avg_epu8(v0, VEC_LD(dst));
+            VEC_ST(dst, v0);
+            src += src_stride;
+            dst += dst_stride;
+        }
+    } else if (w >= 8) {
+        PRAGMA_E2K("ivdep")
+        for (y = 0; y < h; y++) {
+            h0 = *(__m64*)src;
+            h1 = *(__m64*)dst;
+            h0 = _mm_avg_pu8(h0, h1);
+            *(__m64*)dst = h0;
+            src += src_stride;
+            dst += dst_stride;
+        }
+    } else {
+        PRAGMA_E2K("ivdep")
+        for (y = 0; y < h; y++) {
+            h0 = _mm_cvtsi32_si64(*(uint32_t*)src);
+            h1 = _mm_cvtsi32_si64(*(uint32_t*)dst);
+            h0 = _mm_avg_pu8(h0, h1);
+            *(uint32_t*)dst = _mm_cvtsi64_si32(h0);
+            src += src_stride;
+            dst += dst_stride;
+        }
+    }
+}
+
+#define fpel_fn(type, sz) \
+static void type##sz##_e2k(uint8_t *dst, ptrdiff_t dst_stride, \
+                           const uint8_t *src, ptrdiff_t src_stride, \
+                           int h, int mx, int my) \
+{ \
+    type##_e2k(dst, dst_stride, src, src_stride, sz, h); \
+}
+
+#define copy_avg_fn(sz) \
+fpel_fn(copy, sz) \
+fpel_fn(avg,  sz)
+
+copy_avg_fn(64)
+copy_avg_fn(32)
+copy_avg_fn(16)
+copy_avg_fn(8)
+copy_avg_fn(4)
+
+#undef fpel_fn
+#undef copy_avg_fn
+
+#endif /* BIT_DEPTH != 12 */
+
+static av_always_inline void do_8tap_1d_v_e2k(uint8_t *dst, ptrdiff_t dst_stride,
+                                              const uint8_t *src, ptrdiff_t src_stride,
+                                              int w, int h, const int16_t *filter, int avg)
+{
+    int x, y;
+    const uint8_t *s; uint8_t *d;
+
+    if (w >= 8) {
+        __m128i a0, a1, a2, a3, a4, a5, a6, a7;
+        __m128i v0, v1, v2, v3;
+        __m128i f0, f1, f2, f3, c64; __m64 h0, h1;
+        f0 = _mm_set1_epi16((filter[0] & 255) | filter[1] << 8);
+        f1 = _mm_set1_epi16((filter[2] & 255) | filter[3] << 8);
+        f2 = _mm_set1_epi16((filter[4] & 255) | filter[5] << 8);
+        f3 = _mm_set1_epi16((filter[6] & 255) | filter[7] << 8);
+        c64 = _mm_set1_epi16(64);
+
+        for (x = 0; x < w; x += 8) {
+            a0 = VEC_LD8(&src[x - 3 * src_stride]);
+            a1 = VEC_LD8(&src[x - 2 * src_stride]);
+            a2 = VEC_LD8(&src[x - 1 * src_stride]);
+            a3 = VEC_LD8(&src[x + 0 * src_stride]);
+            a4 = VEC_LD8(&src[x + 1 * src_stride]);
+            a5 = VEC_LD8(&src[x + 2 * src_stride]);
+            a6 = VEC_LD8(&src[x + 3 * src_stride]);
+            s = src + x + 4 * src_stride;
+            d = dst + x;
+
+            PRAGMA_E2K("ivdep")
+            for (y = 0; y < h; y++) {
+                a7 = VEC_LD8(s);
+                v0 = _mm_unpacklo_epi8(a0, a1);
+                v1 = _mm_unpacklo_epi8(a2, a3);
+                v2 = _mm_unpacklo_epi8(a4, a5);
+                v3 = _mm_unpacklo_epi8(a6, a7);
+                v0 = _mm_maddubs_epi16(v0, f0);
+                v1 = _mm_maddubs_epi16(v1, f1);
+                v2 = _mm_maddubs_epi16(v2, f2);
+                v3 = _mm_maddubs_epi16(v3, f3);
+                v0 = _mm_add_epi16(v0, v2);
+                v1 = _mm_add_epi16(v1, v3);
+                v0 = _mm_add_epi16(v0, c64);
+                v0 = _mm_adds_epi16(v0, v1);
+                v0 = _mm_srai_epi16(v0, 7);
+                v0 = _mm_packus_epi16(v0, v0);
+                h0 = _mm_movepi64_pi64(v0);
+                if (avg) {
+                    h1 = *(__m64*)d;
+                    h0 = _mm_avg_pu8(h0, h1);
+                }
+                *(__m64*)d = h0;
+                s += src_stride;
+                d += dst_stride;
+                a0 = a1; a1 = a2; a2 = a3; a3 = a4;
+                a4 = a5; a5 = a6; a6 = a7;
+            }
+        }
+    } else {
+        __m64 a0, a1, a2, a3, a4, a5, a6, a7;
+        __m64 v0, v1, v2, v3;
+        __m64 f0, f1, f2, f3, c64;
+        f0 = _mm_set1_pi16((filter[0] & 255) | filter[1] << 8);
+        f1 = _mm_set1_pi16((filter[2] & 255) | filter[3] << 8);
+        f2 = _mm_set1_pi16((filter[4] & 255) | filter[5] << 8);
+        f3 = _mm_set1_pi16((filter[6] & 255) | filter[7] << 8);
+        c64 = _mm_set1_pi16(64);
+
+        a0 = _mm_cvtsi32_si64(*(uint32_t*)(src - 3 * src_stride));
+        a1 = _mm_cvtsi32_si64(*(uint32_t*)(src - 2 * src_stride));
+        a2 = _mm_cvtsi32_si64(*(uint32_t*)(src - 1 * src_stride));
+        a3 = _mm_cvtsi32_si64(*(uint32_t*)(src + 0 * src_stride));
+        a4 = _mm_cvtsi32_si64(*(uint32_t*)(src + 1 * src_stride));
+        a5 = _mm_cvtsi32_si64(*(uint32_t*)(src + 2 * src_stride));
+        a6 = _mm_cvtsi32_si64(*(uint32_t*)(src + 3 * src_stride));
+        s = src + 4 * src_stride;
+        d = dst;
+
+        PRAGMA_E2K("ivdep")
+        for (y = 0; y < h; y++) {
+            a7 = _mm_cvtsi32_si64(*(uint32_t*)s);
+            v0 = _mm_unpacklo_pi8(a0, a1);
+            v1 = _mm_unpacklo_pi8(a2, a3);
+            v2 = _mm_unpacklo_pi8(a4, a5);
+            v3 = _mm_unpacklo_pi8(a6, a7);
+            v0 = _mm_maddubs_pi16(v0, f0);
+            v1 = _mm_maddubs_pi16(v1, f1);
+            v2 = _mm_maddubs_pi16(v2, f2);
+            v3 = _mm_maddubs_pi16(v3, f3);
+            v0 = _mm_add_pi16(v0, v2);
+            v1 = _mm_add_pi16(v1, v3);
+            v0 = _mm_add_pi16(v0, c64);
+            v0 = _mm_adds_pi16(v0, v1);
+            v0 = _mm_srai_pi16(v0, 7);
+            v0 = _mm_packs_pu16(v0, v0);
+            if (avg) {
+                v1 = _mm_cvtsi32_si64(*(uint32_t*)d);
+                v0 = _mm_avg_pu8(v0, v1);
+            }
+            *(uint32_t*)d = _mm_cvtsi64_si32(v0);
+            s += src_stride;
+            d += dst_stride;
+            a0 = a1; a1 = a2; a2 = a3; a3 = a4;
+            a4 = a5; a5 = a6; a6 = a7;
+        }
+    }
+}
+
+static av_always_inline void do_8tap_1d_h_e2k(uint8_t *dst, ptrdiff_t dst_stride,
+                                              const uint8_t *src, ptrdiff_t src_stride,
+                                              int w, int h, const int16_t *filter, int avg)
+{
+    int x, y;
+
+    if (w >= 8) {
+        __m64 a0, a1, a2, a3;
+        __m128i v0, v1, v2, v3;
+        __m128i f0, f1, f2, f3, c64; __m64 h0, h1;
+        f0 = _mm_set1_epi16((filter[0] & 255) | filter[1] << 8);
+        f1 = _mm_set1_epi16((filter[2] & 255) | filter[3] << 8);
+        f2 = _mm_set1_epi16((filter[4] & 255) | filter[5] << 8);
+        f3 = _mm_set1_epi16((filter[6] & 255) | filter[7] << 8);
+        c64 = _mm_set1_epi16(64);
+
+        for (y = 0; y < h; y++) {
+            PRAGMA_E2K("ivdep")
+            for (x = 0; x < w; x += 8) {
+                a0 = *(__m64*)(src + x - 3);
+                a1 = *(__m64*)(src + x - 3 + 7);
+                a0 = _mm_slli_si64(a0, 8);
+                a2 = _mm_alignr_pi8(a1, a0, 1);
+                a3 = _mm_alignr_pi8(a1, a0, 2);
+                v0 = _mm_unpacklo_epi8(_mm_movpi64_epi64(a2), _mm_movpi64_epi64(a3));
+                a2 = _mm_alignr_pi8(a1, a0, 3);
+                a3 = _mm_alignr_pi8(a1, a0, 4);
+                v1 = _mm_unpacklo_epi8(_mm_movpi64_epi64(a2), _mm_movpi64_epi64(a3));
+                a2 = _mm_alignr_pi8(a1, a0, 5);
+                a3 = _mm_alignr_pi8(a1, a0, 6);
+                v2 = _mm_unpacklo_epi8(_mm_movpi64_epi64(a2), _mm_movpi64_epi64(a3));
+                a2 = _mm_alignr_pi8(a1, a0, 7);
+                v3 = _mm_unpacklo_epi8(_mm_movpi64_epi64(a2), _mm_movpi64_epi64(a1));
+
+                v0 = _mm_maddubs_epi16(v0, f0);
+                v1 = _mm_maddubs_epi16(v1, f1);
+                v2 = _mm_maddubs_epi16(v2, f2);
+                v3 = _mm_maddubs_epi16(v3, f3);
+                v0 = _mm_add_epi16(v0, v2);
+                v1 = _mm_add_epi16(v1, v3);
+                v0 = _mm_add_epi16(v0, c64);
+                v0 = _mm_adds_epi16(v0, v1);
+                v0 = _mm_srai_epi16(v0, 7);
+                v0 = _mm_packus_epi16(v0, v0);
+                h0 = _mm_movepi64_pi64(v0);
+                if (avg) {
+                    h1 = *(__m64*)(dst + x);
+                    h0 = _mm_avg_pu8(h0, h1);
+                }
+                *(__m64*)(dst + x) = h0;
+            }
+            src += src_stride;
+            dst += dst_stride;
+        }
+    } else {
+        __m64 a0, a1, a2, a3;
+        __m64 v0, v1, v2, v3;
+        __m64 f0, f1, f2, f3, c64;
+        f0 = _mm_set1_pi16((filter[0] & 255) | filter[1] << 8);
+        f1 = _mm_set1_pi16((filter[2] & 255) | filter[3] << 8);
+        f2 = _mm_set1_pi16((filter[4] & 255) | filter[5] << 8);
+        f3 = _mm_set1_pi16((filter[6] & 255) | filter[7] << 8);
+        c64 = _mm_set1_pi16(64);
+
+        PRAGMA_E2K("ivdep")
+        for (y = 0; y < h; y++) {
+            a0 = *(__m64*)(src - 3);
+            a1 = _mm_cvtsi32_si64(*(uint32_t*)(src - 3 + 7));
+            a0 = _mm_slli_si64(a0, 8);
+            a2 = _mm_alignr_pi8(a1, a0, 1);
+            a3 = _mm_alignr_pi8(a1, a0, 2);
+            v0 = _mm_unpacklo_pi8(a2, a3);
+            a2 = _mm_alignr_pi8(a1, a0, 3);
+            a3 = _mm_alignr_pi8(a1, a0, 4);
+            v1 = _mm_unpacklo_pi8(a2, a3);
+            a2 = _mm_alignr_pi8(a1, a0, 5);
+            a3 = _mm_alignr_pi8(a1, a0, 6);
+            v2 = _mm_unpacklo_pi8(a2, a3);
+            a2 = _mm_alignr_pi8(a1, a0, 7);
+            v3 = _mm_unpacklo_pi8(a2, a1);
+
+            v0 = _mm_maddubs_pi16(v0, f0);
+            v1 = _mm_maddubs_pi16(v1, f1);
+            v2 = _mm_maddubs_pi16(v2, f2);
+            v3 = _mm_maddubs_pi16(v3, f3);
+            v0 = _mm_add_pi16(v0, v2);
+            v1 = _mm_add_pi16(v1, v3);
+            v0 = _mm_add_pi16(v0, c64);
+            v0 = _mm_adds_pi16(v0, v1);
+            v0 = _mm_srai_pi16(v0, 7);
+            v0 = _mm_packs_pu16(v0, v0);
+            if (avg) {
+                v1 = _mm_cvtsi32_si64(*(uint32_t*)dst);
+                v0 = _mm_avg_pu8(v0, v1);
+            }
+            *(uint32_t*)dst = _mm_cvtsi64_si32(v0);
+            src += src_stride;
+            dst += dst_stride;
+        }
+    }
+}
+
+#define filter_8tap_1d_fn(opn, opa) \
+static av_noinline void opn##_8tap_1d_v_e2k(uint8_t *dst, ptrdiff_t dst_stride, \
+                                            const uint8_t *src, ptrdiff_t src_stride, \
+                                            int w, int h, const int16_t *filter) \
+{ \
+    do_8tap_1d_v_e2k(dst, dst_stride, src, src_stride, w, h, filter, opa); \
+} \
+static av_noinline void opn##_8tap_1d_h_e2k(uint8_t *dst, ptrdiff_t dst_stride, \
+                                            const uint8_t *src, ptrdiff_t src_stride, \
+                                            int w, int h, const int16_t *filter) \
+{ \
+    do_8tap_1d_h_e2k(dst, dst_stride, src, src_stride, w, h, filter, opa); \
+}
+
+filter_8tap_1d_fn(put, 0)
+filter_8tap_1d_fn(avg, 1)
+
+#undef filter_8tap_1d_fn
+
+#define filter_fn_1d(sz, dir, dir_m, type, type_idx, avg) \
+static void avg##_8tap_##type##_##sz##dir##_e2k(uint8_t *dst, ptrdiff_t dst_stride, \
+                                                const uint8_t *src, ptrdiff_t src_stride, \
+                                                int h, int mx, int my) \
+{ \
+    avg##_8tap_1d_##dir##_e2k(dst, dst_stride, src, src_stride, sz, h, \
+                              ff_vp9_subpel_filters[type_idx][dir_m]); \
+}
+
+#define put_opa 0
+#define avg_opa 1
+#define filter_fn_2d(sz, type, type_idx, avg) \
+static void avg##_8tap_##type##_##sz##hv_e2k(uint8_t *dst, ptrdiff_t dst_stride, \
+                                             const uint8_t *src, ptrdiff_t src_stride, \
+                                             int h, int mx, int my) \
+{ \
+    int w = sz; \
+    pixel tmp[sz * (64 + 7)]; \
+    src -= src_stride * 3; \
+    do_8tap_1d_h_e2k(tmp, sz, src, src_stride, w, h + 7, ff_vp9_subpel_filters[type_idx][mx], 0); \
+    do_8tap_1d_v_e2k(dst, dst_stride, tmp + sz * 3, sz, w, h, ff_vp9_subpel_filters[type_idx][my], avg##_opa); \
+}
+
+#define bilinf_fn_1d(sz, dir, dir_m, avg)
+#define bilinf_fn_2d(sz, avg)
+
+#define filter_fn(sz, avg) \
+filter_fn_1d(sz, h, mx, regular, FILTER_8TAP_REGULAR, avg) \
+filter_fn_1d(sz, v, my, regular, FILTER_8TAP_REGULAR, avg) \
+filter_fn_2d(sz,        regular, FILTER_8TAP_REGULAR, avg) \
+filter_fn_1d(sz, h, mx, smooth,  FILTER_8TAP_SMOOTH,  avg) \
+filter_fn_1d(sz, v, my, smooth,  FILTER_8TAP_SMOOTH,  avg) \
+filter_fn_2d(sz,        smooth,  FILTER_8TAP_SMOOTH,  avg) \
+filter_fn_1d(sz, h, mx, sharp,   FILTER_8TAP_SHARP,   avg) \
+filter_fn_1d(sz, v, my, sharp,   FILTER_8TAP_SHARP,   avg) \
+filter_fn_2d(sz,        sharp,   FILTER_8TAP_SHARP,   avg) \
+bilinf_fn_1d(sz, h, mx,                               avg) \
+bilinf_fn_1d(sz, v, my,                               avg) \
+bilinf_fn_2d(sz,                                      avg)
+
+#define filter_fn_set(avg) \
+filter_fn(64, avg) \
+filter_fn(32, avg) \
+filter_fn(16, avg) \
+filter_fn(8,  avg) \
+filter_fn(4,  avg)
+
+filter_fn_set(put)
+filter_fn_set(avg)
+
+#undef filter_fn
+#undef filter_fn_set
+#undef filter_fn_1d
+#undef filter_fn_2d
+#undef bilinf_fn_1d
+#undef bilinf_fn_2d
+
+static av_cold void ff_vp9dsp_mc_init_8_e2k(VP9DSPContext *dsp)
+{
+#define init_fpel(idx1, idx2, sz, type) \
+    dsp->mc[idx1][FILTER_8TAP_SMOOTH ][idx2][0][0] = type##sz##_e2k; \
+    dsp->mc[idx1][FILTER_8TAP_REGULAR][idx2][0][0] = type##sz##_e2k; \
+    dsp->mc[idx1][FILTER_8TAP_SHARP  ][idx2][0][0] = type##sz##_e2k
+
+#define init_copy_avg(idx, sz) \
+    init_fpel(idx, 0, sz, copy); \
+    init_fpel(idx, 1, sz, avg)
+
+    init_copy_avg(0, 64);
+    init_copy_avg(1, 32);
+    init_copy_avg(2, 16);
+    init_copy_avg(3,  8);
+    init_copy_avg(4,  4);
+
+#undef init_copy_avg
+#undef init_fpel
+
+#define init_subpel1_bd_aware(idx1, idx2, idxh, idxv, sz, dir, type) \
+    dsp->mc[idx1][FILTER_8TAP_SMOOTH ][idx2][idxh][idxv] = type##_8tap_smooth_##sz##dir##_e2k; \
+    dsp->mc[idx1][FILTER_8TAP_REGULAR][idx2][idxh][idxv] = type##_8tap_regular_##sz##dir##_e2k; \
+    dsp->mc[idx1][FILTER_8TAP_SHARP  ][idx2][idxh][idxv] = type##_8tap_sharp_##sz##dir##_e2k
+
+#define init_subpel1(idx1, idx2, idxh, idxv, sz, dir, type) \
+    init_subpel1_bd_aware(idx1, idx2, idxh, idxv, sz, dir, type)
+
+#define init_subpel2(idx, idxh, idxv, dir, type) \
+    init_subpel1(0, idx, idxh, idxv, 64, dir, type); \
+    init_subpel1(1, idx, idxh, idxv, 32, dir, type); \
+    init_subpel1(2, idx, idxh, idxv, 16, dir, type); \
+    init_subpel1(3, idx, idxh, idxv,  8, dir, type); \
+    init_subpel1(4, idx, idxh, idxv,  4, dir, type)
+
+#define init_subpel3(idx, type) \
+    init_subpel2(idx, 1, 1, hv, type); \
+    init_subpel2(idx, 0, 1, v, type); \
+    init_subpel2(idx, 1, 0, h, type)
+
+    init_subpel3(0, put);
+    init_subpel3(1, avg);
+
+#undef init_subpel1
+#undef init_subpel2
+#undef init_subpel3
+#undef init_subpel1_bd_aware
+}
+
+av_cold void ff_vp9dsp_init_e2k(VP9DSPContext *dsp, int bpp, int bitexact)
+{
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    // checkasm
+    // doesn't check all cases for loopfilter 
+    if (bpp == 8) {
+        ff_vp9dsp_itxfm_init_8_e2k(dsp);
+        ff_vp9dsp_loopfilter_init_8_e2k(dsp);
+        ff_vp9dsp_mc_init_8_e2k(dsp);
+    }
+}
diff --git a/libavcodec/e2k/xvididct.c b/libavcodec/e2k/xvididct.c
new file mode 100644
index 0000000..efd6b5f
--- /dev/null
+++ b/libavcodec/e2k/xvididct.c
@@ -0,0 +1,198 @@
+/*
+ * SIMD-optimized XVID IDCT for Elbrus
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/mem_internal.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/idctdsp.h"
+#include "libavcodec/xvididct.h"
+
+#include "dctdsp.h"
+
+DECLARE_ALIGNED(16, static const int16_t, tab_e2k)[32 * 4] = {
+    0x4000, 0x539f, 0xc000, 0xac61, 0x4000, 0xdd5d, 0x4000, 0xdd5d,
+    0x4000, 0x22a3, 0x4000, 0x22a3, 0xc000, 0x539f, 0x4000, 0xac61,
+    0x3249, 0x11a8, 0x4b42, 0xee58, 0x11a8, 0x4b42, 0x11a8, 0xcdb7,
+    0x58c5, 0x4b42, 0xa73b, 0xcdb7, 0x3249, 0xa73b, 0x4b42, 0xa73b,
+    0x58c5, 0x73fc, 0xa73b, 0x8c04, 0x58c5, 0xcff5, 0x58c5, 0xcff5,
+    0x58c5, 0x300b, 0x58c5, 0x300b, 0xa73b, 0x73fc, 0x58c5, 0x8c04,
+    0x45bf, 0x187e, 0x6862, 0xe782, 0x187e, 0x6862, 0x187e, 0xba41,
+    0x7b21, 0x6862, 0x84df, 0xba41, 0x45bf, 0x84df, 0x6862, 0x84df,
+    0x539f, 0x6d41, 0xac61, 0x92bf, 0x539f, 0xd2bf, 0x539f, 0xd2bf,
+    0x539f, 0x2d41, 0x539f, 0x2d41, 0xac61, 0x6d41, 0x539f, 0x92bf,
+    0x41b3, 0x1712, 0x6254, 0xe8ee, 0x1712, 0x6254, 0x1712, 0xbe4d,
+    0x73fc, 0x6254, 0x8c04, 0xbe4d, 0x41b3, 0x8c04, 0x6254, 0x8c04,
+    0x4b42, 0x6254, 0xb4be, 0x9dac, 0x4b42, 0xd746, 0x4b42, 0xd746,
+    0x4b42, 0x28ba, 0x4b42, 0x28ba, 0xb4be, 0x6254, 0x4b42, 0x9dac,
+    0x3b21, 0x14c3, 0x587e, 0xeb3d, 0x14c3, 0x587e, 0x14c3, 0xc4df,
+    0x6862, 0x587e, 0x979e, 0xc4df, 0x3b21, 0x979e, 0x587e, 0x979e
+};
+
+#define XVID_IDCT_ROW(i, t, r) \
+    v3 = _mm_load_si128((__m128i*)(block + i * 8)); \
+    v0 = _mm_unpacklo_epi64(v3, v3);  /* 0246 */ \
+    v1 = _mm_shuffle_epi32(v3, 0x11); /* 4602 */ \
+    v2 = _mm_shuffle_epi32(v3, 0xbb); /* 5713 */ \
+    v3 = _mm_unpackhi_epi64(v3, v3);  /* 1357 */ \
+    v0 = _mm_madd_epi16(v0, _mm_load_si128((__m128i*)(tab_e2k + 32 * t))); \
+    v1 = _mm_madd_epi16(v1, _mm_load_si128((__m128i*)(tab_e2k + 32 * t + 8))); \
+    v2 = _mm_madd_epi16(v2, _mm_load_si128((__m128i*)(tab_e2k + 32 * t + 16))); \
+    v3 = _mm_madd_epi16(v3, _mm_load_si128((__m128i*)(tab_e2k + 32 * t + 24))); \
+    v0 = _mm_add_epi32(v0, v1); \
+    v1 = _mm_add_epi32(v2, v3); \
+    if (r) v0 = _mm_add_epi32(v0, _mm_set1_epi32(r)); \
+    v2 = _mm_srai_epi32(_mm_add_epi32(v0, v1), 11); \
+    v3 = _mm_srai_epi32(_mm_sub_epi32(v0, v1), 11); \
+    x##i = _mm_packs_epi32(v2, _mm_shuffle_epi32(v3, 0x1b));
+
+#define XVID_IDCT \
+    __m128i x0, x1, x2, x3, x4, x5, x6, x7; \
+    __m128i v0, v1, v2, v3, v4, v5, v6, v7; \
+    __m128i t0, t1, t2, t3; \
+    \
+    XVID_IDCT_ROW(0, 0, 65536) \
+    XVID_IDCT_ROW(1, 1, 3597) \
+    XVID_IDCT_ROW(2, 2, 2260) \
+    XVID_IDCT_ROW(3, 3, 1203) \
+    XVID_IDCT_ROW(4, 0, 0) \
+    XVID_IDCT_ROW(5, 3, 120) \
+    XVID_IDCT_ROW(6, 2, 512) \
+    XVID_IDCT_ROW(7, 1, 512) \
+    \
+    t1 = _mm_set1_epi16(13036);   /* tan1 */ \
+    t2 = _mm_set1_epi16(27146);   /* tan2 */ \
+    t3 = _mm_set1_epi16(-21746);  /* tan3 */ \
+    t0 = _mm_set1_epi16(23170);   /* sqrt2 */ \
+    \
+    v0 = _mm_mulhi_epi16(t3, x3); /* x3*(tg_3_16-1) */ \
+    v1 = _mm_mulhi_epi16(t3, x5); /* x5*(tg_3_16-1) */ \
+    v4 = _mm_mulhi_epi16(t1, x7); /* x7*tg_1_16 */ \
+    v2 = _mm_mulhi_epi16(t1, x1); /* x1*tg_1_16 */ \
+    v0 = _mm_adds_epi16(v0, x3);  /* x3*tg_3_16 */ \
+    v1 = _mm_adds_epi16(v1, x3);  /* x3+x5*(tg_3_16-1) */ \
+    v0 = _mm_subs_epi16(v0, x5);  /* x3*tg_3_16-x5 = tm35 */ \
+    v1 = _mm_adds_epi16(v1, x5);  /* x3+x5*tg_3_16 = tp35 */ \
+    v4 = _mm_adds_epi16(v4, x1);  /* x1+tg_1_16*x7 = tp17 */ \
+    v2 = _mm_subs_epi16(v2, x7);  /* x1*tg_1_16-x7 = tm17 */ \
+    t1 = _mm_adds_epi16(v4, v1);  /* tp17+tp35 = b0 */ \
+    t3 = _mm_subs_epi16(v2, v0);  /* tm17-tm35 = b3 */ \
+    v4 = _mm_subs_epi16(v4, v1);  /* tp17-tp35 = t1 */ \
+    v2 = _mm_adds_epi16(v2, v0);  /* tm17+tm35 = t2 */ \
+    v1 = _mm_adds_epi16(v4, v2);  /* t1+t2 */ \
+    v4 = _mm_subs_epi16(v4, v2);  /* t1-t2 */ \
+    v0 = _mm_mulhi_epi16(t2, x2); /* x2*tg_2_16 */ \
+    v7 = _mm_mulhi_epi16(t2, x6); /* x6*tg_2_16 */ \
+    v1 = _mm_mulhi_epi16(v1, t0); /* ocos_4_16*(t1+t2) = b1/2 */ \
+    v4 = _mm_mulhi_epi16(v4, t0); /* ocos_4_16*(t1-t2) = b2/2 */ \
+    v0 = _mm_subs_epi16(v0, x6);  /* t2*tg_2_16-x6 = tm26 */ \
+    v7 = _mm_adds_epi16(v7, x2);  /* x2+x6*tg_2_16 = tp26 */ \
+    v5 = _mm_adds_epi16(x0, x4);  /* x0+x4 = tp04 */ \
+    v6 = _mm_subs_epi16(x0, x4);  /* x0-x4 = tm04 */ \
+    v2 = _mm_subs_epi16(v5, v7);  /* tp04-tp26 = a3 */ \
+    v3 = _mm_adds_epi16(v6, v0);  /* tm04+tm26 = a1 */ \
+    v1 = _mm_adds_epi16(v1, v1);  /* b1 */ \
+    v4 = _mm_adds_epi16(v4, v4);  /* b2 */ \
+    v5 = _mm_adds_epi16(v5, v7);  /* tp04+tp26 = a0 */ \
+    v6 = _mm_subs_epi16(v6, v0);  /* tm04-tm26 = a2 */ \
+    x0 = _mm_adds_epi16(v5, t1);  /* a0+b0 */ \
+    x7 = _mm_subs_epi16(v5, t1);  /* a0-b0 */ \
+    x1 = _mm_adds_epi16(v3, v1);  /* a1+b1 */ \
+    x6 = _mm_subs_epi16(v3, v1);  /* a1-b1 */ \
+    x2 = _mm_adds_epi16(v6, v4);  /* a2+b2 */ \
+    x5 = _mm_subs_epi16(v6, v4);  /* a2-b2 */ \
+    x3 = _mm_adds_epi16(v2, t3);  /* a3+b3 */ \
+    x4 = _mm_subs_epi16(v2, t3);  /* a3-b3 */ \
+    x0 = _mm_srai_epi16(x0, 6);   /* dst0 */ \
+    x1 = _mm_srai_epi16(x1, 6);   /* dst1 */ \
+    x2 = _mm_srai_epi16(x2, 6);   /* dst2 */ \
+    x3 = _mm_srai_epi16(x3, 6);   /* dst3 */ \
+    x4 = _mm_srai_epi16(x4, 6);   /* dst4 */ \
+    x5 = _mm_srai_epi16(x5, 6);   /* dst5 */ \
+    x6 = _mm_srai_epi16(x6, 6);   /* dst6 */ \
+    x7 = _mm_srai_epi16(x7, 6);   /* dst7 */
+
+void ff_xvid_idct_e2k(int16_t *block) {
+    XVID_IDCT
+    VEC_ST(block + 8 * 0, x0);
+    VEC_ST(block + 8 * 1, x1);
+    VEC_ST(block + 8 * 2, x2);
+    VEC_ST(block + 8 * 3, x3);
+    VEC_ST(block + 8 * 4, x4);
+    VEC_ST(block + 8 * 5, x5);
+    VEC_ST(block + 8 * 6, x6);
+    VEC_ST(block + 8 * 7, x7);
+}
+
+#define PUT(a, b)                             \
+    t0 = _mm_packus_epi16(x##a, x##b);        \
+    VEC_STL(pixels, t0); pixels += line_size; \
+    VEC_STH(pixels, t0); pixels += line_size;
+
+#define ADD(a, b)                             \
+    t0 = VEC_LD8(pixels);                     \
+    t1 = VEC_LD8(pixels + line_size);         \
+    t0 = _mm_unpacklo_epi8(t0, t2);           \
+    t1 = _mm_unpacklo_epi8(t1, t2);           \
+    t0 = _mm_adds_epi16(t0, x##a);            \
+    t1 = _mm_adds_epi16(t1, x##b);            \
+    t0 = _mm_packus_epi16(t0, t1);            \
+    VEC_STL(pixels, t0); pixels += line_size; \
+    VEC_STH(pixels, t0); pixels += line_size;
+
+static void xvid_idct_put_e2k(uint8_t *pixels, ptrdiff_t line_size, int16_t *block)
+{
+    XVID_IDCT
+    PUT(0, 1) PUT(2, 3) PUT(4, 5) PUT(6, 7)
+}
+
+static void xvid_idct_add_e2k(uint8_t *pixels, ptrdiff_t line_size, int16_t *block)
+{
+    XVID_IDCT
+    t2 = _mm_setzero_si128();
+    ADD(0, 1) ADD(2, 3) ADD(4, 5) ADD(6, 7)
+}
+
+av_cold void ff_xvid_idct_init_e2k(IDCTDSPContext *c, AVCodecContext *avctx,
+                                   unsigned high_bit_depth)
+{
+    int cpu_flags = av_get_cpu_flags();
+    if (!E2K_SIMD(cpu_flags))
+        return;
+
+    // !checkasm
+    // libavcodec/tests/dct -i
+
+    if (!high_bit_depth) {
+        if (avctx->idct_algo == FF_IDCT_AUTO ||
+            avctx->idct_algo == FF_IDCT_XVID) {
+
+						c->idct      = ff_xvid_idct_e2k;
+						c->idct_put  = xvid_idct_put_e2k;
+						c->idct_add  = xvid_idct_add_e2k;
+    				// same as FF_IDCT_PERM_SSE2, but already initialized
+            c->perm_type = FF_IDCT_PERM_LIBMPEG2;
+        }
+    }
+}
diff --git a/libavcodec/fdctdsp.c b/libavcodec/fdctdsp.c
index f8ba174..b399ded 100644
--- a/libavcodec/fdctdsp.c
+++ b/libavcodec/fdctdsp.c
@@ -44,6 +44,8 @@ av_cold void ff_fdctdsp_init(FDCTDSPContext *c, AVCodecContext *avctx)
 
 #if ARCH_PPC
     ff_fdctdsp_init_ppc(c, avctx, high_bit_depth);
+#elif ARCH_E2K
+    ff_fdctdsp_init_e2k(c, avctx, high_bit_depth);
 #elif ARCH_X86
     ff_fdctdsp_init_x86(c, avctx, high_bit_depth);
 #endif
diff --git a/libavcodec/fdctdsp.h b/libavcodec/fdctdsp.h
index 7378eab..296562f 100644
--- a/libavcodec/fdctdsp.h
+++ b/libavcodec/fdctdsp.h
@@ -34,6 +34,8 @@ FF_VISIBILITY_PUSH_HIDDEN
 void ff_fdctdsp_init(FDCTDSPContext *c, struct AVCodecContext *avctx);
 void ff_fdctdsp_init_ppc(FDCTDSPContext *c, struct AVCodecContext *avctx,
                          unsigned high_bit_depth);
+void ff_fdctdsp_init_e2k(FDCTDSPContext *c, struct AVCodecContext *avctx,
+                         unsigned high_bit_depth);
 void ff_fdctdsp_init_x86(FDCTDSPContext *c, struct AVCodecContext *avctx,
                          unsigned high_bit_depth);
 
diff --git a/libavcodec/fmtconvert.c b/libavcodec/fmtconvert.c
index d889e61..09190f8 100644
--- a/libavcodec/fmtconvert.c
+++ b/libavcodec/fmtconvert.c
@@ -52,6 +52,8 @@ av_cold void ff_fmt_convert_init(FmtConvertContext *c)
     ff_fmt_convert_init_arm(c);
 #elif ARCH_PPC
     ff_fmt_convert_init_ppc(c);
+#elif ARCH_E2K
+    ff_fmt_convert_init_e2k(c);
 #elif ARCH_RISCV
     ff_fmt_convert_init_riscv(c);
 #elif ARCH_X86
diff --git a/libavcodec/fmtconvert.h b/libavcodec/fmtconvert.h
index 1cb4628..8d89d2c 100644
--- a/libavcodec/fmtconvert.h
+++ b/libavcodec/fmtconvert.h
@@ -61,6 +61,7 @@ void ff_fmt_convert_init(FmtConvertContext *c);
 void ff_fmt_convert_init_aarch64(FmtConvertContext *c);
 void ff_fmt_convert_init_arm(FmtConvertContext *c);
 void ff_fmt_convert_init_ppc(FmtConvertContext *c);
+void ff_fmt_convert_init_e2k(FmtConvertContext *c);
 void ff_fmt_convert_init_riscv(FmtConvertContext *c);
 void ff_fmt_convert_init_x86(FmtConvertContext *c);
 void ff_fmt_convert_init_mips(FmtConvertContext *c);
diff --git a/libavcodec/h264chroma.c b/libavcodec/h264chroma.c
index 1eeab7b..d67d3b6 100644
--- a/libavcodec/h264chroma.c
+++ b/libavcodec/h264chroma.c
@@ -52,6 +52,8 @@ av_cold void ff_h264chroma_init(H264ChromaContext *c, int bit_depth)
     ff_h264chroma_init_arm(c, bit_depth);
 #elif ARCH_PPC
     ff_h264chroma_init_ppc(c, bit_depth);
+#elif ARCH_E2K
+    ff_h264chroma_init_e2k(c, bit_depth);
 #elif ARCH_X86
     ff_h264chroma_init_x86(c, bit_depth);
 #elif ARCH_MIPS
diff --git a/libavcodec/h264chroma.h b/libavcodec/h264chroma.h
index 9c81c18..ff6589b 100644
--- a/libavcodec/h264chroma.h
+++ b/libavcodec/h264chroma.h
@@ -34,6 +34,7 @@ void ff_h264chroma_init(H264ChromaContext *c, int bit_depth);
 void ff_h264chroma_init_aarch64(H264ChromaContext *c, int bit_depth);
 void ff_h264chroma_init_arm(H264ChromaContext *c, int bit_depth);
 void ff_h264chroma_init_ppc(H264ChromaContext *c, int bit_depth);
+void ff_h264chroma_init_e2k(H264ChromaContext *c, int bit_depth);
 void ff_h264chroma_init_x86(H264ChromaContext *c, int bit_depth);
 void ff_h264chroma_init_mips(H264ChromaContext *c, int bit_depth);
 void ff_h264chroma_init_loongarch(H264ChromaContext *c, int bit_depth);
diff --git a/libavcodec/h264dsp.c b/libavcodec/h264dsp.c
index 4d2ee10..d72cfcf 100644
--- a/libavcodec/h264dsp.c
+++ b/libavcodec/h264dsp.c
@@ -158,6 +158,8 @@ av_cold void ff_h264dsp_init(H264DSPContext *c, const int bit_depth,
     ff_h264dsp_init_arm(c, bit_depth, chroma_format_idc);
 #elif ARCH_PPC
     ff_h264dsp_init_ppc(c, bit_depth, chroma_format_idc);
+#elif ARCH_E2K
+    ff_h264dsp_init_e2k(c, bit_depth, chroma_format_idc);
 #elif ARCH_X86
     ff_h264dsp_init_x86(c, bit_depth, chroma_format_idc);
 #elif ARCH_MIPS
diff --git a/libavcodec/h264dsp.h b/libavcodec/h264dsp.h
index e0880c4..6dff79d 100644
--- a/libavcodec/h264dsp.h
+++ b/libavcodec/h264dsp.h
@@ -125,6 +125,8 @@ void ff_h264dsp_init_arm(H264DSPContext *c, const int bit_depth,
                          const int chroma_format_idc);
 void ff_h264dsp_init_ppc(H264DSPContext *c, const int bit_depth,
                          const int chroma_format_idc);
+void ff_h264dsp_init_e2k(H264DSPContext *c, const int bit_depth,
+                         const int chroma_format_idc);
 void ff_h264dsp_init_x86(H264DSPContext *c, const int bit_depth,
                          const int chroma_format_idc);
 void ff_h264dsp_init_mips(H264DSPContext *c, const int bit_depth,
diff --git a/libavcodec/h264qpel.c b/libavcodec/h264qpel.c
index 65fef03..f15af00 100644
--- a/libavcodec/h264qpel.c
+++ b/libavcodec/h264qpel.c
@@ -102,6 +102,8 @@ av_cold void ff_h264qpel_init(H264QpelContext *c, int bit_depth)
     ff_h264qpel_init_arm(c, bit_depth);
 #elif ARCH_PPC
     ff_h264qpel_init_ppc(c, bit_depth);
+#elif ARCH_E2K
+    ff_h264qpel_init_e2k(c, bit_depth);
 #elif ARCH_X86
     ff_h264qpel_init_x86(c, bit_depth);
 #elif ARCH_MIPS
diff --git a/libavcodec/h264qpel.h b/libavcodec/h264qpel.h
index 0259e8d..f9bf8b8 100644
--- a/libavcodec/h264qpel.h
+++ b/libavcodec/h264qpel.h
@@ -34,6 +34,7 @@ void ff_h264qpel_init(H264QpelContext *c, int bit_depth);
 void ff_h264qpel_init_aarch64(H264QpelContext *c, int bit_depth);
 void ff_h264qpel_init_arm(H264QpelContext *c, int bit_depth);
 void ff_h264qpel_init_ppc(H264QpelContext *c, int bit_depth);
+void ff_h264qpel_init_e2k(H264QpelContext *c, int bit_depth);
 void ff_h264qpel_init_x86(H264QpelContext *c, int bit_depth);
 void ff_h264qpel_init_mips(H264QpelContext *c, int bit_depth);
 void ff_h264qpel_init_loongarch(H264QpelContext *c, int bit_depth);
diff --git a/libavcodec/hevcdsp.c b/libavcodec/hevcdsp.c
index 2ca551d..a6f38f2 100644
--- a/libavcodec/hevcdsp.c
+++ b/libavcodec/hevcdsp.c
@@ -263,6 +263,8 @@ int i = 0;
     ff_hevc_dsp_init_arm(hevcdsp, bit_depth);
 #elif ARCH_PPC
     ff_hevc_dsp_init_ppc(hevcdsp, bit_depth);
+#elif ARCH_E2K
+    ff_hevc_dsp_init_e2k(hevcdsp, bit_depth);
 #elif ARCH_X86
     ff_hevc_dsp_init_x86(hevcdsp, bit_depth);
 #elif ARCH_MIPS
diff --git a/libavcodec/hevcdsp.h b/libavcodec/hevcdsp.h
index 1b9c5bb..6e301ca 100644
--- a/libavcodec/hevcdsp.h
+++ b/libavcodec/hevcdsp.h
@@ -132,6 +132,7 @@ extern const int8_t ff_hevc_qpel_filters[3][16];
 void ff_hevc_dsp_init_aarch64(HEVCDSPContext *c, const int bit_depth);
 void ff_hevc_dsp_init_arm(HEVCDSPContext *c, const int bit_depth);
 void ff_hevc_dsp_init_ppc(HEVCDSPContext *c, const int bit_depth);
+void ff_hevc_dsp_init_e2k(HEVCDSPContext *c, const int bit_depth);
 void ff_hevc_dsp_init_x86(HEVCDSPContext *c, const int bit_depth);
 void ff_hevc_dsp_init_mips(HEVCDSPContext *c, const int bit_depth);
 void ff_hevc_dsp_init_loongarch(HEVCDSPContext *c, const int bit_depth);
diff --git a/libavcodec/hpeldsp.c b/libavcodec/hpeldsp.c
index 1ec76e7..99492b1 100644
--- a/libavcodec/hpeldsp.c
+++ b/libavcodec/hpeldsp.c
@@ -363,6 +363,8 @@ av_cold void ff_hpeldsp_init(HpelDSPContext *c, int flags)
     ff_hpeldsp_init_arm(c, flags);
 #elif ARCH_PPC
     ff_hpeldsp_init_ppc(c, flags);
+#elif ARCH_E2K
+    ff_hpeldsp_init_e2k(c, flags);
 #elif ARCH_X86
     ff_hpeldsp_init_x86(c, flags);
 #elif ARCH_MIPS
diff --git a/libavcodec/hpeldsp.h b/libavcodec/hpeldsp.h
index 45e81b1..12920fd 100644
--- a/libavcodec/hpeldsp.h
+++ b/libavcodec/hpeldsp.h
@@ -100,6 +100,7 @@ void ff_hpeldsp_init_aarch64(HpelDSPContext *c, int flags);
 void ff_hpeldsp_init_alpha(HpelDSPContext *c, int flags);
 void ff_hpeldsp_init_arm(HpelDSPContext *c, int flags);
 void ff_hpeldsp_init_ppc(HpelDSPContext *c, int flags);
+void ff_hpeldsp_init_e2k(HpelDSPContext *c, int flags);
 void ff_hpeldsp_init_x86(HpelDSPContext *c, int flags);
 void ff_hpeldsp_init_mips(HpelDSPContext *c, int flags);
 void ff_hpeldsp_init_loongarch(HpelDSPContext *c, int flags);
diff --git a/libavcodec/idctdsp.c b/libavcodec/idctdsp.c
index 7216afb..6f53d90 100644
--- a/libavcodec/idctdsp.c
+++ b/libavcodec/idctdsp.c
@@ -300,6 +300,8 @@ av_cold void ff_idctdsp_init(IDCTDSPContext *c, AVCodecContext *avctx)
     ff_idctdsp_init_arm(c, avctx, high_bit_depth);
 #elif ARCH_PPC
     ff_idctdsp_init_ppc(c, avctx, high_bit_depth);
+#elif ARCH_E2K
+    ff_idctdsp_init_e2k(c, avctx, high_bit_depth);
 #elif ARCH_RISCV
     ff_idctdsp_init_riscv(c, avctx, high_bit_depth);
 #elif ARCH_X86
diff --git a/libavcodec/idctdsp.h b/libavcodec/idctdsp.h
index c840a51..d6bcf4b 100644
--- a/libavcodec/idctdsp.h
+++ b/libavcodec/idctdsp.h
@@ -106,6 +106,8 @@ void ff_idctdsp_init_arm(IDCTDSPContext *c, struct AVCodecContext *avctx,
                          unsigned high_bit_depth);
 void ff_idctdsp_init_ppc(IDCTDSPContext *c, struct AVCodecContext *avctx,
                          unsigned high_bit_depth);
+void ff_idctdsp_init_e2k(IDCTDSPContext *c, struct AVCodecContext *avctx,
+                         unsigned high_bit_depth);
 void ff_idctdsp_init_riscv(IDCTDSPContext *c, struct AVCodecContext *avctx,
                            unsigned high_bit_depth);
 void ff_idctdsp_init_x86(IDCTDSPContext *c, struct AVCodecContext *avctx,
diff --git a/libavcodec/librsvgdec.c b/libavcodec/librsvgdec.c
index c328fbc..756c26d 100644
--- a/libavcodec/librsvgdec.c
+++ b/libavcodec/librsvgdec.c
@@ -90,8 +90,10 @@ static int librsvg_decode_frame(AVCodecContext *avctx, AVFrame *frame,
         goto end;
 
     avctx->pix_fmt = AV_PIX_FMT_RGB32;
+#if LIBRSVG_MAJOR_VERSION > 2 || LIBRSVG_MAJOR_VERSION == 2 && LIBRSVG_MINOR_VERSION >= 52
     viewport.width = dimensions.width;
     viewport.height = dimensions.height;
+#endif
 
     ret = ff_get_buffer(avctx, frame, 0);
     if (ret < 0)
diff --git a/libavcodec/lossless_audiodsp.c b/libavcodec/lossless_audiodsp.c
index 1daf2e4..6eb5c64 100644
--- a/libavcodec/lossless_audiodsp.c
+++ b/libavcodec/lossless_audiodsp.c
@@ -63,6 +63,8 @@ av_cold void ff_llauddsp_init(LLAudDSPContext *c)
     ff_llauddsp_init_arm(c);
 #elif ARCH_PPC
     ff_llauddsp_init_ppc(c);
+#elif ARCH_E2K
+    ff_llauddsp_init_e2k(c);
 #elif ARCH_X86
     ff_llauddsp_init_x86(c);
 #endif
diff --git a/libavcodec/lossless_audiodsp.h b/libavcodec/lossless_audiodsp.h
index eea5d49..3de02d5 100644
--- a/libavcodec/lossless_audiodsp.h
+++ b/libavcodec/lossless_audiodsp.h
@@ -46,6 +46,7 @@ typedef struct LLAudDSPContext {
 void ff_llauddsp_init(LLAudDSPContext *c);
 void ff_llauddsp_init_arm(LLAudDSPContext *c);
 void ff_llauddsp_init_ppc(LLAudDSPContext *c);
+void ff_llauddsp_init_e2k(LLAudDSPContext *c);
 void ff_llauddsp_init_x86(LLAudDSPContext *c);
 
 #endif /* AVCODEC_LOSSLESS_AUDIODSP_H */
diff --git a/libavcodec/lossless_videodsp.c b/libavcodec/lossless_videodsp.c
index 3596069..b285b68 100644
--- a/libavcodec/lossless_videodsp.c
+++ b/libavcodec/lossless_videodsp.c
@@ -121,6 +121,8 @@ void ff_llviddsp_init(LLVidDSPContext *c)
 
 #if ARCH_PPC
     ff_llviddsp_init_ppc(c);
+#elif ARCH_E2K
+    ff_llviddsp_init_e2k(c);
 #elif ARCH_X86
     ff_llviddsp_init_x86(c);
 #endif
diff --git a/libavcodec/lossless_videodsp.h b/libavcodec/lossless_videodsp.h
index da4baa1..c0960fe 100644
--- a/libavcodec/lossless_videodsp.h
+++ b/libavcodec/lossless_videodsp.h
@@ -42,5 +42,6 @@ typedef struct LLVidDSPContext {
 void ff_llviddsp_init(LLVidDSPContext *llviddsp);
 void ff_llviddsp_init_x86(LLVidDSPContext *llviddsp);
 void ff_llviddsp_init_ppc(LLVidDSPContext *llviddsp);
+void ff_llviddsp_init_e2k(LLVidDSPContext *llviddsp);
 
 #endif //AVCODEC_LOSSLESS_VIDEODSP_H
diff --git a/libavcodec/me_cmp.c b/libavcodec/me_cmp.c
index cd05e63..c734108 100644
--- a/libavcodec/me_cmp.c
+++ b/libavcodec/me_cmp.c
@@ -1064,6 +1064,8 @@ av_cold void ff_me_cmp_init(MECmpContext *c, AVCodecContext *avctx)
     ff_me_cmp_init_arm(c, avctx);
 #elif ARCH_PPC
     ff_me_cmp_init_ppc(c, avctx);
+#elif ARCH_E2K
+    ff_me_cmp_init_e2k(c, avctx);
 #elif ARCH_X86
     ff_me_cmp_init_x86(c, avctx);
 #elif ARCH_MIPS
diff --git a/libavcodec/me_cmp.h b/libavcodec/me_cmp.h
index aefd32a..f2cf80d 100644
--- a/libavcodec/me_cmp.h
+++ b/libavcodec/me_cmp.h
@@ -86,6 +86,7 @@ void ff_me_cmp_init_aarch64(MECmpContext *c, AVCodecContext *avctx);
 void ff_me_cmp_init_alpha(MECmpContext *c, AVCodecContext *avctx);
 void ff_me_cmp_init_arm(MECmpContext *c, AVCodecContext *avctx);
 void ff_me_cmp_init_ppc(MECmpContext *c, AVCodecContext *avctx);
+void ff_me_cmp_init_e2k(MECmpContext *c, AVCodecContext *avctx);
 void ff_me_cmp_init_x86(MECmpContext *c, AVCodecContext *avctx);
 void ff_me_cmp_init_mips(MECmpContext *c, AVCodecContext *avctx);
 
diff --git a/libavcodec/mpeg4videodsp.c b/libavcodec/mpeg4videodsp.c
index 1c5661a..2012c54 100644
--- a/libavcodec/mpeg4videodsp.c
+++ b/libavcodec/mpeg4videodsp.c
@@ -114,6 +114,8 @@ av_cold void ff_mpeg4videodsp_init(Mpeg4VideoDSPContext *c)
 
 #if ARCH_PPC
     ff_mpeg4videodsp_init_ppc(c);
+#elif ARCH_E2K
+    ff_mpeg4videodsp_init_e2k(c);
 #elif ARCH_X86
     ff_mpeg4videodsp_init_x86(c);
 #endif
diff --git a/libavcodec/mpeg4videodsp.h b/libavcodec/mpeg4videodsp.h
index e1ccb71..d27f4fd 100644
--- a/libavcodec/mpeg4videodsp.h
+++ b/libavcodec/mpeg4videodsp.h
@@ -42,6 +42,7 @@ typedef struct Mpeg4VideoDSPContext {
 
 void ff_mpeg4videodsp_init(Mpeg4VideoDSPContext *c);
 void ff_mpeg4videodsp_init_ppc(Mpeg4VideoDSPContext *c);
+void ff_mpeg4videodsp_init_e2k(Mpeg4VideoDSPContext *c);
 void ff_mpeg4videodsp_init_x86(Mpeg4VideoDSPContext *c);
 
 #endif /* AVCODEC_MPEG4VIDEODSP_H */
diff --git a/libavcodec/mpegaudiodsp.c b/libavcodec/mpegaudiodsp.c
index 0971c28..0cf198c 100644
--- a/libavcodec/mpegaudiodsp.c
+++ b/libavcodec/mpegaudiodsp.c
@@ -97,6 +97,8 @@ av_cold void ff_mpadsp_init(MPADSPContext *s)
     ff_mpadsp_init_arm(s);
 #elif ARCH_PPC
     ff_mpadsp_init_ppc(s);
+#elif ARCH_E2K
+    ff_mpadsp_init_e2k(s);
 #elif ARCH_X86
     ff_mpadsp_init_x86(s);
 #endif
diff --git a/libavcodec/mpegaudiodsp.h b/libavcodec/mpegaudiodsp.h
index 7bc6351..55a00a9 100644
--- a/libavcodec/mpegaudiodsp.h
+++ b/libavcodec/mpegaudiodsp.h
@@ -62,6 +62,7 @@ void ff_mpa_synth_filter_float(MPADSPContext *s,
 void ff_mpadsp_init_aarch64(MPADSPContext *s);
 void ff_mpadsp_init_arm(MPADSPContext *s);
 void ff_mpadsp_init_ppc(MPADSPContext *s);
+void ff_mpadsp_init_e2k(MPADSPContext *s);
 void ff_mpadsp_init_x86(MPADSPContext *s);
 void ff_mpadsp_init_x86_tabs(void);
 void ff_mpadsp_init_mipsfpu(MPADSPContext *s);
diff --git a/libavcodec/mpegvideo.c b/libavcodec/mpegvideo.c
index 81796e4..3390813 100644
--- a/libavcodec/mpegvideo.c
+++ b/libavcodec/mpegvideo.c
@@ -309,6 +309,8 @@ static av_cold int dct_init(MpegEncContext *s)
     ff_mpv_common_init_arm(s);
 #elif ARCH_PPC
     ff_mpv_common_init_ppc(s);
+#elif ARCH_E2K
+    ff_mpv_common_init_e2k(s);
 #elif ARCH_X86
     ff_mpv_common_init_x86(s);
 #elif ARCH_MIPS
diff --git a/libavcodec/mpegvideo.h b/libavcodec/mpegvideo.h
index d7c2f57..62bbf76 100644
--- a/libavcodec/mpegvideo.h
+++ b/libavcodec/mpegvideo.h
@@ -552,6 +552,7 @@ void ff_mpv_common_init_arm(MpegEncContext *s);
 void ff_mpv_common_init_axp(MpegEncContext *s);
 void ff_mpv_common_init_neon(MpegEncContext *s);
 void ff_mpv_common_init_ppc(MpegEncContext *s);
+void ff_mpv_common_init_e2k(MpegEncContext *s);
 void ff_mpv_common_init_x86(MpegEncContext *s);
 void ff_mpv_common_init_mips(MpegEncContext *s);
 /**
diff --git a/libavcodec/mpegvideoencdsp.c b/libavcodec/mpegvideoencdsp.c
index 997d048..9e05955 100644
--- a/libavcodec/mpegvideoencdsp.c
+++ b/libavcodec/mpegvideoencdsp.c
@@ -249,6 +249,8 @@ av_cold void ff_mpegvideoencdsp_init(MpegvideoEncDSPContext *c,
     ff_mpegvideoencdsp_init_arm(c, avctx);
 #elif ARCH_PPC
     ff_mpegvideoencdsp_init_ppc(c, avctx);
+#elif ARCH_E2K
+    ff_mpegvideoencdsp_init_e2k(c, avctx);
 #elif ARCH_X86
     ff_mpegvideoencdsp_init_x86(c, avctx);
 #elif ARCH_MIPS
diff --git a/libavcodec/mpegvideoencdsp.h b/libavcodec/mpegvideoencdsp.h
index 9508467..5fc8432 100644
--- a/libavcodec/mpegvideoencdsp.h
+++ b/libavcodec/mpegvideoencdsp.h
@@ -50,6 +50,8 @@ void ff_mpegvideoencdsp_init_arm(MpegvideoEncDSPContext *c,
                                  AVCodecContext *avctx);
 void ff_mpegvideoencdsp_init_ppc(MpegvideoEncDSPContext *c,
                                  AVCodecContext *avctx);
+void ff_mpegvideoencdsp_init_e2k(MpegvideoEncDSPContext *c,
+                                 AVCodecContext *avctx);
 void ff_mpegvideoencdsp_init_x86(MpegvideoEncDSPContext *c,
                                  AVCodecContext *avctx);
 void ff_mpegvideoencdsp_init_mips(MpegvideoEncDSPContext *c,
diff --git a/libavcodec/pixblockdsp.c b/libavcodec/pixblockdsp.c
index 4294075..74848a4 100644
--- a/libavcodec/pixblockdsp.c
+++ b/libavcodec/pixblockdsp.c
@@ -109,6 +109,8 @@ av_cold void ff_pixblockdsp_init(PixblockDSPContext *c, AVCodecContext *avctx)
     ff_pixblockdsp_init_arm(c, avctx, high_bit_depth);
 #elif ARCH_PPC
     ff_pixblockdsp_init_ppc(c, avctx, high_bit_depth);
+#elif ARCH_E2K
+    ff_pixblockdsp_init_e2k(c, avctx, high_bit_depth);
 #elif ARCH_RISCV
     ff_pixblockdsp_init_riscv(c, avctx, high_bit_depth);
 #elif ARCH_X86
diff --git a/libavcodec/pixblockdsp.h b/libavcodec/pixblockdsp.h
index 9b002aa..518b4a5 100644
--- a/libavcodec/pixblockdsp.h
+++ b/libavcodec/pixblockdsp.h
@@ -52,6 +52,8 @@ void ff_pixblockdsp_init_arm(PixblockDSPContext *c, AVCodecContext *avctx,
                              unsigned high_bit_depth);
 void ff_pixblockdsp_init_ppc(PixblockDSPContext *c, AVCodecContext *avctx,
                              unsigned high_bit_depth);
+void ff_pixblockdsp_init_e2k(PixblockDSPContext *c, AVCodecContext *avctx,
+                             unsigned high_bit_depth);
 void ff_pixblockdsp_init_riscv(PixblockDSPContext *c, AVCodecContext *avctx,
                                unsigned high_bit_depth);
 void ff_pixblockdsp_init_x86(PixblockDSPContext *c, AVCodecContext *avctx,
diff --git a/libavcodec/svq1enc.c b/libavcodec/svq1enc.c
index 894ae55..1067373 100644
--- a/libavcodec/svq1enc.c
+++ b/libavcodec/svq1enc.c
@@ -643,6 +643,8 @@ static av_cold int svq1_encode_init(AVCodecContext *avctx)
 
 #if ARCH_PPC
     ff_svq1enc_init_ppc(&s->svq1encdsp);
+#elif ARCH_E2K
+    ff_svq1enc_init_e2k(&s->svq1encdsp);
 #elif ARCH_X86
     ff_svq1enc_init_x86(&s->svq1encdsp);
 #endif
diff --git a/libavcodec/svq1encdsp.h b/libavcodec/svq1encdsp.h
index 91b3673..a8c9ca2 100644
--- a/libavcodec/svq1encdsp.h
+++ b/libavcodec/svq1encdsp.h
@@ -29,6 +29,7 @@ typedef struct SVQ1EncDSPContext {
 } SVQ1EncDSPContext;
 
 void ff_svq1enc_init_ppc(SVQ1EncDSPContext *c);
+void ff_svq1enc_init_e2k(SVQ1EncDSPContext *c);
 void ff_svq1enc_init_x86(SVQ1EncDSPContext *c);
 
 #endif /* AVCODEC_SVQ1ENCDSP_H */
diff --git a/libavcodec/tests/dct.c b/libavcodec/tests/dct.c
index 010d0c1..20f8273 100644
--- a/libavcodec/tests/dct.c
+++ b/libavcodec/tests/dct.c
@@ -104,6 +104,8 @@ static const struct algo idct_tab[] = {
 #include "arm/dct.c"
 #elif ARCH_PPC
 #include "ppc/dct.c"
+#elif ARCH_E2K
+#include "e2k/dct.c"
 #elif ARCH_X86
 #include "x86/dct.c"
 #else
diff --git a/libavcodec/tests/e2k/dct.c b/libavcodec/tests/e2k/dct.c
new file mode 100644
index 0000000..4783f06
--- /dev/null
+++ b/libavcodec/tests/e2k/dct.c
@@ -0,0 +1,33 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavcodec/e2k/dctdsp.h"
+
+static const struct algo fdct_tab_arch[] = {
+    { "FDCT-E2K", ff_fdct_e2k, FF_IDCT_PERM_NONE, AV_CPU_FLAG_E2K_SIMD },
+    { 0 }
+};
+
+static const struct algo idct_tab_arch[] = {
+    { "FAST-E2K", ff_fast_idct_e2k, FF_IDCT_PERM_TRANSPOSE, AV_CPU_FLAG_E2K_SIMD },
+    { "SIMPLE-E2K", ff_simple_idct_e2k, FF_IDCT_PERM_LIBMPEG2, AV_CPU_FLAG_E2K_SIMD },
+    { "XVID-E2K", ff_xvid_idct_e2k, FF_IDCT_PERM_LIBMPEG2, AV_CPU_FLAG_E2K_SIMD, 1 },
+    { 0 }
+};
diff --git a/libavcodec/vc1dsp.c b/libavcodec/vc1dsp.c
index 62c8eb2..1070a3b 100644
--- a/libavcodec/vc1dsp.c
+++ b/libavcodec/vc1dsp.c
@@ -1039,6 +1039,8 @@ av_cold void ff_vc1dsp_init(VC1DSPContext *dsp)
     ff_vc1dsp_init_arm(dsp);
 #elif ARCH_PPC
     ff_vc1dsp_init_ppc(dsp);
+#elif ARCH_E2K
+    ff_vc1dsp_init_e2k(dsp);
 #elif ARCH_X86
     ff_vc1dsp_init_x86(dsp);
 #elif ARCH_MIPS
diff --git a/libavcodec/vc1dsp.h b/libavcodec/vc1dsp.h
index 7ed1776..35c5c73 100644
--- a/libavcodec/vc1dsp.h
+++ b/libavcodec/vc1dsp.h
@@ -89,6 +89,7 @@ void ff_vc1dsp_init(VC1DSPContext* c);
 void ff_vc1dsp_init_aarch64(VC1DSPContext* dsp);
 void ff_vc1dsp_init_arm(VC1DSPContext* dsp);
 void ff_vc1dsp_init_ppc(VC1DSPContext *c);
+void ff_vc1dsp_init_e2k(VC1DSPContext *c);
 void ff_vc1dsp_init_x86(VC1DSPContext* dsp);
 void ff_vc1dsp_init_mips(VC1DSPContext* dsp);
 void ff_vc1dsp_init_loongarch(VC1DSPContext* dsp);
diff --git a/libavcodec/videodsp.c b/libavcodec/videodsp.c
index bdff2e7..e9979af 100644
--- a/libavcodec/videodsp.c
+++ b/libavcodec/videodsp.c
@@ -51,6 +51,8 @@ av_cold void ff_videodsp_init(VideoDSPContext *ctx, int bpc)
     ff_videodsp_init_arm(ctx, bpc);
 #elif ARCH_PPC
     ff_videodsp_init_ppc(ctx, bpc);
+#elif ARCH_E2K
+    ff_videodsp_init_e2k(ctx, bpc);
 #elif ARCH_X86
     ff_videodsp_init_x86(ctx, bpc);
 #elif ARCH_MIPS
diff --git a/libavcodec/videodsp.h b/libavcodec/videodsp.h
index e8960b6..78617c4 100644
--- a/libavcodec/videodsp.h
+++ b/libavcodec/videodsp.h
@@ -81,6 +81,7 @@ void ff_videodsp_init(VideoDSPContext *ctx, int bpc);
 void ff_videodsp_init_aarch64(VideoDSPContext *ctx, int bpc);
 void ff_videodsp_init_arm(VideoDSPContext *ctx, int bpc);
 void ff_videodsp_init_ppc(VideoDSPContext *ctx, int bpc);
+void ff_videodsp_init_e2k(VideoDSPContext *ctx, int bpc);
 void ff_videodsp_init_x86(VideoDSPContext *ctx, int bpc);
 void ff_videodsp_init_mips(VideoDSPContext *ctx, int bpc);
 void ff_videodsp_init_loongarch(VideoDSPContext *ctx, int bpc);
diff --git a/libavcodec/vorbisdsp.c b/libavcodec/vorbisdsp.c
index 70022bd..9374566 100644
--- a/libavcodec/vorbisdsp.c
+++ b/libavcodec/vorbisdsp.c
@@ -53,6 +53,8 @@ av_cold void ff_vorbisdsp_init(VorbisDSPContext *dsp)
     ff_vorbisdsp_init_arm(dsp);
 #elif ARCH_PPC
     ff_vorbisdsp_init_ppc(dsp);
+#elif ARCH_E2K
+    ff_vorbisdsp_init_e2k(dsp);
 #elif ARCH_RISCV
     ff_vorbisdsp_init_riscv(dsp);
 #elif ARCH_X86
diff --git a/libavcodec/vorbisdsp.h b/libavcodec/vorbisdsp.h
index 5c369ec..a9a322a 100644
--- a/libavcodec/vorbisdsp.h
+++ b/libavcodec/vorbisdsp.h
@@ -34,6 +34,7 @@ void ff_vorbisdsp_init_aarch64(VorbisDSPContext *dsp);
 void ff_vorbisdsp_init_x86(VorbisDSPContext *dsp);
 void ff_vorbisdsp_init_arm(VorbisDSPContext *dsp);
 void ff_vorbisdsp_init_ppc(VorbisDSPContext *dsp);
+void ff_vorbisdsp_init_e2k(VorbisDSPContext *dsp);
 void ff_vorbisdsp_init_riscv(VorbisDSPContext *dsp);
 
 #endif /* AVCODEC_VORBISDSP_H */
diff --git a/libavcodec/vp3dsp.c b/libavcodec/vp3dsp.c
index b4621f0..e2e4036 100644
--- a/libavcodec/vp3dsp.c
+++ b/libavcodec/vp3dsp.c
@@ -459,6 +459,8 @@ av_cold void ff_vp3dsp_init(VP3DSPContext *c, int flags)
     ff_vp3dsp_init_arm(c, flags);
 #elif ARCH_PPC
     ff_vp3dsp_init_ppc(c, flags);
+#elif ARCH_E2K
+    ff_vp3dsp_init_e2k(c, flags);
 #elif ARCH_X86
     ff_vp3dsp_init_x86(c, flags);
 #elif ARCH_MIPS
diff --git a/libavcodec/vp3dsp.h b/libavcodec/vp3dsp.h
index 3b849ec..a01bfd4 100644
--- a/libavcodec/vp3dsp.h
+++ b/libavcodec/vp3dsp.h
@@ -56,6 +56,7 @@ void ff_vp3dsp_idct10_add(uint8_t *dest, ptrdiff_t stride, int16_t *block);
 void ff_vp3dsp_init(VP3DSPContext *c, int flags);
 void ff_vp3dsp_init_arm(VP3DSPContext *c, int flags);
 void ff_vp3dsp_init_ppc(VP3DSPContext *c, int flags);
+void ff_vp3dsp_init_e2k(VP3DSPContext *c, int flags);
 void ff_vp3dsp_init_x86(VP3DSPContext *c, int flags);
 void ff_vp3dsp_init_mips(VP3DSPContext *c, int flags);
 
diff --git a/libavcodec/vp8dsp.c b/libavcodec/vp8dsp.c
index 7a85e9f..99f73e7 100644
--- a/libavcodec/vp8dsp.c
+++ b/libavcodec/vp8dsp.c
@@ -681,6 +681,8 @@ av_cold void ff_vp78dsp_init(VP8DSPContext *dsp)
     ff_vp78dsp_init_arm(dsp);
 #elif ARCH_PPC
     ff_vp78dsp_init_ppc(dsp);
+#elif ARCH_E2K
+    ff_vp78dsp_init_e2k(dsp);
 #elif ARCH_X86
     ff_vp78dsp_init_x86(dsp);
 #endif
diff --git a/libavcodec/vp8dsp.h b/libavcodec/vp8dsp.h
index 16b5e9c..ae0b9f1 100644
--- a/libavcodec/vp8dsp.h
+++ b/libavcodec/vp8dsp.h
@@ -87,6 +87,7 @@ void ff_vp78dsp_init(VP8DSPContext *c);
 void ff_vp78dsp_init_aarch64(VP8DSPContext *c);
 void ff_vp78dsp_init_arm(VP8DSPContext *c);
 void ff_vp78dsp_init_ppc(VP8DSPContext *c);
+void ff_vp78dsp_init_e2k(VP8DSPContext *c);
 void ff_vp78dsp_init_x86(VP8DSPContext *c);
 
 void ff_vp8dsp_init(VP8DSPContext *c);
diff --git a/libavcodec/vp9dsp.c b/libavcodec/vp9dsp.c
index d8ddf74..01efb35 100644
--- a/libavcodec/vp9dsp.c
+++ b/libavcodec/vp9dsp.c
@@ -102,6 +102,8 @@ av_cold void ff_vp9dsp_init(VP9DSPContext *dsp, int bpp, int bitexact)
     ff_vp9dsp_init_arm(dsp, bpp);
 #elif ARCH_X86
     ff_vp9dsp_init_x86(dsp, bpp, bitexact);
+#elif ARCH_E2K
+    ff_vp9dsp_init_e2k(dsp, bpp, bitexact);
 #elif ARCH_MIPS
     ff_vp9dsp_init_mips(dsp, bpp);
 #elif ARCH_LOONGARCH
diff --git a/libavcodec/vp9dsp.h b/libavcodec/vp9dsp.h
index be0ac0b..281dabe 100644
--- a/libavcodec/vp9dsp.h
+++ b/libavcodec/vp9dsp.h
@@ -132,6 +132,7 @@ void ff_vp9dsp_init_12(VP9DSPContext *dsp);
 void ff_vp9dsp_init_aarch64(VP9DSPContext *dsp, int bpp);
 void ff_vp9dsp_init_arm(VP9DSPContext *dsp, int bpp);
 void ff_vp9dsp_init_x86(VP9DSPContext *dsp, int bpp, int bitexact);
+void ff_vp9dsp_init_e2k(VP9DSPContext *dsp, int bpp, int bitexact);
 void ff_vp9dsp_init_mips(VP9DSPContext *dsp, int bpp);
 void ff_vp9dsp_init_loongarch(VP9DSPContext *dsp, int bpp);
 
diff --git a/libavcodec/xvididct.c b/libavcodec/xvididct.c
index 01072d8..a60b44f 100644
--- a/libavcodec/xvididct.c
+++ b/libavcodec/xvididct.c
@@ -348,6 +348,8 @@ av_cold void ff_xvid_idct_init(IDCTDSPContext *c, AVCodecContext *avctx)
 
 #if ARCH_X86
     ff_xvid_idct_init_x86(c, avctx, high_bit_depth);
+#elif ARCH_E2K
+    ff_xvid_idct_init_e2k(c, avctx, high_bit_depth);
 #elif ARCH_MIPS
     ff_xvid_idct_init_mips(c, avctx, high_bit_depth);
 #endif
diff --git a/libavcodec/xvididct.h b/libavcodec/xvididct.h
index e0bc1a2..6e5ff91 100644
--- a/libavcodec/xvididct.h
+++ b/libavcodec/xvididct.h
@@ -30,6 +30,8 @@ void ff_xvid_idct_init(IDCTDSPContext *c, AVCodecContext *avctx);
 
 void ff_xvid_idct_init_x86(IDCTDSPContext *c, AVCodecContext *avctx,
                            unsigned high_bit_depth);
+void ff_xvid_idct_init_e2k(IDCTDSPContext *c, AVCodecContext *avctx,
+                           unsigned high_bit_depth);
 void ff_xvid_idct_init_mips(IDCTDSPContext *c, AVCodecContext *avctx,
                             unsigned high_bit_depth);
 
diff --git a/libavfilter/vf_drawtext.c b/libavfilter/vf_drawtext.c
index c5477cb..848de38 100644
--- a/libavfilter/vf_drawtext.c
+++ b/libavfilter/vf_drawtext.c
@@ -1391,6 +1391,17 @@ static int func_eval_expr_int_format(AVFilterContext *ctx, AVBPrint *bp,
         }
     }
 
+#ifdef __e2k__
+    {
+        union { double d; long long i; unsigned long long u; } d2i = { res };
+        ret = d2i.i >= 0x41eLL << 52 || d2i.u >= (0xc1eULL << 52 | 1 << 21);
+    }
+    intval = res;
+    if (ret) {
+        av_log(ctx, AV_LOG_ERROR, "Conversion of floating-point result to int failed. Result: %g. Conversion result: %d\n", res, intval);
+        return AVERROR(EINVAL);
+    }
+#else
     feclearexcept(FE_ALL_EXCEPT);
     intval = res;
 #if defined(FE_INVALID) && defined(FE_OVERFLOW) && defined(FE_UNDERFLOW)
@@ -1398,6 +1409,7 @@ static int func_eval_expr_int_format(AVFilterContext *ctx, AVBPrint *bp,
         av_log(ctx, AV_LOG_ERROR, "Conversion of floating-point result to int failed. Control register: 0x%08x. Conversion result: %d\n", ret, intval);
         return AVERROR(EINVAL);
     }
+#endif
 #endif
 
     if (argc == 3)
diff --git a/libavutil/cpu.c b/libavutil/cpu.c
index 1e0607d..fa400c1 100644
--- a/libavutil/cpu.c
+++ b/libavutil/cpu.c
@@ -62,6 +62,8 @@ static int get_cpu_flags(void)
     return ff_get_cpu_flags_arm();
 #elif ARCH_PPC
     return ff_get_cpu_flags_ppc();
+#elif ARCH_E2K
+    return ff_get_cpu_flags_e2k();
 #elif ARCH_RISCV
     return ff_get_cpu_flags_riscv();
 #elif ARCH_X86
@@ -116,6 +118,8 @@ int av_parse_cpu_caps(unsigned *flags, const char *s)
         { "flags"   , NULL, 0, AV_OPT_TYPE_FLAGS, { .i64 = 0 }, INT64_MIN, INT64_MAX, .unit = "flags" },
 #if   ARCH_PPC
         { "altivec" , NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_ALTIVEC  },    .unit = "flags" },
+#elif ARCH_E2K
+        { "e2k"     , NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_E2K_SIMD },    .unit = "flags" },
 #elif ARCH_X86
         { "mmx"     , NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_MMX      },    .unit = "flags" },
         { "mmx2"    , NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_MMX2     },    .unit = "flags" },
@@ -275,6 +279,8 @@ size_t av_cpu_max_align(void)
     return ff_get_cpu_max_align_arm();
 #elif ARCH_PPC
     return ff_get_cpu_max_align_ppc();
+#elif ARCH_E2K
+    return ff_get_cpu_max_align_e2k();
 #elif ARCH_X86
     return ff_get_cpu_max_align_x86();
 #elif ARCH_LOONGARCH
diff --git a/libavutil/cpu.h b/libavutil/cpu.h
index 8dff341..8577a68 100644
--- a/libavutil/cpu.h
+++ b/libavutil/cpu.h
@@ -61,6 +61,8 @@
 #define AV_CPU_FLAG_VSX          0x0002 ///< ISA 2.06
 #define AV_CPU_FLAG_POWER8       0x0004 ///< ISA 2.07
 
+#define AV_CPU_FLAG_E2K_SIMD     0x0001
+
 #define AV_CPU_FLAG_ARMV5TE      (1 << 0)
 #define AV_CPU_FLAG_ARMV6        (1 << 1)
 #define AV_CPU_FLAG_ARMV6T2      (1 << 2)
diff --git a/libavutil/cpu_internal.h b/libavutil/cpu_internal.h
index 634f28b..8f6c98a 100644
--- a/libavutil/cpu_internal.h
+++ b/libavutil/cpu_internal.h
@@ -48,6 +48,7 @@ int ff_get_cpu_flags_mips(void);
 int ff_get_cpu_flags_aarch64(void);
 int ff_get_cpu_flags_arm(void);
 int ff_get_cpu_flags_ppc(void);
+int ff_get_cpu_flags_e2k(void);
 int ff_get_cpu_flags_riscv(void);
 int ff_get_cpu_flags_x86(void);
 int ff_get_cpu_flags_loongarch(void);
@@ -56,6 +57,7 @@ size_t ff_get_cpu_max_align_mips(void);
 size_t ff_get_cpu_max_align_aarch64(void);
 size_t ff_get_cpu_max_align_arm(void);
 size_t ff_get_cpu_max_align_ppc(void);
+size_t ff_get_cpu_max_align_e2k(void);
 size_t ff_get_cpu_max_align_x86(void);
 size_t ff_get_cpu_max_align_loongarch(void);
 
diff --git a/libavutil/e2k/Makefile b/libavutil/e2k/Makefile
new file mode 100644
index 0000000..67892b4
--- /dev/null
+++ b/libavutil/e2k/Makefile
@@ -0,0 +1,2 @@
+OBJS += e2k/cpu.o \
+        e2k/float_dsp.o
diff --git a/libavutil/e2k/cpu.c b/libavutil/e2k/cpu.c
new file mode 100644
index 0000000..2b6fa60
--- /dev/null
+++ b/libavutil/e2k/cpu.c
@@ -0,0 +1,41 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+#include "libavutil/avassert.h"
+#include "libavutil/cpu.h"
+#include "libavutil/cpu_internal.h"
+
+int ff_get_cpu_flags_e2k(void)
+{
+#if HAVE_E2K_SIMD
+    return AV_CPU_FLAG_E2K_SIMD;
+#else
+    return 0;
+#endif
+}
+
+size_t ff_get_cpu_max_align_e2k(void)
+{
+    int flags = av_get_cpu_flags();
+
+    if (flags & AV_CPU_FLAG_E2K_SIMD)
+        return 16;
+
+    return 8;
+}
diff --git a/libavutil/e2k/cpu.h b/libavutil/e2k/cpu.h
new file mode 100644
index 0000000..216a30f
--- /dev/null
+++ b/libavutil/e2k/cpu.h
@@ -0,0 +1,27 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVUTIL_E2K_CPU_H
+#define AVUTIL_E2K_CPU_H
+
+#include "libavutil/cpu.h"
+#include "libavutil/cpu_internal.h"
+
+#define E2K_SIMD(flags) CPUEXT(flags, E2K_SIMD)
+
+#endif /* AVUTIL_E2K_CPU_H */
diff --git a/libavutil/e2k/float_dsp.c b/libavutil/e2k/float_dsp.c
new file mode 100644
index 0000000..1aed70a
--- /dev/null
+++ b/libavutil/e2k/float_dsp.c
@@ -0,0 +1,187 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2006 Luca Barbato <lu_zero@gentoo.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+#include "libavutil/attributes.h"
+#include "libavutil/float_dsp.h"
+#include "libavutil/e2k/cpu.h"
+#include "util_e2k.h"
+
+static void ff_vector_fmul_e2k(float *dst,
+                               const float *src0, const float *src1,
+                               int len)
+{
+    int i = 0;
+    __m128 d0, d1;
+
+    PRAGMA_E2K("ivdep")
+    for (; i < len; i += 8) {
+        d0 = _mm_load_ps(src0 + i);
+        d1 = _mm_load_ps(src0 + i + 4);
+        d0 = _mm_mul_ps(d0, _mm_load_ps(src1 + i));
+        d1 = _mm_mul_ps(d1, _mm_load_ps(src1 + i + 4));
+        _mm_store_ps(dst + i, d0);
+        _mm_store_ps(dst + i + 4, d1);
+    }
+}
+
+static void vector_fmac_scalar_e2k(float *dst, const float *src, float mul,
+                                   int len)
+{
+    int i = 0;
+    __m128 d0, d1, d2 = _mm_set1_ps(mul);
+
+    PRAGMA_E2K("ivdep")
+    for (; i < len; i += 8) {
+        d0 = _mm_load_ps(src + i);
+        d1 = _mm_load_ps(src + i + 4);
+        d0 = _mm_mul_ps(d0, d2);
+        d1 = _mm_mul_ps(d1, d2);
+        d0 = _mm_add_ps(d0, _mm_load_ps(dst + i));
+        d1 = _mm_add_ps(d1, _mm_load_ps(dst + i + 4));
+        _mm_store_ps(dst + i, d0);
+        _mm_store_ps(dst + i + 4, d1);
+    }
+}
+
+static void vector_fmul_scalar_e2k(float *dst, const float *src, float mul,
+                                   int len)
+{
+    int i = 0;
+    __m128 d0, d1, d2 = _mm_set1_ps(mul);
+
+    PRAGMA_E2K("ivdep")
+    for (; i < len - 4; i += 8) {
+        d0 = _mm_load_ps(src + i);
+        d1 = _mm_load_ps(src + i + 4);
+        d0 = _mm_mul_ps(d0, d2);
+        d1 = _mm_mul_ps(d1, d2);
+        _mm_store_ps(dst + i, d0);
+        _mm_store_ps(dst + i + 4, d1);
+    }
+    if (i < len) {
+        d0 = _mm_load_ps(src + i);
+        d0 = _mm_mul_ps(d0, d2);
+        _mm_store_ps(dst + i, d0);
+    }
+}
+
+static void ff_vector_fmul_window_e2k(float *dst, const float *src0,
+                                      const float *src1, const float *win,
+                                      int len)
+{
+    __m128 t0, t1, s0, s1, wi, wj;
+    int i, j;
+
+    dst  += len;
+    win  += len;
+    src0 += len;
+
+    PRAGMA_E2K("ivdep")
+    for (i = -len, j = len - 4; i < 0; i += 4, j -= 4) {
+        s0 = _mm_load_ps(src0 + i);
+        s1 = _mm_load_ps(src1 + j);
+        wi = _mm_load_ps(win + i);
+        wj = _mm_load_ps(win + j);
+
+        s1 = _mm_shuffle_ps(s1, s1, 0x1b);
+        wj = _mm_shuffle_ps(wj, wj, 0x1b);
+
+        t0 = _mm_mul_ps(s0, wj);
+        t1 = _mm_mul_ps(s0, wi);
+        t0 = _mm_sub_ps(t0, _mm_mul_ps(s1, wi));
+        t1 = _mm_add_ps(t1, _mm_mul_ps(s1, wj));
+        t1 = _mm_shuffle_ps(t1, t1, 0x1b);
+
+        _mm_store_ps(dst + i, t0);
+        _mm_store_ps(dst + j, t1);
+    }
+}
+
+static void ff_vector_fmul_add_e2k(float *dst, const float *src0,
+                                   const float *src1, const float *src2,
+                                   int len)
+{
+    int i;
+    __m128 d, s0, s1, s2;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < len; i += 4) {
+        s0 = _mm_load_ps(src0 + i);
+        s1 = _mm_load_ps(src1 + i);
+        s2 = _mm_load_ps(src2 + i);
+        d = _mm_add_ps(_mm_mul_ps(s0, s1), s2);
+        _mm_store_ps(dst + i, d);
+    }
+}
+
+static void ff_vector_fmul_reverse_e2k(float *dst, const float *src0,
+                                       const float *src1, int len)
+{
+    int i;
+    __m128 s0, s1, s2, s3;
+    src1 += len - 4;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < len; i += 8) {
+        s1 = _mm_load_ps(src1 - i);
+        s0 = _mm_load_ps(src0 + i);
+        s3 = _mm_load_ps(src1 - i - 4);
+        s2 = _mm_load_ps(src0 + i + 4);
+        s1 = _mm_shuffle_ps(s1, s1, 0x1b);
+        s3 = _mm_shuffle_ps(s3, s3, 0x1b);
+        s0 = _mm_mul_ps(s0, s1);
+        s2 = _mm_mul_ps(s2, s3);
+        _mm_store_ps(dst + i, s0);
+        _mm_store_ps(dst + i + 4, s2);
+    }
+}
+
+static void butterflies_float_e2k(float *av_restrict src0,
+                                  float *av_restrict src1, int len)
+{
+    int i;
+    __m128 s0, s1, s2;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < len; i += 4) {
+        s0 = _mm_load_ps(src0 + i);
+        s1 = _mm_load_ps(src1 + i);
+        s2 = _mm_sub_ps(s0, s1);
+        s0 = _mm_add_ps(s0, s1);
+        _mm_store_ps(src1 + i, s2);
+        _mm_store_ps(src0 + i, s0);
+    }
+}
+
+av_cold void ff_float_dsp_init_e2k(AVFloatDSPContext *fdsp, int bit_exact)
+{
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    fdsp->vector_fmul = ff_vector_fmul_e2k; 
+    fdsp->vector_fmac_scalar = vector_fmac_scalar_e2k;
+    fdsp->vector_fmul_scalar = vector_fmul_scalar_e2k;
+    fdsp->vector_fmul_window = ff_vector_fmul_window_e2k;
+    fdsp->vector_fmul_add = ff_vector_fmul_add_e2k;
+    fdsp->vector_fmul_reverse = ff_vector_fmul_reverse_e2k;
+    fdsp->butterflies_float = butterflies_float_e2k;
+}
diff --git a/libavutil/e2k/intreadwrite.h b/libavutil/e2k/intreadwrite.h
new file mode 100644
index 0000000..0387475
--- /dev/null
+++ b/libavutil/e2k/intreadwrite.h
@@ -0,0 +1,54 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVUTIL_E2K_INTREADWRITE_H
+#define AVUTIL_E2K_INTREADWRITE_H
+
+#include <stdint.h>
+#include "config.h"
+
+#include <x86intrin.h>
+
+#define AV_RB32 av_read_bswap32
+#define AV_WB32 av_write_bswap32
+#define AV_RB64 av_read_bswap64
+#define AV_WB64 av_write_bswap64
+
+static av_always_inline uint32_t av_read_bswap32(const void *p)
+{
+    return _bswap(*(const uint32_t*)p);
+}
+
+static av_always_inline void av_write_bswap32(void *p, uint32_t v)
+{
+    *(uint32_t*)p = _bswap(v);
+}
+
+static av_always_inline uint64_t av_read_bswap64(const void *p)
+{
+    return _bswap64(*(const uint64_t*)p);
+}
+
+static av_always_inline void av_write_bswap64(void *p, uint64_t v)
+{
+    *(uint64_t*)p = _bswap64(v);
+}
+
+#endif /* AVUTIL_E2K_INTREADWRITE_H */
diff --git a/libavutil/e2k/timer.h b/libavutil/e2k/timer.h
new file mode 100644
index 0000000..ea78175
--- /dev/null
+++ b/libavutil/e2k/timer.h
@@ -0,0 +1,35 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVUTIL_E2K_TIMER_H
+#define AVUTIL_E2K_TIMER_H
+
+#include <stdint.h>
+#include <x86intrin.h>
+
+#define AV_READ_TIME read_time
+
+static inline uint64_t read_time(void)
+{
+    unsigned aux;
+    return __rdtscp(&aux);
+}
+
+#endif /* AVUTIL_E2K_TIMER_H */
diff --git a/libavutil/e2k/util_e2k.h b/libavutil/e2k/util_e2k.h
new file mode 100644
index 0000000..f5cea7c
--- /dev/null
+++ b/libavutil/e2k/util_e2k.h
@@ -0,0 +1,146 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * Contains misc utility macros and inline functions
+ */
+
+#ifndef AVUTIL_E2K_UTIL_E2K_H
+#define AVUTIL_E2K_UTIL_E2K_H
+
+#include <stdint.h>
+#include "config.h"
+#include <smmintrin.h> /* SSE4.1 */
+
+#define ALWAYS_INLINE __attribute__((__always_inline__)) inline
+#define ALIGNED(n) __attribute__((aligned(n))) 
+
+#ifdef __e2k__
+#define PRAGMA_E2K _Pragma
+#define _mm_shuffle2_pi8(a, b, c) \
+    ((__m64)__builtin_e2k_pshufb((uint64_t)(b), (uint64_t)(a), (uint64_t)(c)))
+#define _mm_shuffle2_epi8(a, b, c) \
+    ((__m128i)__builtin_e2k_qppermb((__v2di)(b), (__v2di)(a), (__v2di)(c)))
+#define _mm_blendv_pi8(a, b, c) \
+    ((__m64)__builtin_e2k_pmerge((uint64_t)(a), (uint64_t)(b), (uint64_t)(c)))
+#else
+#define PRAGMA_E2K(x)
+#define _mm_shuffle2_pi8(a, b, c) \
+    _mm_movepi64_pi64(_mm_shuffle_epi8(_mm_unpacklo_epi64( \
+        _mm_movpi64_epi64(a), _mm_movpi64_epi64(b)), _mm_movpi64_epi64(c)))
+#define _mm_shuffle2_epi8(a, b, c) \
+    _mm_blendv_epi8(_mm_shuffle_epi8(a, c), _mm_shuffle_epi8(b, c), \
+        _mm_slli_epi16(c, 3))
+#define _mm_blendv_pi8(a, b, c) \
+    _mm_movepi64_pi64(_mm_blendv_epi8(_mm_movpi64_epi64(a), \
+        _mm_movpi64_epi64(b), _mm_movpi64_epi64(c)))
+
+static ALWAYS_INLINE uint64_t __builtin_e2k_insfd(uint64_t a, uint64_t b, uint64_t c) {
+  int n = b & 63;
+  a = a >> n | a << (64 - n);
+  return c ^ ((a ^ c) & (~0ll << (b >> 6 & 63)));
+}
+#endif
+
+#define _mm_extract_pi32(a, b) _mm_extract_epi32(_mm_movpi64_epi64(a), b)
+#define VEC_ALIGNR8(a, b) _mm_castpd_si128(_mm_shuffle_pd(_mm_castsi128_pd(b), _mm_castsi128_pd(a), 1))
+
+#define _mm_unpacklo_ps2(a, b) _mm_castpd_ps(_mm_unpacklo_pd(_mm_castps_pd(a), _mm_castps_pd(b)));
+#define _mm_unpackhi_ps2(a, b) _mm_castpd_ps(_mm_unpackhi_pd(_mm_castps_pd(a), _mm_castps_pd(b)));
+#define _mm_alignr_ps(a, b, n) _mm_castsi128_ps(_mm_alignr_epi8(_mm_castps_si128(a), _mm_castps_si128(b), (n) * 4))
+#define _mm_bsrli_ps(a, n) _mm_castsi128_ps(_mm_bsrli_si128(_mm_castps_si128(a), (n) * 4))
+
+/***********************************************************************
+ * Vector types
+ **********************************************************************/
+#define vec_u8  __m128i
+#define vec_s8  __m128i
+#define vec_u16 __m128i
+#define vec_s16 __m128i
+#define vec_u32 __m128i
+#define vec_s32 __m128i
+#define vec_f   __m128
+
+/***********************************************************************
+ * Null vector
+ **********************************************************************/
+#define LOAD_ZERO const __m128i zerov = _mm_setzero_si128()
+
+// Transpose 8x8 matrix of 16-bit elements (in-place)
+#define TRANSPOSE8(a0, a1, a2, a3, a4, a5, a6, a7) \
+do { \
+    vec_s16 _b0, _b1, _b2, _b3, _b4, _b5, _b6, _b7; \
+    vec_s16 _c0, _c1, _c2, _c3, _c4, _c5, _c6, _c7; \
+    _b0 = _mm_unpacklo_epi16(a0, a2); \
+    _b1 = _mm_unpackhi_epi16(a0, a2); \
+    _b2 = _mm_unpacklo_epi16(a1, a3); \
+    _b3 = _mm_unpackhi_epi16(a1, a3); \
+    _b4 = _mm_unpacklo_epi16(a4, a6); \
+    _b5 = _mm_unpackhi_epi16(a4, a6); \
+    _b6 = _mm_unpacklo_epi16(a5, a7); \
+    _b7 = _mm_unpackhi_epi16(a5, a7); \
+    \
+    _c0 = _mm_unpacklo_epi16(_b0, _b2); \
+    _c1 = _mm_unpackhi_epi16(_b0, _b2); \
+    _c2 = _mm_unpacklo_epi16(_b1, _b3); \
+    _c3 = _mm_unpackhi_epi16(_b1, _b3); \
+    _c4 = _mm_unpacklo_epi16(_b4, _b6); \
+    _c5 = _mm_unpackhi_epi16(_b4, _b6); \
+    _c6 = _mm_unpacklo_epi16(_b5, _b7); \
+    _c7 = _mm_unpackhi_epi16(_b5, _b7); \
+    \
+    a0 = _mm_unpacklo_epi64(_c0, _c4); \
+    a1 = _mm_unpackhi_epi64(_c0, _c4); \
+    a2 = _mm_unpacklo_epi64(_c1, _c5); \
+    a3 = _mm_unpackhi_epi64(_c1, _c5); \
+    a4 = _mm_unpacklo_epi64(_c2, _c6); \
+    a5 = _mm_unpackhi_epi64(_c2, _c6); \
+    a6 = _mm_unpacklo_epi64(_c3, _c7); \
+    a7 = _mm_unpackhi_epi64(_c3, _c7); \
+} while (0)
+
+#define VEC_LD(a)     _mm_loadu_si128((const __m128i*)(a))
+#define VEC_ST(a, b)  _mm_storeu_si128((__m128i*)(a), b)
+#define VEC_LD8(a)    _mm_loadl_epi64((const __m128i*)(a))
+#define VEC_STL(a, b) _mm_storel_epi64((__m128i*)(a), b)
+#define VEC_STH(a, b) _mm_storeh_pd((double*)(a), _mm_castsi128_pd(b));
+
+#define VEC_SPLAT16(v, i) _mm_shuffle_epi8(v, _mm_set1_epi16((i) * 2 | ((i) * 2 + 1) << 8))
+
+#if !defined(__iset__) || __iset__ < 5
+#define NEED_ALIGN8
+#define ALIGN8_COMMON uint64_t src_shr; __m64 src_tmp0, src_tmp1;
+#define ALIGN8_VARS(src) __m64 *src##_ptr, src##_next, src##_index;
+#define ALIGN8_START(ptr, src) \
+  src_shr = (intptr_t)(ptr - 1) & 7; \
+  src##_ptr = (__m64*)((intptr_t)(ptr - 1) & -8); \
+  src##_next = src##_ptr[src_shr == 7]; \
+  src##_index = _mm_add_pi8(_mm_set1_pi8(src_shr), \
+                            _mm_setr_pi8(1, 2, 3, 4, 5, 6, 7, 8));
+#define ALIGN8_READ16(v0, src, i) \
+  src_tmp1 = src##_ptr[i * 2 + 1]; \
+  src_tmp0 = _mm_shuffle2_pi8(src##_next, src_tmp1, src##_index); \
+  src##_next = src##_ptr[i * 2 + 2]; \
+  src_tmp1 = _mm_shuffle2_pi8(src_tmp1, src##_next, src##_index); \
+  v0 = _mm_setr_epi64(src_tmp0, src_tmp1);
+#endif
+
+#endif /* AVUTIL_E2K_UTIL_E2K_H */
diff --git a/libavutil/float_dsp.c b/libavutil/float_dsp.c
index 742dd67..45e7a5f 100644
--- a/libavutil/float_dsp.c
+++ b/libavutil/float_dsp.c
@@ -156,6 +156,8 @@ av_cold AVFloatDSPContext *avpriv_float_dsp_alloc(int bit_exact)
     ff_float_dsp_init_arm(fdsp);
 #elif ARCH_PPC
     ff_float_dsp_init_ppc(fdsp, bit_exact);
+#elif ARCH_E2K
+    ff_float_dsp_init_e2k(fdsp, bit_exact);
 #elif ARCH_RISCV
     ff_float_dsp_init_riscv(fdsp);
 #elif ARCH_X86
diff --git a/libavutil/float_dsp.h b/libavutil/float_dsp.h
index 7cad9fc..b3ccde0 100644
--- a/libavutil/float_dsp.h
+++ b/libavutil/float_dsp.h
@@ -205,6 +205,7 @@ float avpriv_scalarproduct_float_c(const float *v1, const float *v2, int len);
 void ff_float_dsp_init_aarch64(AVFloatDSPContext *fdsp);
 void ff_float_dsp_init_arm(AVFloatDSPContext *fdsp);
 void ff_float_dsp_init_ppc(AVFloatDSPContext *fdsp, int strict);
+void ff_float_dsp_init_e2k(AVFloatDSPContext *fdsp, int strict);
 void ff_float_dsp_init_riscv(AVFloatDSPContext *fdsp);
 void ff_float_dsp_init_x86(AVFloatDSPContext *fdsp);
 void ff_float_dsp_init_mips(AVFloatDSPContext *fdsp);
diff --git a/libavutil/intreadwrite.h b/libavutil/intreadwrite.h
index 21df788..81c95a1 100644
--- a/libavutil/intreadwrite.h
+++ b/libavutil/intreadwrite.h
@@ -72,6 +72,8 @@ typedef union {
 #   include "mips/intreadwrite.h"
 #elif ARCH_PPC
 #   include "ppc/intreadwrite.h"
+#elif ARCH_E2K
+#   include "e2k/intreadwrite.h"
 #elif ARCH_X86
 #   include "x86/intreadwrite.h"
 #endif
diff --git a/libavutil/tests/cpu.c b/libavutil/tests/cpu.c
index 200f203..a2bf8c1 100644
--- a/libavutil/tests/cpu.c
+++ b/libavutil/tests/cpu.c
@@ -51,6 +51,8 @@ static const struct {
     { AV_CPU_FLAG_SETEND,    "setend"     },
 #elif ARCH_PPC
     { AV_CPU_FLAG_ALTIVEC,   "altivec"    },
+#elif ARCH_E2K
+    { AV_CPU_FLAG_E2K_SIMD,  "e2k"        },
 #elif ARCH_MIPS
     { AV_CPU_FLAG_MMI,       "mmi"        },
     { AV_CPU_FLAG_MSA,       "msa"        },
diff --git a/libavutil/timer.h b/libavutil/timer.h
index 2cd299e..463f272 100644
--- a/libavutil/timer.h
+++ b/libavutil/timer.h
@@ -57,6 +57,8 @@
 #   include "arm/timer.h"
 #elif ARCH_PPC
 #   include "ppc/timer.h"
+#elif ARCH_E2K
+#   include "e2k/timer.h"
 #elif ARCH_RISCV
 #   include "riscv/timer.h"
 #elif ARCH_X86
diff --git a/libswresample/audioconvert.c b/libswresample/audioconvert.c
index 1d75ba1..a24472d 100644
--- a/libswresample/audioconvert.c
+++ b/libswresample/audioconvert.c
@@ -182,6 +182,8 @@ AudioConvert *swri_audio_convert_alloc(enum AVSampleFormat out_fmt,
     swri_audio_convert_init_arm(ctx, out_fmt, in_fmt, channels);
 #elif ARCH_AARCH64
     swri_audio_convert_init_aarch64(ctx, out_fmt, in_fmt, channels);
+#elif ARCH_E2K
+    swri_audio_convert_init_e2k(ctx, out_fmt, in_fmt, channels);
 #endif
 
     return ctx;
diff --git a/libswresample/e2k/Makefile b/libswresample/e2k/Makefile
new file mode 100644
index 0000000..a90ab9e
--- /dev/null
+++ b/libswresample/e2k/Makefile
@@ -0,0 +1 @@
+OBJS += e2k/audio_convert.o
diff --git a/libswresample/e2k/audio_convert.c b/libswresample/e2k/audio_convert.c
new file mode 100644
index 0000000..6b3dab0
--- /dev/null
+++ b/libswresample/e2k/audio_convert.c
@@ -0,0 +1,109 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libswresample/swresample_internal.h"
+#include "libswresample/audioconvert.h"
+
+// length is aligned by 16 in "audioconvert.c"
+
+static void conv_flt_to_s16_e2k(uint8_t **_dst, const uint8_t **_src, int len) {
+    const float *src = (const float*)_src[0];
+    int16_t *dst = (int16_t*)_dst[0];
+    int i = 0;
+    __m128 f0, f1, c1 = _mm_set1_ps(1 << 15);
+    __m128i v0, v1;
+
+    PRAGMA_E2K("ivdep")
+    for (; i + 7 < len; i += 8) {
+        f0 = _mm_loadu_ps(src);
+        f1 = _mm_loadu_ps(src + 4);
+        v0 = _mm_cvtps_epi32(_mm_mul_ps(f0, c1));
+        v1 = _mm_cvtps_epi32(_mm_mul_ps(f1, c1));
+        v0 = _mm_packs_epi32(v0, v1);
+        VEC_ST(dst, v0);
+        src += 8; dst += 8;
+    }
+/*
+    PRAGMA_E2K("ivdep")
+    for (; i < len; i++)
+        *dst++ = av_clip_int16(lrintf(*src++ * (1 << 15)));
+*/
+}
+
+static void conv_fltp_to_s16_2ch_e2k(uint8_t **_dst, const uint8_t **_src, int len) {
+    const float *src0 = (const float*)_src[0];
+    const float *src1 = (const float*)_src[1];
+    int16_t *dst = (int16_t*)_dst[0];
+    int i = 0;
+    __m128 f0, f1, c1 = _mm_set1_ps(1 << 15);
+    __m128i v0, v1, v2, v3;
+
+    PRAGMA_E2K("ivdep")
+    for (; i + 7 < len; i += 8) {
+        f0 = _mm_loadu_ps(src0);
+        f1 = _mm_loadu_ps(src0 + 4);
+        v0 = _mm_cvtps_epi32(_mm_mul_ps(f0, c1));
+        v1 = _mm_cvtps_epi32(_mm_mul_ps(f1, c1));
+        v2 = _mm_packs_epi32(v0, v1);
+        f0 = _mm_loadu_ps(src1);
+        f1 = _mm_loadu_ps(src1 + 4);
+        v0 = _mm_cvtps_epi32(_mm_mul_ps(f0, c1));
+        v1 = _mm_cvtps_epi32(_mm_mul_ps(f1, c1));
+        v3 = _mm_packs_epi32(v0, v1);
+        v0 = _mm_unpacklo_epi16(v2, v3);
+        v1 = _mm_unpackhi_epi16(v2, v3);
+        VEC_ST(dst, v0);
+        VEC_ST(dst + 8, v1);
+        src0 += 8; src1 += 8; dst += 16;
+    }
+/*
+    PRAGMA_E2K("ivdep")
+    for (; i < len; i++) {
+        dst[0] = av_clip_int16(lrintf(*src0++ * (1 << 15)));
+        dst[1] = av_clip_int16(lrintf(*src1++ * (1 << 15)));
+        dst += 2;
+    }
+*/
+}
+
+av_cold void swri_audio_convert_init_e2k(struct AudioConvert *ac,
+                                         enum AVSampleFormat out_fmt,
+                                         enum AVSampleFormat in_fmt,
+                                         int channels){
+
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    ac->simd_f = NULL;
+
+    if (out_fmt == AV_SAMPLE_FMT_S16 && in_fmt == AV_SAMPLE_FMT_FLT ||
+        out_fmt == AV_SAMPLE_FMT_S16P && in_fmt == AV_SAMPLE_FMT_FLTP)
+        ac->simd_f = conv_flt_to_s16_e2k;
+
+    if (out_fmt == AV_SAMPLE_FMT_S16 && in_fmt == AV_SAMPLE_FMT_FLTP && channels == 2)
+        ac->simd_f = conv_fltp_to_s16_2ch_e2k;
+
+    // if(ac->simd_f) ac->in_simd_align_mask = 7;
+}
+
diff --git a/libswresample/swresample_internal.h b/libswresample/swresample_internal.h
index ad902d7..ac25b62 100644
--- a/libswresample/swresample_internal.h
+++ b/libswresample/swresample_internal.h
@@ -223,5 +223,9 @@ void swri_audio_convert_init_x86(struct AudioConvert *ac,
                                  enum AVSampleFormat out_fmt,
                                  enum AVSampleFormat in_fmt,
                                  int channels);
+void swri_audio_convert_init_e2k(struct AudioConvert *ac,
+                                 enum AVSampleFormat out_fmt,
+                                 enum AVSampleFormat in_fmt,
+                                 int channels);
 
 #endif
diff --git a/libswscale/e2k/Makefile b/libswscale/e2k/Makefile
new file mode 100644
index 0000000..f35371d
--- /dev/null
+++ b/libswscale/e2k/Makefile
@@ -0,0 +1,3 @@
+OBJS += e2k/swscale.o \
+        e2k/yuv2rgb.o \
+        e2k/yuv2yuv.o
diff --git a/libswscale/e2k/swscale.c b/libswscale/e2k/swscale.c
new file mode 100644
index 0000000..348e60b
--- /dev/null
+++ b/libswscale/e2k/swscale.c
@@ -0,0 +1,2075 @@
+/*
+ * Elbrus-enhanced yuv2yuvX
+ *
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (C) 2004 Romain Dolbeau <romain@dolbeau.org>
+ * based on the equivalent C code in swscale.c
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include <inttypes.h>
+
+#include "config.h"
+#include "libswscale/swscale.h"
+#include "libswscale/swscale_internal.h"
+#include "libavutil/attributes.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "yuv2rgb.h"
+
+static void yuv2planeX_8_e2k(const int16_t *filter, int filterSize,
+                           const int16_t **src, uint8_t *dest, int dstW,
+                           const uint8_t *dither, int offset)
+{
+    int i = 0, j;
+    __m64 h0;
+    __m128i d0, d1, zerov = _mm_setzero_si128();
+    h0 = (__m64)__builtin_e2k_insfd(*(uint64_t*)dither, ((offset + i) & 7) * 8, 0);
+    d1 = _mm_unpacklo_epi8(_mm_movpi64_epi64(h0), zerov);
+    d0 = _mm_unpacklo_epi16(d1, zerov);
+    d1 = _mm_unpackhi_epi16(d1, zerov);
+    d0 = _mm_slli_epi32(d0, 12);
+    d1 = _mm_slli_epi32(d1, 12);
+
+    for (; i < dstW - 15; i += 16) {
+        __m128i r0, r1, r2, r3, v0, v1, v2, v3;
+
+        r2 = r0 = d0;
+        r3 = r1 = d1;
+        for (j = 0; j < filterSize; j++) {
+            v1 = _mm_set1_epi16(filter[j]);
+
+            v0 = VEC_LD(src[j] + i);
+            v2 = _mm_mullo_epi16(v0, v1);
+            v3 = _mm_mulhi_epi16(v0, v1);
+            v0 = _mm_unpacklo_epi16(v2, v3);
+            v3 = _mm_unpackhi_epi16(v2, v3);
+            r0 = _mm_add_epi32(r0, v0);
+            r1 = _mm_add_epi32(r1, v3);
+
+            v0 = VEC_LD(src[j] + i + 8);
+            v2 = _mm_mullo_epi16(v0, v1);
+            v3 = _mm_mulhi_epi16(v0, v1);
+            v0 = _mm_unpacklo_epi16(v2, v3);
+            v3 = _mm_unpackhi_epi16(v2, v3);
+            r2 = _mm_add_epi32(r2, v0);
+            r3 = _mm_add_epi32(r3, v3);
+        }
+        r0 = _mm_srai_epi32(r0, 19);
+        r1 = _mm_srai_epi32(r1, 19);
+        r2 = _mm_srai_epi32(r2, 19);
+        r3 = _mm_srai_epi32(r3, 19);
+        r0 = _mm_packs_epi32(r0, r1);
+        r2 = _mm_packs_epi32(r2, r3);
+        r0 = _mm_packus_epi16(r0, r2);
+        VEC_ST(dest + i, r0);
+    }
+
+    for (; i < dstW; i++) {
+        int val = dither[(i + offset) & 7] << 12;
+        for (j = 0; j < filterSize; j++)
+            val += src[j][i] * filter[j];
+        dest[i] = av_clip_uint8(val >> 19);
+    }
+}
+
+static void hScale_real_e2k(SwsContext *c, int16_t *dst, int dstW,
+                            const uint8_t *src, const int16_t *filter,
+                            const int32_t *filterPos, int filterSize)
+{
+    int i;
+    LOAD_ZERO;
+    switch (filterSize) {
+
+    case 1:
+        PRAGMA_E2K("ivdep")
+        for (i = 0; i < dstW; i++) {
+            int val, srcPos = filterPos[i];
+            val = (int)src[srcPos] * filter[filterSize * i];
+            dst[i] = FFMIN(val >> 7, (1 << 15) - 1);
+        }
+    break;
+
+    case 2:
+        PRAGMA_E2K("ivdep")
+        for (i = 0; i < dstW; i++) {
+            int val, srcPos = filterPos[i];
+            val = (int)src[srcPos] * filter[filterSize * i];
+            val += (int)src[srcPos + 1] * filter[filterSize * i + 1];
+            dst[i] = FFMIN(val >> 7, (1 << 15) - 1);
+        }
+    break;
+
+    case 4: {
+        __m64 zerov = _mm_setzero_si64();
+        PRAGMA_E2K("ivdep")
+        for (i = 0; i < dstW; i++, filter += filterSize) {
+            int val;
+            __m64 v0, v2, accv;
+            const uint8_t *srci = src + filterPos[i];
+
+            v0 = _mm_cvtsi32_si64(*(uint32_t*)srci);
+            v0 = _mm_unpacklo_pi8(v0, zerov);
+            v2 = *(__m64*)filter;
+            accv = _mm_madd_pi16(v0, v2);
+            val = _mm_extract_pi32(accv, 0) + _mm_extract_pi32(accv, 1);
+            dst[i] = FFMIN(val >> 7, (1 << 15) - 1);
+        }
+    }
+    break;
+
+    case 8:
+        PRAGMA_E2K("ivdep")
+        for (i = 0; i < dstW; i++, filter += filterSize) {
+            int val;
+            __m128i v0, v2, accv;
+            const uint8_t *srci = src + filterPos[i];
+
+            v0 = VEC_LD8(srci);
+            v0 = _mm_unpacklo_epi8(v0, zerov);
+            v2 = VEC_LD(filter);
+            accv = _mm_madd_epi16(v0, v2);
+            accv = _mm_hadd_epi32(accv, accv);
+            val = _mm_extract_epi32(accv, 0) + _mm_extract_epi32(accv, 1);
+            dst[i] = FFMIN(val >> 7, (1 << 15) - 1);
+        }
+    break;
+
+    case 12:
+        PRAGMA_E2K("ivdep")
+        for (i = 0; i < dstW; i++, filter += filterSize) {
+            int val;
+            __m128i v0, v2, accv;
+            const uint8_t *srci = src + filterPos[i];
+
+            v0 = VEC_LD8(srci);
+            v0 = _mm_unpacklo_epi8(v0, zerov);
+            v2 = VEC_LD(filter);
+            accv = _mm_madd_epi16(v0, v2);
+            {
+                __m64 v1 = _mm_cvtsi32_si64(*(uint32_t*)&srci[8]);
+                __m64 v0 = _mm_unpacklo_pi8(v1, _mm_setzero_si64());
+                __m64 v2 = *(__m64*)&filter[8];
+                accv = _mm_add_epi32(accv, _mm_movpi64_epi64(_mm_madd_pi16(v0, v2)));
+            }
+            accv = _mm_hadd_epi32(accv, accv);
+            val = _mm_extract_epi32(accv, 0) + _mm_extract_epi32(accv, 1);
+            dst[i] = FFMIN(val >> 7, (1 << 15) - 1);
+        }
+    break;
+
+    case 16:
+        PRAGMA_E2K("ivdep")
+        for (i = 0; i < dstW; i++, filter += filterSize) {
+            int val, j = 0;
+            __m128i v0, v1, v2, v3, accv;
+            const uint8_t *srci = src + filterPos[i];
+
+            v1 = VEC_LD(srci + j);
+            v0 = _mm_unpacklo_epi8(v1, zerov);
+            v1 = _mm_unpackhi_epi8(v1, zerov);
+            v2 = VEC_LD(filter + j);
+            v3 = VEC_LD(filter + j + 8);
+            accv = _mm_madd_epi16(v0, v2);
+            accv = _mm_add_epi32(accv, _mm_madd_epi16(v1, v3));
+            accv = _mm_hadd_epi32(accv, accv);
+            val = _mm_extract_epi32(accv, 0) + _mm_extract_epi32(accv, 1);
+            dst[i] = FFMIN(val >> 7, (1 << 15) - 1);
+        }
+    break;
+
+    default:
+        av_assert0((filterSize & 7) == 0);
+
+        for (i = 0; i < dstW; i++, filter += filterSize) {
+            int val, j = 0;
+            __m128i v0, v1, v2, v3, accv = zerov;
+            const uint8_t *srci = src + filterPos[i];
+
+            for (; j < filterSize - 15; j += 16) {
+                v1 = VEC_LD(srci + j);
+                v0 = _mm_unpacklo_epi8(v1, zerov);
+                v1 = _mm_unpackhi_epi8(v1, zerov);
+                v2 = VEC_LD(filter + j);
+                v3 = VEC_LD(filter + j + 8);
+                accv = _mm_add_epi32(accv, _mm_madd_epi16(v0, v2));
+                accv = _mm_add_epi32(accv, _mm_madd_epi16(v1, v3));
+            }
+            if (filterSize & 8) {
+                v1 = VEC_LD8(srci + j);
+                v0 = _mm_unpacklo_epi8(v1, zerov);
+                v2 = VEC_LD(filter + j);
+                accv = _mm_add_epi32(accv, _mm_madd_epi16(v0, v2));
+#if 0
+                j += 8;
+            }
+            if (filterSize & 4) {
+                __m64 v1 = _mm_cvtsi32_si64(*(uint32_t*)&srci[j]);
+                __m64 v0 = _mm_unpacklo_pi8(v1, _mm_setzero_si64());
+                __m64 v2 = *(__m64*)&filter[j];
+                accv = _mm_add_epi32(accv, _mm_movpi64_epi64(_mm_madd_pi16(v0, v2)));
+#endif
+            }
+            accv = _mm_hadd_epi32(accv, accv);
+            val = _mm_extract_epi32(accv, 0) + _mm_extract_epi32(accv, 1);
+            dst[i] = FFMIN(val >> 7, (1 << 15) - 1);
+        }
+    }
+}
+
+static void yuv2plane1_floatLE_e2k(const int16_t *_src, uint8_t *_dest,
+                                   int dstW, const uint8_t *dither, int offset)
+{
+    const int32_t *src = (const int32_t*)_src;
+    float *dest = (float*)_dest;
+    int shift = 3;
+    int add = (1 << shift) >> 1;
+    int clip = (1 << 16) - 1;
+    float fmult = 1.0f / 65535.0f;
+    LOAD_ZERO;
+    vec_u32 vadd = _mm_set1_epi32(add);
+    vec_u32 vlargest = _mm_set1_epi32(clip);
+    vec_f vmul = _mm_set1_ps(fmult);
+    vec_u32 v0;
+    vec_f v1;
+    int i = 0;
+
+    PRAGMA_E2K("ivdep")
+    for (; i < dstW - 3; i += 4) {
+        v0 = VEC_LD(src + i);
+        v0 = _mm_add_epi32(v0, vadd);
+        v0 = _mm_srai_epi32(v0, shift);
+        v0 = _mm_max_epi32(v0, zerov);
+        v0 = _mm_min_epi32(v0, vlargest);
+        v1 = _mm_mul_ps(_mm_cvtepi32_ps(v0), vmul);
+        _mm_storeu_ps(dest + i, v1);
+    }
+
+    PRAGMA_E2K("ivdep")
+    for (; i < dstW; ++i){
+        int val = src[i] + add;
+        val = av_clip_uint16(val >> shift);
+        dest[i] = fmult * (float)val;
+    }
+}
+
+static void yuv2plane1_floatBE_e2k(const int16_t *_src, uint8_t *_dest,
+                                   int dstW, const uint8_t *dither, int offset)
+{
+    const int32_t *src = (const int32_t*)_src;
+    uint32_t *dest = (uint32_t*)_dest;
+    int shift = 3;
+    int add = (1 << shift) >> 1;
+    int clip = (1 << 16) - 1;
+    float fmult = 1.0f / 65535.0f;
+    LOAD_ZERO;
+    vec_u32 vadd = _mm_set1_epi32(add);
+    vec_u32 vlargest = _mm_set1_epi32(clip);
+    vec_f vmul = _mm_set1_ps(fmult);
+    vec_u8 vswap = _mm_setr_epi8(3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12);
+    vec_u32 v0;
+    vec_f v1;
+    int i = 0;
+
+    PRAGMA_E2K("ivdep")
+    for (; i < dstW - 3; i += 4) {
+        v0 = VEC_LD(src + i);
+        v0 = _mm_add_epi32(v0, vadd);
+        v0 = _mm_srai_epi32(v0, shift);
+        v0 = _mm_max_epi32(v0, zerov);
+        v0 = _mm_min_epi32(v0, vlargest);
+        v1 = _mm_mul_ps(_mm_cvtepi32_ps(v0), vmul);
+        v0 = _mm_shuffle_epi8(_mm_castps_si128(v1), vswap);
+        VEC_ST(dest + i, v0);
+    }
+
+    PRAGMA_E2K("ivdep")
+    for (; i < dstW; i++) {
+        int val = src[i] + add;
+        val = av_clip_uint16(val >> shift);
+        dest[i] = av_bswap32(av_float2int(fmult * (float)val));
+    }
+}
+
+static void yuv2plane1_8_e2k(const int16_t *src, uint8_t *dest, int dstW,
+                             const uint8_t *dither, int offset)
+{
+    int i = 0;
+    __m128i v0, v1, ditherv;
+    LOAD_ZERO;
+    __m64 h0;
+    h0 = (__m64)__builtin_e2k_insfd(*(uint64_t*)dither, ((offset + i) & 7) * 8, 0);
+    ditherv = _mm_unpacklo_epi8(_mm_movpi64_epi64(h0), zerov);
+
+    PRAGMA_E2K("ivdep")
+    for (; i < dstW - 15; i += 16) {
+        v0 = VEC_LD(src + i);
+        v1 = VEC_LD(src + i + 8);
+        v0 = _mm_adds_epi16(v0, ditherv);
+        v1 = _mm_adds_epi16(v1, ditherv);
+        v0 = _mm_srai_epi16(v0, 7);
+        v1 = _mm_srai_epi16(v1, 7);
+        v0 = _mm_packus_epi16(v0, v1);
+        VEC_ST(dest + i, v0);
+    }
+
+    PRAGMA_E2K("ivdep")
+    for (; i < dstW; i++) {
+        int val = (src[i] + dither[(i + offset) & 7]) >> 7;
+        dest[i] = av_clip_uint8(val);
+    }
+}
+
+#define output_pixel(pos, val) \
+    if (big_endian) { \
+        AV_WB16(pos, av_clip_uintp2(val >> shift, output_bits)); \
+    } else { \
+        AV_WL16(pos, av_clip_uintp2(val >> shift, output_bits)); \
+    }
+
+static av_always_inline
+void yuv2plane1_10_e2k(const int16_t *src, uint16_t *dest, int dstW,
+                       const int big_endian, const int output_bits)
+{
+    int shift = 15 - output_bits;
+    int add = 1 << (shift - 1);
+    int clip = (1 << output_bits) - 1;
+    vec_u16 vadd = _mm_set1_epi16(add);
+    vec_u16 vlargest = _mm_set1_epi16(clip);
+    vec_u8 vswap = _mm_setr_epi8(1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14);
+    LOAD_ZERO;
+    __m128i v0;
+    int i = 0;
+
+    PRAGMA_E2K("ivdep")
+    for (; i < dstW - 7; i += 8) {
+        v0 = VEC_LD(src + i);
+        v0 = _mm_adds_epi16(v0, vadd);
+        v0 = _mm_srai_epi16(v0, shift);
+        v0 = _mm_max_epi16(v0, zerov);
+        v0 = _mm_min_epu16(v0, vlargest);
+        if (big_endian) {
+            v0 = _mm_shuffle_epi8(v0, vswap);
+        }
+        VEC_ST(dest + i, v0);
+    }
+
+    PRAGMA_E2K("ivdep")
+    for (; i < dstW; i++) {
+        int val = src[i] + add;
+        output_pixel(&dest[i], val);
+    }
+}
+
+static av_always_inline
+void yuv2planeX_10_e2k(const int16_t *filter, int filterSize,
+                       const int16_t **src, uint16_t *dest, int dstW,
+                       int big_endian, int output_bits)
+{
+    int shift = 11 + 16 - output_bits;
+    int add = 1 << (shift - 1);
+    int clip = (1 << output_bits) - 1;
+    vec_u16 vadd = _mm_set1_epi32(add);
+    vec_u16 vlargest = _mm_set1_epi16(clip);
+    vec_u8 vswap = _mm_setr_epi8(1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14);
+    __m128i v0, v1, v2, v3, v4, v5;
+    int i = 0, j;
+
+    for (; i < dstW - 7; i += 8) {
+        v4 = v5 = vadd;
+        for (j = 0; j < filterSize; j++) {
+            v0 = VEC_LD(src[j] + i);
+            v1 = _mm_set1_epi16(filter[j]);
+            v2 = _mm_mullo_epi16(v0, v1);
+            v3 = _mm_mulhi_epi16(v0, v1);
+            v0 = _mm_unpacklo_epi16(v2, v3);
+            v1 = _mm_unpackhi_epi16(v2, v3);
+            v4 = _mm_add_epi32(v4, v0);
+            v5 = _mm_add_epi32(v5, v1);
+        }
+        v4 = _mm_srai_epi32(v4, shift);
+        v5 = _mm_srai_epi32(v5, shift);
+        v0 = _mm_packus_epi32(v4, v5);
+        v0 = _mm_min_epu16(v0, vlargest);
+        if (big_endian) {
+            v0 = _mm_shuffle_epi8(v0, vswap);
+        }
+        VEC_ST(dest + i, v0);
+    }
+
+    for (; i < dstW; i++) {
+        int val = 1 << (shift - 1);
+        for (j = 0; j < filterSize; j++)
+            val += src[j][i] * filter[j];
+        output_pixel(&dest[i], val);
+    }
+}
+
+#undef output_pixel
+
+#define output_pixel(pos, val, bias, signedness) \
+    if (big_endian) { \
+        AV_WB16(pos, bias + av_clip_##signedness##16(val >> shift)); \
+    } else { \
+        AV_WL16(pos, bias + av_clip_##signedness##16(val >> shift)); \
+    }
+
+static av_always_inline
+void yuv2plane1_16_e2k(const int32_t *src, uint16_t *dest, int dstW,
+                       const int big_endian, int output_bits)
+{
+    int shift = 3;
+    int add = 1 << (shift - 1);
+    vec_u32 vadd = _mm_set1_epi32(add);
+    vec_u8 vswap = _mm_setr_epi8(1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14);
+    __m128i v0, v1;
+    int i = 0;
+
+    PRAGMA_E2K("ivdep")
+    for (; i < dstW - 7; i += 8) {
+        v0 = VEC_LD(src + i);
+        v1 = VEC_LD(src + i + 4);
+        v0 = _mm_add_epi32(v0, vadd);
+        v1 = _mm_add_epi32(v1, vadd);
+        v0 = _mm_srai_epi32(v0, shift);
+        v1 = _mm_srai_epi32(v1, shift);
+        v0 = _mm_packus_epi32(v0, v1);
+        if (big_endian) {
+            v0 = _mm_shuffle_epi8(v0, vswap);
+        }
+        VEC_ST(dest + i, v0);
+    }
+
+    PRAGMA_E2K("ivdep")
+    for (; i < dstW; i++) {
+        int val = src[i] + add;
+        output_pixel(&dest[i], val, 0, uint);
+    }
+}
+
+/* range of val is [0,0x7FFFFFFF], so 31 bits, but with lanczos/spline
+ * filters (or anything with negative coeffs, the range can be slightly
+ * wider in both directions. To account for this overflow, we subtract
+ * a constant so it always fits in the signed range (assuming a
+ * reasonable filterSize), and re-add that at the end. */
+
+static av_always_inline
+void yuv2planeX_16_e2k(const int16_t *filter, int filterSize,
+                       const int32_t **src, uint16_t *dest, int dstW,
+                       int big_endian, int output_bits)
+{
+    int shift = 15, bias = 0x8000;
+    int add = (1 << (shift - 1)) - 0x40000000;
+    vec_u32 vadd = _mm_set1_epi32(add);
+    vec_u16 vbias = _mm_set1_epi16(bias);
+    vec_u8 vswap = _mm_setr_epi8(1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14);
+    __m128i v0, v1, v2, v4, v5;
+    int i = 0, j;
+
+    for (; i < dstW - 7; i += 8) {
+        v4 = v5 = vadd;
+        for (j = 0; j < filterSize; j++) {
+            v0 = VEC_LD(src[j] + i);
+            v1 = VEC_LD(src[j] + i + 4);
+            v2 = _mm_set1_epi32(filter[j]);
+            v4 = _mm_add_epi32(v4, _mm_mullo_epi32(v0, v2));
+            v5 = _mm_add_epi32(v5, _mm_mullo_epi32(v1, v2));
+        }
+        v4 = _mm_srai_epi32(v4, shift);
+        v5 = _mm_srai_epi32(v5, shift);
+        v0 = _mm_packs_epi32(v4, v5);
+        v0 = _mm_add_epi16(v0, vbias);
+        if (big_endian) {
+            v0 = _mm_shuffle_epi8(v0, vswap);
+        }
+        VEC_ST(dest + i, v0);
+    }
+
+    for (; i < dstW; i++) {
+        int val = add;
+        for (j = 0; j < filterSize; j++)
+            val += src[j][i] * (unsigned)filter[j];
+        output_pixel(&dest[i], val, bias, int);
+    }
+}
+
+#undef output_pixel
+
+#define yuv2NBPS(bits, BE_LE, is_be, template_size, typeX_t) \
+static void yuv2plane1_##bits##BE_LE##_e2k(const int16_t *src, \
+                             uint8_t *dest, int dstW, \
+                             const uint8_t *dither, int offset) \
+{ \
+    yuv2plane1_##template_size##_e2k((const typeX_t *) src, \
+                         (uint16_t *) dest, dstW, is_be, bits); \
+} \
+static void yuv2planeX_##bits##BE_LE##_e2k(const int16_t *filter, int filterSize, \
+                              const int16_t **src, uint8_t *dest, int dstW, \
+                              const uint8_t *dither, int offset)\
+{ \
+    yuv2planeX_##template_size##_e2k(filter, \
+                         filterSize, (const typeX_t **) src, \
+                         (uint16_t *) dest, dstW, is_be, bits); \
+}
+
+yuv2NBPS( 9, BE, 1, 10, int16_t)
+yuv2NBPS( 9, LE, 0, 10, int16_t)
+yuv2NBPS(10, BE, 1, 10, int16_t)
+yuv2NBPS(10, LE, 0, 10, int16_t)
+yuv2NBPS(12, BE, 1, 10, int16_t)
+yuv2NBPS(12, LE, 0, 10, int16_t)
+yuv2NBPS(14, BE, 1, 10, int16_t)
+yuv2NBPS(14, LE, 0, 10, int16_t)
+yuv2NBPS(16, BE, 1, 16, int32_t)
+yuv2NBPS(16, LE, 0, 16, int32_t)
+
+#define INIT_RGB(R, B) \
+    __m64 rgb_index0 = _mm_setr_pi8(0, 1, 3, 4, 5, 7, 8, 9); \
+    __m64 rgb_index1 = _mm_setr_pi8(3, 4, 5, 7, 8, 9, 11, 12); \
+    __m64 rgb_index2 = _mm_setr_pi8(5, 7, 8, 9, 11, 12, 13, 15);
+
+#define INIT_RGBX(R, B) \
+    __m128i A_h = _mm_set1_epi16(-256);
+
+#define INIT_XRGB(R, B) \
+    __m128i A_l = _mm_set1_epi16(255);
+
+#define WRITE_RGB(R, B) \
+    v0 = _mm_srai_epi32(R##_l, 22); \
+    v1 = _mm_srai_epi32(R##_h, 22); \
+    v4 = _mm_srai_epi32(G_l, 22 - 16); \
+    v5 = _mm_srai_epi32(G_h, 22 - 16); \
+    v2 = _mm_srai_epi32(B##_l, 22 - 8); \
+    v3 = _mm_srai_epi32(B##_h, 22 - 8); \
+    v0 = _mm_blend_epi16(v0, v4, 0xaa); \
+    v1 = _mm_blend_epi16(v1, v5, 0xaa); \
+    v2 = _mm_packus_epi32(v2, v3); \
+    v0 = _mm_packus_epi16(v0, v1); \
+    v1 = _mm_unpacklo_epi16(v0, v2); \
+    v2 = _mm_unpackhi_epi16(v0, v2); \
+    { \
+        union { __m128i v; __m64 d[2]; } a = { v1 }, b = { v2 }; \
+        __m64 *p = (__m64*)dest; \
+        p[0] = _mm_shuffle2_pi8(a.d[0], a.d[1], rgb_index0); \
+        p[1] = _mm_shuffle2_pi8(a.d[1], b.d[0], rgb_index1); \
+        p[2] = _mm_shuffle2_pi8(b.d[0], b.d[1], rgb_index2); \
+        dest += 24; \
+    }
+
+#define WRITE_RGBX(R, B) \
+    v0 = _mm_srai_epi32(R##_l, 22); \
+    v1 = _mm_srai_epi32(R##_h, 22); \
+    v4 = _mm_srai_epi32(G_l, 22 - 16); \
+    v5 = _mm_srai_epi32(G_h, 22 - 16); \
+    v2 = _mm_srai_epi32(B##_l, 22); \
+    v3 = _mm_srai_epi32(B##_h, 22); \
+    v0 = _mm_blend_epi16(v0, v4, 0xaa); \
+    v1 = _mm_blend_epi16(v1, v5, 0xaa); \
+    v0 = _mm_packus_epi16(v0, v1); \
+    v2 = _mm_packus_epi16(v2, v3); \
+    v2 = _mm_or_si128(v2, A_h); \
+    v1 = _mm_unpacklo_epi16(v0, v2); \
+    v3 = _mm_unpackhi_epi16(v0, v2); \
+    VEC_ST(dest, v1); \
+    VEC_ST(dest + 16, v3); \
+    dest += 32;
+
+#define WRITE_XRGB(R, B) \
+    v0 = _mm_srai_epi32(R##_l, 22 - 16); \
+    v1 = _mm_srai_epi32(R##_h, 22 - 16); \
+    v4 = _mm_srai_epi32(G_l, 22); \
+    v5 = _mm_srai_epi32(G_h, 22); \
+    v2 = _mm_srai_epi32(B##_l, 22 - 16); \
+    v3 = _mm_srai_epi32(B##_h, 22 - 16); \
+    v2 = _mm_blend_epi16(v4, v2, 0xaa); \
+    v3 = _mm_blend_epi16(v5, v3, 0xaa); \
+    v0 = _mm_packus_epi16(v0, v1); \
+    v2 = _mm_packus_epi16(v2, v3); \
+    v0 = _mm_or_si128(v0, A_l); \
+    v1 = _mm_unpacklo_epi16(v0, v2); \
+    v3 = _mm_unpackhi_epi16(v0, v2); \
+    VEC_ST(dest, v1); \
+    VEC_ST(dest + 16, v3); \
+    dest += 32;
+
+#define CALC_RGB \
+    vy_l = _mm_add_epi32(_mm_mullo_epi32(vy_l, y_coeff), y_add); \
+    vy_h = _mm_add_epi32(_mm_mullo_epi32(vy_h, y_coeff), y_add); \
+    \
+    v0 = _mm_mullo_epi32(vv_l, v2g_coeff); \
+    v1 = _mm_mullo_epi32(vu_l, u2g_coeff); \
+    v2 = _mm_mullo_epi32(vv_h, v2g_coeff); \
+    v3 = _mm_mullo_epi32(vu_h, u2g_coeff); \
+    G_l = _mm_add_epi32(_mm_add_epi32(v0, vy_l), v1); \
+    G_h = _mm_add_epi32(_mm_add_epi32(v2, vy_h), v3); \
+    \
+    R_l = _mm_add_epi32(vy_l, _mm_mullo_epi32(vv_l, v2r_coeff)); \
+    R_h = _mm_add_epi32(vy_h, _mm_mullo_epi32(vv_h, v2r_coeff)); \
+    B_l = _mm_add_epi32(vy_l, _mm_mullo_epi32(vu_l, u2b_coeff)); \
+    B_h = _mm_add_epi32(vy_h, _mm_mullo_epi32(vu_h, u2b_coeff));
+
+#define WITH_ALPHA(...) __VA_ARGS__
+#define NO_ALPHA(...)
+
+#define YUV2RGBWRAPPERXF(ext, fmt, R, B, hasAlpha) \
+static void yuv2##ext##_X_e2k(SwsContext *c, const int16_t *lumFilter, \
+                              const int16_t **lumSrc, int lumFilterSize, \
+                              const int16_t *chrFilter, const int16_t **chrUSrc, \
+                              const int16_t **chrVSrc, int chrFilterSize, \
+                              const int16_t **alpSrc, uint8_t *dest, int dstW, \
+                              int y) \
+{ \
+    vec_s32 vy_l, vy_h, vu_l, vu_h, vv_l, vv_h; \
+    vec_s32 hasAlpha(A_l, A_h,) R_l, R_h, G_l, G_h, B_l, B_h; \
+    vec_s32 ystart = _mm_set1_epi32(1 << 9); \
+    vec_s32 uvstart = _mm_set1_epi32((1 << 9) - (128 << 19)); \
+    vec_s32 y_coeff = _mm_set1_epi32(c->yuv2rgb_y_coeff); \
+    vec_s32 y_add = _mm_set1_epi32((1 << 21) - c->yuv2rgb_y_offset * c->yuv2rgb_y_coeff); \
+    vec_s32 v2r_coeff = _mm_set1_epi32(c->yuv2rgb_v2r_coeff); \
+    vec_s32 v2g_coeff = _mm_set1_epi32(c->yuv2rgb_v2g_coeff); \
+    vec_s32 u2g_coeff = _mm_set1_epi32(c->yuv2rgb_u2g_coeff); \
+    vec_s32 u2b_coeff = _mm_set1_epi32(c->yuv2rgb_u2b_coeff); \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    int i, j; \
+    INIT_##fmt(R, B) \
+    \
+    for (i = 0; i < dstW; i += 8) { \
+        vy_l = vy_h = ystart; \
+        for (j = 0; j < lumFilterSize; j++) { \
+            v0 = VEC_LD(lumSrc[j] + i); \
+            v1 = _mm_set1_epi16(lumFilter[j]); \
+            v2 = _mm_mullo_epi16(v0, v1); \
+            v3 = _mm_mulhi_epi16(v0, v1); \
+            v0 = _mm_unpacklo_epi16(v2, v3); \
+            v1 = _mm_unpackhi_epi16(v2, v3); \
+            vy_l = _mm_add_epi32(vy_l, v0); \
+            vy_h = _mm_add_epi32(vy_h, v1); \
+        } \
+        vy_l = _mm_srai_epi32(vy_l, 10); \
+        vy_h = _mm_srai_epi32(vy_h, 10); \
+        \
+        vu_l = vu_h = vv_l = vv_h = uvstart; \
+        for (j = 0; j < chrFilterSize; j++) { \
+            v0 = VEC_LD(chrUSrc[j] + i); \
+            v1 = VEC_LD(chrVSrc[j] + i); \
+            v5 = _mm_set1_epi16(chrFilter[j]); \
+            v2 = _mm_mullo_epi16(v0, v5); \
+            v3 = _mm_mulhi_epi16(v0, v5); \
+            v4 = _mm_mullo_epi16(v1, v5); \
+            v5 = _mm_mulhi_epi16(v1, v5); \
+            v0 = _mm_unpacklo_epi16(v2, v3); \
+            v1 = _mm_unpackhi_epi16(v2, v3); \
+            v2 = _mm_unpacklo_epi16(v4, v5); \
+            v3 = _mm_unpackhi_epi16(v4, v5); \
+            vu_l = _mm_add_epi32(vu_l, v0); \
+            vu_h = _mm_add_epi32(vu_h, v1); \
+            vv_l = _mm_add_epi32(vv_l, v2); \
+            vv_h = _mm_add_epi32(vv_h, v3); \
+        } \
+        vu_l = _mm_srai_epi32(vu_l, 10); \
+        vu_h = _mm_srai_epi32(vu_h, 10); \
+        vv_l = _mm_srai_epi32(vv_l, 10); \
+        vv_h = _mm_srai_epi32(vv_h, 10); \
+        \
+        CALC_RGB \
+        WRITE_##fmt(R, B) \
+    } \
+}
+
+#define SETUP(buf, i, alpha, r0, r1) { \
+    v0 = VEC_LD(buf##0 + i); \
+    v1 = VEC_LD(buf##1 + i); \
+    v2 = _mm_unpacklo_epi16(v0, v1); \
+    v3 = _mm_unpackhi_epi16(v0, v1); \
+    r0 = _mm_madd_epi16(v2, alpha); \
+    r1 = _mm_madd_epi16(v3, alpha); \
+}
+
+#define YUV2RGBWRAPPER2F(ext, fmt, R, B, hasAlpha) \
+static void yuv2##ext##_2_e2k(SwsContext *c, const int16_t *buf[2], \
+                              const int16_t *ubuf[2], const int16_t *vbuf[2], \
+                              const int16_t *abuf[2], uint8_t *dest, int dstW, \
+                              int yalpha, int uvalpha, int y) \
+{ \
+    const int16_t hasAlpha(*abuf0 = abuf[0], *abuf1 = abuf[1],) \
+                  *buf0 = buf[0], *buf1 = buf[1], \
+                  *ubuf0 = ubuf[0], *ubuf1 = ubuf[1], \
+                  *vbuf0 = vbuf[0], *vbuf1 = vbuf[1]; \
+    vec_s32 vy_l, vy_h, vu_l, vu_h, vv_l, vv_h; \
+    vec_s32 hasAlpha(A_l, A_h,) R_l, R_h, G_l, G_h, B_l, B_h; \
+    vec_s16 vyalpha = _mm_set1_epi32(4096 + yalpha * 0xffff); \
+    vec_s16 vuvalpha = _mm_set1_epi32(4096 + uvalpha * 0xffff); \
+    vec_s32 y_coeff = _mm_set1_epi32(c->yuv2rgb_y_coeff); \
+    vec_s32 y_add = _mm_set1_epi32((1 << 21) - c->yuv2rgb_y_offset * c->yuv2rgb_y_coeff); \
+    vec_s32 v2r_coeff = _mm_set1_epi32(c->yuv2rgb_v2r_coeff); \
+    vec_s32 v2g_coeff = _mm_set1_epi32(c->yuv2rgb_v2g_coeff); \
+    vec_s32 u2g_coeff = _mm_set1_epi32(c->yuv2rgb_u2g_coeff); \
+    vec_s32 u2b_coeff = _mm_set1_epi32(c->yuv2rgb_u2b_coeff); \
+    vec_s32 dec128 = _mm_set1_epi32(128 << 19); \
+    hasAlpha(vec_s32 add18 = _mm_set1_epi32(1 << 18);) \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    int i; \
+    INIT_##fmt(R, B) \
+    \
+    av_assert2(yalpha <= 4096U); \
+    av_assert2(uvalpha <= 4096U); \
+    \
+    for (i = 0; i < dstW; i += 8) { \
+        SETUP(buf, i, vyalpha, v0, v1); \
+        vy_l = _mm_srai_epi32(v0, 10); \
+        vy_h = _mm_srai_epi32(v1, 10); \
+        \
+        SETUP(ubuf, i, vuvalpha, v0, v1); \
+        vu_l = _mm_srai_epi32(_mm_sub_epi32(v0, dec128), 10); \
+        vu_h = _mm_srai_epi32(_mm_sub_epi32(v1, dec128), 10); \
+        \
+        SETUP(vbuf, i, vuvalpha, v0, v1); \
+        vv_l = _mm_srai_epi32(_mm_sub_epi32(v0, dec128), 10); \
+        vv_h = _mm_srai_epi32(_mm_sub_epi32(v1, dec128), 10); \
+        \
+        hasAlpha( \
+            SETUP(abuf, i, vyalpha, v0, v1); \
+            A_l = _mm_add_epi32(v0, add18); \
+            A_h = _mm_add_epi32(v1, add18); \
+        ) \
+        \
+        CALC_RGB \
+        WRITE_##fmt(R, B) \
+    } \
+}
+
+#define YUV2RGBWRAPPER1F(ext, fmt, R, B, hasAlpha) \
+static void yuv2##ext##_1_e2k(SwsContext *c, const int16_t *buf0, \
+                              const int16_t *ubuf[2], const int16_t *vbuf[2], \
+                              const int16_t *abuf0, uint8_t *dest, int dstW, \
+                              int uvalpha, int y) \
+{ \
+    const int16_t *ubuf0 = ubuf[0], *vbuf0 = vbuf[0]; \
+    const int16_t *ubuf1 = ubuf[1], *vbuf1 = vbuf[1]; \
+    int uvshl = uvalpha < 2048 ? 2 : 1; \
+    vec_s32 vy_l, vy_h, vu_l, vu_h, vv_l, vv_h; \
+    vec_s32 R_l, R_h, G_l, G_h, B_l, B_h; \
+    vec_s32 y_coeff = _mm_set1_epi32(c->yuv2rgb_y_coeff * 4); \
+    vec_s32 y_add = _mm_set1_epi32((1 << 21) - c->yuv2rgb_y_offset * c->yuv2rgb_y_coeff); \
+    vec_s32 v2r_coeff = _mm_set1_epi32(c->yuv2rgb_v2r_coeff << uvshl); \
+    vec_s32 v2g_coeff = _mm_set1_epi32(c->yuv2rgb_v2g_coeff << uvshl); \
+    vec_s32 u2g_coeff = _mm_set1_epi32(c->yuv2rgb_u2g_coeff << uvshl); \
+    vec_s32 u2b_coeff = _mm_set1_epi32(c->yuv2rgb_u2b_coeff << uvshl); \
+    vec_u16 uvsub = _mm_set1_epi16(uvalpha < 2048 ? 128 << 7 : 128 << 8); \
+    hasAlpha(vec_s16 A, add64 = _mm_set1_epi16(64);) \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    int i; \
+    INIT_##fmt(R, B) \
+    \
+    for (i = 0; i < dstW; i += 8) { \
+        v0 = VEC_LD(buf0 + i); \
+        v2 = _mm_unpacklo_epi16(v0, v0); \
+        v3 = _mm_unpackhi_epi16(v0, v0); \
+        vy_l = _mm_srai_epi32(v2, 16); \
+        vy_h = _mm_srai_epi32(v3, 16); \
+        \
+        v0 = VEC_LD(ubuf0 + i); \
+        v1 = VEC_LD(vbuf0 + i); \
+        if (uvalpha >= 2048) { \
+            v2 = VEC_LD(ubuf1 + i); \
+            v3 = VEC_LD(vbuf1 + i); \
+            v0 = _mm_add_epi16(v0, v2); \
+            v1 = _mm_add_epi16(v1, v3); \
+        } \
+        v0 = _mm_sub_epi16(v0, uvsub); \
+        v1 = _mm_sub_epi16(v1, uvsub); \
+        v2 = _mm_unpacklo_epi16(v0, v0); \
+        v3 = _mm_unpackhi_epi16(v0, v0); \
+        vu_l = _mm_srai_epi32(v2, 16); \
+        vu_h = _mm_srai_epi32(v3, 16); \
+        v2 = _mm_unpacklo_epi16(v1, v1); \
+        v3 = _mm_unpackhi_epi16(v1, v1); \
+        vv_l = _mm_srai_epi32(v2, 16); \
+        vv_h = _mm_srai_epi32(v3, 16); \
+        \
+        hasAlpha( \
+            A = VEC_LD(abuf0 + i); \
+            A = _mm_add_epi16(A, add64); \
+            A = _mm_srai_epi16(A, 7); \
+        ) \
+        \
+        CALC_RGB \
+        WRITE_##fmt(R, B) \
+    } \
+}
+
+YUV2RGBWRAPPERXF(rgbx32_full, RGBX, R, B, NO_ALPHA)
+YUV2RGBWRAPPERXF(bgrx32_full, RGBX, B, R, NO_ALPHA)
+YUV2RGBWRAPPERXF(xrgb32_full, XRGB, R, B, NO_ALPHA)
+YUV2RGBWRAPPERXF(xbgr32_full, XRGB, B, R, NO_ALPHA)
+YUV2RGBWRAPPERXF(rgb24_full, RGB, R, B, NO_ALPHA)
+YUV2RGBWRAPPERXF(bgr24_full, RGB, B, R, NO_ALPHA)
+
+YUV2RGBWRAPPER2F(rgbx32_full, RGBX, R, B, NO_ALPHA)
+YUV2RGBWRAPPER2F(bgrx32_full, RGBX, B, R, NO_ALPHA)
+YUV2RGBWRAPPER2F(xrgb32_full, XRGB, R, B, NO_ALPHA)
+YUV2RGBWRAPPER2F(xbgr32_full, XRGB, B, R, NO_ALPHA)
+YUV2RGBWRAPPER2F(rgb24_full, RGB, R, B, NO_ALPHA)
+YUV2RGBWRAPPER2F(bgr24_full, RGB, B, R, NO_ALPHA)
+
+YUV2RGBWRAPPER1F(rgbx32_full, RGBX, R, B, NO_ALPHA)
+YUV2RGBWRAPPER1F(bgrx32_full, RGBX, B, R, NO_ALPHA)
+YUV2RGBWRAPPER1F(xrgb32_full, XRGB, R, B, NO_ALPHA)
+YUV2RGBWRAPPER1F(xbgr32_full, XRGB, B, R, NO_ALPHA)
+YUV2RGBWRAPPER1F(rgb24_full, RGB, R, B, NO_ALPHA)
+YUV2RGBWRAPPER1F(bgr24_full, RGB, B, R, NO_ALPHA)
+
+#if 1 // performance
+
+#define INIT2_RGB(R, B) \
+    __m128i perm_unp8 = _mm_setr_epi8( \
+        0, 8, 1, 9, 2, 10, 3, 11, 4, 12, 5, 13, 6, 14, 7, 15); \
+    __m64 rgb_index0 = _mm_setr_pi8(0, 1, 3, 4, 5, 7, 8, 9); \
+    __m64 rgb_index1 = _mm_setr_pi8(3, 4, 5, 7, 8, 9, 11, 12); \
+    __m64 rgb_index2 = _mm_setr_pi8(5, 7, 8, 9, 11, 12, 13, 15);
+
+#define INIT2_RGBX(R, B) INIT2_XRGB(R, B)
+#define INIT2_XRGB(R, B) \
+    __m128i A_l = _mm_set1_epi16(255); \
+    __m128i perm_unp8 = _mm_setr_epi8( \
+        0, 8, 1, 9, 2, 10, 3, 11, 4, 12, 5, 13, 6, 14, 7, 15);
+
+#define WRITE2_RGB(R, B) \
+    v4 = _mm_packus_epi16(R##_l, G_l); \
+    v5 = _mm_packus_epi16(B##_l, B##_l); \
+    v0 = _mm_shuffle_epi8(v4, perm_unp8); \
+    v1 = _mm_unpacklo_epi8(v5, v5); \
+    v2 = _mm_unpacklo_epi16(v0, v1); \
+    v3 = _mm_unpackhi_epi16(v0, v1); \
+    { \
+        union { __m128i v; __m64 d[2]; } a = { v2 }, b = { v3 }; \
+        __m64 *p = (__m64*)dest; \
+        p[0] = _mm_shuffle2_pi8(a.d[0], a.d[1], rgb_index0); \
+        p[1] = _mm_shuffle2_pi8(a.d[1], b.d[0], rgb_index1); \
+        p[2] = _mm_shuffle2_pi8(b.d[0], b.d[1], rgb_index2); \
+        dest += 24; \
+    }
+
+#define WRITE2_RGBX(R, B) \
+    v4 = _mm_packus_epi16(R##_l, G_l); \
+    v5 = _mm_packus_epi16(B##_l, A_l); \
+    v0 = _mm_shuffle_epi8(v4, perm_unp8); \
+    v1 = _mm_shuffle_epi8(v5, perm_unp8); \
+    v2 = _mm_unpacklo_epi16(v0, v1); \
+    v3 = _mm_unpackhi_epi16(v0, v1); \
+    VEC_ST(dest, v2); \
+    VEC_ST(dest + 16, v3); \
+    dest += 32;
+
+#define WRITE2_XRGB(R, B) \
+    v4 = _mm_packus_epi16(A_l, R##_l); \
+    v5 = _mm_packus_epi16(G_l, B##_l); \
+    v0 = _mm_shuffle_epi8(v4, perm_unp8); \
+    v1 = _mm_shuffle_epi8(v5, perm_unp8); \
+    v2 = _mm_unpacklo_epi16(v0, v1); \
+    v3 = _mm_unpackhi_epi16(v0, v1); \
+    VEC_ST(dest, v2); \
+    VEC_ST(dest + 16, v3); \
+    dest += 32;
+
+#define CALC2_RGB \
+    vy_l = _mm_mulhrs_epi16(_mm_sub_epi16(vy_l, y_sub), y_coeff); \
+    \
+    v0 = _mm_mulhrs_epi16(vv_l, v2g_coeff); \
+    v1 = _mm_mulhrs_epi16(vu_l, u2g_coeff); \
+    G_l = _mm_add_epi16(_mm_add_epi16(v0, vy_l), v1); \
+    \
+    R_l = _mm_add_epi16(vy_l, _mm_mulhrs_epi16(vv_l, v2r_coeff)); \
+    B_l = _mm_add_epi16(vy_l, _mm_mulhrs_epi16(vu_l, u2b_coeff));
+
+#define YUV2RGBWRAPPERX(ext, fmt, R, B, hasAlpha) \
+static void yuv2##ext##_X_e2k(SwsContext *c, const int16_t *lumFilter, \
+                              const int16_t **lumSrc, int lumFilterSize, \
+                              const int16_t *chrFilter, const int16_t **chrUSrc, \
+                              const int16_t **chrVSrc, int chrFilterSize, \
+                              const int16_t **alpSrc, uint8_t *dest, int dstW, \
+                              int y) \
+{ \
+    vec_s32 vy_l, vy_h, vu_l, vv_l, vu2_l, vu2_h, vv2_l, vv2_h; \
+    vec_s32 hasAlpha(A_l,) R_l, G_l, B_l; \
+    vec_s32 ystart = _mm_set1_epi32(0); \
+    vec_s32 uvstart = _mm_set1_epi32(-(128 << 19)); \
+    vec_s32 y_coeff = _mm_set1_epi16(c->yuv2rgb_y_coeff); \
+    vec_s32 y_sub = _mm_set1_epi16((c->yuv2rgb_y_offset + 64) >> 7); \
+    vec_s32 v2r_coeff = _mm_set1_epi16(c->yuv2rgb_v2r_coeff); \
+    vec_s32 v2g_coeff = _mm_set1_epi16(c->yuv2rgb_v2g_coeff); \
+    vec_s32 u2g_coeff = _mm_set1_epi16(c->yuv2rgb_u2g_coeff); \
+    vec_s32 u2b_coeff = _mm_set1_epi16(c->yuv2rgb_u2b_coeff); \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    int i, j; \
+    INIT2_##fmt(R, B) \
+    \
+    for (i = 0; i < (dstW + 1) >> 1; i += 8) { \
+        vy_l = vy_h = ystart; \
+        for (j = 0; j < lumFilterSize; j++) { \
+            v0 = VEC_LD(lumSrc[j] + i * 2); \
+            v1 = _mm_set1_epi16(lumFilter[j]); \
+            v2 = _mm_mullo_epi16(v0, v1); \
+            v3 = _mm_mulhi_epi16(v0, v1); \
+            v0 = _mm_unpacklo_epi16(v2, v3); \
+            v1 = _mm_unpackhi_epi16(v2, v3); \
+            vy_l = _mm_add_epi32(vy_l, v0); \
+            vy_h = _mm_add_epi32(vy_h, v1); \
+        } \
+        vy_l = _mm_srai_epi32(vy_l, 17); \
+        vy_h = _mm_srai_epi32(vy_h, 17); \
+        \
+        vu2_l = vu2_h = vv2_l = vv2_h = uvstart; \
+        for (j = 0; j < chrFilterSize; j++) { \
+            v0 = VEC_LD(chrUSrc[j] + i); \
+            v1 = VEC_LD(chrVSrc[j] + i); \
+            v5 = _mm_set1_epi16(chrFilter[j]); \
+            v2 = _mm_mullo_epi16(v0, v5); \
+            v3 = _mm_mulhi_epi16(v0, v5); \
+            v4 = _mm_mullo_epi16(v1, v5); \
+            v5 = _mm_mulhi_epi16(v1, v5); \
+            v0 = _mm_unpacklo_epi16(v2, v3); \
+            v1 = _mm_unpackhi_epi16(v2, v3); \
+            v2 = _mm_unpacklo_epi16(v4, v5); \
+            v3 = _mm_unpackhi_epi16(v4, v5); \
+            vu2_l = _mm_add_epi32(vu2_l, v0); \
+            vu2_h = _mm_add_epi32(vu2_h, v1); \
+            vv2_l = _mm_add_epi32(vv2_l, v2); \
+            vv2_h = _mm_add_epi32(vv2_h, v3); \
+        } \
+        vu2_l = _mm_srai_epi32(vu2_l, 17); \
+        vu2_h = _mm_srai_epi32(vu2_h, 17); \
+        vv2_l = _mm_srai_epi32(vv2_l, 17); \
+        vv2_h = _mm_srai_epi32(vv2_h, 17); \
+        vu2_l = _mm_packs_epi32(vu2_l, vu2_h); \
+        vv2_l = _mm_packs_epi32(vv2_l, vv2_h); \
+        \
+        vu_l = _mm_unpacklo_epi16(vu2_l, vu2_l); \
+        vv_l = _mm_unpacklo_epi16(vv2_l, vv2_l); \
+        vy_l = _mm_packs_epi32(vy_l, vy_h); \
+        \
+        CALC2_RGB \
+        WRITE2_##fmt(R, B) \
+        \
+        vy_l = vy_h = ystart; \
+        for (j = 0; j < lumFilterSize; j++) { \
+            v0 = VEC_LD(lumSrc[j] + i * 2 + 8); \
+            v1 = _mm_set1_epi16(lumFilter[j]); \
+            v2 = _mm_mullo_epi16(v0, v1); \
+            v3 = _mm_mulhi_epi16(v0, v1); \
+            v0 = _mm_unpacklo_epi16(v2, v3); \
+            v1 = _mm_unpackhi_epi16(v2, v3); \
+            vy_l = _mm_add_epi32(vy_l, v0); \
+            vy_h = _mm_add_epi32(vy_h, v1); \
+        } \
+        vy_l = _mm_srai_epi32(vy_l, 17); \
+        vy_h = _mm_srai_epi32(vy_h, 17); \
+        \
+        vu_l = _mm_unpackhi_epi16(vu2_l, vu2_l); \
+        vv_l = _mm_unpackhi_epi16(vv2_l, vv2_l); \
+        vy_l = _mm_packs_epi32(vy_l, vy_h); \
+        \
+        CALC2_RGB \
+        WRITE2_##fmt(R, B) \
+    } \
+}
+
+#define SETUP2(buf, i, alpha, r0) { \
+    v0 = VEC_LD(buf##0 + i); \
+    v1 = VEC_LD(buf##1 + i); \
+    v1 = _mm_subs_epi16(v0, v1); \
+    v1 = _mm_mulhrs_epi16(v1, alpha); \
+    r0 = _mm_add_epi16(v0, v1); \
+}
+
+#define YUV2RGBWRAPPER2(ext, fmt, R, B, hasAlpha) \
+static void yuv2##ext##_2_e2k(SwsContext *c, const int16_t *buf[2], \
+                              const int16_t *ubuf[2], const int16_t *vbuf[2], \
+                              const int16_t *abuf[2], uint8_t *dest, int dstW, \
+                              int yalpha, int uvalpha, int y) \
+{ \
+    const int16_t hasAlpha(*abuf0 = abuf[0], *abuf1 = abuf[1],) \
+                  *buf0 = buf[0], *buf1 = buf[1], \
+                  *ubuf0 = ubuf[0], *ubuf1 = ubuf[1], \
+                  *vbuf0 = vbuf[0], *vbuf1 = vbuf[1]; \
+    vec_s32 vy_l, vu_l, vv_l, vu2_l, vv2_l; \
+    vec_s32 hasAlpha(A_l,) R_l, G_l, B_l; \
+    vec_s16 vyalpha = _mm_set1_epi16(-yalpha << 3); \
+    vec_s16 vuvalpha = _mm_set1_epi16(-uvalpha << 3); \
+    vec_s32 y_coeff = _mm_set1_epi16(c->yuv2rgb_y_coeff); \
+    vec_s32 y_sub = _mm_set1_epi16((c->yuv2rgb_y_offset + 64) >> 7); \
+    vec_s32 v2r_coeff = _mm_set1_epi16(c->yuv2rgb_v2r_coeff); \
+    vec_s32 v2g_coeff = _mm_set1_epi16(c->yuv2rgb_v2g_coeff); \
+    vec_s32 u2g_coeff = _mm_set1_epi16(c->yuv2rgb_u2g_coeff); \
+    vec_s32 u2b_coeff = _mm_set1_epi16(c->yuv2rgb_u2b_coeff); \
+    vec_s32 dec128 = _mm_set1_epi16(128 << 2); \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    int i; \
+    INIT2_##fmt(R, B) \
+    \
+    av_assert2(yalpha <= 4096U); \
+    av_assert2(uvalpha <= 4096U); \
+    \
+    for (i = 0; i < (dstW + 1) >> 1; i += 8) { \
+        SETUP2(buf, i * 2, vyalpha, vy_l); \
+        vy_l = _mm_srai_epi16(vy_l, 5); \
+        \
+        SETUP2(ubuf, i, vuvalpha, vu2_l); \
+        vu2_l = _mm_srai_epi16(vu2_l, 5); \
+        \
+        SETUP2(vbuf, i, vuvalpha, vv2_l); \
+        vv2_l = _mm_srai_epi16(vv2_l, 5); \
+        \
+        vu2_l = _mm_sub_epi16(vu2_l, dec128); \
+        vv2_l = _mm_sub_epi16(vv2_l, dec128); \
+        \
+        hasAlpha( \
+            SETUP2(abuf, i * 2, vyalpha, A_l); \
+        ) \
+        \
+        vu_l = _mm_unpacklo_epi16(vu2_l, vu2_l); \
+        vv_l = _mm_unpacklo_epi16(vv2_l, vv2_l); \
+        \
+        CALC2_RGB \
+        WRITE2_##fmt(R, B) \
+        \
+        SETUP2(buf, i * 2 + 8, vyalpha, vy_l); \
+        vy_l = _mm_srai_epi16(vy_l, 5); \
+        \
+        hasAlpha( \
+            SETUP2(abuf, i * 2 + 8, vyalpha, A_l); \
+        ) \
+        \
+        vu_l = _mm_unpackhi_epi16(vu2_l, vu2_l); \
+        vv_l = _mm_unpackhi_epi16(vv2_l, vv2_l); \
+        \
+        CALC2_RGB \
+        WRITE2_##fmt(R, B) \
+    } \
+}
+
+#define YUV2RGBWRAPPER1(ext, fmt, R, B, hasAlpha) \
+static void yuv2##ext##_1_e2k(SwsContext *c, const int16_t *buf0, \
+                              const int16_t *ubuf[2], const int16_t *vbuf[2], \
+                              const int16_t *abuf0, uint8_t *dest, int dstW, \
+                              int uvalpha, int y) \
+{ \
+    const int16_t *ubuf0 = ubuf[0], *vbuf0 = vbuf[0]; \
+    const int16_t *ubuf1 = ubuf[1], *vbuf1 = vbuf[1]; \
+    int uvshr = uvalpha < 2048 ? 5 : 6; \
+    vec_s32 vy_l, vu_l, vv_l, vu2_l, vv2_l; \
+    vec_s32 hasAlpha(A_l,) R_l, G_l, B_l; \
+    vec_s32 y_coeff = _mm_set1_epi16(c->yuv2rgb_y_coeff); \
+    vec_s32 y_sub = _mm_set1_epi16((c->yuv2rgb_y_offset + 64) >> 7); \
+    vec_s32 v2r_coeff = _mm_set1_epi16(c->yuv2rgb_v2r_coeff); \
+    vec_s32 v2g_coeff = _mm_set1_epi16(c->yuv2rgb_v2g_coeff); \
+    vec_s32 u2g_coeff = _mm_set1_epi16(c->yuv2rgb_u2g_coeff); \
+    vec_s32 u2b_coeff = _mm_set1_epi16(c->yuv2rgb_u2b_coeff); \
+    vec_u16 uvsub = _mm_set1_epi16(uvalpha < 2048 ? 128 << 7 : 128 << 8); \
+    hasAlpha(vec_s16 A, add64 = _mm_set1_epi16(64);) \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    int i; \
+    INIT2_##fmt(R, B) \
+    \
+    for (i = 0; i < (dstW + 1) >> 1; i += 8) { \
+        v0 = VEC_LD(buf0 + i * 2); \
+        vy_l = _mm_srai_epi16(v0, 5); \
+        \
+        v0 = VEC_LD(ubuf0 + i); \
+        v1 = VEC_LD(vbuf0 + i); \
+        if (uvalpha >= 2048) { \
+            v2 = VEC_LD(ubuf1 + i); \
+            v3 = VEC_LD(vbuf1 + i); \
+            v0 = _mm_add_epi16(v0, v2); \
+            v1 = _mm_add_epi16(v1, v3); \
+        } \
+        v0 = _mm_sub_epi16(v0, uvsub); \
+        v1 = _mm_sub_epi16(v1, uvsub); \
+        vu2_l = _mm_srai_epi16(v0, uvshr); \
+        vv2_l = _mm_srai_epi16(v1, uvshr); \
+        \
+        hasAlpha( \
+            A_l = VEC_LD(abuf0 + i * 2); \
+            A_l = _mm_add_epi16(A_l, add64); \
+            A_l = _mm_srai_epi16(A_l, 7); \
+        ) \
+        \
+        vu_l = _mm_unpacklo_epi16(vu2_l, vu2_l); \
+        vv_l = _mm_unpacklo_epi16(vv2_l, vv2_l); \
+        \
+        CALC2_RGB \
+        WRITE2_##fmt(R, B) \
+        \
+        v0 = VEC_LD(buf0 + i * 2 + 8); \
+        vy_l = _mm_srai_epi16(v0, 5); \
+        \
+        hasAlpha( \
+            A_l = VEC_LD(abuf0 + i * 2 + 8); \
+            A_l = _mm_add_epi16(A_l, add64); \
+            A_l = _mm_srai_epi16(A_l, 7); \
+        ) \
+        \
+        vu_l = _mm_unpackhi_epi16(vu2_l, vu2_l); \
+        vv_l = _mm_unpackhi_epi16(vv2_l, vv2_l); \
+        \
+        CALC2_RGB \
+        WRITE2_##fmt(R, B) \
+    } \
+}
+
+#else // quality
+
+#define YUV2RGBWRAPPERX(ext, fmt, R, B, hasAlpha) \
+static void yuv2##ext##_X_e2k(SwsContext *c, const int16_t *lumFilter, \
+                              const int16_t **lumSrc, int lumFilterSize, \
+                              const int16_t *chrFilter, const int16_t **chrUSrc, \
+                              const int16_t **chrVSrc, int chrFilterSize, \
+                              const int16_t **alpSrc, uint8_t *dest, int dstW, \
+                              int y) \
+{ \
+    vec_s32 vy_l, vy_h, vu_l, vu_h, vv_l, vv_h, vu2_l, vu2_h, vv2_l, vv2_h; \
+    vec_s32 hasAlpha(A_l, A_h,) R_l, R_h, G_l, G_h, B_l, B_h; \
+    vec_s32 ystart = _mm_set1_epi32(1 << 9); \
+    vec_s32 uvstart = _mm_set1_epi32((1 << 9) - (128 << 19)); \
+    vec_s32 y_coeff = _mm_set1_epi32(c->yuv2rgb_y_coeff); \
+    vec_s32 y_add = _mm_set1_epi32((1 << 21) - c->yuv2rgb_y_offset * c->yuv2rgb_y_coeff); \
+    vec_s32 v2r_coeff = _mm_set1_epi32(c->yuv2rgb_v2r_coeff); \
+    vec_s32 v2g_coeff = _mm_set1_epi32(c->yuv2rgb_v2g_coeff); \
+    vec_s32 u2g_coeff = _mm_set1_epi32(c->yuv2rgb_u2g_coeff); \
+    vec_s32 u2b_coeff = _mm_set1_epi32(c->yuv2rgb_u2b_coeff); \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    int i, j; \
+    INIT_##fmt(R, B) \
+    \
+    for (i = 0; i < (dstW + 1) >> 1; i += 8) { \
+        vy_l = vy_h = ystart; \
+        for (j = 0; j < lumFilterSize; j++) { \
+            v0 = VEC_LD(lumSrc[j] + i * 2); \
+            v1 = _mm_set1_epi16(lumFilter[j]); \
+            v2 = _mm_mullo_epi16(v0, v1); \
+            v3 = _mm_mulhi_epi16(v0, v1); \
+            v0 = _mm_unpacklo_epi16(v2, v3); \
+            v1 = _mm_unpackhi_epi16(v2, v3); \
+            vy_l = _mm_add_epi32(vy_l, v0); \
+            vy_h = _mm_add_epi32(vy_h, v1); \
+        } \
+        vy_l = _mm_srai_epi32(vy_l, 10); \
+        vy_h = _mm_srai_epi32(vy_h, 10); \
+        \
+        vu2_l = vu2_h = vv2_l = vv2_h = uvstart; \
+        for (j = 0; j < chrFilterSize; j++) { \
+            v0 = VEC_LD(chrUSrc[j] + i); \
+            v1 = VEC_LD(chrVSrc[j] + i); \
+            v5 = _mm_set1_epi16(chrFilter[j]); \
+            v2 = _mm_mullo_epi16(v0, v5); \
+            v3 = _mm_mulhi_epi16(v0, v5); \
+            v4 = _mm_mullo_epi16(v1, v5); \
+            v5 = _mm_mulhi_epi16(v1, v5); \
+            v0 = _mm_unpacklo_epi16(v2, v3); \
+            v1 = _mm_unpackhi_epi16(v2, v3); \
+            v2 = _mm_unpacklo_epi16(v4, v5); \
+            v3 = _mm_unpackhi_epi16(v4, v5); \
+            vu2_l = _mm_add_epi32(vu2_l, v0); \
+            vu2_h = _mm_add_epi32(vu2_h, v1); \
+            vv2_l = _mm_add_epi32(vv2_l, v2); \
+            vv2_h = _mm_add_epi32(vv2_h, v3); \
+        } \
+        vu2_l = _mm_srai_epi32(vu2_l, 10); \
+        vu2_h = _mm_srai_epi32(vu2_h, 10); \
+        vv2_l = _mm_srai_epi32(vv2_l, 10); \
+        vv2_h = _mm_srai_epi32(vv2_h, 10); \
+        \
+        vu_l = _mm_unpacklo_epi32(vu2_l, vu2_l); \
+        vu_h = _mm_unpackhi_epi32(vu2_l, vu2_l); \
+        vv_l = _mm_unpacklo_epi32(vv2_l, vv2_l); \
+        vv_h = _mm_unpackhi_epi32(vv2_l, vv2_l); \
+        \
+        CALC_RGB \
+        WRITE_##fmt(R, B) \
+        \
+        vy_l = vy_h = ystart; \
+        for (j = 0; j < lumFilterSize; j++) { \
+            v0 = VEC_LD(lumSrc[j] + i * 2 + 8); \
+            v1 = _mm_set1_epi16(lumFilter[j]); \
+            v2 = _mm_mullo_epi16(v0, v1); \
+            v3 = _mm_mulhi_epi16(v0, v1); \
+            v0 = _mm_unpacklo_epi16(v2, v3); \
+            v1 = _mm_unpackhi_epi16(v2, v3); \
+            vy_l = _mm_add_epi32(vy_l, v0); \
+            vy_h = _mm_add_epi32(vy_h, v1); \
+        } \
+        vy_l = _mm_srai_epi32(vy_l, 10); \
+        vy_h = _mm_srai_epi32(vy_h, 10); \
+        \
+        vu_l = _mm_unpacklo_epi32(vu2_h, vu2_h); \
+        vu_h = _mm_unpackhi_epi32(vu2_h, vu2_h); \
+        vv_l = _mm_unpacklo_epi32(vv2_h, vv2_h); \
+        vv_h = _mm_unpackhi_epi32(vv2_h, vv2_h); \
+        \
+        CALC_RGB \
+        WRITE_##fmt(R, B) \
+    } \
+}
+
+#define YUV2RGBWRAPPER2(ext, fmt, R, B, hasAlpha) \
+static void yuv2##ext##_2_e2k(SwsContext *c, const int16_t *buf[2], \
+                              const int16_t *ubuf[2], const int16_t *vbuf[2], \
+                              const int16_t *abuf[2], uint8_t *dest, int dstW, \
+                              int yalpha, int uvalpha, int y) \
+{ \
+    const int16_t hasAlpha(*abuf0 = abuf[0], *abuf1 = abuf[1],) \
+                  *buf0 = buf[0], *buf1 = buf[1], \
+                  *ubuf0 = ubuf[0], *ubuf1 = ubuf[1], \
+                  *vbuf0 = vbuf[0], *vbuf1 = vbuf[1]; \
+    vec_s32 vy_l, vy_h, vu_l, vu_h, vv_l, vv_h, vu2_l, vu2_h, vv2_l, vv2_h; \
+    vec_s32 hasAlpha(A_l, A_h,) R_l, R_h, G_l, G_h, B_l, B_h; \
+    vec_s16 vyalpha = _mm_set1_epi32(4096 + yalpha * 0xffff); \
+    vec_s16 vuvalpha = _mm_set1_epi32(4096 + uvalpha * 0xffff); \
+    vec_s32 y_coeff = _mm_set1_epi32(c->yuv2rgb_y_coeff); \
+    vec_s32 y_add = _mm_set1_epi32((1 << 21) - c->yuv2rgb_y_offset * c->yuv2rgb_y_coeff); \
+    vec_s32 v2r_coeff = _mm_set1_epi32(c->yuv2rgb_v2r_coeff); \
+    vec_s32 v2g_coeff = _mm_set1_epi32(c->yuv2rgb_v2g_coeff); \
+    vec_s32 u2g_coeff = _mm_set1_epi32(c->yuv2rgb_u2g_coeff); \
+    vec_s32 u2b_coeff = _mm_set1_epi32(c->yuv2rgb_u2b_coeff); \
+    vec_s32 dec128 = _mm_set1_epi32(128 << 19); \
+    hasAlpha(vec_s32 add18 = _mm_set1_epi32(1 << 18);) \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    int i; \
+    INIT_##fmt(R, B) \
+    \
+    av_assert2(yalpha <= 4096U); \
+    av_assert2(uvalpha <= 4096U); \
+    \
+    for (i = 0; i < (dstW + 1) >> 1; i += 8) { \
+        SETUP(buf, i * 2, vyalpha, v0, v1); \
+        vy_l = _mm_srai_epi32(v0, 10); \
+        vy_h = _mm_srai_epi32(v1, 10); \
+        \
+        SETUP(ubuf, i, vuvalpha, v0, v1); \
+        vu2_l = _mm_srai_epi32(_mm_sub_epi32(v0, dec128), 10); \
+        vu2_h = _mm_srai_epi32(_mm_sub_epi32(v1, dec128), 10); \
+        \
+        SETUP(vbuf, i, vuvalpha, v0, v1); \
+        vv2_l = _mm_srai_epi32(_mm_sub_epi32(v0, dec128), 10); \
+        vv2_h = _mm_srai_epi32(_mm_sub_epi32(v1, dec128), 10); \
+        \
+        hasAlpha( \
+            SETUP(abuf, i * 2, vyalpha, v0, v1); \
+            A_l = _mm_add_epi32(v0, add18); \
+            A_h = _mm_add_epi32(v1, add18); \
+        ) \
+        \
+        vu_l = _mm_unpacklo_epi32(vu2_l, vu2_l); \
+        vu_h = _mm_unpackhi_epi32(vu2_l, vu2_l); \
+        vv_l = _mm_unpacklo_epi32(vv2_l, vv2_l); \
+        vv_h = _mm_unpackhi_epi32(vv2_l, vv2_l); \
+        \
+        CALC_RGB \
+        WRITE_##fmt(R, B) \
+        \
+        SETUP(buf, i * 2 + 8, vyalpha, v0, v1); \
+        vy_l = _mm_srai_epi32(v0, 10); \
+        vy_h = _mm_srai_epi32(v1, 10); \
+        \
+        hasAlpha( \
+            SETUP(abuf, i * 2 + 8, vyalpha, v0, v1); \
+            A_l = _mm_add_epi32(v0, add18); \
+            A_h = _mm_add_epi32(v1, add18); \
+        ) \
+        \
+        vu_l = _mm_unpacklo_epi32(vu2_h, vu2_h); \
+        vu_h = _mm_unpackhi_epi32(vu2_h, vu2_h); \
+        vv_l = _mm_unpacklo_epi32(vv2_h, vv2_h); \
+        vv_h = _mm_unpackhi_epi32(vv2_h, vv2_h); \
+        \
+        CALC_RGB \
+        WRITE_##fmt(R, B) \
+    } \
+}
+
+#define YUV2RGBWRAPPER1(ext, fmt, R, B, hasAlpha) \
+static void yuv2##ext##_1_e2k(SwsContext *c, const int16_t *buf0, \
+                              const int16_t *ubuf[2], const int16_t *vbuf[2], \
+                              const int16_t *abuf0, uint8_t *dest, int dstW, \
+                              int uvalpha, int y) \
+{ \
+    const int16_t *ubuf0 = ubuf[0], *vbuf0 = vbuf[0]; \
+    const int16_t *ubuf1 = ubuf[1], *vbuf1 = vbuf[1]; \
+    int uvshl = uvalpha < 2048 ? 2 : 1; \
+    vec_s32 vy_l, vy_h, vu_l, vu_h, vv_l, vv_h, vu2_l, vu2_h, vv2_l, vv2_h; \
+    vec_s32 R_l, R_h, G_l, G_h, B_l, B_h; \
+    vec_s32 y_coeff = _mm_set1_epi32(c->yuv2rgb_y_coeff * 4); \
+    vec_s32 y_add = _mm_set1_epi32((1 << 21) - c->yuv2rgb_y_offset * c->yuv2rgb_y_coeff); \
+    vec_s32 v2r_coeff = _mm_set1_epi32(c->yuv2rgb_v2r_coeff << uvshl); \
+    vec_s32 v2g_coeff = _mm_set1_epi32(c->yuv2rgb_v2g_coeff << uvshl); \
+    vec_s32 u2g_coeff = _mm_set1_epi32(c->yuv2rgb_u2g_coeff << uvshl); \
+    vec_s32 u2b_coeff = _mm_set1_epi32(c->yuv2rgb_u2b_coeff << uvshl); \
+    vec_u16 uvsub = _mm_set1_epi16(uvalpha < 2048 ? 128 << 7 : 128 << 8); \
+    hasAlpha(vec_s16 A, add64 = _mm_set1_epi16(64);) \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    int i; \
+    INIT_##fmt(R, B) \
+    \
+    for (i = 0; i < (dstW + 1) >> 1; i += 8) { \
+        v0 = VEC_LD(buf0 + i * 2); \
+        v2 = _mm_unpacklo_epi16(v0, v0); \
+        v3 = _mm_unpackhi_epi16(v0, v0); \
+        vy_l = _mm_srai_epi32(v2, 16); \
+        vy_h = _mm_srai_epi32(v3, 16); \
+        \
+        v0 = VEC_LD(ubuf0 + i); \
+        v1 = VEC_LD(vbuf0 + i); \
+        if (uvalpha >= 2048) { \
+            v2 = VEC_LD(ubuf1 + i); \
+            v3 = VEC_LD(vbuf1 + i); \
+            v0 = _mm_add_epi16(v0, v2); \
+            v1 = _mm_add_epi16(v1, v3); \
+        } \
+        v0 = _mm_sub_epi16(v0, uvsub); \
+        v1 = _mm_sub_epi16(v1, uvsub); \
+        v2 = _mm_unpacklo_epi16(v0, v0); \
+        v3 = _mm_unpackhi_epi16(v0, v0); \
+        vu2_l = _mm_srai_epi32(v2, 16); \
+        vu2_h = _mm_srai_epi32(v3, 16); \
+        v2 = _mm_unpacklo_epi16(v1, v1); \
+        v3 = _mm_unpackhi_epi16(v1, v1); \
+        vv2_l = _mm_srai_epi32(v2, 16); \
+        vv2_h = _mm_srai_epi32(v3, 16); \
+        \
+        hasAlpha( \
+            A_l = VEC_LD(abuf0 + i * 2); \
+            A_l = _mm_add_epi16(A_l, add64); \
+            A_l = _mm_srai_epi16(A_l, 7); \
+        ) \
+        \
+        vu_l = _mm_unpacklo_epi32(vu2_l, vu2_l); \
+        vu_h = _mm_unpackhi_epi32(vu2_l, vu2_l); \
+        vv_l = _mm_unpacklo_epi32(vv2_l, vv2_l); \
+        vv_h = _mm_unpackhi_epi32(vv2_l, vv2_l); \
+        \
+        CALC_RGB \
+        WRITE_##fmt(R, B) \
+        \
+        v0 = VEC_LD(buf0 + i * 2 + 8); \
+        v2 = _mm_unpacklo_epi16(v0, v0); \
+        v3 = _mm_unpackhi_epi16(v0, v0); \
+        vy_l = _mm_srai_epi32(v2, 16); \
+        vy_h = _mm_srai_epi32(v3, 16); \
+        \
+        hasAlpha( \
+            A_l = VEC_LD(abuf0 + i * 2 + 8); \
+            A_l = _mm_add_epi16(A_l, add64); \
+            A_l = _mm_srai_epi16(A_l, 7); \
+        ) \
+        \
+        vu_l = _mm_unpacklo_epi32(vu2_h, vu2_h); \
+        vu_h = _mm_unpackhi_epi32(vu2_h, vu2_h); \
+        vv_l = _mm_unpacklo_epi32(vv2_h, vv2_h); \
+        vv_h = _mm_unpackhi_epi32(vv2_h, vv2_h); \
+        \
+        CALC_RGB \
+        WRITE_##fmt(R, B) \
+    } \
+}
+
+#endif
+
+YUV2RGBWRAPPERX(rgbx32, RGBX, R, B, NO_ALPHA)
+YUV2RGBWRAPPERX(bgrx32, RGBX, B, R, NO_ALPHA)
+YUV2RGBWRAPPERX(xrgb32, XRGB, R, B, NO_ALPHA)
+YUV2RGBWRAPPERX(xbgr32, XRGB, B, R, NO_ALPHA)
+YUV2RGBWRAPPERX(rgb24, RGB, R, B, NO_ALPHA)
+YUV2RGBWRAPPERX(bgr24, RGB, B, R, NO_ALPHA)
+
+YUV2RGBWRAPPER2(rgbx32, RGBX, R, B, NO_ALPHA)
+YUV2RGBWRAPPER2(bgrx32, RGBX, B, R, NO_ALPHA)
+YUV2RGBWRAPPER2(xrgb32, XRGB, R, B, NO_ALPHA)
+YUV2RGBWRAPPER2(xbgr32, XRGB, B, R, NO_ALPHA)
+YUV2RGBWRAPPER2(rgb24, RGB, R, B, NO_ALPHA)
+YUV2RGBWRAPPER2(bgr24, RGB, B, R, NO_ALPHA)
+
+YUV2RGBWRAPPER1(rgbx32, RGBX, R, B, NO_ALPHA)
+YUV2RGBWRAPPER1(bgrx32, RGBX, B, R, NO_ALPHA)
+YUV2RGBWRAPPER1(xrgb32, XRGB, R, B, NO_ALPHA)
+YUV2RGBWRAPPER1(xbgr32, XRGB, B, R, NO_ALPHA)
+YUV2RGBWRAPPER1(rgb24, RGB, R, B, NO_ALPHA)
+YUV2RGBWRAPPER1(bgr24, RGB, B, R, NO_ALPHA)
+
+#define WRITE_422(vu, vv, x0, x1) \
+    vy0 = _mm_srai_epi32(vy0, 19); \
+    vy1 = _mm_srai_epi32(vy1, 19); \
+    vy2 = _mm_srai_epi32(vy2, 19); \
+    vy3 = _mm_srai_epi32(vy3, 19); \
+    vu##0 = _mm_srai_epi32(vu##0, 19); \
+    vu##1 = _mm_srai_epi32(vu##1, 19); \
+    vv##0 = _mm_srai_epi32(vv##0, 19 - 16); \
+    vv##1 = _mm_srai_epi32(vv##1, 19 - 16); \
+    v0 = _mm_packs_epi32(vy0, vy1); \
+    v1 = _mm_packs_epi32(vy2, vy3); \
+    v2 = _mm_blend_epi16(vu##0, vv##0, 0xaa); \
+    v3 = _mm_blend_epi16(vu##1, vv##1, 0xaa); \
+    v4 = _mm_packus_epi16(v0, v1); \
+    v5 = _mm_packus_epi16(v2, v3); \
+    v0 = _mm_unpacklo_epi8(x0, x1); \
+    v1 = _mm_unpackhi_epi8(x0, x1); \
+    VEC_ST(dest, v0); \
+    VEC_ST(dest + 16, v1); \
+    dest += 32;
+
+#define WRITE_YUYV422 WRITE_422(vu, vv, v4, v5)
+#define WRITE_YVYU422 WRITE_422(vv, vu, v4, v5)
+#define WRITE_UYVY422 WRITE_422(vu, vv, v5, v4)
+
+#define YUV2PACKEDWRAPPERX(ext, fmt) \
+static void yuv2##ext##_X_e2k(SwsContext *c, const int16_t *lumFilter, \
+                              const int16_t **lumSrc, int lumFilterSize, \
+                              const int16_t *chrFilter, const int16_t **chrUSrc, \
+                              const int16_t **chrVSrc, int chrFilterSize, \
+                              const int16_t **alpSrc, uint8_t *dest, int dstW, \
+                              int y) \
+{ \
+    int i, j; \
+    __m128i vy0, vy1, vy2, vy3, vu0, vu1, vv0, vv1; \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    vec_s32 start = _mm_set1_epi32(1 << 18); \
+    \
+    for (i = 0; i < (dstW + 1) >> 1; i += 8) { \
+        vy0 = vy1 = vy2 = vy3 = start; \
+        for (j = 0; j < lumFilterSize; j++) { \
+            v0 = VEC_LD(lumSrc[j] + i * 2); \
+            v1 = VEC_LD(lumSrc[j] + i * 2 + 8); \
+            v5 = _mm_set1_epi16(lumFilter[j]); \
+            v2 = _mm_mullo_epi16(v0, v5); \
+            v3 = _mm_mulhi_epi16(v0, v5); \
+            v4 = _mm_mullo_epi16(v1, v5); \
+            v5 = _mm_mulhi_epi16(v1, v5); \
+            v0 = _mm_unpacklo_epi16(v2, v3); \
+            v1 = _mm_unpackhi_epi16(v2, v3); \
+            v2 = _mm_unpacklo_epi16(v4, v5); \
+            v3 = _mm_unpackhi_epi16(v4, v5); \
+            vy0 = _mm_add_epi32(vy0, v0); \
+            vy1 = _mm_add_epi32(vy1, v1); \
+            vy2 = _mm_add_epi32(vy2, v2); \
+            vy3 = _mm_add_epi32(vy3, v3); \
+        } \
+        \
+        vu0 = vu1 = vv0 = vv1 = start; \
+        for (j = 0; j < chrFilterSize; j++) { \
+            v0 = VEC_LD(chrUSrc[j] + i); \
+            v1 = VEC_LD(chrVSrc[j] + i); \
+            v5 = _mm_set1_epi16(chrFilter[j]); \
+            v2 = _mm_mullo_epi16(v0, v5); \
+            v3 = _mm_mulhi_epi16(v0, v5); \
+            v4 = _mm_mullo_epi16(v1, v5); \
+            v5 = _mm_mulhi_epi16(v1, v5); \
+            v0 = _mm_unpacklo_epi16(v2, v3); \
+            v1 = _mm_unpackhi_epi16(v2, v3); \
+            v2 = _mm_unpacklo_epi16(v4, v5); \
+            v3 = _mm_unpackhi_epi16(v4, v5); \
+            vu0 = _mm_add_epi32(vu0, v0); \
+            vu1 = _mm_add_epi32(vu1, v1); \
+            vv0 = _mm_add_epi32(vv0, v2); \
+            vv1 = _mm_add_epi32(vv1, v3); \
+        } \
+        \
+        WRITE_##fmt##422 \
+    } \
+}
+
+#define YUV2PACKEDWRAPPER2(ext, fmt) \
+static void yuv2##ext##_2_e2k(SwsContext *c, const int16_t *buf[2], \
+                              const int16_t *ubuf[2], const int16_t *vbuf[2], \
+                              const int16_t *abuf[2], uint8_t *dest, int dstW, \
+                              int yalpha, int uvalpha, int y) \
+{ \
+    const int16_t *buf0 = buf[0], *buf1 = buf[1], \
+                  *ubuf0 = ubuf[0], *ubuf1 = ubuf[1], \
+                  *vbuf0 = vbuf[0], *vbuf1 = vbuf[1]; \
+    vec_s16 vyalpha = _mm_set1_epi32(4096 + yalpha * 0xffff); \
+    vec_s16 vuvalpha = _mm_set1_epi32(4096 + uvalpha * 0xffff); \
+    __m128i vy0, vy1, vy2, vy3, vu0, vu1, vv0, vv1; \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    int i; \
+    av_assert2(yalpha <= 4096U); \
+    av_assert2(uvalpha <= 4096U); \
+    \
+    for (i = 0; i < (dstW + 1) >> 1; i += 8) { \
+        SETUP(buf, i * 2, vyalpha, vy0, vy1); \
+        SETUP(buf, i * 2 + 8, vyalpha, vy2, vy3); \
+        SETUP(ubuf, i, vuvalpha, vu0, vu1); \
+        SETUP(vbuf, i, vuvalpha, vv0, vv1); \
+        \
+        WRITE_##fmt##422 \
+    } \
+}
+
+#define INIT1_422 __m128i blenduv = _mm_set1_epi16(255);
+
+#define WRITE1_422(vu, vv, x0, x1) \
+    v5 = _mm_srli_epi16(vu, 8); \
+    v4 = _mm_packus_epi16(vy0, vy1); \
+    v5 = _mm_blendv_epi8(vv, v5, blenduv); \
+    v0 = _mm_unpacklo_epi8(x0, x1); \
+    v1 = _mm_unpackhi_epi8(x0, x1); \
+    VEC_ST(dest, v0); \
+    VEC_ST(dest + 16, v1); \
+    dest += 32;
+
+#define WRITE1_YUYV422 WRITE1_422(vu, vv, v4, v5)
+#define WRITE1_YVYU422 WRITE1_422(vv, vu, v4, v5)
+#define WRITE1_UYVY422 WRITE1_422(vu, vv, v5, v4)
+
+#define YUV2PACKEDWRAPPER1(ext, fmt) \
+static void yuv2##ext##_1_e2k(SwsContext *c, const int16_t *buf0, \
+                              const int16_t *ubuf[2], const int16_t *vbuf[2], \
+                              const int16_t *abuf0, uint8_t *dest, int dstW, \
+                              int uvalpha, int y) \
+{ \
+    const int16_t *ubuf0 = ubuf[0], *vbuf0 = vbuf[0]; \
+    vec_s16 vy0, vy1, vu, vv; \
+    vec_s16 add64 = _mm_set1_epi16(64); \
+    int i; \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    LOAD_ZERO; \
+    INIT1_422 \
+    \
+    if (uvalpha < 2048) { \
+        for (i = 0; i < (dstW + 1) >> 1; i += 8) { \
+            vy0 = VEC_LD(buf0 + i * 2); \
+            vy1 = VEC_LD(buf0 + i * 2 + 8); \
+            vu = VEC_LD(ubuf0 + i); \
+            vv = VEC_LD(vbuf0 + i); \
+            vy0 = _mm_adds_epi16(vy0, add64); \
+            vy1 = _mm_adds_epi16(vy1, add64); \
+            vu = _mm_max_epi16(vu, zerov); \
+            vv = _mm_max_epi16(vv, zerov); \
+            vy0 = _mm_srai_epi16(vy0, 7); \
+            vy1 = _mm_srai_epi16(vy1, 7); \
+            vu = _mm_add_epi16(vu, add64); \
+            vv = _mm_add_epi16(vv, add64); \
+            vu = _mm_adds_epu16(vu, vu); \
+            vv = _mm_adds_epu16(vv, vv); \
+            \
+            WRITE1_##fmt##422 \
+        } \
+    } else { \
+        const int16_t *ubuf1 = ubuf[1], *vbuf1 = vbuf[1]; \
+        vec_s16 add128 = _mm_add_epi16(add64, add64); \
+        for (i = 0; i < (dstW + 1) >> 1; i += 8) { \
+            vy0 = VEC_LD(buf0 + i * 2); \
+            vy1 = VEC_LD(buf0 + i * 2 + 8); \
+            v0 = VEC_LD(ubuf0 + i); \
+            v1 = VEC_LD(vbuf0 + i); \
+            v2 = VEC_LD(ubuf1 + i); \
+            v3 = VEC_LD(vbuf1 + i); \
+            vy0 = _mm_adds_epi16(vy0, add64); \
+            vy1 = _mm_adds_epi16(vy1, add64); \
+            v0 = _mm_max_epi16(v0, zerov); \
+            v1 = _mm_max_epi16(v1, zerov); \
+            v2 = _mm_max_epi16(v2, zerov); \
+            v3 = _mm_max_epi16(v3, zerov); \
+            v0 = _mm_add_epi16(v0, add128); \
+            v1 = _mm_add_epi16(v1, add128); \
+            vy0 = _mm_srai_epi16(vy0, 7); \
+            vy1 = _mm_srai_epi16(vy1, 7); \
+            vu = _mm_adds_epu16(v0, v2); \
+            vv = _mm_adds_epu16(v1, v3); \
+            \
+            WRITE1_##fmt##422 \
+        } \
+    } \
+}
+
+YUV2PACKEDWRAPPERX(yuyv422, YUYV)
+YUV2PACKEDWRAPPERX(yvyu422, YVYU)
+YUV2PACKEDWRAPPERX(uyvy422, UYVY)
+
+YUV2PACKEDWRAPPER2(yuyv422, YUYV)
+YUV2PACKEDWRAPPER2(yvyu422, YVYU)
+YUV2PACKEDWRAPPER2(uyvy422, UYVY)
+
+YUV2PACKEDWRAPPER1(yuyv422, YUYV)
+YUV2PACKEDWRAPPER1(yvyu422, YVYU)
+YUV2PACKEDWRAPPER1(uyvy422, UYVY)
+
+#define HSCALE_INIT() \
+    __m128i v0, v1, v2, v3, v4, v5, v6; \
+    vec_u32 vadd = _mm_setr_epi32(0, xInc, xInc * 2, xInc * 3); \
+    vec_u16 vadd16 = _mm_setr_epi16(0, xInc, xInc * 2, xInc * 3, \
+        xInc * 4, xInc * 5, xInc * 6, xInc * 7)
+
+#define HSCALE1() \
+    v4 = _mm_set1_epi16(xpos); \
+    v5 = _mm_set1_epi16(xpos + xInc * 8); \
+    v4 = _mm_add_epi16(v4, vadd16); \
+    v5 = _mm_add_epi16(v5, vadd16); \
+    v4 = _mm_srli_epi16(v4, 9); \
+    v5 = _mm_srli_epi16(v5, 9); \
+    \
+    v0 = _mm_set1_epi32(xpos & 0xffff); \
+    v1 = _mm_set1_epi32((xpos & 0xffff) + xInc * 4); \
+    v2 = _mm_set1_epi32((xpos & 0xffff) + xInc * 8); \
+    v3 = _mm_set1_epi32((xpos & 0xffff) + xInc * 12); \
+    v0 = _mm_add_epi32(v0, vadd); \
+    v1 = _mm_add_epi32(v1, vadd); \
+    v2 = _mm_add_epi32(v2, vadd); \
+    v3 = _mm_add_epi32(v3, vadd); \
+    v0 = _mm_srli_epi32(v0, 16); \
+    v1 = _mm_srli_epi32(v1, 16); \
+    v2 = _mm_srli_epi32(v2, 16); \
+    v3 = _mm_srli_epi32(v3, 16); \
+    v0 = _mm_packs_epi32(v0, v1); \
+    v2 = _mm_packs_epi32(v2, v3); \
+    v6 = _mm_packus_epi16(v0, v2); \
+    \
+    xx = xpos >> 16
+
+static void hyscale_fast_e2k(SwsContext *c, int16_t *dst, int dstWidth,
+                           const uint8_t *src, int srcW, int xInc)
+{
+    int i, xpos = 0, xx, a1;
+    LOAD_ZERO;
+    HSCALE_INIT();
+
+    for (i = 0; i < dstWidth; i += 16) {
+        HSCALE1();
+
+        v1 = VEC_LD(src + xx);
+        v3 = VEC_LD(src + xx + 1);
+
+        v1 = _mm_shuffle_epi8(v1, v6);
+        v3 = _mm_shuffle_epi8(v3, v6);
+        v0 = _mm_unpacklo_epi8(v1, zerov);
+        v1 = _mm_unpackhi_epi8(v1, zerov);
+        v2 = _mm_unpacklo_epi8(v3, zerov);
+        v3 = _mm_unpackhi_epi8(v3, zerov);
+        v2 = _mm_sub_epi16(v2, v0);
+        v3 = _mm_sub_epi16(v3, v1);
+        v0 = _mm_slli_epi16(v0, 7);
+        v1 = _mm_slli_epi16(v1, 7);
+        v2 = _mm_mullo_epi16(v2, v4);
+        v3 = _mm_mullo_epi16(v3, v5);
+        v0 = _mm_add_epi16(v0, v2);
+        v1 = _mm_add_epi16(v1, v3);
+
+        VEC_ST(dst + i, v0);
+        VEC_ST(dst + i + 8, v1);
+        xpos += xInc * 16;
+    }
+
+    a1 = src[srcW - 1] * 128;
+    for (i = dstWidth - 1; (i * xInc) >> 16 >= srcW - 1; i--)
+        dst[i] = a1;
+}
+
+#define HSCALE2() \
+    v0 = _mm_shuffle_epi8(v0, v6); \
+    v1 = _mm_shuffle_epi8(v1, v6); \
+    v2 = _mm_unpacklo_epi8(v0, v1); \
+    v3 = _mm_unpackhi_epi8(v0, v1); \
+    v2 = _mm_maddubs_epi16(v2, v4); \
+    v3 = _mm_maddubs_epi16(v3, v5)
+
+static void hcscale_fast_e2k(SwsContext *c, int16_t *dst1, int16_t *dst2,
+                           int dstWidth, const uint8_t *src1,
+                           const uint8_t *src2, int srcW, int xInc)
+{
+    int i, xpos = 0, xx, a1, a2;
+    HSCALE_INIT();
+    __m128i xorv = _mm_set1_epi8(127);
+
+    for (i = 0; i < dstWidth; i += 16) {
+        HSCALE1();
+
+        v0 = _mm_packus_epi16(v4, v5);
+        v1 = _mm_xor_si128(v0, xorv);
+        v4 = _mm_unpacklo_epi8(v1, v0);
+        v5 = _mm_unpackhi_epi8(v1, v0);
+
+        v0 = VEC_LD(src1 + xx);
+        v1 = VEC_LD(src1 + xx + 1);
+        HSCALE2();
+        v0 = VEC_LD(src2 + xx);
+        v1 = VEC_LD(src2 + xx + 1);
+        VEC_ST(dst1 + i, v2);
+        VEC_ST(dst1 + i + 8, v3);
+        HSCALE2();
+        VEC_ST(dst2 + i, v2);
+        VEC_ST(dst2 + i + 8, v3);
+        xpos += xInc * 16;
+    }
+
+    a1 = src1[srcW - 1] * 128;
+    a2 = src2[srcW - 1] * 128;
+    for (i = dstWidth - 1; (i * xInc) >> 16 >= srcW - 1; i--) {
+        dst1[i] = a1;
+        dst2[i] = a2;
+    }
+}
+
+static void hScale8To19_e2k(SwsContext *c, int16_t *_dst, int dstW,
+                            const uint8_t *src, const int16_t *filter,
+                            const int32_t *filterPos, int filterSize)
+{
+    int i, j;
+    int32_t *dst = (int32_t*)_dst;
+    LOAD_ZERO;
+    __m128i v0, v1, accv;
+
+    if (filterSize == 1) {
+        for (i = 0; i < dstW; i++, filter += filterSize) {
+            int val = 0, srcPos = filterPos[i];
+            for (j = 0; j < filterSize; j++) {
+                val += (int)src[srcPos + j] * filter[j];
+            }
+            dst[i] = FFMIN(val >> 3, (1 << 19) - 1); // the cubic equation does overflow ...
+        }
+    } else {
+        __m64 h0, maskv;
+        uint64_t mask = ~0ll;
+        mask >>= (-filterSize & 7) * 8;
+        maskv = (__m64)mask; // 8, 1, 2, 3, 4, 5, 6, 7
+
+        for (i = 0; i < dstW; i++, filter += filterSize) {
+            int val;
+            const uint8_t *srci = src + filterPos[i];
+            accv = zerov;
+            for (j = 0; j + 7 < filterSize; j += 8) {
+                v0 = VEC_LD8(srci + j);
+                v1 = VEC_LD(filter + j);
+                v0 = _mm_unpacklo_epi8(v0, zerov);
+                accv = _mm_add_epi32(accv, _mm_madd_epi16(v0, v1));
+            }
+            if (filterSize & 7) {
+                h0 = *(__m64*)(srci + j);
+                // Remove the unused elements on the last round
+                h0 = _mm_and_si64(h0, maskv);
+                v0 = _mm_movpi64_epi64(h0);
+                v1 = VEC_LD(filter + j);
+                v0 = _mm_unpacklo_epi8(v0, zerov);
+                accv = _mm_add_epi32(accv, _mm_madd_epi16(v0, v1));
+            }
+            accv = _mm_hadd_epi32(accv, accv);
+            val = _mm_extract_epi32(accv, 0) + _mm_extract_epi32(accv, 1);
+            dst[i] = FFMIN(val >> 3, (1 << 19) - 1);
+        }
+    }
+}
+
+static void hScale16To19_e2k(SwsContext *c, int16_t *_dst, int dstW,
+                             const uint8_t *_src, const int16_t *filter,
+                             const int32_t *filterPos, int filterSize)
+{
+    const AVPixFmtDescriptor *desc = av_pix_fmt_desc_get(c->srcFormat);
+    int i, j;
+    int32_t *dst = (int32_t*)_dst;
+    const uint16_t *src = (const uint16_t*)_src;
+    int bits = desc->comp[0].depth - 1;
+    int sh = bits - 4;
+    LOAD_ZERO;
+    __m128i v0, v1, accv;
+
+    if ((isAnyRGB(c->srcFormat) || c->srcFormat == AV_PIX_FMT_PAL8) && desc->comp[0].depth < 16) {
+        sh = 9;
+    } else if (desc->flags & AV_PIX_FMT_FLAG_FLOAT) { /* float input are process like uint 16bpc */
+        sh = 16 - 1 - 4;
+    }
+
+    if (filterSize == 1) {
+        for (i = 0; i < dstW; i++) {
+            int val = 0, srcPos = filterPos[i];
+            for (j = 0; j < filterSize; j++) {
+                val += (int)src[srcPos + j] * filter[filterSize * i + j];
+            }
+            // filter=14 bit, input=16 bit, output=30 bit, >> 11 makes 19 bit
+            dst[i] = FFMIN(val >> sh, (1 << 19) - 1);
+        }
+    } else {
+        __m128i maskv, signv = _mm_set1_epi16(-0x8000), initv = zerov;
+        uint64_t mask = ~0ll;
+        mask >>= (-filterSize & 7) * 8;
+        maskv = _mm_movpi64_epi64((__m64)mask);
+        maskv = _mm_unpacklo_epi8(maskv, maskv);
+
+        for (j = 0; j + 7 < filterSize; j += 8) {
+            v1 = VEC_LD(filter + j);
+            initv = _mm_sub_epi32(initv, _mm_madd_epi16(signv, v1));
+        }
+        if (filterSize & 7) {
+            v1 = VEC_LD(filter + j);
+            v1 = _mm_and_si128(v1, maskv);
+            initv = _mm_sub_epi32(initv, _mm_madd_epi16(signv, v1));
+        }
+
+        for (i = 0; i < dstW; i++, filter += filterSize) {
+            int val;
+            const int16_t *srci = src + filterPos[i];
+            accv = initv;
+            for (j = 0; j + 7 < filterSize; j += 8) {
+                v0 = VEC_LD(srci + j);
+                v0 = _mm_xor_si128(v0, signv);
+                v1 = VEC_LD(filter + j);
+                accv = _mm_add_epi32(accv, _mm_madd_epi16(v0, v1));
+            }
+            if (filterSize & 7) {
+                v0 = VEC_LD(srci + j);
+                v0 = _mm_xor_si128(v0, signv);
+                v1 = VEC_LD(filter + j);
+                // Remove the unused elements on the last round
+                v1 = _mm_and_si128(v1, maskv);
+                accv = _mm_add_epi32(accv, _mm_madd_epi16(v0, v1));
+            }
+            accv = _mm_hadd_epi32(accv, accv);
+            val = _mm_extract_epi32(accv, 0) + _mm_extract_epi32(accv, 1);
+            dst[i] = FFMIN(val >> sh, (1 << 19) - 1);
+        }
+    }
+}
+
+static void hScale16To15_e2k(SwsContext *c, int16_t *dst, int dstW,
+                             const uint8_t *_src, const int16_t *filter,
+                             const int32_t *filterPos, int filterSize)
+{
+    const AVPixFmtDescriptor *desc = av_pix_fmt_desc_get(c->srcFormat);
+    int i, j;
+    const uint16_t *src = (const uint16_t*)_src;
+    int sh = desc->comp[0].depth - 1;
+    LOAD_ZERO;
+    __m128i v0, v1, accv;
+
+    if (sh < 15) {
+        sh = isAnyRGB(c->srcFormat) || c->srcFormat == AV_PIX_FMT_PAL8 ? 13 : (desc->comp[0].depth - 1);
+    } else if (desc->flags & AV_PIX_FMT_FLAG_FLOAT) { /* float input are process like uint 16bpc */
+        sh = 16 - 1;
+    }
+
+    if (filterSize == 1) {
+        for (i = 0; i < dstW; i++) {
+            int val = 0, srcPos = filterPos[i];
+            for (j = 0; j < filterSize; j++) {
+                val += (int)src[srcPos + j] * filter[filterSize * i + j];
+            }
+            // filter=14 bit, input=16 bit, output=30 bit, >> 15 makes 15 bit
+            dst[i] = FFMIN(val >> sh, (1 << 15) - 1);
+        }
+    } else {
+        __m128i maskv, signv = _mm_set1_epi16(-0x8000), initv = zerov;
+        uint64_t mask = ~0ll;
+        mask >>= (-filterSize & 7) * 8;
+        maskv = _mm_movpi64_epi64((__m64)mask);
+        maskv = _mm_unpacklo_epi8(maskv, maskv);
+
+        for (j = 0; j + 7 < filterSize; j += 8) {
+            v1 = VEC_LD(filter + j);
+            initv = _mm_sub_epi32(initv, _mm_madd_epi16(signv, v1));
+        }
+        if (filterSize & 7) {
+            v1 = VEC_LD(filter + j);
+            v1 = _mm_and_si128(v1, maskv);
+            initv = _mm_sub_epi32(initv, _mm_madd_epi16(signv, v1));
+        }
+
+        for (i = 0; i < dstW; i++, filter += filterSize) {
+            int val;
+            const int16_t *srci = src + filterPos[i];
+            accv = initv;
+            for (j = 0; j + 7 < filterSize; j += 8) {
+                v0 = VEC_LD(srci + j);
+                v0 = _mm_xor_si128(v0, signv);
+                v1 = VEC_LD(filter + j);
+                accv = _mm_add_epi32(accv, _mm_madd_epi16(v0, v1));
+            }
+            if (filterSize & 7) {
+                v0 = VEC_LD(srci + j);
+                v0 = _mm_xor_si128(v0, signv);
+                // Remove the unused elements on the last round
+                v1 = VEC_LD(filter + j);
+                v1 = _mm_and_si128(v1, maskv);
+                accv = _mm_add_epi32(accv, _mm_madd_epi16(v0, v1));
+            }
+            accv = _mm_hadd_epi32(accv, accv);
+            val = _mm_extract_epi32(accv, 0) + _mm_extract_epi32(accv, 1);
+            dst[i] = FFMIN(val >> sh, (1 << 15) - 1);
+        }
+    }
+}
+
+av_cold void ff_sws_init_swscale_e2k(SwsContext *c)
+{
+    enum AVPixelFormat dstFormat = c->dstFormat;
+    const AVPixFmtDescriptor *desc = av_pix_fmt_desc_get(dstFormat);
+
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    if (isSemiPlanarYUV(dstFormat) && isDataInHighBits(dstFormat)) {
+        // ...
+    } else if (is16BPS(dstFormat)) {
+        c->yuv2planeX = isBE(dstFormat) ? yuv2planeX_16BE_e2k  : yuv2planeX_16LE_e2k;
+        c->yuv2plane1 = isBE(dstFormat) ? yuv2plane1_16BE_e2k  : yuv2plane1_16LE_e2k;
+        if (isSemiPlanarYUV(dstFormat)) {
+          // c->yuv2nv12cX = isBE(dstFormat) ? yuv2nv12cX_16BE_e2k : yuv2nv12cX_16LE_e2k;
+        }
+    } else if (isNBPS(dstFormat)) {
+        if (desc->comp[0].depth == 9) {
+            c->yuv2planeX = isBE(dstFormat) ? yuv2planeX_9BE_e2k  : yuv2planeX_9LE_e2k;
+            c->yuv2plane1 = isBE(dstFormat) ? yuv2plane1_9BE_e2k  : yuv2plane1_9LE_e2k;
+        } else if (desc->comp[0].depth == 10) {
+            c->yuv2planeX = isBE(dstFormat) ? yuv2planeX_10BE_e2k  : yuv2planeX_10LE_e2k;
+            c->yuv2plane1 = isBE(dstFormat) ? yuv2plane1_10BE_e2k  : yuv2plane1_10LE_e2k;
+        } else if (desc->comp[0].depth == 12) {
+            c->yuv2planeX = isBE(dstFormat) ? yuv2planeX_12BE_e2k  : yuv2planeX_12LE_e2k;
+            c->yuv2plane1 = isBE(dstFormat) ? yuv2plane1_12BE_e2k  : yuv2plane1_12LE_e2k;
+        } else if (desc->comp[0].depth == 14) {
+            c->yuv2planeX = isBE(dstFormat) ? yuv2planeX_14BE_e2k  : yuv2planeX_14LE_e2k;
+            c->yuv2plane1 = isBE(dstFormat) ? yuv2plane1_14BE_e2k  : yuv2plane1_14LE_e2k;
+        } else
+            av_assert0(0);
+    } else if (dstFormat == AV_PIX_FMT_GRAYF32BE) {
+        // c->yuv2planeX = yuv2planeX_floatBE_e2k;
+        c->yuv2plane1 = yuv2plane1_floatBE_e2k;
+    } else if (dstFormat == AV_PIX_FMT_GRAYF32LE) {
+        // c->yuv2planeX = yuv2planeX_floatLE_e2k;
+        c->yuv2plane1 = yuv2plane1_floatLE_e2k;
+    } else {
+        c->yuv2plane1 = yuv2plane1_8_e2k;
+        c->yuv2planeX = yuv2planeX_8_e2k;
+#if 0
+        if (isSemiPlanarYUV(dstFormat))
+            c->yuv2nv12cX = yuv2nv12cX_e2k;
+#endif
+    }
+
+    if (c->srcBpc == 8) {
+        if (c->dstBpc <= 14) {
+            c->hyScale = c->hcScale = hScale_real_e2k;
+            if (c->flags & SWS_FAST_BILINEAR && c->dstW >= c->srcW && c->chrDstW >= c->chrSrcW) {
+                c->hyscale_fast = hyscale_fast_e2k;
+                c->hcscale_fast = hcscale_fast_e2k;
+            }
+        } else {
+            c->hyScale = c->hcScale = hScale8To19_e2k;
+        }
+    } else {
+            c->hyScale = c->hcScale = c->dstBpc > 14 ? hScale16To19_e2k
+                                                     : hScale16To15_e2k;
+    }
+
+    if (c->flags & SWS_FULL_CHR_H_INT) {
+        switch (dstFormat) {
+            case AV_PIX_FMT_RGB24:
+                c->yuv2packed1 = yuv2rgb24_full_1_e2k;
+                c->yuv2packed2 = yuv2rgb24_full_2_e2k;
+                c->yuv2packedX = yuv2rgb24_full_X_e2k;
+            break;
+            case AV_PIX_FMT_BGR24:
+                c->yuv2packed1 = yuv2bgr24_full_1_e2k;
+                c->yuv2packed2 = yuv2bgr24_full_2_e2k;
+                c->yuv2packedX = yuv2bgr24_full_X_e2k;
+            break;
+            case AV_PIX_FMT_BGRA:
+                if (!c->needAlpha) {
+                    c->yuv2packed1 = yuv2bgrx32_full_1_e2k;
+                    c->yuv2packed2 = yuv2bgrx32_full_2_e2k;
+                    c->yuv2packedX = yuv2bgrx32_full_X_e2k;
+                }
+            break;
+            case AV_PIX_FMT_RGBA:
+                if (!c->needAlpha) {
+                    c->yuv2packed1 = yuv2rgbx32_full_1_e2k;
+                    c->yuv2packed2 = yuv2rgbx32_full_2_e2k;
+                    c->yuv2packedX = yuv2rgbx32_full_X_e2k;
+                }
+            break;
+            case AV_PIX_FMT_ARGB:
+                if (!c->needAlpha) {
+                    c->yuv2packed1 = yuv2xrgb32_full_1_e2k;
+                    c->yuv2packed2 = yuv2xrgb32_full_2_e2k;
+                    c->yuv2packedX = yuv2xrgb32_full_X_e2k;
+                }
+            break;
+            case AV_PIX_FMT_ABGR:
+                if (!c->needAlpha) {
+                    c->yuv2packed1 = yuv2xbgr32_full_1_e2k;
+                    c->yuv2packed2 = yuv2xbgr32_full_2_e2k;
+                    c->yuv2packedX = yuv2xbgr32_full_X_e2k;
+                 }
+            break;
+        }
+    } else if (!(c->flags & SWS_BITEXACT)) { /* !SWS_FULL_CHR_H_INT */
+        switch (dstFormat) {
+            case AV_PIX_FMT_RGB24:
+                c->yuv2packed1 = yuv2rgb24_1_e2k;
+                c->yuv2packed2 = yuv2rgb24_2_e2k;
+                c->yuv2packedX = yuv2rgb24_X_e2k;
+            break;
+            case AV_PIX_FMT_BGR24:
+                c->yuv2packed1 = yuv2bgr24_1_e2k;
+                c->yuv2packed2 = yuv2bgr24_2_e2k;
+                c->yuv2packedX = yuv2bgr24_X_e2k;
+            break;
+            case AV_PIX_FMT_BGRA:
+                if (!c->needAlpha) {
+                    c->yuv2packed1 = yuv2bgrx32_1_e2k;
+                    c->yuv2packed2 = yuv2bgrx32_2_e2k;
+                    c->yuv2packedX = yuv2bgrx32_X_e2k;
+                }
+            break;
+            case AV_PIX_FMT_RGBA:
+                if (!c->needAlpha) {
+                    c->yuv2packed1 = yuv2rgbx32_1_e2k;
+                    c->yuv2packed2 = yuv2rgbx32_2_e2k;
+                    c->yuv2packedX = yuv2rgbx32_X_e2k;
+                }
+            break;
+            case AV_PIX_FMT_ARGB:
+                if (!c->needAlpha) {
+                    c->yuv2packed1 = yuv2xrgb32_1_e2k;
+                    c->yuv2packed2 = yuv2xrgb32_2_e2k;
+                    c->yuv2packedX = yuv2xrgb32_X_e2k;
+                }
+            break;
+            case AV_PIX_FMT_ABGR:
+                if (!c->needAlpha) {
+                    c->yuv2packed1 = yuv2xbgr32_1_e2k;
+                    c->yuv2packed2 = yuv2xbgr32_2_e2k;
+                    c->yuv2packedX = yuv2xbgr32_X_e2k;
+                }
+            break;
+        }
+    }
+
+    switch (dstFormat) {
+        case AV_PIX_FMT_YUYV422:
+            c->yuv2packed1 = yuv2yuyv422_1_e2k;
+            c->yuv2packed2 = yuv2yuyv422_2_e2k;
+            c->yuv2packedX = yuv2yuyv422_X_e2k;
+        break;
+        case AV_PIX_FMT_YVYU422:
+            c->yuv2packed1 = yuv2yvyu422_1_e2k;
+            c->yuv2packed2 = yuv2yvyu422_2_e2k;
+            c->yuv2packedX = yuv2yvyu422_X_e2k;
+        break;
+        case AV_PIX_FMT_UYVY422:
+            c->yuv2packed1 = yuv2uyvy422_1_e2k;
+            c->yuv2packed2 = yuv2uyvy422_2_e2k;
+            c->yuv2packedX = yuv2uyvy422_X_e2k;
+        break;
+    }
+}
diff --git a/libswscale/e2k/yuv2rgb.c b/libswscale/e2k/yuv2rgb.c
new file mode 100644
index 0000000..20074f9
--- /dev/null
+++ b/libswscale/e2k/yuv2rgb.c
@@ -0,0 +1,248 @@
+/*
+ * Elbrus acceleration for colorspace conversion
+ *
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * copyright (C) 2004 Marc Hoffman <marc.hoffman@analog.com>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <inttypes.h>
+
+#include "config.h"
+#include "libswscale/rgb2rgb.h"
+#include "libswscale/swscale.h"
+#include "libswscale/swscale_internal.h"
+#include "libavutil/attributes.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+#include "libavutil/pixdesc.h"
+#include "yuv2rgb.h"
+
+/*
+ * ------------------------------------------------------------------------------
+ * CS converters
+ * ------------------------------------------------------------------------------
+ */
+
+#define INIT2_RGB(R, B) \
+    __m128i perm_unp8 = _mm_setr_epi8( \
+        0, 8, 1, 9, 2, 10, 3, 11, 4, 12, 5, 13, 6, 14, 7, 15); \
+    __m64 rgb_index0 = _mm_setr_pi8(0, 1, 3, 4, 5, 7, 8, 9); \
+    __m64 rgb_index1 = _mm_setr_pi8(3, 4, 5, 7, 8, 9, 11, 12); \
+    __m64 rgb_index2 = _mm_setr_pi8(5, 7, 8, 9, 11, 12, 13, 15);
+
+#define INIT2_RGBX(R, B) INIT2_XRGB(R, B)
+#define INIT2_XRGB(R, B) \
+    __m128i A_l = _mm_set1_epi16(255); \
+    __m128i perm_unp8 = _mm_setr_epi8( \
+        0, 8, 1, 9, 2, 10, 3, 11, 4, 12, 5, 13, 6, 14, 7, 15);
+
+#define WRITE2_RGB(dest, R, B) \
+    v4 = _mm_packus_epi16(R##_l, G_l); \
+    v5 = _mm_packus_epi16(B##_l, B##_l); \
+    v0 = _mm_shuffle_epi8(v4, perm_unp8); \
+    v1 = _mm_unpacklo_epi8(v5, v5); \
+    v2 = _mm_unpacklo_epi16(v0, v1); \
+    v3 = _mm_unpackhi_epi16(v0, v1); \
+    { \
+        union { __m128i v; __m64 d[2]; } a = { v2 }, b = { v3 }; \
+        __m64 *p = (__m64*)dest; \
+        p[0] = _mm_shuffle2_pi8(a.d[0], a.d[1], rgb_index0); \
+        p[1] = _mm_shuffle2_pi8(a.d[1], b.d[0], rgb_index1); \
+        p[2] = _mm_shuffle2_pi8(b.d[0], b.d[1], rgb_index2); \
+        dest += 24; \
+    }
+
+#define WRITE2_RGBX(dest, R, B) \
+    v4 = _mm_packus_epi16(R##_l, G_l); \
+    v5 = _mm_packus_epi16(B##_l, A_l); \
+    v0 = _mm_shuffle_epi8(v4, perm_unp8); \
+    v1 = _mm_shuffle_epi8(v5, perm_unp8); \
+    v2 = _mm_unpacklo_epi16(v0, v1); \
+    v3 = _mm_unpackhi_epi16(v0, v1); \
+    VEC_ST(dest, v2); \
+    VEC_ST(dest + 16, v3); \
+    dest += 32;
+
+#define WRITE2_XRGB(dest, R, B) \
+    v4 = _mm_packus_epi16(A_l, R##_l); \
+    v5 = _mm_packus_epi16(G_l, B##_l); \
+    v0 = _mm_shuffle_epi8(v4, perm_unp8); \
+    v1 = _mm_shuffle_epi8(v5, perm_unp8); \
+    v2 = _mm_unpacklo_epi16(v0, v1); \
+    v3 = _mm_unpackhi_epi16(v0, v1); \
+    VEC_ST(dest, v2); \
+    VEC_ST(dest + 16, v3); \
+    dest += 32;
+
+#define DEFCSP420_CVT(name, fmt, R, B) \
+static int yuv2##name##_e2k(SwsContext *c, const unsigned char **in, \
+                      int *instrides, int srcSliceY, int srcSliceH, \
+                      unsigned char **oplanes, int *outstrides) \
+{ \
+    vec_s32 R_l, G_l, B_l; \
+    vec_s32 y_coeff = _mm_set1_epi16(c->yuv2rgb_y_coeff); \
+    vec_s32 y_sub = _mm_set1_epi16((c->yuv2rgb_y_offset + 64) >> 7); \
+    vec_s32 v2r_coeff = _mm_set1_epi16(c->yuv2rgb_v2r_coeff); \
+    vec_s32 v2g_coeff = _mm_set1_epi16(c->yuv2rgb_v2g_coeff); \
+    vec_s32 u2g_coeff = _mm_set1_epi16(c->yuv2rgb_u2g_coeff); \
+    vec_s32 u2b_coeff = _mm_set1_epi16(c->yuv2rgb_u2b_coeff); \
+    vec_s32 dec128 = _mm_set1_epi16(128); \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    LOAD_ZERO; \
+    INIT2_##fmt(R, B) \
+    int i, j, w = c->dstW & -16, h = srcSliceH & -2; \
+    vec_s16 Y0, Y1, Y2, Y3, U, V; \
+    vec_s16 vx, ux, uvx, vx0, ux0, uvx0, vx1, ux1, uvx1; \
+    const uint8_t *y1i = in[0]; \
+    const uint8_t *y2i = in[0] + instrides[0]; \
+    const uint8_t *ui = in[1], *vi = in[2]; \
+    uint8_t *out0, *out1; \
+    int vshift = c->srcFormat == AV_PIX_FMT_YUV422P; \
+    int instrides0 = instrides[0] * 2 - w; \
+    int instrides1 = (instrides[1] << vshift) - w / 2; \
+    int instrides2 = (instrides[2] << vshift) - w / 2; \
+    \
+    for (i = 0; i < h; i += 2) { \
+        out0 = oplanes[0] + (i + srcSliceY) * outstrides[0]; \
+        out1 = out0 + outstrides[0]; \
+        for (j = 0; j < w >> 4; j++) { \
+            Y1 = VEC_LD(y1i); \
+            Y3 = VEC_LD(y2i); \
+            U = VEC_LD8(ui); \
+            V = VEC_LD8(vi); \
+            U = _mm_unpacklo_epi8(U, zerov); \
+            V = _mm_unpacklo_epi8(V, zerov); \
+            Y0 = _mm_unpacklo_epi8(Y1, zerov); \
+            Y1 = _mm_unpackhi_epi8(Y1, zerov); \
+            Y2 = _mm_unpacklo_epi8(Y3, zerov); \
+            Y3 = _mm_unpackhi_epi8(Y3, zerov); \
+            U = _mm_sub_epi16(U, dec128); \
+            V = _mm_sub_epi16(V, dec128); \
+            U = _mm_slli_epi16(U, 2); \
+            V = _mm_slli_epi16(V, 2); \
+            Y0 = _mm_slli_epi16(Y0, 2); \
+            Y1 = _mm_slli_epi16(Y1, 2); \
+            Y2 = _mm_slli_epi16(Y2, 2); \
+            Y3 = _mm_slli_epi16(Y3, 2); \
+            \
+            Y0 = _mm_mulhrs_epi16(_mm_sub_epi16(Y0, y_sub), y_coeff); \
+            Y1 = _mm_mulhrs_epi16(_mm_sub_epi16(Y1, y_sub), y_coeff); \
+            Y2 = _mm_mulhrs_epi16(_mm_sub_epi16(Y2, y_sub), y_coeff); \
+            Y3 = _mm_mulhrs_epi16(_mm_sub_epi16(Y3, y_sub), y_coeff); \
+            \
+            ux = _mm_mulhrs_epi16(U, u2b_coeff); \
+            vx = _mm_mulhrs_epi16(V, v2r_coeff); \
+            ux0 = _mm_unpacklo_epi16(ux, ux); \
+            ux1 = _mm_unpackhi_epi16(ux, ux); \
+            vx0 = _mm_unpacklo_epi16(vx, vx); \
+            vx1 = _mm_unpackhi_epi16(vx, vx); \
+            \
+            uvx = _mm_mulhrs_epi16(U, u2g_coeff); \
+            uvx = _mm_add_epi16(_mm_mulhrs_epi16(V, v2g_coeff), uvx); \
+            uvx0 = _mm_unpacklo_epi16(uvx, uvx); \
+            uvx1 = _mm_unpackhi_epi16(uvx, uvx); \
+            \
+            R_l = _mm_add_epi16(Y0, vx0); \
+            G_l = _mm_add_epi16(Y0, uvx0); \
+            B_l = _mm_add_epi16(Y0, ux0); \
+            \
+            WRITE2_##fmt(out0, R, B) \
+            \
+            R_l = _mm_add_epi16(Y1, vx1); \
+            G_l = _mm_add_epi16(Y1, uvx1); \
+            B_l = _mm_add_epi16(Y1, ux1); \
+            \
+            WRITE2_##fmt(out0, R, B) \
+            \
+            R_l = _mm_add_epi16(Y2, vx0); \
+            G_l = _mm_add_epi16(Y2, uvx0); \
+            B_l = _mm_add_epi16(Y2, ux0); \
+            \
+            WRITE2_##fmt(out1, R, B) \
+            \
+            R_l = _mm_add_epi16(Y3, vx1); \
+            G_l = _mm_add_epi16(Y3, uvx1); \
+            B_l = _mm_add_epi16(Y3, ux1); \
+            \
+            WRITE2_##fmt(out1, R, B) \
+            \
+            y1i += 16; ui += 8; \
+            y2i += 16; vi += 8; \
+        } \
+        y1i += instrides0; ui += instrides1; \
+        y2i += instrides0; vi += instrides2; \
+    } \
+    return srcSliceH; \
+}
+
+DEFCSP420_CVT(rgbx32, RGBX, R, B)
+DEFCSP420_CVT(bgrx32, RGBX, B, R)
+DEFCSP420_CVT(xrgb32, XRGB, R, B)
+DEFCSP420_CVT(xbgr32, XRGB, B, R)
+DEFCSP420_CVT(rgb24, RGB, R, B)
+DEFCSP420_CVT(bgr24, RGB, B, R)
+
+/* Ok currently the acceleration routine only supports
+ * inputs of widths a multiple of 16
+ * and heights a multiple 2
+ *
+ * So we just fall back to the C codes for this.
+ */
+av_cold SwsFunc ff_yuv2rgb_init_e2k(SwsContext *c)
+{
+    SwsFunc ret;
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return NULL;
+
+    if (c->flags & SWS_BITEXACT || c->needAlpha)
+        return NULL;
+
+    ret = NULL;
+    switch (c->srcFormat) {
+    case AV_PIX_FMT_YUV422P:
+    case AV_PIX_FMT_YUV420P:
+        if (c->dstW & 15 || c->dstH & 1) break;
+        switch (c->dstFormat) {
+        case AV_PIX_FMT_RGB24:
+            ret = yuv2rgb24_e2k; break;
+        case AV_PIX_FMT_BGR24:
+            ret = yuv2bgr24_e2k; break;
+        case AV_PIX_FMT_ARGB:
+            ret = yuv2xrgb32_e2k; break;
+        case AV_PIX_FMT_ABGR:
+            ret = yuv2xbgr32_e2k; break;
+        case AV_PIX_FMT_RGBA:
+            ret = yuv2rgbx32_e2k; break;
+        case AV_PIX_FMT_BGRA:
+            ret = yuv2bgrx32_e2k; break;
+        default: break;
+        }
+        break;
+    }
+    if (ret) {
+        av_log(c, AV_LOG_WARNING, "E2K: yuv2rgb(%s, %s)\n",
+                av_get_pix_fmt_name(c->srcFormat),
+                av_get_pix_fmt_name(c->dstFormat));
+    }
+    return ret;
+}
+
diff --git a/libswscale/e2k/yuv2rgb.h b/libswscale/e2k/yuv2rgb.h
new file mode 100644
index 0000000..59637bc
--- /dev/null
+++ b/libswscale/e2k/yuv2rgb.h
@@ -0,0 +1,52 @@
+/*
+ * Elbrus-enhanced yuv2yuvX
+ *
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (C) 2004 Romain Dolbeau <romain@dolbeau.org>
+ * based on the equivalent C code in swscale.c
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef SWSCALE_E2K_YUV2RGB_H
+#define SWSCALE_E2K_YUV2RGB_H
+
+#include <stdint.h>
+
+#include "libswscale/swscale_internal.h"
+
+#define YUV2PACKEDX_HEADER(suffix)                              \
+    void ff_yuv2##suffix##_X_e2k(SwsContext *c,             \
+                                     const int16_t *lumFilter,  \
+                                     const int16_t **lumSrc,    \
+                                     int lumFilterSize,         \
+                                     const int16_t *chrFilter,  \
+                                     const int16_t **chrUSrc,   \
+                                     const int16_t **chrVSrc,   \
+                                     int chrFilterSize,         \
+                                     const int16_t **alpSrc,    \
+                                     uint8_t *dest,             \
+                                     int dstW, int dstY);
+
+YUV2PACKEDX_HEADER(abgr);
+YUV2PACKEDX_HEADER(bgra);
+YUV2PACKEDX_HEADER(argb);
+YUV2PACKEDX_HEADER(rgba);
+YUV2PACKEDX_HEADER(rgb24);
+YUV2PACKEDX_HEADER(bgr24);
+
+#endif /* SWSCALE_E2K_YUV2RGB_H */
diff --git a/libswscale/e2k/yuv2yuv.c b/libswscale/e2k/yuv2yuv.c
new file mode 100644
index 0000000..c31cfd8
--- /dev/null
+++ b/libswscale/e2k/yuv2yuv.c
@@ -0,0 +1,146 @@
+/*
+ * Elbrus-enhanced yuv-to-yuv conversion routines.
+ *
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (C) 2004 Romain Dolbeau <romain@dolbeau.org>
+ * based on the equivalent C code in swscale.c
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include <inttypes.h>
+
+#include "config.h"
+#include "libavutil/attributes.h"
+#include "libavutil/e2k/cpu.h"
+#include "libswscale/swscale.h"
+#include "libswscale/swscale_internal.h"
+#include "libavutil/e2k/util_e2k.h"
+
+/* This code assumes:
+ *
+ * 1) dst is 16 bytes-aligned
+ * 2) dstStride is a multiple of 16
+ * 3) width is a multiple of 16
+ * 4) lum & chrom stride are multiples of 8
+ */
+
+static int yv12toyuy2_unscaled_e2k(SwsContext *c, const uint8_t *src[],
+                                   int srcStride[], int srcSliceY,
+                                   int srcSliceH, uint8_t *dstParam[],
+                                   int dstStride_a[])
+{
+    const uint8_t *ysrc = src[0], *usrc = src[1], *vsrc = src[2];
+    int dstStride = dstStride_a[0];
+    uint8_t *dst = dstParam[0] + dstStride * srcSliceY;
+    int width = (c->dstW + 1) >> 1, height = srcSliceH;
+    int lumStride = srcStride[0];
+    int chromStride = srcStride[1];
+    int y, i;
+
+    for (y = 0; y < height; y++) {
+        PRAGMA_E2K("ivdep")
+        for (i = 0; i < width - 7; i += 8) {
+            __m128i v0, v1, v2, v3;
+            v0 = VEC_LD(ysrc + i * 2);
+            v2 = VEC_LD8(usrc + i);
+            v3 = VEC_LD8(vsrc + i);
+            v1 = _mm_unpacklo_epi8(v2, v3);
+            VEC_ST(dst + i * 4, _mm_unpacklo_epi8(v0, v1));
+            VEC_ST(dst + i * 4 + 16, _mm_unpackhi_epi8(v0, v1));
+        }
+
+        PRAGMA_E2K("ivdep")
+        for (; i < width; i++) {
+            *(uint32_t*)(dst + i * 4) =
+                ysrc[i * 2] | usrc[i] << 8 |
+                ysrc[i * 2 + 1] << 16 | vsrc[i] << 24;
+        }
+
+        if (y & 1) {
+            usrc += chromStride;
+            vsrc += chromStride;
+        }
+        ysrc += lumStride;
+        dst += dstStride;
+    }
+
+    return srcSliceH;
+}
+
+static int yv12touyvy_unscaled_e2k(SwsContext *c, const uint8_t *src[],
+                                   int srcStride[], int srcSliceY,
+                                   int srcSliceH, uint8_t *dstParam[],
+                                   int dstStride_a[])
+{
+    const uint8_t *ysrc = src[0], *usrc = src[1], *vsrc = src[2];
+    int dstStride = dstStride_a[0];
+    uint8_t *dst = dstParam[0] + dstStride * srcSliceY;
+    int width = (c->dstW + 1) >> 1, height = srcSliceH;
+    int lumStride = srcStride[0];
+    int chromStride = srcStride[1];
+    int y, i;
+
+    for (y = 0; y < height; y++) {
+        PRAGMA_E2K("ivdep")
+        for (i = 0; i < width - 7; i += 8) {
+            __m128i v0, v1, v2, v3;
+            v0 = VEC_LD(ysrc + i * 2);
+            v2 = VEC_LD8(usrc + i);
+            v3 = VEC_LD8(vsrc + i);
+            v1 = _mm_unpacklo_epi8(v2, v3);
+            VEC_ST(dst + i * 4, _mm_unpacklo_epi8(v1, v0));
+            VEC_ST(dst + i * 4 + 16, _mm_unpackhi_epi8(v1, v0));
+        }
+
+        PRAGMA_E2K("ivdep")
+        for (; i < width; i++) {
+            *(uint32_t*)(dst + i * 4) =
+                usrc[i] | ysrc[i * 2] << 8 |
+                vsrc[i] << 16 | ysrc[i * 2 + 1] << 24;
+        }
+
+        if (y & 1) {
+            usrc += chromStride;
+            vsrc += chromStride;
+        }
+        ysrc += lumStride;
+        dst += dstStride;
+    }
+    return srcSliceH;
+}
+
+av_cold void ff_get_unscaled_swscale_e2k(SwsContext *c)
+{
+    if (!E2K_SIMD(av_get_cpu_flags()))
+        return;
+
+    if (c->flags & SWS_BITEXACT)
+        return;
+
+    if (c->srcFormat == AV_PIX_FMT_YUV420P) {
+        enum AVPixelFormat dstFormat = c->dstFormat;
+        switch (dstFormat) {
+        case AV_PIX_FMT_YUYV422:
+            c->convert_unscaled = yv12toyuy2_unscaled_e2k;
+            break;
+        case AV_PIX_FMT_UYVY422:
+            c->convert_unscaled = yv12touyvy_unscaled_e2k;
+            break;
+        }
+    }
+}
diff --git a/libswscale/swscale.c b/libswscale/swscale.c
index 90e5b29..c2fc8e4 100644
--- a/libswscale/swscale.c
+++ b/libswscale/swscale.c
@@ -593,6 +593,8 @@ void ff_sws_init_scale(SwsContext *c)
 
 #if ARCH_PPC
     ff_sws_init_swscale_ppc(c);
+#elif ARCH_E2K
+    ff_sws_init_swscale_e2k(c);
 #elif ARCH_X86
     ff_sws_init_swscale_x86(c);
 #elif ARCH_AARCH64
diff --git a/libswscale/swscale_internal.h b/libswscale/swscale_internal.h
index abeebbb..d1263a4 100644
--- a/libswscale/swscale_internal.h
+++ b/libswscale/swscale_internal.h
@@ -34,7 +34,9 @@
 #include "libavutil/pixfmt.h"
 #include "libavutil/pixdesc.h"
 #include "libavutil/slicethread.h"
+#if HAVE_ALTIVEC
 #include "libavutil/ppc/util_altivec.h"
+#endif
 #include "libavutil/half2float.h"
 
 #define STR(s) AV_TOSTRING(s) // AV_STRINGIFY is too long
@@ -698,6 +700,7 @@ av_cold void ff_sws_init_range_convert(SwsContext *c);
 
 SwsFunc ff_yuv2rgb_init_x86(SwsContext *c);
 SwsFunc ff_yuv2rgb_init_ppc(SwsContext *c);
+SwsFunc ff_yuv2rgb_init_e2k(SwsContext *c);
 SwsFunc ff_yuv2rgb_init_loongarch(SwsContext *c);
 
 static av_always_inline int is16BPS(enum AVPixelFormat pix_fmt)
@@ -965,6 +968,7 @@ extern const AVClass ff_sws_context_class;
  */
 void ff_get_unscaled_swscale(SwsContext *c);
 void ff_get_unscaled_swscale_ppc(SwsContext *c);
+void ff_get_unscaled_swscale_e2k(SwsContext *c);
 void ff_get_unscaled_swscale_arm(SwsContext *c);
 void ff_get_unscaled_swscale_aarch64(SwsContext *c);
 
@@ -981,6 +985,7 @@ void ff_sws_init_output_funcs(SwsContext *c,
                               yuv2anyX_fn *yuv2anyX);
 void ff_sws_init_swscale_ppc(SwsContext *c);
 void ff_sws_init_swscale_vsx(SwsContext *c);
+void ff_sws_init_swscale_e2k(SwsContext *c);
 void ff_sws_init_swscale_x86(SwsContext *c);
 void ff_sws_init_swscale_aarch64(SwsContext *c);
 void ff_sws_init_swscale_arm(SwsContext *c);
diff --git a/libswscale/swscale_unscaled.c b/libswscale/swscale_unscaled.c
index 5d054c0..20f99dc 100644
--- a/libswscale/swscale_unscaled.c
+++ b/libswscale/swscale_unscaled.c
@@ -2283,6 +2283,8 @@ void ff_get_unscaled_swscale(SwsContext *c)
 
 #if ARCH_PPC
     ff_get_unscaled_swscale_ppc(c);
+#elif ARCH_E2K
+    ff_get_unscaled_swscale_e2k(c);
 #elif ARCH_ARM
     ff_get_unscaled_swscale_arm(c);
 #elif ARCH_AARCH64
diff --git a/libswscale/utils.c b/libswscale/utils.c
index a878456..fa9055e 100644
--- a/libswscale/utils.c
+++ b/libswscale/utils.c
@@ -51,6 +51,7 @@
 #include "libavutil/thread.h"
 #include "libavutil/aarch64/cpu.h"
 #include "libavutil/ppc/cpu.h"
+#include "libavutil/e2k/cpu.h"
 #include "libavutil/x86/asm.h"
 #include "libavutil/x86/cpu.h"
 #include "libavutil/loongarch/cpu.h"
@@ -655,6 +656,14 @@ static av_cold int initFilter(int16_t **outFilter, int32_t **filterPos,
             filterAlign = 1;
     }
 
+    if (E2K_SIMD(cpu_flags)) {
+        if (minFilterSize < 5)
+            filterAlign = 4;
+
+        if (minFilterSize < 3)
+            filterAlign = 1;
+    }
+
     if (HAVE_MMX && cpu_flags & AV_CPU_FLAG_MMX) {
         // special case for unscaled vertical filtering
         if (minFilterSize == 1 && filterAlign == 2)
@@ -1817,6 +1826,7 @@ static av_cold int sws_init_single_context(SwsContext *c, SwsFilter *srcFilter,
         {
             const int filterAlign = X86_MMX(cpu_flags)     ? 4 :
                                     PPC_ALTIVEC(cpu_flags) ? 8 :
+                                    E2K_SIMD(cpu_flags)    ? 8 :
                                     have_neon(cpu_flags)   ? 4 :
                                     have_lsx(cpu_flags)    ? 8 :
                                     have_lasx(cpu_flags)   ? 8 : 1;
@@ -1850,6 +1860,7 @@ static av_cold int sws_init_single_context(SwsContext *c, SwsFilter *srcFilter,
     {
         const int filterAlign = X86_MMX(cpu_flags)     ? 2 :
                                 PPC_ALTIVEC(cpu_flags) ? 8 :
+                                E2K_SIMD(cpu_flags)    ? 8 :
                                 have_neon(cpu_flags)   ? 2 : 1;
 
         if ((ret = initFilter(&c->vLumFilter, &c->vLumFilterPos, &c->vLumFilterSize,
@@ -1934,6 +1945,8 @@ static av_cold int sws_init_single_context(SwsContext *c, SwsFilter *srcFilter,
             cpucaps = "MMX";
         else if (PPC_ALTIVEC(cpu_flags))
             cpucaps = "AltiVec";
+        else if (E2K_SIMD(cpu_flags))
+            cpucaps = "Elbrus";
         else
             cpucaps = "C";
 
diff --git a/libswscale/yuv2rgb.c b/libswscale/yuv2rgb.c
index 9c3f5e2..3ea8b23 100644
--- a/libswscale/yuv2rgb.c
+++ b/libswscale/yuv2rgb.c
@@ -681,6 +681,8 @@ SwsFunc ff_yuv2rgb_get_func_ptr(SwsContext *c)
 
 #if ARCH_PPC
     t = ff_yuv2rgb_init_ppc(c);
+#elif ARCH_E2K
+    t = ff_yuv2rgb_init_e2k(c);
 #elif ARCH_X86
     t = ff_yuv2rgb_init_x86(c);
 #elif ARCH_LOONGARCH64
diff --git a/tests/checkasm/checkasm.c b/tests/checkasm/checkasm.c
index f521d30..bdfb980 100644
--- a/tests/checkasm/checkasm.c
+++ b/tests/checkasm/checkasm.c
@@ -244,6 +244,8 @@ static const struct {
     { "ALTIVEC",  "altivec",  AV_CPU_FLAG_ALTIVEC },
     { "VSX",      "vsx",      AV_CPU_FLAG_VSX },
     { "POWER8",   "power8",   AV_CPU_FLAG_POWER8 },
+#elif ARCH_E2K
+    { "E2K",      "e2k",      AV_CPU_FLAG_E2K_SIMD },
 #elif ARCH_RISCV
     { "RVI",      "rvi",      AV_CPU_FLAG_RVI },
     { "RVF",      "rvf",      AV_CPU_FLAG_RVF },
diff --git a/tests/checkasm/huffyuvdsp.c b/tests/checkasm/huffyuvdsp.c
index 6ba27e2..4fbf9fb 100644
--- a/tests/checkasm/huffyuvdsp.c
+++ b/tests/checkasm/huffyuvdsp.c
@@ -24,10 +24,14 @@
 #include "libavutil/intreadwrite.h"
 #include "libavutil/mem.h"
 
-#include "libavcodec/huffyuvdsp.h"
-
 #include "checkasm.h"
 
+/* Short defines (B,G,R,A) in "huffyuvdsp.h" cause problems for Elbrus (e2k)
+ * system includes, so this header file must be included after "checkasm.h".
+ * Ilya Kurdyukov <jpegqs@gmail.com>
+ */
+#include "libavcodec/huffyuvdsp.h"
+
 #define randomize_buffers(buf, size)     \
     do {                                 \
         int j;                           \
-- 
2.34.1

