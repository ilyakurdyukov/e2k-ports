From f6683d5cbaa6b7941e6463b146515d3ad60d51eb Mon Sep 17 00:00:00 2001
From: Ilya Kurdyukov <jpegqs@gmail.com>
Date: Mon, 31 May 2021 23:39:40 +0700
Subject: [PATCH] libaom-3.0.0 e2k support

---
 aom_dsp/aom_dsp.cmake                    |   18 +
 aom_dsp/aom_simd.h                       |    2 +-
 aom_dsp/e2k/aom_high_subpixel_8t_e2k.c   |  416 +++++
 aom_dsp/e2k/aom_subpixel_8t_e2k.c        |  450 ++++++
 aom_dsp/e2k/highbd_intrapred_e2k.c       |  143 ++
 aom_dsp/e2k/highbd_sad4d_e2k.c           |  326 ++++
 aom_dsp/e2k/highbd_sad_e2k.c             |  233 +++
 aom_dsp/e2k/highbd_subpel_variance_e2k.c |  254 ++++
 aom_dsp/e2k/highbd_variance_impl_e2k.c   |  104 ++
 aom_dsp/e2k/intrapred_e2k.c              |  305 ++++
 aom_dsp/e2k/sad4d_e2k.c                  |  270 ++++
 aom_dsp/e2k/sad_e2k.c                    |  265 ++++
 aom_dsp/e2k/subpel_variance_e2k.c        | 1761 ++++++++++++++++++++++
 aom_dsp/e2k/subtract_e2k.c               |  111 ++
 aom_dsp/simd/v256_intrinsics_c.h         |    8 +
 aom_dsp/simd/v256_intrinsics_x86.h       |    2 +-
 aom_dsp/x86/aom_asm_stubs.c              |    2 +
 aom_dsp/x86/variance_sse2.c              |    4 +
 aom_ports/e2k.h                          |   98 ++
 av1/av1.cmake                            |    5 +
 av1/encoder/e2k/error_e2k.c              |   68 +
 build/cmake/aom_config_defaults.cmake    |    1 +
 build/cmake/aom_configure.cmake          |    2 +
 build/cmake/aom_optimization.cmake       |    3 +
 build/cmake/cpu.cmake                    |   16 +
 build/cmake/rtcd.pl                      |   33 +
 test/variance_test.cc                    |    2 +
 27 files changed, 4900 insertions(+), 2 deletions(-)
 create mode 100644 aom_dsp/e2k/aom_high_subpixel_8t_e2k.c
 create mode 100644 aom_dsp/e2k/aom_subpixel_8t_e2k.c
 create mode 100644 aom_dsp/e2k/highbd_intrapred_e2k.c
 create mode 100644 aom_dsp/e2k/highbd_sad4d_e2k.c
 create mode 100644 aom_dsp/e2k/highbd_sad_e2k.c
 create mode 100644 aom_dsp/e2k/highbd_subpel_variance_e2k.c
 create mode 100644 aom_dsp/e2k/highbd_variance_impl_e2k.c
 create mode 100644 aom_dsp/e2k/intrapred_e2k.c
 create mode 100644 aom_dsp/e2k/sad4d_e2k.c
 create mode 100644 aom_dsp/e2k/sad_e2k.c
 create mode 100644 aom_dsp/e2k/subpel_variance_e2k.c
 create mode 100644 aom_dsp/e2k/subtract_e2k.c
 create mode 100644 aom_ports/e2k.h
 create mode 100644 av1/encoder/e2k/error_e2k.c

diff --git a/aom_dsp/aom_dsp.cmake b/aom_dsp/aom_dsp.cmake
index fa58f85..68bc126 100644
--- a/aom_dsp/aom_dsp.cmake
+++ b/aom_dsp/aom_dsp.cmake
@@ -295,6 +295,24 @@ if(CONFIG_AV1_ENCODER)
                      "${AOM_ROOT}/aom_dsp/x86/obmc_variance_sse4.c")
   endif()
 
+  if("${AOM_TARGET_CPU}" STREQUAL "e2k")
+    list(APPEND AOM_DSP_COMMON_INTRIN_SSE4_1
+                "${AOM_ROOT}/aom_dsp/e2k/aom_high_subpixel_8t_e2k.c"
+                "${AOM_ROOT}/aom_dsp/e2k/aom_subpixel_8t_e2k.c"
+                "${AOM_ROOT}/aom_dsp/e2k/highbd_intrapred_e2k.c"
+                "${AOM_ROOT}/aom_dsp/e2k/intrapred_e2k.c")
+
+    list(APPEND AOM_DSP_ENCODER_INTRIN_SSE4_1
+                "${AOM_ROOT}/aom_dsp/e2k/highbd_sad4d_e2k.c"
+                "${AOM_ROOT}/aom_dsp/e2k/highbd_sad_e2k.c"
+                "${AOM_ROOT}/aom_dsp/e2k/highbd_subpel_variance_e2k.c"
+                "${AOM_ROOT}/aom_dsp/e2k/highbd_variance_impl_e2k.c"
+                "${AOM_ROOT}/aom_dsp/e2k/sad4d_e2k.c"
+                "${AOM_ROOT}/aom_dsp/e2k/sad_e2k.c"
+                "${AOM_ROOT}/aom_dsp/e2k/subpel_variance_e2k.c"
+                "${AOM_ROOT}/aom_dsp/e2k/subtract_e2k.c")
+  endif()
+
   list(APPEND AOM_DSP_ENCODER_INTRIN_NEON "${AOM_ROOT}/aom_dsp/arm/sad4d_neon.c"
               "${AOM_ROOT}/aom_dsp/arm/sad_neon.c"
               "${AOM_ROOT}/aom_dsp/arm/subpel_variance_neon.c"
diff --git a/aom_dsp/aom_simd.h b/aom_dsp/aom_simd.h
index ab950ca..a2e8516 100644
--- a/aom_dsp/aom_simd.h
+++ b/aom_dsp/aom_simd.h
@@ -29,7 +29,7 @@
 // VS compiling for 32 bit targets does not support vector types in
 // structs as arguments, which makes the v256 type of the intrinsics
 // hard to support, so optimizations for this target are disabled.
-#elif HAVE_SSE2 && (defined(_WIN64) || !defined(_MSC_VER) || defined(__clang__))
+#elif HAVE_SSE2 && (defined(_WIN64) || !defined(_MSC_VER) || defined(__clang__) || ARCH_E2K)
 #include "simd/v256_intrinsics_x86.h"
 #else
 #include "simd/v256_intrinsics.h"
diff --git a/aom_dsp/e2k/aom_high_subpixel_8t_e2k.c b/aom_dsp/e2k/aom_high_subpixel_8t_e2k.c
new file mode 100644
index 0000000..94ebf26
--- /dev/null
+++ b/aom_dsp/e2k/aom_high_subpixel_8t_e2k.c
@@ -0,0 +1,416 @@
+/*
+ * Copyright (c) 2021, Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2016, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "config/av1_rtcd.h"
+
+DECLARE_ALIGNED(16, static const uint8_t, filt_high_sse2[]) = {
+   0,  1,   2,  3,   2,  3,   4,  5,   4,  5,   6,  7,   6,  7,   8,  9,
+   4,  5,   6,  7,   6,  7,   8,  9,   8,  9,  10, 11,  10, 11,  12, 13,
+   2,  3,   4,  5,   4,  5,   6,  7,   6,  7,   8,  9,   8,  9,  10, 11, /* -6 */
+   6,  7,   8,  9,   8,  9,  10, 11,  10, 11,  12, 13,  12, 13,  14, 15,
+};
+
+#define INIT_H8 \
+  i = 0x10001; \
+  vmax = _mm_set1_epi32((i << bd) - i); \
+  round = _mm_set1_epi32(64); \
+  v0 = _mm_loadu_si128((const __m128i *)filter); \
+  firstFilters = _mm_shuffle_epi32(v0, 0); \
+  secondFilters = _mm_shuffle_epi32(v0, 0x55); \
+  thirdFilters = _mm_shuffle_epi32(v0, 0xaa); \
+  forthFilters = _mm_shuffle_epi32(v0, 0xff); \
+  filt1Reg = _mm_load_si128((__m128i const *)filt_high_sse2); \
+  filt2Reg = _mm_load_si128((__m128i const *)(filt_high_sse2 + 16)); \
+  filt3Reg = _mm_load_si128((__m128i const *)(filt_high_sse2 + 16 * 2)); \
+  filt4Reg = _mm_load_si128((__m128i const *)(filt_high_sse2 + 16 * 3));
+
+#define FILTER_H8(src, v0) \
+  v1 = _mm_loadu_si128((const __m128i *)(src - 3)); \
+  v3 = _mm_loadu_si128((const __m128i *)(src)); \
+  v0 = _mm_shuffle_epi8(v1, filt1Reg); \
+  v1 = _mm_shuffle_epi8(v1, filt2Reg); \
+  v0 = _mm_madd_epi16(v0, firstFilters); \
+  v1 = _mm_madd_epi16(v1, secondFilters); \
+  v0 = _mm_add_epi32(v0, v1); \
+  v2 = _mm_shuffle_epi8(v3, filt3Reg); \
+  v3 = _mm_shuffle_epi8(v3, filt4Reg); \
+  v2 = _mm_madd_epi16(v2, thirdFilters); \
+  v3 = _mm_madd_epi16(v3, forthFilters); \
+  v0 = _mm_add_epi32(v0, _mm_add_epi32(v2, v3)); \
+  v0 = _mm_add_epi32(v0, round); \
+  v0 = _mm_srai_epi32(v0, 7);
+
+void aom_highbd_filter_block1d4_h8_sse2(
+    const uint16_t *src_ptr, ptrdiff_t src_pixels_per_line, uint16_t *output_ptr,
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter, int bd) {
+  __m128i round, vmax, filt1Reg, filt2Reg, filt3Reg, filt4Reg, v0, v1, v2, v3;
+  __m128i firstFilters, secondFilters, thirdFilters, forthFilters;
+  unsigned int i;
+
+  INIT_H8
+
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < output_height; i++) {
+    FILTER_H8(src_ptr, v0)
+    v0 = _mm_packus_epi32(v0, v0);
+    v0 = _mm_min_epu16(v0, vmax);
+    _mm_storel_epi64((__m128i *)output_ptr, v0);
+
+    src_ptr += src_pixels_per_line;
+    output_ptr += output_pitch;
+  }
+}
+
+void aom_highbd_filter_block1d8_h8_sse2(
+    const uint16_t *src_ptr, ptrdiff_t src_pixels_per_line, uint16_t *output_ptr,
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter, int bd) {
+  __m128i round, vmax, filt1Reg, filt2Reg, filt3Reg, filt4Reg, v0, v1, v2, v3, v4;
+  __m128i firstFilters, secondFilters, thirdFilters, forthFilters;
+  unsigned int i;
+
+  INIT_H8
+
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < output_height; i++) {
+    FILTER_H8(src_ptr, v4)
+    FILTER_H8(src_ptr + 4, v0)
+    v0 = _mm_packus_epi32(v4, v0);
+    v0 = _mm_min_epu16(v0, vmax);
+    _mm_store_si128((__m128i *)output_ptr, v0);
+
+    src_ptr += src_pixels_per_line;
+    output_ptr += output_pitch;
+  }
+}
+
+void aom_highbd_filter_block1d16_h8_sse2(
+    const uint16_t *src_ptr, ptrdiff_t src_pixels_per_line, uint16_t *output_ptr,
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter, int bd) {
+  __m128i round, vmax, filt1Reg, filt2Reg, filt3Reg, filt4Reg, v0, v1, v2, v3, v4, v5;
+  __m128i firstFilters, secondFilters, thirdFilters, forthFilters;
+  unsigned int i;
+
+  INIT_H8
+
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < output_height; i++) {
+    FILTER_H8(src_ptr, v4)
+    FILTER_H8(src_ptr + 4, v0)
+    v4 = _mm_packus_epi32(v4, v0);
+    v4 = _mm_min_epu16(v4, vmax);
+    FILTER_H8(src_ptr + 8, v5)
+    FILTER_H8(src_ptr + 12, v0)
+    v0 = _mm_packus_epi32(v5, v0);
+    v0 = _mm_min_epu16(v0, vmax);
+    _mm_store_si128((__m128i *)output_ptr, v4);
+    _mm_store_si128((__m128i *)(output_ptr + 8), v0);
+
+    src_ptr += src_pixels_per_line;
+    output_ptr += output_pitch;
+  }
+}
+
+#define INIT_X2 \
+  i = 0x10001; \
+  vmax = _mm_set1_epi32((i << bd) - i); \
+  round = _mm_set1_epi32(64); \
+  v0 = _mm_cvtsi32_si128(*(const uint32_t *)(filter + 3)); \
+  firstFilters = _mm_shuffle_epi32(v0, 0);
+
+void aom_highbd_filter_block1d4_h2_sse2(
+    const uint16_t *src_ptr, ptrdiff_t src_pixels_per_line, uint16_t *output_ptr,
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter, int bd) {
+  __m128i round, vmax, firstFilters, v0, v1;
+  unsigned int i;
+
+  INIT_X2
+
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < output_height; i++) {
+    v0 = _mm_loadl_epi64((const __m128i *)src_ptr);
+    v1 = _mm_loadl_epi64((const __m128i *)(src_ptr + 1));
+
+    v0 = _mm_unpacklo_epi16(v0, v1);
+    v0 = _mm_madd_epi16(v0, firstFilters);
+    v0 = _mm_add_epi32(v0, round);
+    v0 = _mm_srai_epi32(v0, 7);
+    v0 = _mm_packus_epi32(v0, v0);
+    v0 = _mm_min_epu16(v0, vmax);
+    _mm_storel_epi64((__m128i *)output_ptr, v0);
+
+    src_ptr += src_pixels_per_line;
+    output_ptr += output_pitch;
+  }
+}
+
+#define FILTER_H2(src, v0) \
+  v2 = _mm_loadu_si128((const __m128i *)(src)); \
+  v3 = _mm_loadu_si128((const __m128i *)(src + 1)); \
+  v0 = _mm_unpacklo_epi16(v2, v3); \
+  v1 = _mm_unpackhi_epi16(v2, v3); \
+  v0 = _mm_madd_epi16(v0, firstFilters); \
+  v1 = _mm_madd_epi16(v1, firstFilters); \
+  v0 = _mm_add_epi32(v0, round); \
+  v1 = _mm_add_epi32(v1, round); \
+  v0 = _mm_srai_epi32(v0, 7); \
+  v1 = _mm_srai_epi32(v1, 7); \
+  v0 = _mm_packus_epi32(v0, v1); \
+  v0 = _mm_min_epu16(v0, vmax);
+
+void aom_highbd_filter_block1d8_h2_sse2(
+    const uint16_t *src_ptr, ptrdiff_t src_pixels_per_line, uint16_t *output_ptr,
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter, int bd) {
+  __m128i round, vmax, firstFilters, v0, v1, v2, v3;
+  unsigned int i;
+
+  INIT_X2
+
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < output_height; i++) {
+    FILTER_H2(src_ptr, v0)
+    _mm_store_si128((__m128i *)output_ptr, v0);
+
+    src_ptr += src_pixels_per_line;
+    output_ptr += output_pitch;
+  }
+}
+
+void aom_highbd_filter_block1d16_h2_sse2(
+    const uint16_t *src_ptr, ptrdiff_t src_pixels_per_line, uint16_t *output_ptr,
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter, int bd) {
+  __m128i round, vmax, firstFilters, v0, v1, v2, v3, v4;
+  unsigned int i;
+
+  INIT_X2
+
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < output_height; i++) {
+    FILTER_H2(src_ptr, v4)
+    FILTER_H2(src_ptr + 8, v0)
+    _mm_store_si128((__m128i *)output_ptr, v4);
+    _mm_store_si128((__m128i *)(output_ptr + 8), v0);
+
+    src_ptr += src_pixels_per_line;
+    output_ptr += output_pitch;
+  }
+}
+
+#define INIT_V8(LOAD) \
+  i = 0x10001; \
+  vmax = _mm_set1_epi32((i << bd) - i); \
+  round = _mm_set1_epi32(64); \
+  v0 = _mm_loadu_si128((const __m128i *)filter); \
+  firstFilters = _mm_shuffle_epi32(v0, 0); \
+  secondFilters = _mm_shuffle_epi32(v0, 0x55); \
+  thirdFilters = _mm_shuffle_epi32(v0, 0xaa); \
+  forthFilters = _mm_shuffle_epi32(v0, 0xff); \
+  i0 = LOAD((const __m128i *)src_ptr); \
+  i1 = LOAD((const __m128i *)(src_ptr + src_pixels_per_line)); \
+  i2 = LOAD((const __m128i *)(src_ptr + src_pixels_per_line * 2)); \
+  i3 = LOAD((const __m128i *)(src_ptr + src_pixels_per_line * 3)); \
+  i4 = LOAD((const __m128i *)(src_ptr + src_pixels_per_line * 4)); \
+  i5 = LOAD((const __m128i *)(src_ptr + src_pixels_per_line * 5)); \
+  i6 = LOAD((const __m128i *)(src_ptr + src_pixels_per_line * 6)); \
+  src_ptr += src_pixels_per_line * 7;
+
+#define FILTER_V8(lo, v0, i) \
+  v0 = _mm_unpack##lo##_epi16(i##0, i##1); \
+  v1 = _mm_unpack##lo##_epi16(i##6, i##7); \
+  v0 = _mm_madd_epi16(v0, firstFilters); \
+  v1 = _mm_madd_epi16(v1, forthFilters); \
+  v0 = _mm_add_epi32(v0, v1); \
+  v2 = _mm_unpack##lo##_epi16(i##2, i##3); \
+  v3 = _mm_unpack##lo##_epi16(i##4, i##5); \
+  v2 = _mm_madd_epi16(v2, secondFilters); \
+  v3 = _mm_madd_epi16(v3, thirdFilters); \
+  v0 = _mm_add_epi32(v0, _mm_add_epi32(v2, v3)); \
+  v0 = _mm_add_epi32(v0, round); \
+  v0 = _mm_srai_epi32(v0, 7);
+
+void aom_highbd_filter_block1d4_v8_sse2(
+    const uint16_t *src_ptr, ptrdiff_t src_pixels_per_line, uint16_t *output_ptr,
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter, int bd) {
+  __m128i round, vmax, i0, i1, i2, i3, i4, i5, i6, i7, v0, v1, v2, v3;
+  __m128i firstFilters, secondFilters, thirdFilters, forthFilters;
+  unsigned int i;
+
+  INIT_V8(_mm_loadl_epi64)
+
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < output_height; i++) {
+    i7 = _mm_loadl_epi64((const __m128i *)src_ptr);
+    src_ptr += src_pixels_per_line;
+    FILTER_V8(lo, v0, i)
+    v0 = _mm_packus_epi32(v0, v0);
+    v0 = _mm_min_epu16(v0, vmax);
+    _mm_storel_epi64((__m128i *)output_ptr, v0);
+    output_ptr += output_pitch;
+    i0 = i1; i1 = i2; i2 = i3; i3 = i4; i4 = i5; i5 = i6; i6 = i7;
+  }
+}
+
+void aom_highbd_filter_block1d8_v8_sse2(
+    const uint16_t *src_ptr, ptrdiff_t src_pixels_per_line, uint16_t *output_ptr,
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter, int bd) {
+  __m128i round, vmax, i0, i1, i2, i3, i4, i5, i6, i7, v0, v1, v2, v3, v4;
+  __m128i firstFilters, secondFilters, thirdFilters, forthFilters;
+  unsigned int i;
+
+  INIT_V8(_mm_loadu_si128)
+
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < output_height; i++) {
+    i7 = _mm_loadu_si128((const __m128i *)src_ptr);
+    src_ptr += src_pixels_per_line;
+    FILTER_V8(lo, v4, i)
+    FILTER_V8(hi, v0, i)
+    v0 = _mm_packus_epi32(v4, v0);
+    v0 = _mm_min_epu16(v0, vmax);
+    _mm_store_si128((__m128i *)output_ptr, v0);
+    output_ptr += output_pitch;
+    i0 = i1; i1 = i2; i2 = i3; i3 = i4; i4 = i5; i5 = i6; i6 = i7;
+  }
+}
+
+void aom_highbd_filter_block1d16_v8_sse2(
+    const uint16_t *src_ptr, ptrdiff_t src_pixels_per_line, uint16_t *output_ptr,
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter, int bd) {
+  __m128i round, vmax, i0, i1, i2, i3, i4, i5, i6, i7, v0, v1, v2, v3, v4, v5;
+  __m128i j0, j1, j2, j3, j4, j5, j6, j7;
+  __m128i firstFilters, secondFilters, thirdFilters, forthFilters;
+  unsigned int i;
+
+  i = 0x10001;
+  vmax = _mm_set1_epi32((i << bd) - i);
+  round = _mm_set1_epi32(64);
+  v0 = _mm_loadu_si128((const __m128i *)filter);
+  firstFilters = _mm_shuffle_epi32(v0, 0);
+  secondFilters = _mm_shuffle_epi32(v0, 0x55);
+  thirdFilters = _mm_shuffle_epi32(v0, 0xaa);
+  forthFilters = _mm_shuffle_epi32(v0, 0xff);
+  i0 = _mm_loadu_si128((const __m128i *)src_ptr);
+  j0 = _mm_loadu_si128((const __m128i *)(src_ptr + 8));
+  i1 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pixels_per_line));
+  j1 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pixels_per_line + 8));
+  i2 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pixels_per_line * 2));
+  j2 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pixels_per_line * 2 + 8));
+  i3 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pixels_per_line * 3));
+  j3 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pixels_per_line * 3 + 8));
+  i4 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pixels_per_line * 4));
+  j4 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pixels_per_line * 4 + 8));
+  i5 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pixels_per_line * 5));
+  j5 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pixels_per_line * 5 + 8));
+  i6 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pixels_per_line * 6));
+  j6 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pixels_per_line * 6 + 8));
+  src_ptr += src_pixels_per_line * 7;
+
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < output_height; i++) {
+    i7 = _mm_loadu_si128((const __m128i *)src_ptr);
+    j7 = _mm_loadu_si128((const __m128i *)(src_ptr + 8));
+    src_ptr += src_pixels_per_line;
+    FILTER_V8(lo, v4, i)
+    FILTER_V8(hi, v0, i)
+    v4 = _mm_packus_epi32(v4, v0);
+    v4 = _mm_min_epu16(v4, vmax);
+    FILTER_V8(lo, v5, j)
+    FILTER_V8(hi, v0, j)
+    v0 = _mm_packus_epi32(v5, v0);
+    v0 = _mm_min_epu16(v0, vmax);
+    _mm_store_si128((__m128i *)output_ptr, v4);
+    _mm_store_si128((__m128i *)(output_ptr + 8), v0);
+    output_ptr += output_pitch;
+    i0 = i1; i1 = i2; i2 = i3; i3 = i4; i4 = i5; i5 = i6; i6 = i7;
+    j0 = j1; j1 = j2; j2 = j3; j3 = j4; j4 = j5; j5 = j6; j6 = j7;
+  }
+}
+
+void aom_highbd_filter_block1d4_v2_sse2(
+    const uint16_t *src_ptr, ptrdiff_t src_pixels_per_line, uint16_t *output_ptr,
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter, int bd) {
+  __m128i round, vmax, firstFilters, v0, v2, v3;
+  unsigned int i;
+
+  INIT_X2
+
+  v2 = _mm_loadl_epi64((const __m128i *)src_ptr);
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < output_height; i++) {
+    src_ptr += src_pixels_per_line;
+    v3 = _mm_loadl_epi64((const __m128i *)src_ptr);
+
+    v0 = _mm_unpacklo_epi16(v2, v3);
+    v0 = _mm_madd_epi16(v0, firstFilters);
+    v0 = _mm_add_epi32(v0, round);
+    v0 = _mm_srai_epi32(v0, 7);
+    v0 = _mm_packus_epi32(v0, v0);
+    v0 = _mm_min_epu16(v0, vmax);
+    _mm_storel_epi64((__m128i *)output_ptr, v0);
+    output_ptr += output_pitch;
+    v2 = v3;
+  }
+}
+
+#define FILTER_V2(src, v2, v3, v0) \
+  v3 = _mm_loadu_si128((const __m128i *)(src)); \
+  v0 = _mm_unpacklo_epi16(v2, v3); \
+  v2 = _mm_unpackhi_epi16(v2, v3); \
+  v0 = _mm_madd_epi16(v0, firstFilters); \
+  v2 = _mm_madd_epi16(v2, firstFilters); \
+  v0 = _mm_add_epi32(v0, round); \
+  v2 = _mm_add_epi32(v2, round); \
+  v0 = _mm_srai_epi32(v0, 7); \
+  v2 = _mm_srai_epi32(v2, 7); \
+  v0 = _mm_packus_epi32(v0, v2); \
+  v0 = _mm_min_epu16(v0, vmax); \
+  v2 = v3;
+
+void aom_highbd_filter_block1d8_v2_sse2(
+    const uint16_t *src_ptr, ptrdiff_t src_pixels_per_line, uint16_t *output_ptr,
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter, int bd) {
+  __m128i round, vmax, firstFilters, v0, v2, v3;
+  unsigned int i;
+
+  INIT_X2
+
+  v2 = _mm_loadu_si128((const __m128i *)src_ptr);
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < output_height; i++) {
+    src_ptr += src_pixels_per_line;
+    FILTER_V2(src_ptr, v2, v3, v0)
+    _mm_store_si128((__m128i *)output_ptr, v0);
+    output_ptr += output_pitch;
+  }
+}
+
+void aom_highbd_filter_block1d16_v2_sse2(
+    const uint16_t *src_ptr, ptrdiff_t src_pixels_per_line, uint16_t *output_ptr,
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter, int bd) {
+  __m128i round, vmax, firstFilters, v0, v1, v2, v3, v4, v5;
+  unsigned int i;
+
+  INIT_X2
+
+  v2 = _mm_loadu_si128((const __m128i *)src_ptr);
+  v4 = _mm_loadu_si128((const __m128i *)(src_ptr + 8));
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < output_height; i++) {
+    src_ptr += src_pixels_per_line;
+    FILTER_V2(src_ptr, v2, v3, v0)
+    FILTER_V2(src_ptr + 8, v4, v5, v1)
+    _mm_store_si128((__m128i *)output_ptr, v0);
+    _mm_store_si128((__m128i *)(output_ptr + 8), v1);
+    output_ptr += output_pitch;
+  }
+}
diff --git a/aom_dsp/e2k/aom_subpixel_8t_e2k.c b/aom_dsp/e2k/aom_subpixel_8t_e2k.c
new file mode 100644
index 0000000..117241c
--- /dev/null
+++ b/aom_dsp/e2k/aom_subpixel_8t_e2k.c
@@ -0,0 +1,450 @@
+/*
+ * Copyright (c) 2021, Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2016, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "config/av1_rtcd.h"
+
+DECLARE_ALIGNED(16, static const uint8_t, filt_sse2[]) = {
+  0,  1,  1,  2,  2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  7,   8,
+  2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  7,  8,  8,  9,  9,  10,
+  4,  5,  5,  6,  6,  7,  7,  8,  8,  9,  9,  10, 10, 11, 11, 12,
+  6,  7,  7,  8,  8,  9,  9,  10, 10, 11, 11, 12, 12, 13, 13, 14
+};
+
+DECLARE_ALIGNED(16, static const uint8_t, filt_d4_sse2[]) = {
+  0, 1, 2, 3,  1, 2, 3, 4,  2, 3, 4, 5,  3, 4, 5, 6,
+  4, 5, 6, 7,  5, 6, 7, 8,  6, 7, 8, 9,  7, 8, 9, 10
+};
+
+void aom_filter_block1d4_h8_ssse3(
+    const uint8_t *src_ptr, ptrdiff_t src_pixels_per_line, uint8_t *output_ptr,
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter) {
+  __m128i round, filt1Reg, filt2Reg, v0, v1;
+  __m128i firstFilters, secondFilters;
+  unsigned int i;
+
+  round = _mm_set1_epi16(32);
+  v0 = _mm_loadu_si128((const __m128i *)filter);
+  v0 = _mm_srai_epi16(v0, 1);
+  v0 = _mm_packs_epi16(v0, v0);
+  firstFilters = _mm_shuffle_epi32(v0, 0);
+  secondFilters = _mm_shuffle_epi32(v0, 0x55);
+
+  filt1Reg = _mm_load_si128((__m128i const *)filt_d4_sse2);
+  filt2Reg = _mm_load_si128((__m128i const *)(filt_d4_sse2 + 16));
+
+  src_ptr -= 3;
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < output_height; i++) {
+    v1 = _mm_loadu_si128((const __m128i *)(src_ptr));
+
+    v0 = _mm_shuffle_epi8(v1, filt1Reg);
+    v1 = _mm_shuffle_epi8(v1, filt2Reg);
+    v0 = _mm_maddubs_epi16(v0, firstFilters);
+    v1 = _mm_maddubs_epi16(v1, secondFilters);
+
+    v0 = _mm_adds_epi16(v0, v1);
+    v0 = _mm_hadds_epi16(v0, v0);
+    v0 = _mm_adds_epi16(v0, round);
+    v0 = _mm_srai_epi16(v0, 6);
+    v0 = _mm_packus_epi16(v0, v0);
+    *(uint32_t *)output_ptr = _mm_cvtsi128_si32(v0);
+
+    src_ptr += src_pixels_per_line;
+    output_ptr += output_pitch;
+  }
+}
+
+#define INIT_H8 \
+  round = _mm_set1_epi16(32); \
+  v0 = _mm_loadu_si128((const __m128i *)filter); \
+  v0 = _mm_srai_epi16(v0, 1); \
+  v0 = _mm_packs_epi16(v0, v0); \
+  firstFilters = _mm_shuffle_epi8(v0, _mm_set1_epi16(0x100)); \
+  secondFilters = _mm_shuffle_epi8(v0, _mm_set1_epi16(0x302)); \
+  thirdFilters = _mm_shuffle_epi8(v0, _mm_set1_epi16(0x504)); \
+  forthFilters = _mm_shuffle_epi8(v0, _mm_set1_epi16(0x706)); \
+  filt1Reg = _mm_load_si128((__m128i const *)filt_sse2); \
+  filt2Reg = _mm_load_si128((__m128i const *)(filt_sse2 + 16)); \
+  filt3Reg = _mm_load_si128((__m128i const *)(filt_sse2 + 16 * 2)); \
+  filt4Reg = _mm_load_si128((__m128i const *)(filt_sse2 + 16 * 3)); \
+  src_ptr -= 3;
+
+#define FILTER_H8(src, v0) \
+  v3 = _mm_loadu_si128((const __m128i *)(src)); \
+  v0 = _mm_shuffle_epi8(v3, filt1Reg); \
+  v1 = _mm_shuffle_epi8(v3, filt4Reg); \
+  v0 = _mm_maddubs_epi16(v0, firstFilters); \
+  v1 = _mm_maddubs_epi16(v1, forthFilters); \
+  v0 = _mm_adds_epi16(v0, v1); \
+  v2 = _mm_shuffle_epi8(v3, filt2Reg); \
+  v3 = _mm_shuffle_epi8(v3, filt3Reg); \
+  v2 = _mm_maddubs_epi16(v2, secondFilters); \
+  v3 = _mm_maddubs_epi16(v3, thirdFilters); \
+  v0 = _mm_adds_epi16(v0, _mm_adds_epi16(v2, v3)); \
+  v0 = _mm_adds_epi16(v0, round); \
+  v0 = _mm_srai_epi16(v0, 6);
+
+void aom_filter_block1d8_h8_ssse3(
+    const uint8_t *src_ptr, ptrdiff_t src_pixels_per_line, uint8_t *output_ptr,
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter) {
+  __m128i round, filt1Reg, filt2Reg, filt3Reg, filt4Reg, v0, v1, v2, v3;
+  __m128i firstFilters, secondFilters, thirdFilters, forthFilters;
+  unsigned int i;
+
+  INIT_H8
+
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < output_height; i++) {
+    FILTER_H8(src_ptr, v0)
+
+    v0 = _mm_packus_epi16(v0, v0);
+    _mm_storel_epi64((__m128i *)output_ptr, v0);
+
+    src_ptr += src_pixels_per_line;
+    output_ptr += output_pitch;
+  }
+}
+
+void aom_filter_block1d16_h8_ssse3(
+    const uint8_t *src_ptr, ptrdiff_t src_pixels_per_line, uint8_t *output_ptr,
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter) {
+  __m128i round, filt1Reg, filt2Reg, filt3Reg, filt4Reg, v0, v1, v2, v3, v4;
+  __m128i firstFilters, secondFilters, thirdFilters, forthFilters;
+  unsigned int i;
+
+  INIT_H8
+
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < output_height; i++) {
+    FILTER_H8(src_ptr, v4)
+    FILTER_H8(src_ptr + 8, v0)
+
+    v0 = _mm_packus_epi16(v4, v0);
+    _mm_store_si128((__m128i *)output_ptr, v0);
+
+    src_ptr += src_pixels_per_line;
+    output_ptr += output_pitch;
+  }
+}
+
+void aom_filter_block1d4_h2_ssse3(
+    const uint8_t *src_ptr, ptrdiff_t src_pixels_per_line, uint8_t *output_ptr,
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter) {
+  __m64 round, filt1Reg, firstFilters, v0;
+  unsigned int i;
+
+  round = _mm_set1_pi16(32);
+  v0 = _mm_cvtsi32_si64(*(const uint32_t *)(filter + 3));
+  v0 = _mm_srai_pi16(v0, 1);
+  v0 = _mm_packs_pi16(v0, v0);
+
+  firstFilters = _mm_shuffle_pi16(v0, 0);
+  filt1Reg = _mm_setr_pi8(3, 4, 4, 5, 5, 6, 6, 7);
+
+  src_ptr -= 3;
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < output_height; i++) {
+    v0 = *(const __m64 *)src_ptr;
+
+    v0 = _mm_shuffle_pi8(v0, filt1Reg);
+    v0 = _mm_maddubs_pi16(v0, firstFilters);
+    v0 = _mm_adds_pi16(v0, round);
+    v0 = _mm_srai_pi16(v0, 6);
+    v0 = _mm_packs_pu16(v0, v0);
+    *(uint32_t *)output_ptr = _mm_cvtsi64_si32(v0);
+
+    src_ptr += src_pixels_per_line;
+    output_ptr += output_pitch;
+  }
+}
+
+void aom_filter_block1d8_h2_ssse3(
+    const uint8_t *src_ptr, ptrdiff_t src_pixels_per_line, uint8_t *output_ptr,
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter) {
+  __m128i round, firstFilters, v0, v1;
+  unsigned int i;
+
+  round = _mm_set1_epi16(32);
+  v0 = _mm_cvtsi32_si128(*(const uint32_t *)(filter + 3));
+  v0 = _mm_srai_epi16(v0, 1);
+  v0 = _mm_packs_epi16(v0, v0);
+  firstFilters = _mm_shuffle_epi8(v0, _mm_set1_epi16(0x100));
+
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < output_height; i++) {
+    v0 = _mm_loadl_epi64((const __m128i *)src_ptr);
+    v1 = _mm_loadl_epi64((const __m128i *)(src_ptr + 1));
+
+    v0 = _mm_unpacklo_epi8(v0, v1);
+    v0 = _mm_maddubs_epi16(v0, firstFilters);
+    v0 = _mm_adds_epi16(v0, round);
+    v0 = _mm_srai_epi16(v0, 6);
+    v0 = _mm_packus_epi16(v0, v0);
+    _mm_storel_epi64((__m128i *)output_ptr, v0);
+
+    src_ptr += src_pixels_per_line;
+    output_ptr += output_pitch;
+  }
+}
+
+void aom_filter_block1d16_h2_ssse3(
+    const uint8_t *src_ptr, ptrdiff_t src_pixels_per_line, uint8_t *output_ptr,
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter) {
+  __m128i round, firstFilters, v0, v1, v2, v3;
+  unsigned int i;
+
+  round = _mm_set1_epi16(32);
+  v0 = _mm_cvtsi32_si128(*(const uint32_t *)(filter + 3));
+  v0 = _mm_srai_epi16(v0, 1);
+  v0 = _mm_packs_epi16(v0, v0);
+  firstFilters = _mm_shuffle_epi8(v0, _mm_set1_epi16(0x100));
+
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < output_height; i++) {
+    v2 = _mm_loadu_si128((const __m128i *)src_ptr);
+    v3 = _mm_loadu_si128((const __m128i *)(src_ptr + 1));
+
+    v0 = _mm_unpacklo_epi8(v2, v3);
+    v1 = _mm_unpackhi_epi8(v2, v3);
+    v0 = _mm_maddubs_epi16(v0, firstFilters);
+    v1 = _mm_maddubs_epi16(v1, firstFilters);
+    v0 = _mm_adds_epi16(v0, round);
+    v1 = _mm_adds_epi16(v1, round);
+    v0 = _mm_srai_epi16(v0, 6);
+    v1 = _mm_srai_epi16(v1, 6);
+    v0 = _mm_packus_epi16(v0, v1);
+    _mm_store_si128((__m128i *)output_ptr, v0);
+
+    src_ptr += src_pixels_per_line;
+    output_ptr += output_pitch;
+  }
+}
+
+void aom_filter_block1d4_v8_ssse3(
+    const uint8_t *src_ptr, ptrdiff_t src_pixels_per_line, uint8_t *output_ptr,
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter) {
+  __m64 round, i0, i1, i2, i3, i4, i5, i6, i7, v0, v1, v2, v3;
+  __m64 firstFilters, secondFilters, thirdFilters, forthFilters;
+  unsigned int i;
+
+  round = _mm_set1_pi16(32);
+  v0 = *(const __m64 *)filter;
+  v1 = *(const __m64 *)(filter + 4);
+  v0 = _mm_srai_pi16(v0, 1);
+  v1 = _mm_srai_pi16(v1, 1);
+  v0 = _mm_packs_pi16(v0, v1);
+  firstFilters = _mm_shuffle_pi16(v0, 0);
+  secondFilters = _mm_shuffle_pi16(v0, 0x55);
+  thirdFilters = _mm_shuffle_pi16(v0, 0xaa);
+  forthFilters = _mm_shuffle_pi16(v0, 0xff);
+  i0 = _mm_cvtsi32_si64(*(const uint32_t *)src_ptr);
+  i1 = _mm_cvtsi32_si64(*(const uint32_t *)(src_ptr + src_pixels_per_line));
+  i2 = _mm_cvtsi32_si64(*(const uint32_t *)(src_ptr + src_pixels_per_line * 2));
+  i3 = _mm_cvtsi32_si64(*(const uint32_t *)(src_ptr + src_pixels_per_line * 3));
+  i4 = _mm_cvtsi32_si64(*(const uint32_t *)(src_ptr + src_pixels_per_line * 4));
+  i5 = _mm_cvtsi32_si64(*(const uint32_t *)(src_ptr + src_pixels_per_line * 5));
+  i6 = _mm_cvtsi32_si64(*(const uint32_t *)(src_ptr + src_pixels_per_line * 6));
+  src_ptr += src_pixels_per_line * 7;
+
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < output_height; i++) {
+    i7 = _mm_cvtsi32_si64(*(const uint32_t *)src_ptr);
+    src_ptr += src_pixels_per_line;
+
+    v0 = _mm_unpacklo_pi8(i0, i1);
+    v1 = _mm_unpacklo_pi8(i6, i7);
+    v0 = _mm_maddubs_pi16(v0, firstFilters);
+    v1 = _mm_maddubs_pi16(v1, forthFilters);
+    v0 = _mm_adds_pi16(v0, v1);
+    v2 = _mm_unpacklo_pi8(i2, i3);
+    v3 = _mm_unpacklo_pi8(i4, i5);
+    v2 = _mm_maddubs_pi16(v2, secondFilters);
+    v3 = _mm_maddubs_pi16(v3, thirdFilters);
+    v0 = _mm_adds_pi16(v0, _mm_adds_pi16(v2, v3));
+    v0 = _mm_adds_pi16(v0, round);
+    v0 = _mm_srai_pi16(v0, 6);
+
+    v0 = _mm_packs_pu16(v0, v0);
+    *(uint32_t *)output_ptr = _mm_cvtsi64_si32(v0);
+    output_ptr += output_pitch;
+    i0 = i1; i1 = i2; i2 = i3; i3 = i4; i4 = i5; i5 = i6; i6 = i7;
+  }
+}
+
+#define INIT_V8(LOAD) \
+  round = _mm_set1_epi16(32); \
+  v0 = _mm_loadu_si128((const __m128i *)filter); \
+  v0 = _mm_srai_epi16(v0, 1); \
+  v0 = _mm_packs_epi16(v0, v0); \
+  firstFilters = _mm_shuffle_epi8(v0, _mm_set1_epi16(0x100)); \
+  secondFilters = _mm_shuffle_epi8(v0, _mm_set1_epi16(0x302)); \
+  thirdFilters = _mm_shuffle_epi8(v0, _mm_set1_epi16(0x504)); \
+  forthFilters = _mm_shuffle_epi8(v0, _mm_set1_epi16(0x706)); \
+  i0 = LOAD((const __m128i *)src_ptr); \
+  i1 = LOAD((const __m128i *)(src_ptr + src_pixels_per_line)); \
+  i2 = LOAD((const __m128i *)(src_ptr + src_pixels_per_line * 2)); \
+  i3 = LOAD((const __m128i *)(src_ptr + src_pixels_per_line * 3)); \
+  i4 = LOAD((const __m128i *)(src_ptr + src_pixels_per_line * 4)); \
+  i5 = LOAD((const __m128i *)(src_ptr + src_pixels_per_line * 5)); \
+  i6 = LOAD((const __m128i *)(src_ptr + src_pixels_per_line * 6)); \
+  src_ptr += src_pixels_per_line * 7;
+
+#define FILTER_V8(lo, v0) \
+  v0 = _mm_unpack##lo##_epi8(i0, i1); \
+  v1 = _mm_unpack##lo##_epi8(i6, i7); \
+  v0 = _mm_maddubs_epi16(v0, firstFilters); \
+  v1 = _mm_maddubs_epi16(v1, forthFilters); \
+  v0 = _mm_adds_epi16(v0, v1); \
+  v2 = _mm_unpack##lo##_epi8(i2, i3); \
+  v3 = _mm_unpack##lo##_epi8(i4, i5); \
+  v2 = _mm_maddubs_epi16(v2, secondFilters); \
+  v3 = _mm_maddubs_epi16(v3, thirdFilters); \
+  v0 = _mm_adds_epi16(v0, _mm_adds_epi16(v2, v3)); \
+  v0 = _mm_adds_epi16(v0, round); \
+  v0 = _mm_srai_epi16(v0, 6);
+
+void aom_filter_block1d8_v8_ssse3(
+    const uint8_t *src_ptr, ptrdiff_t src_pixels_per_line, uint8_t *output_ptr,
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter) {
+  __m128i round, i0, i1, i2, i3, i4, i5, i6, i7, v0, v1, v2, v3;
+  __m128i firstFilters, secondFilters, thirdFilters, forthFilters;
+  unsigned int i;
+
+  INIT_V8(_mm_loadl_epi64)
+
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < output_height; i++) {
+    i7 = _mm_loadl_epi64((const __m128i *)src_ptr);
+    src_ptr += src_pixels_per_line;
+    FILTER_V8(lo, v0)
+    v0 = _mm_packus_epi16(v0, v0);
+    _mm_storel_epi64((__m128i *)output_ptr, v0);
+    output_ptr += output_pitch;
+    i0 = i1; i1 = i2; i2 = i3; i3 = i4; i4 = i5; i5 = i6; i6 = i7;
+  }
+}
+
+void aom_filter_block1d16_v8_ssse3(
+    const uint8_t *src_ptr, ptrdiff_t src_pixels_per_line, uint8_t *output_ptr,
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter) {
+  __m128i round, i0, i1, i2, i3, i4, i5, i6, i7, v0, v1, v2, v3, v4;
+  __m128i firstFilters, secondFilters, thirdFilters, forthFilters;
+  unsigned int i;
+
+  INIT_V8(_mm_loadu_si128)
+
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < output_height; i++) {
+    i7 = _mm_loadu_si128((const __m128i *)src_ptr);
+    src_ptr += src_pixels_per_line;
+    FILTER_V8(lo, v4)
+    FILTER_V8(hi, v0)
+    v0 = _mm_packus_epi16(v4, v0);
+    _mm_store_si128((__m128i *)output_ptr, v0);
+    output_ptr += output_pitch;
+    i0 = i1; i1 = i2; i2 = i3; i3 = i4; i4 = i5; i5 = i6; i6 = i7;
+  }
+}
+
+void aom_filter_block1d4_v2_ssse3(
+    const uint8_t *src_ptr, ptrdiff_t src_pixels_per_line, uint8_t *output_ptr,
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter) {
+  __m64 round, firstFilters, v0, v2, v3;
+  unsigned int i;
+
+  round = _mm_set1_pi16(32);
+  v0 = _mm_cvtsi32_si64(*(const uint32_t *)(filter + 3));
+  v0 = _mm_srai_pi16(v0, 1);
+  v0 = _mm_packs_pi16(v0, v0);
+  firstFilters = _mm_shuffle_pi16(v0, 0);
+
+  v2 = _mm_cvtsi32_si64(*(const uint32_t *)src_ptr);
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < output_height; i++) {
+    src_ptr += src_pixels_per_line;
+    v3 = _mm_cvtsi32_si64(*(const uint32_t *)src_ptr);
+
+    v0 = _mm_unpacklo_pi8(v2, v3);
+    v0 = _mm_maddubs_pi16(v0, firstFilters);
+    v0 = _mm_adds_pi16(v0, round);
+    v0 = _mm_srai_pi16(v0, 6);
+    v0 = _mm_packs_pu16(v0, v0);
+    *(uint32_t *)output_ptr = _mm_cvtsi64_si32(v0);
+    output_ptr += output_pitch;
+    v2 = v3;
+  }
+}
+
+void aom_filter_block1d8_v2_ssse3(
+    const uint8_t *src_ptr, ptrdiff_t src_pixels_per_line, uint8_t *output_ptr,
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter) {
+  __m128i round, firstFilters, v0, v2, v3;
+  unsigned int i;
+
+  round = _mm_set1_epi16(32);
+  v0 = _mm_cvtsi32_si128(*(const uint32_t *)(filter + 3));
+  v0 = _mm_srai_epi16(v0, 1);
+  v0 = _mm_packs_epi16(v0, v0);
+  firstFilters = _mm_shuffle_epi8(v0, _mm_set1_epi16(0x100));
+
+  v2 = _mm_loadl_epi64((const __m128i *)src_ptr);
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < output_height; i++) {
+    src_ptr += src_pixels_per_line;
+    v3 = _mm_loadl_epi64((const __m128i *)src_ptr);
+
+    v0 = _mm_unpacklo_epi8(v2, v3);
+    v0 = _mm_maddubs_epi16(v0, firstFilters);
+    v0 = _mm_adds_epi16(v0, round);
+    v0 = _mm_srai_epi16(v0, 6);
+    v0 = _mm_packus_epi16(v0, v0);
+    _mm_storel_epi64((__m128i *)output_ptr, v0);
+    output_ptr += output_pitch;
+    v2 = v3;
+  }
+}
+
+void aom_filter_block1d16_v2_ssse3(
+    const uint8_t *src_ptr, ptrdiff_t src_pixels_per_line, uint8_t *output_ptr,
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter) {
+  __m128i round, firstFilters, v0, v1, v2, v3;
+  unsigned int i;
+
+  round = _mm_set1_epi16(32);
+  v0 = _mm_cvtsi32_si128(*(const uint32_t *)(filter + 3));
+  v0 = _mm_srai_epi16(v0, 1);
+  v0 = _mm_packs_epi16(v0, v0);
+  firstFilters = _mm_shuffle_epi8(v0, _mm_set1_epi16(0x100));
+
+  v2 = _mm_loadu_si128((const __m128i *)src_ptr);
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < output_height; i++) {
+    src_ptr += src_pixels_per_line;
+    v3 = _mm_loadu_si128((const __m128i *)src_ptr);
+
+    v0 = _mm_unpacklo_epi8(v2, v3);
+    v1 = _mm_unpackhi_epi8(v2, v3);
+    v0 = _mm_maddubs_epi16(v0, firstFilters);
+    v1 = _mm_maddubs_epi16(v1, firstFilters);
+    v0 = _mm_adds_epi16(v0, round);
+    v1 = _mm_adds_epi16(v1, round);
+    v0 = _mm_srai_epi16(v0, 6);
+    v1 = _mm_srai_epi16(v1, 6);
+    v0 = _mm_packus_epi16(v0, v1);
+    _mm_store_si128((__m128i *)output_ptr, v0);
+    output_ptr += output_pitch;
+    v2 = v3;
+  }
+}
+
diff --git a/aom_dsp/e2k/highbd_intrapred_e2k.c b/aom_dsp/e2k/highbd_intrapred_e2k.c
new file mode 100644
index 0000000..5a266e6
--- /dev/null
+++ b/aom_dsp/e2k/highbd_intrapred_e2k.c
@@ -0,0 +1,143 @@
+/*
+ * Copyright (c) 2021, Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2016, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "config/av1_rtcd.h"
+
+void aom_highbd_dc_predictor_4x4_sse2(uint16_t *dst, ptrdiff_t stride,
+                                      const uint16_t *above,
+                                      const uint16_t *left, int bd) {
+  __m64 v0;
+  uint64_t sum; int i; (void)bd;
+  sum = *(uint64_t*)above + *(uint64_t*)left;
+  sum += sum >> 32;
+  sum += sum >> 16;
+  sum = ((uint16_t)sum + 4) >> 3;
+  v0 = _mm_set1_pi16(sum);
+  for (i = 0; i < 4; i++, dst += stride) {
+    *(__m64*)dst = v0;
+  }
+}
+
+void aom_highbd_dc_predictor_8x8_sse2(uint16_t *dst, ptrdiff_t stride,
+                                      const uint16_t *above,
+                                      const uint16_t *left, int bd) {
+  __m128i v0;
+  uint64_t sum; int i; (void)bd;
+  sum = *(uint64_t*)above + *(uint64_t*)(above + 4);
+  sum += *(uint64_t*)left + *(uint64_t*)(left + 4);
+  sum += sum >> 32;
+  sum += sum >> 16;
+  sum = ((int64_t)(uint16_t)sum + 8) >> 4;
+  v0 = _mm_set1_epi16(sum);
+  for (i = 0; i < 8; i++, dst += stride) {
+    _mm_store_si128((__m128i *)dst, v0);
+  }
+}
+
+void aom_highbd_dc_predictor_16x16_sse2(uint16_t *dst, ptrdiff_t stride,
+                                        const uint16_t *above,
+                                        const uint16_t *left, int bd) {
+  __m128i v0; __m64 h0, vzero = _mm_setzero_si64();
+  uint64_t sum; int i; (void)bd;
+  sum = *(uint64_t*)above + *(uint64_t*)(above + 4);
+  sum += *(uint64_t*)(above + 8) + *(uint64_t*)(above + 12);
+  sum += *(uint64_t*)left + *(uint64_t*)(left + 4);
+  sum += *(uint64_t*)(left + 8) + *(uint64_t*)(left + 12);
+  h0 = _mm_cvtsi64_m64(sum);
+  sum = _mm_cvtm64_si64(_mm_unpacklo_pi16(h0, vzero));
+  sum += _mm_cvtm64_si64(_mm_unpackhi_pi16(h0, vzero));
+  sum += sum >> 32;
+  sum = ((uint64_t)(uint32_t)sum + 16) >> 5;
+  v0 = _mm_set1_epi16(sum);
+  for (i = 0; i < 16; i++, dst += stride) {
+    _mm_store_si128((__m128i *)dst, v0);
+    _mm_store_si128((__m128i *)(dst + 8), v0);
+  }
+}
+
+void aom_highbd_dc_predictor_32x32_sse2(uint16_t *dst, ptrdiff_t stride,
+                                        const uint16_t *above,
+                                        const uint16_t *left, int bd) {
+  __m128i v0; __m64 h0, vzero = _mm_setzero_si64();
+  uint64_t sum; int i; (void)bd;
+  sum = *(uint64_t*)above + *(uint64_t*)(above + 4);
+  sum += *(uint64_t*)(above + 8) + *(uint64_t*)(above + 12);
+  sum += *(uint64_t*)(above + 16) + *(uint64_t*)(above + 20);
+  sum += *(uint64_t*)(above + 24) + *(uint64_t*)(above + 28);
+  sum += *(uint64_t*)left + *(uint64_t*)(left + 4);
+  sum += *(uint64_t*)(left + 8) + *(uint64_t*)(left + 12);
+  sum += *(uint64_t*)(left + 16) + *(uint64_t*)(left + 20);
+  sum += *(uint64_t*)(left + 24) + *(uint64_t*)(left + 28);
+  h0 = _mm_cvtsi64_m64(sum);
+  sum = _mm_cvtm64_si64(_mm_unpacklo_pi16(h0, vzero));
+  sum += _mm_cvtm64_si64(_mm_unpackhi_pi16(h0, vzero));
+  sum += sum >> 32;
+  sum = ((uint64_t)(uint32_t)sum + 32) >> 6;
+  v0 = _mm_set1_epi16(sum);
+  for (i = 0; i < 32; i++, dst += stride) {
+    _mm_store_si128((__m128i *)dst, v0);
+    _mm_store_si128((__m128i *)(dst + 8), v0);
+    _mm_store_si128((__m128i *)(dst + 16), v0);
+    _mm_store_si128((__m128i *)(dst + 24), v0);
+  }
+}
+
+void aom_highbd_v_predictor_4x4_sse2(uint16_t *dst, ptrdiff_t stride,
+                                     const uint16_t *above,
+                                     const uint16_t *left, int bd) {
+  __m64 v0 = *(__m64 const *)above; int i;
+  (void)left; (void)bd;
+  for (i = 0; i < 4; i++, dst += stride) {
+    *(__m64*)dst = v0;
+  }
+}
+
+void aom_highbd_v_predictor_8x8_sse2(uint16_t *dst, ptrdiff_t stride,
+                                     const uint16_t *above,
+                                     const uint16_t *left, int bd) {
+  __m128i v0 = _mm_load_si128((__m128i const *)above);
+  int i; (void)left; (void)bd;
+  for (i = 0; i < 8; i++, dst += stride) {
+    _mm_store_si128((__m128i *)dst, v0);
+  }
+}
+
+void aom_highbd_v_predictor_16x16_sse2(uint16_t *dst, ptrdiff_t stride,
+                                       const uint16_t *above,
+                                       const uint16_t *left, int bd) {
+  __m128i v0 = _mm_load_si128((__m128i const *)above);
+  __m128i v1 = _mm_load_si128((__m128i const *)(above + 8));
+  int i; (void)left; (void)bd;
+  for (i = 0; i < 16; i++, dst += stride) {
+    _mm_store_si128((__m128i *)dst, v0);
+    _mm_store_si128((__m128i *)(dst + 8), v1);
+  }
+}
+
+void aom_highbd_v_predictor_32x32_sse2(uint16_t *dst, ptrdiff_t stride,
+                                       const uint16_t *above,
+                                       const uint16_t *left, int bd) {
+  __m128i v0 = _mm_load_si128((__m128i const *)above);
+  __m128i v1 = _mm_load_si128((__m128i const *)(above + 8));
+  __m128i v2 = _mm_load_si128((__m128i const *)(above + 16));
+  __m128i v3 = _mm_load_si128((__m128i const *)(above + 24));
+  int i; (void)left; (void)bd;
+  for (i = 0; i < 32; i++, dst += stride) {
+    _mm_store_si128((__m128i *)dst, v0);
+    _mm_store_si128((__m128i *)(dst + 8), v1);
+    _mm_store_si128((__m128i *)(dst + 16), v2);
+    _mm_store_si128((__m128i *)(dst + 24), v3);
+  }
+}
+
diff --git a/aom_dsp/e2k/highbd_sad4d_e2k.c b/aom_dsp/e2k/highbd_sad4d_e2k.c
new file mode 100644
index 0000000..9996081
--- /dev/null
+++ b/aom_dsp/e2k/highbd_sad4d_e2k.c
@@ -0,0 +1,326 @@
+/*
+ * Copyright (c) 2021, Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2016, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "config/aom_dsp_rtcd.h"
+
+#include "aom_ports/mem.h"
+
+#define NO_AVG(...)
+#define WITH_AVG(...) __VA_ARGS__
+
+#define FN(w, h) \
+  SAD##w##XN(h, NO_AVG, aom_highbd_sad, x4d_sse2) \
+  /* SAD##w##XN(h, WITH_AVG, aom_highbd_sad, x4d_avg_sse2) */
+#define SAD_LOOP(h, n) \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < h; i += n)
+#define RESULT_SHL(x, pi) x
+
+#define SAD_START(IS_AVG, vec, si) (const uint8_t *_src, int src_stride, \
+                                    const uint8_t * const ref_ptr[], int ref_stride \
+                                    IS_AVG(, const uint8_t *_second_pred), uint32_t *sad_array) { \
+  vec v0, v1, v2, v3, v4, t0, t1, t2, t3, vzero = _mm_setzero_##si(); \
+  vec vsum0 = vzero, vsum1 = vzero, vsum2 = vzero, vsum3 = vzero; \
+  const uint16_t *src = CONVERT_TO_SHORTPTR(_src); \
+  const uint16_t *ref0 = CONVERT_TO_SHORTPTR(ref_ptr[0]); \
+  const uint16_t *ref1 = CONVERT_TO_SHORTPTR(ref_ptr[1]); \
+  const uint16_t *ref2 = CONVERT_TO_SHORTPTR(ref_ptr[2]); \
+  const uint16_t *ref3 = CONVERT_TO_SHORTPTR(ref_ptr[3]); \
+  IS_AVG(const uint16_t *sec = CONVERT_TO_SHORTPTR(_second_pred);) \
+  int i;
+
+#define PROCESS_8x4(IS_AVG, i, j, k, v0, v1, v2, v3) \
+  v0 = _mm_loadu_si128((const __m128i*)(ref0 + i)); \
+  v1 = _mm_loadu_si128((const __m128i*)(ref1 + i)); \
+  v2 = _mm_loadu_si128((const __m128i*)(ref2 + i)); \
+  v3 = _mm_loadu_si128((const __m128i*)(ref3 + i)); \
+  IS_AVG( \
+    v4 = _mm_loadu_si128((const __m128i*)(sec + j)); \
+    v0 = _mm_avg_epu16(v0, v4); \
+    v1 = _mm_avg_epu16(v1, v4); \
+    v2 = _mm_avg_epu16(v2, v4); \
+    v3 = _mm_avg_epu16(v3, v4); \
+  ) \
+  v4 = _mm_loadu_si128((const __m128i*)(src + k)); \
+  v0 = _mm_abs_epi16(_mm_sub_epi16(v0, v4)); \
+  v1 = _mm_abs_epi16(_mm_sub_epi16(v1, v4)); \
+  v2 = _mm_abs_epi16(_mm_sub_epi16(v2, v4)); \
+  v3 = _mm_abs_epi16(_mm_sub_epi16(v3, v4));
+
+#define PROCESS_4x4(IS_AVG, i, j, k, v0, v1, v2, v3) \
+  v0 = *(const __m64*)(ref0 + i); \
+  v1 = *(const __m64*)(ref1 + i); \
+  v2 = *(const __m64*)(ref2 + i); \
+  v3 = *(const __m64*)(ref3 + i); \
+  IS_AVG( \
+    v4 = *(const __m64*)(sec + j); \
+    v0 = _mm_avg_pu16(v0, v4); \
+    v1 = _mm_avg_pu16(v1, v4); \
+    v2 = _mm_avg_pu16(v2, v4); \
+    v3 = _mm_avg_pu16(v3, v4); \
+  ) \
+  v4 = *(const __m64*)(src + k); \
+  v0 = _mm_abs_pi16(_mm_sub_pi16(v0, v4)); \
+  v1 = _mm_abs_pi16(_mm_sub_pi16(v1, v4)); \
+  v2 = _mm_abs_pi16(_mm_sub_pi16(v2, v4)); \
+  v3 = _mm_abs_pi16(_mm_sub_pi16(v3, v4));
+
+#define SAD64XN(h, IS_AVG, name, end) \
+void name##64x##h##end SAD_START(IS_AVG, __m128i, si128) \
+  SAD_LOOP(h, 1) { \
+    PROCESS_8x4(IS_AVG, 0, 0, 0, t0, t1, t2, t3) \
+    PROCESS_8x4(IS_AVG, 8, 8, 8, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    PROCESS_8x4(IS_AVG, 16, 16, 16, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    PROCESS_8x4(IS_AVG, 24, 24, 24, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    vsum0 = _mm_add_epi32(vsum0, _mm_unpacklo_epi16(t0, vzero)); \
+    vsum0 = _mm_add_epi32(vsum0, _mm_unpackhi_epi16(t0, vzero)); \
+    vsum1 = _mm_add_epi32(vsum1, _mm_unpacklo_epi16(t1, vzero)); \
+    vsum1 = _mm_add_epi32(vsum1, _mm_unpackhi_epi16(t1, vzero)); \
+    vsum2 = _mm_add_epi32(vsum2, _mm_unpacklo_epi16(t2, vzero)); \
+    vsum2 = _mm_add_epi32(vsum2, _mm_unpackhi_epi16(t2, vzero)); \
+    vsum3 = _mm_add_epi32(vsum3, _mm_unpacklo_epi16(t3, vzero)); \
+    vsum3 = _mm_add_epi32(vsum3, _mm_unpackhi_epi16(t3, vzero)); \
+    \
+    PROCESS_8x4(IS_AVG, 32, 32, 32, t0, t1, t2, t3) \
+    PROCESS_8x4(IS_AVG, 40, 40, 40, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    PROCESS_8x4(IS_AVG, 48, 48, 48, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    PROCESS_8x4(IS_AVG, 56, 56, 56, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    vsum0 = _mm_add_epi32(vsum0, _mm_unpacklo_epi16(t0, vzero)); \
+    vsum0 = _mm_add_epi32(vsum0, _mm_unpackhi_epi16(t0, vzero)); \
+    vsum1 = _mm_add_epi32(vsum1, _mm_unpacklo_epi16(t1, vzero)); \
+    vsum1 = _mm_add_epi32(vsum1, _mm_unpackhi_epi16(t1, vzero)); \
+    vsum2 = _mm_add_epi32(vsum2, _mm_unpacklo_epi16(t2, vzero)); \
+    vsum2 = _mm_add_epi32(vsum2, _mm_unpackhi_epi16(t2, vzero)); \
+    vsum3 = _mm_add_epi32(vsum3, _mm_unpacklo_epi16(t3, vzero)); \
+    vsum3 = _mm_add_epi32(vsum3, _mm_unpackhi_epi16(t3, vzero)); \
+    IS_AVG(second_pred += 64;) \
+    ref0 += ref_stride; \
+    ref1 += ref_stride; \
+    ref2 += ref_stride; \
+    ref3 += ref_stride; \
+    src += src_stride; \
+  } \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum1); \
+  vsum2 = _mm_hadd_epi32(vsum2, vsum3); \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum2); \
+  _mm_storeu_si128((__m128i*)sad_array, RESULT_SHL(vsum0, epi)); \
+}
+
+#define SAD32XN(h, IS_AVG, name, end) \
+void name##32x##h##end SAD_START(IS_AVG, __m128i, si128) \
+  SAD_LOOP(h, 1) { \
+    PROCESS_8x4(IS_AVG, 0, 0, 0, t0, t1, t2, t3) \
+    PROCESS_8x4(IS_AVG, 8, 8, 8, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    PROCESS_8x4(IS_AVG, 16, 16, 16, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    PROCESS_8x4(IS_AVG, 24, 24, 24, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    vsum0 = _mm_add_epi32(vsum0, _mm_unpacklo_epi16(t0, vzero)); \
+    vsum0 = _mm_add_epi32(vsum0, _mm_unpackhi_epi16(t0, vzero)); \
+    vsum1 = _mm_add_epi32(vsum1, _mm_unpacklo_epi16(t1, vzero)); \
+    vsum1 = _mm_add_epi32(vsum1, _mm_unpackhi_epi16(t1, vzero)); \
+    vsum2 = _mm_add_epi32(vsum2, _mm_unpacklo_epi16(t2, vzero)); \
+    vsum2 = _mm_add_epi32(vsum2, _mm_unpackhi_epi16(t2, vzero)); \
+    vsum3 = _mm_add_epi32(vsum3, _mm_unpacklo_epi16(t3, vzero)); \
+    vsum3 = _mm_add_epi32(vsum3, _mm_unpackhi_epi16(t3, vzero)); \
+    IS_AVG(second_pred += 32;) \
+    ref0 += ref_stride; \
+    ref1 += ref_stride; \
+    ref2 += ref_stride; \
+    ref3 += ref_stride; \
+    src += src_stride; \
+  } \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum1); \
+  vsum2 = _mm_hadd_epi32(vsum2, vsum3); \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum2); \
+  _mm_storeu_si128((__m128i*)sad_array, RESULT_SHL(vsum0, epi)); \
+}
+
+#define SAD16XN(h, IS_AVG, name, end) \
+void name##16x##h##end SAD_START(IS_AVG, __m128i, si128) \
+  SAD_LOOP(h, 2) { \
+    PROCESS_8x4(IS_AVG, 0, 0, 0, t0, t1, t2, t3) \
+    PROCESS_8x4(IS_AVG, 8, 8, 8, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    PROCESS_8x4(IS_AVG, ref_stride, 16, src_stride, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    PROCESS_8x4(IS_AVG, ref_stride + 8, 24, src_stride + 8, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    vsum0 = _mm_add_epi32(vsum0, _mm_unpacklo_epi16(t0, vzero)); \
+    vsum0 = _mm_add_epi32(vsum0, _mm_unpackhi_epi16(t0, vzero)); \
+    vsum1 = _mm_add_epi32(vsum1, _mm_unpacklo_epi16(t1, vzero)); \
+    vsum1 = _mm_add_epi32(vsum1, _mm_unpackhi_epi16(t1, vzero)); \
+    vsum2 = _mm_add_epi32(vsum2, _mm_unpacklo_epi16(t2, vzero)); \
+    vsum2 = _mm_add_epi32(vsum2, _mm_unpackhi_epi16(t2, vzero)); \
+    vsum3 = _mm_add_epi32(vsum3, _mm_unpacklo_epi16(t3, vzero)); \
+    vsum3 = _mm_add_epi32(vsum3, _mm_unpackhi_epi16(t3, vzero)); \
+    IS_AVG(second_pred += 32;) \
+    ref0 += ref_stride << 1; \
+    ref1 += ref_stride << 1; \
+    ref2 += ref_stride << 1; \
+    ref3 += ref_stride << 1; \
+    src += src_stride << 1; \
+  } \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum1); \
+  vsum2 = _mm_hadd_epi32(vsum2, vsum3); \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum2); \
+  _mm_storeu_si128((__m128i*)sad_array, RESULT_SHL(vsum0, epi)); \
+}
+
+#define SAD8XN(h, IS_AVG, name, end) \
+void name##8x##h##end SAD_START(IS_AVG, __m128i, si128) \
+  SAD_LOOP(h, 4) { \
+    PROCESS_8x4(IS_AVG, 0, 0, 0, t0, t1, t2, t3) \
+    PROCESS_8x4(IS_AVG, ref_stride, 8, src_stride, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    PROCESS_8x4(IS_AVG, ref_stride * 2, 16, src_stride * 2, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    PROCESS_8x4(IS_AVG, ref_stride * 3, 24, src_stride * 3, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    vsum0 = _mm_add_epi32(vsum0, _mm_unpacklo_epi16(t0, vzero)); \
+    vsum0 = _mm_add_epi32(vsum0, _mm_unpackhi_epi16(t0, vzero)); \
+    vsum1 = _mm_add_epi32(vsum1, _mm_unpacklo_epi16(t1, vzero)); \
+    vsum1 = _mm_add_epi32(vsum1, _mm_unpackhi_epi16(t1, vzero)); \
+    vsum2 = _mm_add_epi32(vsum2, _mm_unpacklo_epi16(t2, vzero)); \
+    vsum2 = _mm_add_epi32(vsum2, _mm_unpackhi_epi16(t2, vzero)); \
+    vsum3 = _mm_add_epi32(vsum3, _mm_unpacklo_epi16(t3, vzero)); \
+    vsum3 = _mm_add_epi32(vsum3, _mm_unpackhi_epi16(t3, vzero)); \
+    IS_AVG(second_pred += 32;) \
+    ref0 += ref_stride << 2; \
+    ref1 += ref_stride << 2; \
+    ref2 += ref_stride << 2; \
+    ref3 += ref_stride << 2; \
+    src += src_stride << 2; \
+  } \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum1); \
+  vsum2 = _mm_hadd_epi32(vsum2, vsum3); \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum2); \
+  _mm_storeu_si128((__m128i*)sad_array, RESULT_SHL(vsum0, epi)); \
+}
+
+#define SAD4XN(h, IS_AVG, name, end) \
+void name##4x##h##end SAD_START(IS_AVG, __m64, si64) \
+  SAD_LOOP(h, 4) { \
+    PROCESS_4x4(IS_AVG, 0, 0, 0, t0, t1, t2, t3) \
+    PROCESS_4x4(IS_AVG, ref_stride, 4, src_stride, v0, v1, v2, v3) \
+    t0 = _mm_add_pi16(t0, v0); \
+    t1 = _mm_add_pi16(t1, v1); \
+    t2 = _mm_add_pi16(t2, v2); \
+    t3 = _mm_add_pi16(t3, v3); \
+    PROCESS_4x4(IS_AVG, ref_stride * 2, 8, src_stride * 2, v0, v1, v2, v3) \
+    t0 = _mm_add_pi16(t0, v0); \
+    t1 = _mm_add_pi16(t1, v1); \
+    t2 = _mm_add_pi16(t2, v2); \
+    t3 = _mm_add_pi16(t3, v3); \
+    PROCESS_4x4(IS_AVG, ref_stride * 3, 12, src_stride * 3, v0, v1, v2, v3) \
+    t0 = _mm_add_pi16(t0, v0); \
+    t1 = _mm_add_pi16(t1, v1); \
+    t2 = _mm_add_pi16(t2, v2); \
+    t3 = _mm_add_pi16(t3, v3); \
+    vsum0 = _mm_add_pi32(vsum0, _mm_unpacklo_pi16(t0, vzero)); \
+    vsum0 = _mm_add_pi32(vsum0, _mm_unpackhi_pi16(t0, vzero)); \
+    vsum1 = _mm_add_pi32(vsum1, _mm_unpacklo_pi16(t1, vzero)); \
+    vsum1 = _mm_add_pi32(vsum1, _mm_unpackhi_pi16(t1, vzero)); \
+    vsum2 = _mm_add_pi32(vsum2, _mm_unpacklo_pi16(t2, vzero)); \
+    vsum2 = _mm_add_pi32(vsum2, _mm_unpackhi_pi16(t2, vzero)); \
+    vsum3 = _mm_add_pi32(vsum3, _mm_unpacklo_pi16(t3, vzero)); \
+    vsum3 = _mm_add_pi32(vsum3, _mm_unpackhi_pi16(t3, vzero)); \
+    IS_AVG(second_pred += 16;) \
+    ref0 += ref_stride << 2; \
+    ref1 += ref_stride << 2; \
+    ref2 += ref_stride << 2; \
+    ref3 += ref_stride << 2; \
+    src += src_stride << 2; \
+  } \
+  *(__m64*)sad_array = RESULT_SHL(_mm_hadd_pi32(vsum0, vsum1), pi); \
+  *(__m64*)(sad_array + 2) = RESULT_SHL(_mm_hadd_pi32(vsum2, vsum3), pi); \
+}
+
+FN(64, 64) FN(64, 32) FN(64, 16)
+FN(32, 64) FN(32, 32) FN(32, 16) FN(32, 8)
+FN(16, 64) FN(16, 32) FN(16, 16) FN(16, 8) FN(16, 4)
+FN(8, 32) FN(8, 16) FN(8, 8) FN(8, 4)
+FN(4, 16) FN(4, 8) FN(4, 4)
+
+#undef FN
+#define FN(w, h) SAD##w##XN(h, NO_AVG, aom_highbd_sad_skip_, x4d_sse2)
+#undef SAD_LOOP
+#define SAD_LOOP(h, n) \
+  src_stride <<= 1; ref_stride <<= 1; \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < h / 2; i += n)
+#undef RESULT_SHL
+#define RESULT_SHL(x, pi) _mm_slli_##pi##32(x, 1)
+
+FN(64, 64) FN(64, 32) FN(64, 16)
+FN(32, 64) FN(32, 32) FN(32, 16) FN(32, 8)
+FN(16, 64) FN(16, 32) FN(16, 16) FN(16, 8)
+FN(8, 32) FN(8, 16) FN(8, 8)
+FN(4, 16) FN(4, 8)
+
+#if 0 // Current code cannot handle the case when the height is downsampled to 2
+FN(16, 4) FN(8, 4) FN(4, 4)
+#endif
diff --git a/aom_dsp/e2k/highbd_sad_e2k.c b/aom_dsp/e2k/highbd_sad_e2k.c
new file mode 100644
index 0000000..02c5848
--- /dev/null
+++ b/aom_dsp/e2k/highbd_sad_e2k.c
@@ -0,0 +1,233 @@
+/*
+ * Copyright (c) 2021, Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2016, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "config/aom_dsp_rtcd.h"
+
+#include "aom_ports/mem.h"
+
+#define NO_AVG(...)
+#define WITH_AVG(...) __VA_ARGS__
+
+#define FN(w, h) \
+  SAD##w##XN(h, NO_AVG, aom_highbd_sad, _sse2) \
+  SAD##w##XN(h, WITH_AVG, aom_highbd_sad, _avg_sse2)
+#define SAD_LOOP(h, n) \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < h; i += n)
+#define RESULT_SHL(x) x
+
+#define SAD_START(IS_AVG, vec, si) (const uint8_t *_src, int src_stride, \
+                                    const uint8_t *_ref, int ref_stride \
+                                    IS_AVG(, const uint8_t *_second_pred)) { \
+  vec v0, v1, v2, v3, vzero = _mm_setzero_##si(), vsum = vzero; \
+  const uint16_t *src = CONVERT_TO_SHORTPTR(_src); \
+  const uint16_t *ref = CONVERT_TO_SHORTPTR(_ref); \
+  IS_AVG(const uint16_t *sec = CONVERT_TO_SHORTPTR(_second_pred);) \
+  int i;
+
+#define SAD64XN(h, IS_AVG, name, end) \
+unsigned int name##64x##h##end SAD_START(IS_AVG, __m128i, si128) \
+  SAD_LOOP(h, 1) { \
+    v0 = _mm_loadu_si128((const __m128i*)ref); \
+    v1 = _mm_loadu_si128((const __m128i*)(ref + 8)); \
+    v2 = _mm_loadu_si128((const __m128i*)(ref + 16)); \
+    v3 = _mm_loadu_si128((const __m128i*)(ref + 24)); \
+    IS_AVG( \
+      v0 = _mm_avg_epu16(v0, _mm_loadu_si128((const __m128i*)sec)); \
+      v1 = _mm_avg_epu16(v1, _mm_loadu_si128((const __m128i*)(sec + 8))); \
+      v2 = _mm_avg_epu16(v2, _mm_loadu_si128((const __m128i*)(sec + 16))); \
+      v3 = _mm_avg_epu16(v3, _mm_loadu_si128((const __m128i*)(sec + 24))); \
+    ) \
+    v0 = _mm_sub_epi16(v0, _mm_loadu_si128((const __m128i*)src)); \
+    v1 = _mm_sub_epi16(v1, _mm_loadu_si128((const __m128i*)(src + 8))); \
+    v2 = _mm_sub_epi16(v2, _mm_loadu_si128((const __m128i*)(src + 16))); \
+    v3 = _mm_sub_epi16(v3, _mm_loadu_si128((const __m128i*)(src + 24))); \
+    v0 = _mm_add_epi16(_mm_abs_epi16(v0), _mm_abs_epi16(v1)); \
+    v2 = _mm_add_epi16(_mm_abs_epi16(v2), _mm_abs_epi16(v3)); \
+    v0 = _mm_add_epi16(v0, v2); \
+    vsum = _mm_add_epi32(vsum, _mm_unpacklo_epi16(v0, vzero)); \
+    vsum = _mm_add_epi32(vsum, _mm_unpackhi_epi16(v0, vzero)); \
+    \
+    v0 = _mm_loadu_si128((const __m128i*)(ref + 32)); \
+    v1 = _mm_loadu_si128((const __m128i*)(ref + 40)); \
+    v2 = _mm_loadu_si128((const __m128i*)(ref + 48)); \
+    v3 = _mm_loadu_si128((const __m128i*)(ref + 56)); \
+    IS_AVG( \
+      v0 = _mm_avg_epu16(v0, _mm_loadu_si128((const __m128i*)(sec + 32))); \
+      v1 = _mm_avg_epu16(v1, _mm_loadu_si128((const __m128i*)(sec + 40))); \
+      v2 = _mm_avg_epu16(v2, _mm_loadu_si128((const __m128i*)(sec + 48))); \
+      v3 = _mm_avg_epu16(v3, _mm_loadu_si128((const __m128i*)(sec + 56))); \
+      sec += 64; \
+    ) \
+    v0 = _mm_sub_epi16(v0, _mm_loadu_si128((const __m128i*)(src + 32))); \
+    v1 = _mm_sub_epi16(v1, _mm_loadu_si128((const __m128i*)(src + 40))); \
+    v2 = _mm_sub_epi16(v2, _mm_loadu_si128((const __m128i*)(src + 48))); \
+    v3 = _mm_sub_epi16(v3, _mm_loadu_si128((const __m128i*)(src + 56))); \
+    v0 = _mm_add_epi16(_mm_abs_epi16(v0), _mm_abs_epi16(v1)); \
+    v2 = _mm_add_epi16(_mm_abs_epi16(v2), _mm_abs_epi16(v3)); \
+    v0 = _mm_add_epi16(v0, v2); \
+    vsum = _mm_add_epi32(vsum, _mm_unpacklo_epi16(v0, vzero)); \
+    vsum = _mm_add_epi32(vsum, _mm_unpackhi_epi16(v0, vzero)); \
+    ref += ref_stride; \
+    src += src_stride; \
+  } \
+  vsum = _mm_hadd_epi32(vsum, vsum); \
+  return RESULT_SHL(_mm_cvtsi128_si32(vsum) + _mm_extract_epi32(vsum, 1)); \
+}
+
+#define SAD32XN(h, IS_AVG, name, end) \
+unsigned int name##32x##h##end SAD_START(IS_AVG, __m128i, si128) \
+  SAD_LOOP(h, 1) { \
+    v0 = _mm_loadu_si128((const __m128i*)ref); \
+    v1 = _mm_loadu_si128((const __m128i*)(ref + 8)); \
+    v2 = _mm_loadu_si128((const __m128i*)(ref + 16)); \
+    v3 = _mm_loadu_si128((const __m128i*)(ref + 24)); \
+    IS_AVG( \
+      v0 = _mm_avg_epu16(v0, _mm_loadu_si128((const __m128i*)sec)); \
+      v1 = _mm_avg_epu16(v1, _mm_loadu_si128((const __m128i*)(sec + 8))); \
+      v2 = _mm_avg_epu16(v2, _mm_loadu_si128((const __m128i*)(sec + 16))); \
+      v3 = _mm_avg_epu16(v3, _mm_loadu_si128((const __m128i*)(sec + 24))); \
+      sec += 32; \
+    ) \
+    v0 = _mm_sub_epi16(v0, _mm_loadu_si128((const __m128i*)src)); \
+    v1 = _mm_sub_epi16(v1, _mm_loadu_si128((const __m128i*)(src + 8))); \
+    v2 = _mm_sub_epi16(v2, _mm_loadu_si128((const __m128i*)(src + 16))); \
+    v3 = _mm_sub_epi16(v3, _mm_loadu_si128((const __m128i*)(src + 24))); \
+    v0 = _mm_add_epi16(_mm_abs_epi16(v0), _mm_abs_epi16(v1)); \
+    v2 = _mm_add_epi16(_mm_abs_epi16(v2), _mm_abs_epi16(v3)); \
+    v0 = _mm_add_epi16(v0, v2); \
+    vsum = _mm_add_epi32(vsum, _mm_unpacklo_epi16(v0, vzero)); \
+    vsum = _mm_add_epi32(vsum, _mm_unpackhi_epi16(v0, vzero)); \
+    ref += ref_stride; \
+    src += src_stride; \
+  } \
+  vsum = _mm_hadd_epi32(vsum, vsum); \
+  return RESULT_SHL(_mm_cvtsi128_si32(vsum) + _mm_extract_epi32(vsum, 1)); \
+}
+
+#define SAD16XN(h, IS_AVG, name, end) \
+unsigned int name##16x##h##end SAD_START(IS_AVG, __m128i, si128) \
+  SAD_LOOP(h, 2) { \
+    v0 = _mm_loadu_si128((const __m128i*)ref); \
+    v1 = _mm_loadu_si128((const __m128i*)(ref + 8)); \
+    v2 = _mm_loadu_si128((const __m128i*)(ref + ref_stride)); \
+    v3 = _mm_loadu_si128((const __m128i*)(ref + ref_stride + 8)); \
+    IS_AVG( \
+      v0 = _mm_avg_epu16(v0, _mm_loadu_si128((const __m128i*)sec)); \
+      v1 = _mm_avg_epu16(v1, _mm_loadu_si128((const __m128i*)(sec + 8))); \
+      v2 = _mm_avg_epu16(v2, _mm_loadu_si128((const __m128i*)(sec + 16))); \
+      v3 = _mm_avg_epu16(v3, _mm_loadu_si128((const __m128i*)(sec + 24))); \
+      sec += 32; \
+    ) \
+    v0 = _mm_sub_epi16(v0, _mm_loadu_si128((const __m128i*)src)); \
+    v1 = _mm_sub_epi16(v1, _mm_loadu_si128((const __m128i*)(src + 8))); \
+    v2 = _mm_sub_epi16(v2, _mm_loadu_si128((const __m128i*)(src + src_stride))); \
+    v3 = _mm_sub_epi16(v3, _mm_loadu_si128((const __m128i*)(src + src_stride + 8))); \
+    v0 = _mm_add_epi16(_mm_abs_epi16(v0), _mm_abs_epi16(v1)); \
+    v2 = _mm_add_epi16(_mm_abs_epi16(v2), _mm_abs_epi16(v3)); \
+    v0 = _mm_add_epi16(v0, v2); \
+    vsum = _mm_add_epi32(vsum, _mm_unpacklo_epi16(v0, vzero)); \
+    vsum = _mm_add_epi32(vsum, _mm_unpackhi_epi16(v0, vzero)); \
+    ref += ref_stride << 1; \
+    src += src_stride << 1; \
+  } \
+  vsum = _mm_hadd_epi32(vsum, vsum); \
+  return RESULT_SHL(_mm_cvtsi128_si32(vsum) + _mm_extract_epi32(vsum, 1)); \
+}
+
+#define SAD8XN(h, IS_AVG, name, end) \
+unsigned int name##8x##h##end SAD_START(IS_AVG, __m128i, si128) \
+  SAD_LOOP(h, 4) { \
+    v0 = _mm_loadu_si128((const __m128i*)ref); \
+    v1 = _mm_loadu_si128((const __m128i*)(ref + ref_stride)); \
+    v2 = _mm_loadu_si128((const __m128i*)(ref + ref_stride * 2)); \
+    v3 = _mm_loadu_si128((const __m128i*)(ref + ref_stride * 3)); \
+    IS_AVG( \
+      v0 = _mm_avg_epu16(v0, _mm_loadu_si128((const __m128i*)sec)); \
+      v1 = _mm_avg_epu16(v1, _mm_loadu_si128((const __m128i*)(sec + 8))); \
+      v2 = _mm_avg_epu16(v2, _mm_loadu_si128((const __m128i*)(sec + 16))); \
+      v3 = _mm_avg_epu16(v3, _mm_loadu_si128((const __m128i*)(sec + 24))); \
+      sec += 32; \
+    ) \
+    v0 = _mm_sub_epi16(v0, _mm_loadu_si128((const __m128i*)src)); \
+    v1 = _mm_sub_epi16(v1, _mm_loadu_si128((const __m128i*)(src + src_stride))); \
+    v2 = _mm_sub_epi16(v2, _mm_loadu_si128((const __m128i*)(src + src_stride * 2))); \
+    v3 = _mm_sub_epi16(v3, _mm_loadu_si128((const __m128i*)(src + src_stride * 3))); \
+    v0 = _mm_add_epi16(_mm_abs_epi16(v0), _mm_abs_epi16(v1)); \
+    v2 = _mm_add_epi16(_mm_abs_epi16(v2), _mm_abs_epi16(v3)); \
+    v0 = _mm_add_epi16(v0, v2); \
+    vsum = _mm_add_epi32(vsum, _mm_unpacklo_epi16(v0, vzero)); \
+    vsum = _mm_add_epi32(vsum, _mm_unpackhi_epi16(v0, vzero)); \
+    ref += ref_stride << 2; \
+    src += src_stride << 2; \
+  } \
+  vsum = _mm_hadd_epi32(vsum, vsum); \
+  return RESULT_SHL(_mm_cvtsi128_si32(vsum) + _mm_extract_epi32(vsum, 1)); \
+}
+
+#define SAD4XN(h, IS_AVG, name, end) \
+unsigned int name##4x##h##end SAD_START(IS_AVG, __m64, si64) \
+  SAD_LOOP(h, 4) { \
+    v0 = *(const __m64*)ref; \
+    v1 = *(const __m64*)(ref + ref_stride); \
+    v2 = *(const __m64*)(ref + ref_stride * 2); \
+    v3 = *(const __m64*)(ref + ref_stride * 3); \
+    IS_AVG( \
+      v0 = _mm_avg_pu16(v0, *(const __m64*)sec); \
+      v1 = _mm_avg_pu16(v1, *(const __m64*)(sec + 4)); \
+      v2 = _mm_avg_pu16(v2, *(const __m64*)(sec + 8)); \
+      v3 = _mm_avg_pu16(v3, *(const __m64*)(sec + 12)); \
+      sec += 16; \
+    ) \
+    v0 = _mm_sub_pi16(v0, *(const __m64*)src); \
+    v1 = _mm_sub_pi16(v1, *(const __m64*)(src + src_stride)); \
+    v2 = _mm_sub_pi16(v2, *(const __m64*)(src + src_stride * 2)); \
+    v3 = _mm_sub_pi16(v3, *(const __m64*)(src + src_stride * 3)); \
+    v0 = _mm_add_pi16(_mm_abs_pi16(v0), _mm_abs_pi16(v1)); \
+    v2 = _mm_add_pi16(_mm_abs_pi16(v2), _mm_abs_pi16(v3)); \
+    v0 = _mm_add_pi16(v0, v2); \
+    vsum = _mm_add_pi32(vsum, _mm_unpacklo_pi16(v0, vzero)); \
+    vsum = _mm_add_pi32(vsum, _mm_unpackhi_pi16(v0, vzero)); \
+    ref += ref_stride << 2; \
+    src += src_stride << 2; \
+  } \
+  return RESULT_SHL(_mm_cvtsi64_si32(vsum) + _mm_extract_pi32(vsum, 1)); \
+}
+
+FN(64, 64) FN(64, 32) FN(64, 16)
+FN(32, 64) FN(32, 32) FN(32, 16) FN(32, 8)
+FN(16, 64) FN(16, 32) FN(16, 16) FN(16, 8) FN(16, 4)
+FN(8, 32) FN(8, 16) FN(8, 8) FN(8, 4)
+FN(4, 16) FN(4, 8) FN(4, 4)
+
+#undef FN
+#define FN(w, h) SAD##w##XN(h, NO_AVG, aom_highbd_sad_skip_, _sse2)
+#undef SAD_LOOP
+#define SAD_LOOP(h, n) \
+  src_stride <<= 1; ref_stride <<= 1; \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < h / 2; i += n)
+#undef RESULT_SHL
+#define RESULT_SHL(x) (x) << 1
+
+FN(64, 64) FN(64, 32) FN(64, 16)
+FN(32, 64) FN(32, 32) FN(32, 16) FN(32, 8)
+FN(16, 64) FN(16, 32) FN(16, 16) FN(16, 8)
+FN(8, 32) FN(8, 16) FN(8, 8)
+FN(4, 16) FN(4, 8)
+
+#if 0 // Different assembly is needed when the height gets subsampled to 2
+FN(16, 4) FN(8, 4) FN(4, 4)
+#endif
+
diff --git a/aom_dsp/e2k/highbd_subpel_variance_e2k.c b/aom_dsp/e2k/highbd_subpel_variance_e2k.c
new file mode 100644
index 0000000..e62b3e1
--- /dev/null
+++ b/aom_dsp/e2k/highbd_subpel_variance_e2k.c
@@ -0,0 +1,254 @@
+/*
+ * Copyright (c) 2021, Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2016, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "config/aom_dsp_rtcd.h"
+
+#define SEC_NONE
+#define SEC_AVG16 \
+  t0 = _mm_loadu_si128((__m128i const *)sec); \
+  t1 = _mm_loadu_si128((__m128i const *)(sec + 8)); \
+  v0 = _mm_avg_epu16(v0, t0); \
+  v1 = _mm_avg_epu16(v1, t1); \
+  sec += sec_stride;
+#define SEC_AVG8 \
+  t0 = _mm_loadu_si128((__m128i const *)sec); \
+  t1 = _mm_loadu_si128((__m128i const *)(sec + sec_stride)); \
+  v0 = _mm_avg_epu16(v0, t0); \
+  v1 = _mm_avg_epu16(v1, t1); \
+  sec += sec_stride << 1;
+
+#define UPDATE_SUM_SSE(SEC_AVG, dst_offset, dst_stride) \
+  SEC_AVG \
+  t0 = _mm_loadu_si128((__m128i const *)(dst)); \
+  t1 = _mm_loadu_si128((__m128i const *)(dst + dst_offset)); \
+  dst += dst_stride; \
+  v0 = _mm_sub_epi16(v0, t0); \
+  v1 = _mm_sub_epi16(v1, t1); \
+  t0 = _mm_add_epi16(v0, v1); \
+  t1 = _mm_srai_epi16(t0, 16); \
+  vsse = _mm_add_epi32(vsse, _mm_madd_epi16(v0, v0)); \
+  vsse = _mm_add_epi32(vsse, _mm_madd_epi16(v1, v1)); \
+  vsum = _mm_add_epi32(vsum, _mm_unpacklo_epi16(t0, t1)); \
+  vsum = _mm_add_epi32(vsum, _mm_unpackhi_epi16(t0, t1));
+
+#define RETURN_SUM_SSE \
+  vsse = _mm_add_epi32(vsse, _mm_bsrli_si128(vsse, 8)); \
+  vsum = _mm_add_epi32(vsum, _mm_bsrli_si128(vsum, 8)); \
+  *sse = _mm_cvtsi128_si32(vsse) + _mm_extract_epi32(vsse, 1); \
+  return _mm_cvtsi128_si32(vsum) + _mm_extract_epi32(vsum, 1);
+
+#define LOADX(v0, v1) \
+  v0 = _mm_loadu_si128((__m128i const *)(src)); \
+  v1 = _mm_loadu_si128((__m128i const *)(src + 8));
+
+#define AVGX(v0, v1) \
+  LOADX(t0, t1) \
+  t2 = _mm_loadu_si128((__m128i const *)(src + 1)); \
+  t3 = _mm_loadu_si128((__m128i const *)(src + 8 + 1)); \
+  v0 = _mm_avg_epu16(t0, t2); \
+  v1 = _mm_avg_epu16(t1, t3);
+
+#define FILTERX(v0, v1) \
+  LOADX(t0, t1) \
+  t2 = _mm_loadu_si128((__m128i const *)(src + 1)); \
+  t3 = _mm_loadu_si128((__m128i const *)(src + 8 + 1)); \
+  t2 = _mm_mulhrs_epi16(_mm_sub_epi16(t2, t0), xfilter); \
+  t3 = _mm_mulhrs_epi16(_mm_sub_epi16(t3, t1), xfilter); \
+  v0 = _mm_add_epi16(t0, t2); \
+  v1 = _mm_add_epi16(t1, t3);
+
+#define AVGY(v0, v1, v2, v3) \
+  v0 = _mm_avg_epu16(v0, v2); \
+  v1 = _mm_avg_epu16(v1, v3);
+
+#define FILTERY(v0, v1, v2, v3) \
+  t2 = _mm_mulhrs_epi16(_mm_sub_epi16(v2, v0), yfilter); \
+  t3 = _mm_mulhrs_epi16(_mm_sub_epi16(v3, v1), yfilter); \
+  v0 = _mm_add_epi16(v0, t2); \
+  v1 = _mm_add_epi16(v1, t3);
+
+#define VAR16XH(LOADX, SEC_AVG) \
+  if (y_offset == 0) { \
+    for (i = 0; i < height; i++) { \
+      LOADX(v0, v1) \
+      UPDATE_SUM_SSE(SEC_AVG, 8, dst_stride) \
+      src += src_stride; \
+    } \
+  } else if (y_offset == 4) { \
+    LOADX(v0, v1) \
+    for (i = 0; i < height; i++) { \
+      src += src_stride; \
+      LOADX(v2, v3) \
+      AVGY(v0, v1, v2, v3) \
+      UPDATE_SUM_SSE(SEC_AVG, 8, dst_stride) \
+      v0 = v2; v1 = v3; \
+    } \
+  } else { \
+    yfilter = _mm_set1_epi16(y_offset << 12); \
+    LOADX(v0, v1) \
+    for (i = 0; i < height; i++) { \
+      src += src_stride; \
+      LOADX(v2, v3) \
+      FILTERY(v0, v1, v2, v3) \
+      UPDATE_SUM_SSE(SEC_AVG, 8, dst_stride) \
+      v0 = v2; v1 = v3; \
+    } \
+  }
+
+int aom_highbd_sub_pixel_variance16xh_sse2(
+    const uint16_t *src, ptrdiff_t src_stride, int x_offset, int y_offset,
+    const uint16_t *dst, ptrdiff_t dst_stride, int height,
+    unsigned int *sse, void *unused0, void *unused) {
+  __m128i v0, v1, v2, v3, t0, t1, t2, t3, vzero = _mm_setzero_si128();
+  __m128i vsum = vzero, vsse = vzero, xfilter, yfilter;
+  int i;
+  (void)unused0; (void)unused;
+
+  if (x_offset == 0) {
+    VAR16XH(LOADX, SEC_NONE)
+  } else if (x_offset == 4) {
+    VAR16XH(AVGX, SEC_NONE)
+  } else {
+    xfilter = _mm_set1_epi16(x_offset << 12);
+    VAR16XH(FILTERX, SEC_NONE)
+  }
+  RETURN_SUM_SSE
+}
+
+int aom_highbd_sub_pixel_avg_variance16xh_sse2(
+    const uint16_t *src, ptrdiff_t src_stride, int x_offset, int y_offset,
+    const uint16_t *dst, ptrdiff_t dst_stride, const uint16_t *sec,
+    ptrdiff_t sec_stride, int height, unsigned int *sse, void *unused0,
+    void *unused) {
+  __m128i v0, v1, v2, v3, t0, t1, t2, t3, vzero = _mm_setzero_si128();
+  __m128i vsum = vzero, vsse = vzero, xfilter, yfilter;
+  int i;
+  (void)unused0; (void)unused;
+
+  if (x_offset == 0) {
+    VAR16XH(LOADX, SEC_AVG16)
+  } else if (x_offset == 4) {
+    VAR16XH(AVGX, SEC_AVG16)
+  } else {
+    xfilter = _mm_set1_epi16(x_offset << 12);
+    VAR16XH(FILTERX, SEC_AVG16)
+  }
+  RETURN_SUM_SSE
+}
+
+#define LOADX1 \
+  v0 = _mm_loadu_si128((__m128i const *)src);
+
+#define AVGX1 \
+  LOADX1 \
+  t2 = _mm_loadu_si128((__m128i const *)(src + 1)); \
+  v0 = _mm_avg_epu16(v0, t2);
+
+#define FILTERX1 \
+  LOADX1 \
+  t2 = _mm_loadu_si128((__m128i const *)(src + 1)); \
+  t2 = _mm_mulhrs_epi16(_mm_sub_epi16(t2, v0), xfilter); \
+  v0 = _mm_add_epi16(v0, t2);
+
+#define LOADX2(v0, v1) \
+  v0 = _mm_loadu_si128((__m128i const *)src); \
+  v1 = _mm_loadu_si128((__m128i const *)(src + src_stride));
+
+#define AVGX2(v0, v1) \
+  LOADX2(t0, t1) \
+  t2 = _mm_loadu_si128((__m128i const *)(src + 1)); \
+  t3 = _mm_loadu_si128((__m128i const *)(src + src_stride + 1)); \
+  v0 = _mm_avg_epu16(t0, t2); \
+  v1 = _mm_avg_epu16(t1, t3);
+
+#define FILTERX2(v0, v1) \
+  LOADX2(t0, t1) \
+  t2 = _mm_loadu_si128((__m128i const *)(src + 1)); \
+  t3 = _mm_loadu_si128((__m128i const *)(src + src_stride + 1)); \
+  t2 = _mm_mulhrs_epi16(_mm_sub_epi16(t2, t0), xfilter); \
+  t3 = _mm_mulhrs_epi16(_mm_sub_epi16(t3, t1), xfilter); \
+  v0 = _mm_add_epi16(t0, t2); \
+  v1 = _mm_add_epi16(t1, t3);
+
+#define VAR8XH(LOADX, SEC_AVG) \
+  if (y_offset == 0) { \
+    for (i = 0; i < height; i += 2) { \
+      LOADX##2(v0, v1) \
+      UPDATE_SUM_SSE(SEC_AVG, dst_stride, dst_stride << 1) \
+      src += src_stride << 1; \
+    } \
+  } else if (y_offset == 4) { \
+    LOADX##1 \
+    src += src_stride; \
+    for (i = 0; i < height; i += 2) { \
+      LOADX##2(v1, v2) \
+      src += src_stride << 1; \
+      AVGY(v0, v1, v1, v2) \
+      UPDATE_SUM_SSE(SEC_AVG, dst_stride, dst_stride << 1) \
+      v0 = v2; \
+    } \
+  } else { \
+    yfilter = _mm_set1_epi16(y_offset << 12); \
+    LOADX##1 \
+    src += src_stride; \
+    for (i = 0; i < height; i += 2) { \
+      LOADX##2(v1, v2) \
+      src += src_stride << 1; \
+      FILTERY(v0, v1, v1, v2) \
+      UPDATE_SUM_SSE(SEC_AVG, dst_stride, dst_stride << 1) \
+      v0 = v2; \
+    } \
+  }
+
+int aom_highbd_sub_pixel_variance8xh_sse2(
+    const uint16_t *src, ptrdiff_t src_stride, int x_offset, int y_offset,
+    const uint16_t *dst, ptrdiff_t dst_stride, int height,
+    unsigned int *sse, void *unused0, void *unused) {
+  __m128i v0, v1, v2, t0, t1, t2, t3, vzero = _mm_setzero_si128();
+  __m128i vsum = vzero, vsse = vzero, xfilter, yfilter;
+  int i;
+  (void)unused0; (void)unused;
+
+  if (x_offset == 0) {
+    VAR8XH(LOADX, SEC_NONE)
+  } else if (x_offset == 4) {
+    VAR8XH(AVGX, SEC_NONE)
+  } else {
+    xfilter = _mm_set1_epi16(x_offset << 12);
+    VAR8XH(FILTERX, SEC_NONE)
+  }
+  RETURN_SUM_SSE
+}
+
+int aom_highbd_sub_pixel_avg_variance8xh_sse2(
+    const uint16_t *src, ptrdiff_t src_stride, int x_offset, int y_offset,
+    const uint16_t *dst, ptrdiff_t dst_stride, const uint16_t *sec,
+    ptrdiff_t sec_stride, int height, unsigned int *sse, void *unused0,
+    void *unused) {
+  __m128i v0, v1, v2, t0, t1, t2, t3, vzero = _mm_setzero_si128();
+  __m128i vsum = vzero, vsse = vzero, xfilter, yfilter;
+  int i;
+  (void)unused0; (void)unused;
+
+  if (x_offset == 0) {
+    VAR8XH(LOADX, SEC_AVG8)
+  } else if (x_offset == 4) {
+    VAR8XH(AVGX, SEC_AVG8)
+  } else {
+    xfilter = _mm_set1_epi16(x_offset << 12);
+    VAR8XH(FILTERX, SEC_AVG8)
+  }
+  RETURN_SUM_SSE
+}
+
diff --git a/aom_dsp/e2k/highbd_variance_impl_e2k.c b/aom_dsp/e2k/highbd_variance_impl_e2k.c
new file mode 100644
index 0000000..aa3bcaf
--- /dev/null
+++ b/aom_dsp/e2k/highbd_variance_impl_e2k.c
@@ -0,0 +1,104 @@
+/*
+ * Copyright (c) 2021, Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2016, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "config/aom_dsp_rtcd.h"
+
+void aom_highbd_calc16x16var_sse2(const uint16_t *src, int src_stride,
+                                  const uint16_t *ref, int ref_stride,
+                                  uint32_t *sse, int *sum) {
+  __m128i v0, v1, v2, vzero = _mm_setzero_si128();
+  __m128i vsse = vzero, vsum = vzero;
+  int i;
+
+  for (i = 0; i < 16; i += 2) {
+      v0 = _mm_loadu_si128((const __m128i *)src);
+      v1 = _mm_loadu_si128((const __m128i *)ref);
+      v2 = _mm_sub_epi16(v0, v1);
+      vsse = _mm_add_epi32(vsse, _mm_madd_epi16(v2, v2));
+
+      v0 = _mm_loadu_si128((const __m128i *)(src + 8));
+      v1 = _mm_loadu_si128((const __m128i *)(ref + 8));
+      v0 = _mm_sub_epi16(v0, v1);
+      v2 = _mm_add_epi16(v2, v0);
+      vsse = _mm_add_epi32(vsse, _mm_madd_epi16(v0, v0));
+
+      v0 = _mm_loadu_si128((const __m128i *)(src + src_stride));
+      v1 = _mm_loadu_si128((const __m128i *)(ref + ref_stride));
+      v0 = _mm_sub_epi16(v0, v1);
+      v2 = _mm_add_epi16(v2, v0);
+      vsse = _mm_add_epi32(vsse, _mm_madd_epi16(v0, v0));
+
+      v0 = _mm_loadu_si128((const __m128i *)(src + src_stride + 8));
+      v1 = _mm_loadu_si128((const __m128i *)(ref + ref_stride + 8));
+      v0 = _mm_sub_epi16(v0, v1);
+      v2 = _mm_add_epi16(v2, v0);
+      vsse = _mm_add_epi32(vsse, _mm_madd_epi16(v0, v0));
+
+      v0 = _mm_srai_epi16(v2, 15);
+      vsum = _mm_add_epi32(vsum, _mm_unpacklo_epi16(v2, v0));
+      vsum = _mm_add_epi32(vsum, _mm_unpackhi_epi16(v2, v0));
+      src += src_stride << 1;
+      ref += ref_stride << 1;
+  }
+
+  vsse = _mm_add_epi32(vsse, _mm_bsrli_si128(vsse, 8));
+  vsum = _mm_add_epi32(vsum, _mm_bsrli_si128(vsum, 8));
+  *sse = _mm_cvtsi128_si32(vsse) + _mm_extract_epi32(vsse, 1);
+  *sum = _mm_cvtsi128_si32(vsum) + _mm_extract_epi32(vsum, 1);
+}
+
+void aom_highbd_calc8x8var_sse2(const uint16_t *src, int src_stride,
+                                const uint16_t *ref, int ref_stride,
+                                uint32_t *sse, int *sum) {
+  __m128i v0, v1, v2, vzero = _mm_setzero_si128();
+  __m128i vsse = vzero, vsum = vzero;
+  int i;
+
+  for (i = 0; i < 8; i += 4) {
+      v0 = _mm_loadu_si128((const __m128i *)src);
+      v1 = _mm_loadu_si128((const __m128i *)ref);
+      v2 = _mm_sub_epi16(v0, v1);
+      vsse = _mm_add_epi32(vsse, _mm_madd_epi16(v2, v2));
+
+      v0 = _mm_loadu_si128((const __m128i *)(src + src_stride));
+      v1 = _mm_loadu_si128((const __m128i *)(ref + ref_stride));
+      v0 = _mm_sub_epi16(v0, v1);
+      v2 = _mm_add_epi16(v2, v0);
+      vsse = _mm_add_epi32(vsse, _mm_madd_epi16(v0, v0));
+
+      v0 = _mm_loadu_si128((const __m128i *)(src + src_stride * 2));
+      v1 = _mm_loadu_si128((const __m128i *)(ref + ref_stride * 2));
+      v0 = _mm_sub_epi16(v0, v1);
+      v2 = _mm_add_epi16(v2, v0);
+      vsse = _mm_add_epi32(vsse, _mm_madd_epi16(v0, v0));
+
+      v0 = _mm_loadu_si128((const __m128i *)(src + src_stride * 3));
+      v1 = _mm_loadu_si128((const __m128i *)(ref + ref_stride * 3));
+      v0 = _mm_sub_epi16(v0, v1);
+      v2 = _mm_add_epi16(v2, v0);
+      vsse = _mm_add_epi32(vsse, _mm_madd_epi16(v0, v0));
+
+      v0 = _mm_srai_epi16(v2, 15);
+      vsum = _mm_add_epi32(vsum, _mm_unpacklo_epi16(v2, v0));
+      vsum = _mm_add_epi32(vsum, _mm_unpackhi_epi16(v2, v0));
+      src += src_stride << 2;
+      ref += ref_stride << 2;
+  }
+
+  vsse = _mm_add_epi32(vsse, _mm_bsrli_si128(vsse, 8));
+  vsum = _mm_add_epi32(vsum, _mm_bsrli_si128(vsum, 8));
+  *sse = _mm_cvtsi128_si32(vsse) + _mm_extract_epi32(vsse, 1);
+  *sum = _mm_cvtsi128_si32(vsum) + _mm_extract_epi32(vsum, 1);
+}
+
diff --git a/aom_dsp/e2k/intrapred_e2k.c b/aom_dsp/e2k/intrapred_e2k.c
new file mode 100644
index 0000000..edda753
--- /dev/null
+++ b/aom_dsp/e2k/intrapred_e2k.c
@@ -0,0 +1,305 @@
+/*
+ * Copyright (c) 2021, Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2016, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "config/av1_rtcd.h"
+
+#define DC_SUM_8(ref, v0) \
+  t0 = *(__m64 const *)ref; \
+  v0 = _mm_sad_pu8(t0, vzero); \
+
+#define DC_SUM_16(ref, v0) \
+  t0 = _mm_load_si128((__m128i const *)ref); \
+  v0 = _mm_sad_epu8(t0, vzero); \
+
+#define DC_SUM_32(ref, v0) \
+  t0 = _mm_load_si128((__m128i const *)ref); \
+  t1 = _mm_load_si128((__m128i const *)(ref + 16)); \
+  t0 = _mm_sad_epu8(t0, vzero); \
+  t1 = _mm_sad_epu8(t1, vzero); \
+  v0 = _mm_add_epi64(t0, t1);
+
+#define DC_STORE_4XH(val, h, dst, stride) \
+  sum = _mm_cvtsi64_si32(_mm_set1_pi8(val)); \
+  for (i = 0; i < h; i++, dst += stride) { \
+    *(uint32_t*)dst = sum; \
+  }
+
+#define DC_STORE_8XH(val, h, dst, stride) \
+  v0 = _mm_set1_pi8(val); \
+  for (i = 0; i < h; i++, dst += stride) { \
+    *(__m64*)dst = v0; \
+  }
+
+#define DC_STORE_16XH(val, h, dst, stride) \
+  v0 = _mm_set1_epi8(val); \
+  for (i = 0; i < h; i++, dst += stride) { \
+    _mm_store_si128((__m128i *)dst, v0); \
+  }
+
+#define DC_STORE_32XH(val, h, dst, stride) \
+  v0 = _mm_set1_epi8(val); \
+  for (i = 0; i < h; i++, dst += stride) { \
+    _mm_store_si128((__m128i *)dst, v0); \
+    _mm_store_si128((__m128i *)(dst + 16), v0); \
+  }
+
+void aom_dc_128_predictor_4x4_sse2(uint8_t *dst, ptrdiff_t stride,
+                                   const uint8_t *above, const uint8_t *left) {
+  int32_t sum = 0x80808080; int i;
+  (void)above; (void)left;
+  for (i = 0; i < 4; i++, dst += stride) {
+    *(uint32_t*)dst = sum;
+  }
+}
+
+void aom_dc_128_predictor_8x8_sse2(uint8_t *dst, ptrdiff_t stride,
+                                   const uint8_t *above, const uint8_t *left) {
+  __m64 v0; int i;
+  (void)above; (void)left;
+  DC_STORE_8XH(128, 8, dst, stride);
+}
+
+void aom_dc_128_predictor_16x16_sse2(uint8_t *dst, ptrdiff_t stride,
+                                   const uint8_t *above, const uint8_t *left) {
+  __m128i v0; int i;
+  (void)above; (void)left;
+  DC_STORE_16XH(128, 16, dst, stride);
+}
+
+void aom_dc_128_predictor_32x32_sse2(uint8_t *dst, ptrdiff_t stride,
+                                   const uint8_t *above, const uint8_t *left) {
+  __m128i v0; int i;
+  (void)above; (void)left;
+  DC_STORE_32XH(128, 32, dst, stride);
+}
+
+void aom_dc_top_predictor_4x4_sse2(uint8_t *dst, ptrdiff_t stride,
+                                   const uint8_t *above, const uint8_t *left) {
+  __m64 v0, vzero = _mm_setzero_si64();
+  int32_t sum; int i;
+  (void)left;
+  v0 = _mm_cvtsi32_si64(*(uint32_t const *)above);
+  v0 = _mm_sad_pu8(v0, vzero);
+  sum = _mm_cvtsi64_si32(v0);
+  sum = (sum + 2) >> 2;
+  DC_STORE_4XH(sum, 4, dst, stride);
+}
+
+void aom_dc_top_predictor_8x8_sse2(uint8_t *dst, ptrdiff_t stride,
+                                   const uint8_t *above, const uint8_t *left) {
+  __m64 t0, v0, vzero = _mm_setzero_si64();
+  int32_t sum; int i;
+  (void)left;
+  DC_SUM_8(above, v0);
+  sum = _mm_cvtsi64_si32(v0);
+  sum = (sum + 4) >> 3;
+  DC_STORE_8XH(sum, 8, dst, stride);
+}
+
+void aom_dc_top_predictor_16x16_sse2(uint8_t *dst, ptrdiff_t stride,
+                                   const uint8_t *above, const uint8_t *left) {
+  __m128i t0, v0, vzero = _mm_setzero_si128();
+  int32_t sum; int i;
+  (void)left;
+  DC_SUM_16(above, v0);
+  sum = _mm_cvtsi128_si32(v0) + _mm_extract_epi32(v0, 2);;
+  sum = (sum + 8) >> 4;
+  DC_STORE_16XH(sum, 16, dst, stride);
+}
+
+void aom_dc_top_predictor_32x32_sse2(uint8_t *dst, ptrdiff_t stride,
+                                   const uint8_t *above, const uint8_t *left) {
+  __m128i t0, t1, v0, vzero = _mm_setzero_si128();
+  int32_t sum; int i;
+  (void)left;
+  DC_SUM_32(above, v0);
+  sum = _mm_cvtsi128_si32(v0) + _mm_extract_epi32(v0, 2);;
+  sum = (sum + 16) >> 5;
+  DC_STORE_32XH(sum, 32, dst, stride);
+}
+
+void aom_dc_left_predictor_4x4_sse2(uint8_t *dst, ptrdiff_t stride,
+                                   const uint8_t *above, const uint8_t *left) {
+  __m64 v0, vzero = _mm_setzero_si64();
+  int32_t sum; int i;
+  (void)above;
+  v0 = _mm_cvtsi32_si64(*(uint32_t const *)left);
+  v0 = _mm_sad_pu8(v0, vzero);
+  sum = _mm_cvtsi64_si32(v0);
+  sum = (sum + 2) >> 2;
+  DC_STORE_4XH(sum, 4, dst, stride);
+}
+
+void aom_dc_left_predictor_8x8_sse2(uint8_t *dst, ptrdiff_t stride,
+                                   const uint8_t *above, const uint8_t *left) {
+  __m64 t0, v0, vzero = _mm_setzero_si64();
+  int32_t sum; int i;
+  (void)above;
+  DC_SUM_8(left, v0);
+  sum = _mm_cvtsi64_si32(v0);
+  sum = (sum + 4) >> 3;
+  DC_STORE_8XH(sum, 8, dst, stride);
+}
+
+void aom_dc_left_predictor_16x16_sse2(uint8_t *dst, ptrdiff_t stride,
+                                   const uint8_t *above, const uint8_t *left) {
+  __m128i t0, v0, vzero = _mm_setzero_si128();
+  int32_t sum; int i;
+  (void)above;
+  DC_SUM_16(left, v0);
+  sum = _mm_cvtsi128_si32(v0) + _mm_extract_epi32(v0, 2);;
+  sum = (sum + 8) >> 4;
+  DC_STORE_16XH(sum, 16, dst, stride);
+}
+
+void aom_dc_left_predictor_32x32_sse2(uint8_t *dst, ptrdiff_t stride,
+                                   const uint8_t *above, const uint8_t *left) {
+  __m128i t0, t1, v0, vzero = _mm_setzero_si128();
+  int32_t sum; int i;
+  (void)above;
+  DC_SUM_32(left, v0);
+  sum = _mm_cvtsi128_si32(v0) + _mm_extract_epi32(v0, 2);;
+  sum = (sum + 16) >> 5;
+  DC_STORE_32XH(sum, 32, dst, stride);
+}
+
+void aom_dc_predictor_4x4_sse2(uint8_t *dst, ptrdiff_t stride,
+                               const uint8_t *above, const uint8_t *left) {
+  __m64 v0, v1, vzero = _mm_setzero_si64();
+  int32_t sum; int i;
+  v0 = _mm_cvtsi32_si64(*(uint32_t const *)above);
+  v1 = _mm_cvtsi32_si64(*(uint32_t const *)left);
+  v0 = _mm_sad_pu8(_mm_unpacklo_pi32(v0, v1), vzero);
+  sum = _mm_cvtsi64_si32(v0);
+  sum = (sum + 4) >> 3;
+  DC_STORE_4XH(sum, 4, dst, stride);
+}
+
+void aom_dc_predictor_8x8_sse2(uint8_t *dst, ptrdiff_t stride,
+                               const uint8_t *above, const uint8_t *left) {
+  __m64 t0, v0, v1, vzero = _mm_setzero_si64();
+  int32_t sum; int i;
+  DC_SUM_8(above, v0);
+  DC_SUM_8(left, v1);
+  v0 = _mm_add_pi32(v0, v1);
+  sum = _mm_cvtsi64_si32(v0);
+  sum = (sum + 8) >> 4;
+  DC_STORE_8XH(sum, 8, dst, stride);
+}
+
+void aom_dc_predictor_16x16_sse2(uint8_t *dst, ptrdiff_t stride,
+                                 const uint8_t *above, const uint8_t *left) {
+  __m128i t0, v0, v1, vzero = _mm_setzero_si128();
+  int32_t sum; int i;
+  DC_SUM_16(above, v0);
+  DC_SUM_16(left, v1);
+  v0 = _mm_add_epi64(v0, v1);
+  sum = _mm_cvtsi128_si32(v0) + _mm_extract_epi32(v0, 2);
+  sum = (sum + 16) >> 5;
+  DC_STORE_16XH(sum, 16, dst, stride);
+}
+
+void aom_dc_predictor_32x32_sse2(uint8_t *dst, ptrdiff_t stride,
+                                 const uint8_t *above, const uint8_t *left) {
+  __m128i t0, t1, v0, v1, vzero = _mm_setzero_si128();
+  int32_t sum; int i;
+  DC_SUM_32(above, v0);
+  DC_SUM_32(left, v1);
+  v0 = _mm_add_epi64(v0, v1);
+  sum = _mm_cvtsi128_si32(v0) + _mm_extract_epi32(v0, 2);
+  sum = (sum + 32) >> 6;
+  DC_STORE_32XH(sum, 32, dst, stride);
+}
+
+void aom_v_predictor_4x4_sse2(uint8_t *dst, ptrdiff_t stride,
+                              const uint8_t *above, const uint8_t *left) {
+  uint32_t sum = *(uint32_t const *)above;
+  int i; (void)left;
+  for (i = 0; i < 4; i++, dst += stride) {
+    *(uint32_t*)dst = sum;
+  }
+}
+
+void aom_v_predictor_8x8_sse2(uint8_t *dst, ptrdiff_t stride,
+                              const uint8_t *above, const uint8_t *left) {
+  __m64 v0 = *(__m64 const *)above; int i;
+  (void)left;
+  for (i = 0; i < 8; i++, dst += stride) {
+    *(__m64*)dst = v0;
+  }
+}
+
+void aom_v_predictor_16x16_sse2(uint8_t *dst, ptrdiff_t stride,
+                              const uint8_t *above, const uint8_t *left) {
+  __m128i v0 = _mm_load_si128((__m128i const *)above);
+  int i; (void)left;
+  for (i = 0; i < 16; i++, dst += stride) {
+    _mm_store_si128((__m128i *)dst, v0);
+  }
+}
+
+void aom_v_predictor_32x32_sse2(uint8_t *dst, ptrdiff_t stride,
+                              const uint8_t *above, const uint8_t *left) {
+  __m128i v0 = _mm_load_si128((__m128i const *)above);
+  __m128i v1 = _mm_load_si128((__m128i const *)(above + 16));
+  int i; (void)left;
+  for (i = 0; i < 32; i++, dst += stride) {
+    _mm_store_si128((__m128i *)dst, v0);
+    _mm_store_si128((__m128i *)(dst + 16), v1);
+  }
+}
+
+void aom_h_predictor_4x4_sse2(uint8_t *dst, ptrdiff_t stride,
+                              const uint8_t *above, const uint8_t *left) {
+  __m128i v0; (void)above;
+  v0 = _mm_cvtsi32_si128(*(uint32_t const *)left);
+  v0 = _mm_unpacklo_epi8(v0, v0);
+  v0 = _mm_unpacklo_epi8(v0, v0);
+  *(uint32_t*)dst = _mm_cvtsi128_si32(v0); dst += stride;
+  *(uint32_t*)dst = _mm_extract_epi32(v0, 1); dst += stride;
+  *(uint32_t*)dst = _mm_extract_epi32(v0, 2); dst += stride;
+  *(uint32_t*)dst = _mm_extract_epi32(v0, 3);
+}
+
+void aom_h_predictor_8x8_sse2(uint8_t *dst, ptrdiff_t stride,
+                              const uint8_t *above, const uint8_t *left) {
+  __m64 v0; int i; (void)above;
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < 8; i++, dst += stride) {
+    v0 = _mm_set1_pi8(left[i]);
+    *(__m64*)dst = v0;
+  }
+}
+
+void aom_h_predictor_16x16_sse2(uint8_t *dst, ptrdiff_t stride,
+                                const uint8_t *above, const uint8_t *left) {
+  __m128i v0; int i; (void)above;
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < 16; i++, dst += stride) {
+    v0 = _mm_set1_epi8(left[i]);
+    _mm_store_si128((__m128i *)dst, v0);
+  }
+}
+
+void aom_h_predictor_32x32_sse2(uint8_t *dst, ptrdiff_t stride,
+                                const uint8_t *above, const uint8_t *left) {
+  __m128i v0; int i; (void)above;
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < 32; i++, dst += stride) {
+    v0 = _mm_set1_epi8(left[i]);
+    _mm_store_si128((__m128i *)dst, v0);
+    _mm_store_si128((__m128i *)(dst + 16), v0);
+  }
+}
+
diff --git a/aom_dsp/e2k/sad4d_e2k.c b/aom_dsp/e2k/sad4d_e2k.c
new file mode 100644
index 0000000..c50a2db
--- /dev/null
+++ b/aom_dsp/e2k/sad4d_e2k.c
@@ -0,0 +1,270 @@
+/*
+ * Copyright (c) 2021, Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2016, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "config/aom_dsp_rtcd.h"
+
+#include "aom_ports/mem.h"
+
+#define NO_AVG(...)
+#define WITH_AVG(...) __VA_ARGS__
+
+#define FN(w, h) \
+  SAD##w##XN(h, NO_AVG, aom_sad, x4d_sse2) \
+  SAD##w##XN(h, WITH_AVG, aom_sad, x4d_avg_sse2)
+#define SAD_LOOP(h, n) \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < h; i += n)
+#define RESULT_SHL(x, pi) x
+
+#define PROCESS_16x4(IS_AVG, i) \
+  v0 = _mm_loadu_si128((const __m128i*)(ref0 + i)); \
+  v1 = _mm_loadu_si128((const __m128i*)(ref1 + i)); \
+  v2 = _mm_loadu_si128((const __m128i*)(ref2 + i)); \
+  v3 = _mm_loadu_si128((const __m128i*)(ref3 + i)); \
+  IS_AVG( \
+    v4 = _mm_loadu_si128((const __m128i*)(second_pred + i)); \
+    v0 = _mm_avg_epu8(v0, v4); \
+    v1 = _mm_avg_epu8(v1, v4); \
+    v2 = _mm_avg_epu8(v2, v4); \
+    v3 = _mm_avg_epu8(v3, v4); \
+  ) \
+  v4 = _mm_loadu_si128((const __m128i*)(src + i)); \
+  vsum0 = _mm_add_epi32(vsum0, _mm_sad_epu8(v0, v4)); \
+  vsum1 = _mm_add_epi32(vsum1, _mm_sad_epu8(v1, v4)); \
+  vsum2 = _mm_add_epi32(vsum2, _mm_sad_epu8(v2, v4)); \
+  vsum3 = _mm_add_epi32(vsum3, _mm_sad_epu8(v3, v4)); \
+
+#define SAD128XN(h, IS_AVG, name, end) \
+void name##128x##h##end(const uint8_t *src, int src_stride, \
+                        const uint8_t * const ref_ptr[], int ref_stride \
+                        IS_AVG(, const uint8_t *second_pred), uint32_t *sad_array) { \
+  __m128i v0, v1, v2, v3, v4, vsum0 = _mm_setzero_si128(); \
+  __m128i vsum1 = vsum0, vsum2 = vsum0, vsum3 = vsum0; \
+  const uint8_t *ref0 = ref_ptr[0], *ref1 = ref_ptr[1], *ref2 = ref_ptr[2], *ref3 = ref_ptr[3]; \
+  int i; \
+  SAD_LOOP(h, 1) { \
+    PROCESS_16x4(IS_AVG, 0) \
+    PROCESS_16x4(IS_AVG, 16) \
+    PROCESS_16x4(IS_AVG, 32) \
+    PROCESS_16x4(IS_AVG, 48) \
+    PROCESS_16x4(IS_AVG, 64) \
+    PROCESS_16x4(IS_AVG, 80) \
+    PROCESS_16x4(IS_AVG, 96) \
+    PROCESS_16x4(IS_AVG, 112) \
+    IS_AVG(second_pred += 128;) \
+    ref0 += ref_stride; \
+    ref1 += ref_stride; \
+    ref2 += ref_stride; \
+    ref3 += ref_stride; \
+    src += src_stride; \
+  } \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum1); \
+  vsum2 = _mm_hadd_epi32(vsum2, vsum3); \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum2); \
+  _mm_storeu_si128((__m128i*)sad_array, RESULT_SHL(vsum0, epi)); \
+}
+
+#define SAD64XN(h, IS_AVG, name, end) \
+void name##64x##h##end(const uint8_t *src, int src_stride, \
+                       const uint8_t * const ref_ptr[], int ref_stride \
+                       IS_AVG(, const uint8_t *second_pred), uint32_t *sad_array) { \
+  __m128i v0, v1, v2, v3, v4, vsum0 = _mm_setzero_si128(); \
+  __m128i vsum1 = vsum0, vsum2 = vsum0, vsum3 = vsum0; \
+  const uint8_t *ref0 = ref_ptr[0], *ref1 = ref_ptr[1], *ref2 = ref_ptr[2], *ref3 = ref_ptr[3]; \
+  int i; \
+  SAD_LOOP(h, 1) { \
+    PROCESS_16x4(IS_AVG, 0) \
+    PROCESS_16x4(IS_AVG, 16) \
+    PROCESS_16x4(IS_AVG, 32) \
+    PROCESS_16x4(IS_AVG, 48) \
+    IS_AVG(second_pred += 64;) \
+    ref0 += ref_stride; \
+    ref1 += ref_stride; \
+    ref2 += ref_stride; \
+    ref3 += ref_stride; \
+    src += src_stride; \
+  } \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum1); \
+  vsum2 = _mm_hadd_epi32(vsum2, vsum3); \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum2); \
+  _mm_storeu_si128((__m128i*)sad_array, RESULT_SHL(vsum0, epi)); \
+}
+
+#define SAD32XN(h, IS_AVG, name, end) \
+void name##32x##h##end(const uint8_t *src, int src_stride, \
+                       const uint8_t * const ref_ptr[], int ref_stride \
+                       IS_AVG(, const uint8_t *second_pred), uint32_t *sad_array) { \
+  __m128i v0, v1, v2, v3, v4, vsum0 = _mm_setzero_si128(); \
+  __m128i vsum1 = vsum0, vsum2 = vsum0, vsum3 = vsum0; \
+  const uint8_t *ref0 = ref_ptr[0], *ref1 = ref_ptr[1], *ref2 = ref_ptr[2], *ref3 = ref_ptr[3]; \
+  int i; \
+  SAD_LOOP(h, 1) { \
+    PROCESS_16x4(IS_AVG, 0) \
+    PROCESS_16x4(IS_AVG, 16) \
+    IS_AVG(second_pred += 32;) \
+    ref0 += ref_stride; \
+    ref1 += ref_stride; \
+    ref2 += ref_stride; \
+    ref3 += ref_stride; \
+    src += src_stride; \
+  } \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum1); \
+  vsum2 = _mm_hadd_epi32(vsum2, vsum3); \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum2); \
+  _mm_storeu_si128((__m128i*)sad_array, RESULT_SHL(vsum0, epi)); \
+}
+
+#define SAD16XN(h, IS_AVG, name, end) \
+void name##16x##h##end(const uint8_t *src, int src_stride, \
+                       const uint8_t * const ref_ptr[], int ref_stride \
+                       IS_AVG(, const uint8_t *second_pred), uint32_t *sad_array) { \
+  __m128i v0, v1, v2, v3, v4, vsum0 = _mm_setzero_si128(); \
+  __m128i vsum1 = vsum0, vsum2 = vsum0, vsum3 = vsum0; \
+  const uint8_t *ref0 = ref_ptr[0], *ref1 = ref_ptr[1], *ref2 = ref_ptr[2], *ref3 = ref_ptr[3]; \
+  int i; \
+  SAD_LOOP(h, 1) { \
+    PROCESS_16x4(IS_AVG, 0) \
+    IS_AVG(second_pred += 16;) \
+    ref0 += ref_stride; \
+    ref1 += ref_stride; \
+    ref2 += ref_stride; \
+    ref3 += ref_stride; \
+    src += src_stride; \
+  } \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum1); \
+  vsum2 = _mm_hadd_epi32(vsum2, vsum3); \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum2); \
+  _mm_storeu_si128((__m128i*)sad_array, RESULT_SHL(vsum0, epi)); \
+}
+
+#define SAD8XN(h, IS_AVG, name, end) \
+void name##8x##h##end(const uint8_t *src, int src_stride, \
+                      const uint8_t * const ref_ptr[], int ref_stride \
+                      IS_AVG(, const uint8_t *second_pred), uint32_t *sad_array) { \
+  __m64 v0, v1, v2, v3, v4, vsum0 = _mm_setzero_si64(); \
+  __m64 vsum1 = vsum0, vsum2 = vsum0, vsum3 = vsum0; \
+  const uint8_t *ref0 = ref_ptr[0], *ref1 = ref_ptr[1], *ref2 = ref_ptr[2], *ref3 = ref_ptr[3]; \
+  int i; \
+  SAD_LOOP(h, 1) { \
+    v0 = *(const __m64*)ref0; \
+    v1 = *(const __m64*)ref1; \
+    v2 = *(const __m64*)ref2; \
+    v3 = *(const __m64*)ref3; \
+    IS_AVG( \
+      v4 = *(const __m64*)second_pred; \
+      v0 = _mm_avg_pu8(v0, v4); \
+      v1 = _mm_avg_pu8(v1, v4); \
+      v2 = _mm_avg_pu8(v2, v4); \
+      v3 = _mm_avg_pu8(v3, v4); \
+      second_pred += 8; \
+    ) \
+    v4 = *(const __m64*)src; \
+    vsum0 = _mm_add_pi32(vsum0, _mm_sad_pu8(v0, v4)); \
+    vsum1 = _mm_add_pi32(vsum1, _mm_sad_pu8(v1, v4)); \
+    vsum2 = _mm_add_pi32(vsum2, _mm_sad_pu8(v2, v4)); \
+    vsum3 = _mm_add_pi32(vsum3, _mm_sad_pu8(v3, v4)); \
+    ref0 += ref_stride; \
+    ref1 += ref_stride; \
+    ref2 += ref_stride; \
+    ref3 += ref_stride; \
+    src += src_stride; \
+  } \
+  *(__m64*)sad_array = RESULT_SHL(_mm_unpacklo_pi32(vsum0, vsum1), pi); \
+  *(__m64*)(sad_array + 2) = RESULT_SHL(_mm_unpacklo_pi32(vsum2, vsum3), pi); \
+}
+
+#define SAD4XN(h, IS_AVG, name, end) \
+void name##4x##h##end(const uint8_t *src, int src_stride, \
+                      const uint8_t * const ref_ptr[], int ref_stride \
+                      IS_AVG(, const uint8_t *second_pred), uint32_t *sad_array) { \
+  __m64 v0, v1, v2, v3, v4, v5, vsum0 = _mm_setzero_si64(); \
+  __m64 vsum1 = vsum0, vsum2 = vsum0, vsum3 = vsum0; \
+  const uint8_t *ref0 = ref_ptr[0], *ref1 = ref_ptr[1], *ref2 = ref_ptr[2], *ref3 = ref_ptr[3]; \
+  int i; \
+  SAD_LOOP(h, 2) { \
+    v4 = _mm_cvtsi32_si64(*(uint32_t const *)ref0); \
+    v5 = _mm_cvtsi32_si64(*(uint32_t const *)(ref0 + ref_stride)); \
+    v0 = _mm_unpacklo_pi32(v4, v5); \
+    v4 = _mm_cvtsi32_si64(*(uint32_t const *)ref1); \
+    v5 = _mm_cvtsi32_si64(*(uint32_t const *)(ref1 + ref_stride)); \
+    v1 = _mm_unpacklo_pi32(v4, v5); \
+    v4 = _mm_cvtsi32_si64(*(uint32_t const *)ref2); \
+    v5 = _mm_cvtsi32_si64(*(uint32_t const *)(ref2 + ref_stride)); \
+    v2 = _mm_unpacklo_pi32(v4, v5); \
+    v4 = _mm_cvtsi32_si64(*(uint32_t const *)ref3); \
+    v5 = _mm_cvtsi32_si64(*(uint32_t const *)(ref3 + ref_stride)); \
+    v3 = _mm_unpacklo_pi32(v4, v5); \
+    IS_AVG( \
+      v4 = *(const __m64*)second_pred; \
+      v0 = _mm_avg_pu8(v0, v4); \
+      v1 = _mm_avg_pu8(v1, v4); \
+      v2 = _mm_avg_pu8(v2, v4); \
+      v3 = _mm_avg_pu8(v3, v4); \
+      second_pred += 8; \
+    ) \
+    v4 = _mm_cvtsi32_si64(*(uint32_t const *)src); \
+    v5 = _mm_cvtsi32_si64(*(uint32_t const *)(src + src_stride)); \
+    v4 = _mm_unpacklo_pi32(v4, v5); \
+    vsum0 = _mm_add_pi32(vsum0, _mm_sad_pu8(v0, v4)); \
+    vsum1 = _mm_add_pi32(vsum1, _mm_sad_pu8(v1, v4)); \
+    vsum2 = _mm_add_pi32(vsum2, _mm_sad_pu8(v2, v4)); \
+    vsum3 = _mm_add_pi32(vsum3, _mm_sad_pu8(v3, v4)); \
+    ref0 += ref_stride << 1; \
+    ref1 += ref_stride << 1; \
+    ref2 += ref_stride << 1; \
+    ref3 += ref_stride << 1; \
+    src += src_stride << 1; \
+  } \
+  *(__m64*)sad_array = RESULT_SHL(_mm_unpacklo_pi32(vsum0, vsum1), pi); \
+  *(__m64*)(sad_array + 2) = RESULT_SHL(_mm_unpacklo_pi32(vsum2, vsum3), pi); \
+}
+
+FN(128, 128) FN(128, 64)
+FN(64, 128) FN(64, 64) FN(64, 32)
+FN(32, 64) FN(32, 32) FN(32, 16)
+FN(16, 32) FN(16, 16) FN(16, 8)
+FN(8, 16) FN(8, 8) FN(8, 4)
+FN(4, 8) FN(4, 4)
+#if !CONFIG_REALTIME_ONLY
+FN(4, 16) FN(16, 4)
+FN(8, 32) FN(32, 8)
+FN(16, 64) FN(64, 16)
+#endif
+
+#undef FN
+#define FN(w, h) SAD##w##XN(h, NO_AVG, aom_sad_skip_, x4d_sse2)
+#undef SAD_LOOP
+#define SAD_LOOP(h, n) \
+  src_stride <<= 1; ref_stride <<= 1; \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < h / 2; i += n)
+#undef RESULT_SHL
+#define RESULT_SHL(x, pi) _mm_slli_##pi##32(x, 1)
+
+FN(128, 128) FN(128, 64)
+FN(64, 128) FN(64, 64) FN(64, 32)
+FN(32, 64) FN(32, 32) FN(32, 16)
+FN(16, 32) FN(16, 16) FN(16, 8)
+FN(8, 16) FN(8, 8)
+FN(4, 8)
+#if !CONFIG_REALTIME_ONLY
+FN(4, 16)
+FN(8, 32) FN(32, 8)
+FN(16, 64) FN(64, 16)
+#endif
+
+#if 0 // Current code fails there are only 2 rows
+FN(16, 4) FN(8, 4) FN(4, 4)
+#endif
+
diff --git a/aom_dsp/e2k/sad_e2k.c b/aom_dsp/e2k/sad_e2k.c
new file mode 100644
index 0000000..1f0e231
--- /dev/null
+++ b/aom_dsp/e2k/sad_e2k.c
@@ -0,0 +1,265 @@
+/*
+ * Copyright (c) 2021, Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2016, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "config/aom_dsp_rtcd.h"
+
+#include "aom_ports/mem.h"
+
+#define NO_AVG(...)
+#define WITH_AVG(...) __VA_ARGS__
+
+#define FN(w, h) \
+  SAD##w##XN(h, NO_AVG, aom_sad, _sse2) \
+  SAD##w##XN(h, WITH_AVG, aom_sad, _avg_sse2)
+#define SAD_LOOP(h, n) \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < h; i += n)
+#define RESULT_SHL(x) x
+
+#define SAD128XN(h, IS_AVG, name, end) \
+unsigned int name##128x##h##end(const uint8_t *src, int src_stride, \
+                                const uint8_t *ref, int ref_stride \
+                                IS_AVG(, const uint8_t *second_pred)) { \
+  __m128i v0, v1, v2, v3, vsum = _mm_setzero_si128(); \
+  int i; \
+  SAD_LOOP(h, 1) { \
+    v0 = _mm_loadu_si128((const __m128i*)ref); \
+    v1 = _mm_loadu_si128((const __m128i*)(ref + 16)); \
+    v2 = _mm_loadu_si128((const __m128i*)(ref + 32)); \
+    v3 = _mm_loadu_si128((const __m128i*)(ref + 48)); \
+    IS_AVG( \
+      v0 = _mm_avg_epu8(v0, _mm_loadu_si128((const __m128i*)second_pred)); \
+      v1 = _mm_avg_epu8(v1, _mm_loadu_si128((const __m128i*)(second_pred + 16))); \
+      v2 = _mm_avg_epu8(v2, _mm_loadu_si128((const __m128i*)(second_pred + 32))); \
+      v3 = _mm_avg_epu8(v3, _mm_loadu_si128((const __m128i*)(second_pred + 48))); \
+    ) \
+    v0 = _mm_sad_epu8(v0, _mm_loadu_si128((const __m128i*)src)); \
+    v1 = _mm_sad_epu8(v1, _mm_loadu_si128((const __m128i*)(src + 16))); \
+    v2 = _mm_sad_epu8(v2, _mm_loadu_si128((const __m128i*)(src + 32))); \
+    v3 = _mm_sad_epu8(v3, _mm_loadu_si128((const __m128i*)(src + 48))); \
+    v0 = _mm_add_epi32(v0, v1); \
+    v2 = _mm_add_epi32(v2, v3); \
+    vsum = _mm_add_epi32(vsum, v0); \
+    vsum = _mm_add_epi32(vsum, v2); \
+    \
+    v0 = _mm_loadu_si128((const __m128i*)(ref + 64)); \
+    v1 = _mm_loadu_si128((const __m128i*)(ref + 80)); \
+    v2 = _mm_loadu_si128((const __m128i*)(ref + 96)); \
+    v3 = _mm_loadu_si128((const __m128i*)(ref + 112)); \
+    IS_AVG( \
+      v0 = _mm_avg_epu8(v0, _mm_loadu_si128((const __m128i*)(second_pred + 64))); \
+      v1 = _mm_avg_epu8(v1, _mm_loadu_si128((const __m128i*)(second_pred + 80))); \
+      v2 = _mm_avg_epu8(v2, _mm_loadu_si128((const __m128i*)(second_pred + 96))); \
+      v3 = _mm_avg_epu8(v3, _mm_loadu_si128((const __m128i*)(second_pred + 112))); \
+      second_pred += 128; \
+    ) \
+    v0 = _mm_sad_epu8(v0, _mm_loadu_si128((const __m128i*)(src + 64))); \
+    v1 = _mm_sad_epu8(v1, _mm_loadu_si128((const __m128i*)(src + 80))); \
+    v2 = _mm_sad_epu8(v2, _mm_loadu_si128((const __m128i*)(src + 96))); \
+    v3 = _mm_sad_epu8(v3, _mm_loadu_si128((const __m128i*)(src + 112))); \
+    v0 = _mm_add_epi32(v0, v1); \
+    v2 = _mm_add_epi32(v2, v3); \
+    vsum = _mm_add_epi32(vsum, v0); \
+    vsum = _mm_add_epi32(vsum, v2); \
+    ref += ref_stride; \
+    src += src_stride; \
+  } \
+  return RESULT_SHL(_mm_cvtsi128_si32(vsum) + _mm_extract_epi32(vsum, 2)); \
+}
+
+#define SAD64XN(h, IS_AVG, name, end) \
+unsigned int name##64x##h##end(const uint8_t *src, int src_stride, \
+                               const uint8_t *ref, int ref_stride \
+                               IS_AVG(, const uint8_t *second_pred)) { \
+  __m128i v0, v1, v2, v3, vsum = _mm_setzero_si128(); \
+  int i; \
+  SAD_LOOP(h, 1) { \
+    v0 = _mm_loadu_si128((const __m128i*)ref); \
+    v1 = _mm_loadu_si128((const __m128i*)(ref + 16)); \
+    v2 = _mm_loadu_si128((const __m128i*)(ref + 32)); \
+    v3 = _mm_loadu_si128((const __m128i*)(ref + 48)); \
+    IS_AVG( \
+      v0 = _mm_avg_epu8(v0, _mm_loadu_si128((const __m128i*)second_pred)); \
+      v1 = _mm_avg_epu8(v1, _mm_loadu_si128((const __m128i*)(second_pred + 16))); \
+      v2 = _mm_avg_epu8(v2, _mm_loadu_si128((const __m128i*)(second_pred + 32))); \
+      v3 = _mm_avg_epu8(v3, _mm_loadu_si128((const __m128i*)(second_pred + 48))); \
+      second_pred += 64; \
+    ) \
+    v0 = _mm_sad_epu8(v0, _mm_loadu_si128((const __m128i*)src)); \
+    v1 = _mm_sad_epu8(v1, _mm_loadu_si128((const __m128i*)(src + 16))); \
+    v2 = _mm_sad_epu8(v2, _mm_loadu_si128((const __m128i*)(src + 32))); \
+    v3 = _mm_sad_epu8(v3, _mm_loadu_si128((const __m128i*)(src + 48))); \
+    v0 = _mm_add_epi32(v0, v1); \
+    v2 = _mm_add_epi32(v2, v3); \
+    vsum = _mm_add_epi32(vsum, v0); \
+    vsum = _mm_add_epi32(vsum, v2); \
+    ref += ref_stride; \
+    src += src_stride; \
+  } \
+  return RESULT_SHL(_mm_cvtsi128_si32(vsum) + _mm_extract_epi32(vsum, 2)); \
+}
+
+#define SAD32XN(h, IS_AVG, name, end) \
+unsigned int name##32x##h##end(const uint8_t *src, int src_stride, \
+                               const uint8_t *ref, int ref_stride \
+                               IS_AVG(, const uint8_t *second_pred)) { \
+  __m128i v0, v1, v2, v3, vsum = _mm_setzero_si128(); \
+  int i; \
+  SAD_LOOP(h, 2) { \
+    v0 = _mm_loadu_si128((const __m128i*)ref); \
+    v1 = _mm_loadu_si128((const __m128i*)(ref + 16)); \
+    v2 = _mm_loadu_si128((const __m128i*)(ref + ref_stride)); \
+    v3 = _mm_loadu_si128((const __m128i*)(ref + ref_stride + 16)); \
+    IS_AVG( \
+      v0 = _mm_avg_epu8(v0, _mm_loadu_si128((const __m128i*)second_pred)); \
+      v1 = _mm_avg_epu8(v1, _mm_loadu_si128((const __m128i*)(second_pred + 16))); \
+      v2 = _mm_avg_epu8(v2, _mm_loadu_si128((const __m128i*)(second_pred + 32))); \
+      v3 = _mm_avg_epu8(v3, _mm_loadu_si128((const __m128i*)(second_pred + 48))); \
+      second_pred += 64; \
+    ) \
+    v0 = _mm_sad_epu8(v0, _mm_loadu_si128((const __m128i*)src)); \
+    v1 = _mm_sad_epu8(v1, _mm_loadu_si128((const __m128i*)(src + 16))); \
+    v2 = _mm_sad_epu8(v2, _mm_loadu_si128((const __m128i*)(src + src_stride))); \
+    v3 = _mm_sad_epu8(v3, _mm_loadu_si128((const __m128i*)(src + src_stride + 16))); \
+    v0 = _mm_add_epi32(v0, v1); \
+    v2 = _mm_add_epi32(v2, v3); \
+    vsum = _mm_add_epi32(vsum, v0); \
+    vsum = _mm_add_epi32(vsum, v2); \
+    ref += ref_stride << 1; \
+    src += src_stride << 1; \
+  } \
+  return RESULT_SHL(_mm_cvtsi128_si32(vsum) + _mm_extract_epi32(vsum, 2)); \
+}
+
+
+#define SAD16XN(h, IS_AVG, name, end) \
+unsigned int name##16x##h##end(const uint8_t *src, int src_stride, \
+                               const uint8_t *ref, int ref_stride \
+                               IS_AVG(, const uint8_t *second_pred)) { \
+  __m128i v0, v1, v2, v3, vsum = _mm_setzero_si128(); \
+  int i; \
+  SAD_LOOP(h, 4) { \
+    v0 = _mm_loadu_si128((const __m128i*)ref); \
+    v1 = _mm_loadu_si128((const __m128i*)(ref + ref_stride)); \
+    v2 = _mm_loadu_si128((const __m128i*)(ref + ref_stride * 2)); \
+    v3 = _mm_loadu_si128((const __m128i*)(ref + ref_stride * 3)); \
+    IS_AVG( \
+      v0 = _mm_avg_epu8(v0, _mm_loadu_si128((const __m128i*)second_pred)); \
+      v1 = _mm_avg_epu8(v1, _mm_loadu_si128((const __m128i*)(second_pred + 16))); \
+      v2 = _mm_avg_epu8(v2, _mm_loadu_si128((const __m128i*)(second_pred + 32))); \
+      v3 = _mm_avg_epu8(v3, _mm_loadu_si128((const __m128i*)(second_pred + 48))); \
+      second_pred += 64; \
+    ) \
+    v0 = _mm_sad_epu8(v0, _mm_loadu_si128((const __m128i*)src)); \
+    v1 = _mm_sad_epu8(v1, _mm_loadu_si128((const __m128i*)(src + src_stride))); \
+    v2 = _mm_sad_epu8(v2, _mm_loadu_si128((const __m128i*)(src + src_stride * 2))); \
+    v3 = _mm_sad_epu8(v3, _mm_loadu_si128((const __m128i*)(src + src_stride * 3))); \
+    v0 = _mm_add_epi32(v0, v1); \
+    v2 = _mm_add_epi32(v2, v3); \
+    vsum = _mm_add_epi32(vsum, v0); \
+    vsum = _mm_add_epi32(vsum, v2); \
+    ref += ref_stride << 2; \
+    src += src_stride << 2; \
+  } \
+  return RESULT_SHL(_mm_cvtsi128_si32(vsum) + _mm_extract_epi32(vsum, 2)); \
+}
+
+#define SAD8XN(h, IS_AVG, name, end) \
+unsigned int name##8x##h##end(const uint8_t *src, int src_stride, \
+                              const uint8_t *ref, int ref_stride \
+                              IS_AVG(, const uint8_t *second_pred)) { \
+  __m64 v0, v1, v2, v3, vsum = _mm_setzero_si64(); \
+  int i; \
+  SAD_LOOP(h, 4) { \
+    v0 = *(const __m64*)ref; \
+    v1 = *(const __m64*)(ref + ref_stride); \
+    v2 = *(const __m64*)(ref + ref_stride * 2); \
+    v3 = *(const __m64*)(ref + ref_stride * 3); \
+    IS_AVG( \
+      v0 = _mm_avg_pu8(v0, *(const __m64*)second_pred); \
+      v1 = _mm_avg_pu8(v1, *(const __m64*)(second_pred + 8)); \
+      v2 = _mm_avg_pu8(v2, *(const __m64*)(second_pred + 16)); \
+      v3 = _mm_avg_pu8(v3, *(const __m64*)(second_pred + 24)); \
+      second_pred += 32; \
+    ) \
+    v0 = _mm_sad_pu8(v0, *(const __m64*)src); \
+    v1 = _mm_sad_pu8(v1, *(const __m64*)(src + src_stride)); \
+    v2 = _mm_sad_pu8(v2, *(const __m64*)(src + src_stride * 2)); \
+    v3 = _mm_sad_pu8(v3, *(const __m64*)(src + src_stride * 3)); \
+    v0 = _mm_add_pi32(v0, v1); \
+    v2 = _mm_add_pi32(v2, v3); \
+    vsum = _mm_add_pi32(vsum, v0); \
+    vsum = _mm_add_pi32(vsum, v2); \
+    ref += ref_stride << 2; \
+    src += src_stride << 2; \
+  } \
+  return RESULT_SHL(_mm_cvtsi64_si32(vsum)); \
+}
+
+#define SAD4XN(h, IS_AVG, name, end) \
+unsigned int name##4x##h##end(const uint8_t *src, int src_stride, \
+                              const uint8_t *ref, int ref_stride \
+                              IS_AVG(, const uint8_t *second_pred)) { \
+  __m64 v0, v1, v2, v3, vsum = _mm_setzero_si64(); \
+  int i; \
+  SAD_LOOP(h, 4) { \
+    v2 = _mm_cvtsi32_si64(*(uint32_t const *)ref); \
+    v3 = _mm_cvtsi32_si64(*(uint32_t const *)(ref + ref_stride)); \
+    v0 = _mm_unpacklo_pi32(v2, v3); \
+    v2 = _mm_cvtsi32_si64(*(uint32_t const *)(ref + ref_stride * 2)); \
+    v3 = _mm_cvtsi32_si64(*(uint32_t const *)(ref + ref_stride * 3)); \
+    v1 = _mm_unpacklo_pi32(v2, v3); \
+    IS_AVG( \
+      v0 = _mm_avg_pu8(v0, *(const __m64*)second_pred); \
+      v1 = _mm_avg_pu8(v1, *(const __m64*)(second_pred + 8)); \
+      second_pred += 16; \
+    ) \
+    v2 = _mm_cvtsi32_si64(*(uint32_t const *)src); \
+    v3 = _mm_cvtsi32_si64(*(uint32_t const *)(src + src_stride)); \
+    v0 = _mm_sad_pu8(v0, _mm_unpacklo_pi32(v2, v3)); \
+    v2 = _mm_cvtsi32_si64(*(uint32_t const *)(src + src_stride * 2)); \
+    v3 = _mm_cvtsi32_si64(*(uint32_t const *)(src + src_stride * 3)); \
+    v1 = _mm_sad_pu8(v1, _mm_unpacklo_pi32(v2, v3)); \
+    v0 = _mm_add_pi32(v0, v1); \
+    vsum = _mm_add_pi32(vsum, v0); \
+    ref += ref_stride << 2; \
+    src += src_stride << 2; \
+  } \
+  return RESULT_SHL(_mm_cvtsi64_si32(vsum)); \
+}
+
+FN(128, 128) FN(128, 64)
+FN(64, 128) FN(64, 64) FN(64, 32) FN(64, 16)
+FN(32, 64) FN(32, 32) FN(32, 16) FN(32, 8)
+FN(16, 64) FN(16, 32) FN(16, 16) FN(16, 8) FN(16, 4)
+FN(8, 32) FN(8, 16) FN(8, 8) FN(8, 4)
+FN(4, 16) FN(4, 8) FN(4, 4)
+
+#undef FN
+#define FN(w, h) SAD##w##XN(h, NO_AVG, aom_sad_skip_, _sse2)
+#undef SAD_LOOP
+#define SAD_LOOP(h, n) \
+  src_stride <<= 1; ref_stride <<= 1; \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < h / 2; i += n)
+#undef RESULT_SHL
+#define RESULT_SHL(x) (x) << 1
+
+FN(128, 128) FN(128, 64)
+FN(64, 128) FN(64, 64) FN(64, 32) FN(64, 16)
+FN(32, 64) FN(32, 32) FN(32, 16) FN(32, 8)
+FN(16, 64) FN(16, 32) FN(16, 16) FN(16, 8) FN(16, 4)
+FN(8, 32) FN(8, 16) FN(8, 8) FN(8, 4)
+FN(4, 16) FN(4, 8) FN(4, 4)
+
diff --git a/aom_dsp/e2k/subpel_variance_e2k.c b/aom_dsp/e2k/subpel_variance_e2k.c
new file mode 100644
index 0000000..785c2c6
--- /dev/null
+++ b/aom_dsp/e2k/subpel_variance_e2k.c
@@ -0,0 +1,1761 @@
+/*
+ * Copyright (c) 2021, Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2016, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "config/aom_dsp_rtcd.h"
+
+#include "aom_ports/mem.h"
+
+/* clang-format off */
+DECLARE_ALIGNED(16, static const uint8_t, bilinear_filters_sse2[256]) = {
+  16,  0, 16,  0, 16,  0, 16,  0, 16,  0, 16,  0, 16,  0, 16,  0,
+  14,  2, 14,  2, 14,  2, 14,  2, 14,  2, 14,  2, 14,  2, 14,  2,
+  12,  4, 12,  4, 12,  4, 12,  4, 12,  4, 12,  4, 12,  4, 12,  4,
+  10,  6, 10,  6, 10,  6, 10,  6, 10,  6, 10,  6, 10,  6, 10,  6,
+   8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,
+   6, 10,  6, 10,  6, 10,  6, 10,  6, 10,  6, 10,  6, 10,  6, 10,
+   4, 12,  4, 12,  4, 12,  4, 12,  4, 12,  4, 12,  4, 12,  4, 12,
+   2, 14,  2, 14,  2, 14,  2, 14,  2, 14,  2, 14,  2, 14,  2, 14,
+};
+/* clang-format on */
+
+#define FILTER_SRC(filter)                               \
+  /* filter the source */                                \
+  exp_src_lo = _mm_maddubs_epi16(exp_src_lo, filter);    \
+  exp_src_hi = _mm_maddubs_epi16(exp_src_hi, filter);    \
+                                                         \
+  /* add 8 to source */                                  \
+  exp_src_lo = _mm_add_epi16(exp_src_lo, pw8);           \
+  exp_src_hi = _mm_add_epi16(exp_src_hi, pw8);           \
+                                                         \
+  /* divide source by 16 */                              \
+  exp_src_lo = _mm_srai_epi16(exp_src_lo, 4);            \
+  exp_src_hi = _mm_srai_epi16(exp_src_hi, 4);
+
+#define MERGE_WITH_SRC(src_reg, reg)               \
+  exp_src_lo = _mm_unpacklo_epi8(src_reg, reg);    \
+  exp_src_hi = _mm_unpackhi_epi8(src_reg, reg);
+
+#define LOAD_SRC_DST                                    \
+  /* load source and destination */                     \
+  src_reg = _mm_loadu_si128((__m128i const *)(src));    \
+  dst_reg = _mm_loadu_si128((__m128i const *)(dst));
+
+#define AVG_NEXT_SRC(src_reg, size_stride)                                 \
+  src_next_reg = _mm_loadu_si128((__m128i const *)(src + size_stride));    \
+  /* average between current and next stride source */                     \
+  src_reg = _mm_avg_epu8(src_reg, src_next_reg);
+
+#define MERGE_NEXT_SRC(src_reg, size_stride)                               \
+  src_next_reg = _mm_loadu_si128((__m128i const *)(src + size_stride));    \
+  MERGE_WITH_SRC(src_reg, src_next_reg)
+
+#define CALC_SUM_SSE_INSIDE_LOOP                          \
+  /* expand each byte to 2 bytes */                       \
+  exp_dst_lo = _mm_unpacklo_epi8(dst_reg, zero_reg);      \
+  exp_dst_hi = _mm_unpackhi_epi8(dst_reg, zero_reg);      \
+  /* source - dest */                                     \
+  exp_src_lo = _mm_sub_epi16(exp_src_lo, exp_dst_lo);     \
+  exp_src_hi = _mm_sub_epi16(exp_src_hi, exp_dst_hi);     \
+  /* caculate sum */                                      \
+  sum_reg = _mm_add_epi16(sum_reg, exp_src_lo);           \
+  exp_src_lo = _mm_madd_epi16(exp_src_lo, exp_src_lo);    \
+  sum_reg = _mm_add_epi16(sum_reg, exp_src_hi);           \
+  exp_src_hi = _mm_madd_epi16(exp_src_hi, exp_src_hi);    \
+  /* calculate sse */                                     \
+  sse_reg = _mm_add_epi32(sse_reg, exp_src_lo);           \
+  sse_reg = _mm_add_epi32(sse_reg, exp_src_hi);
+
+// final calculation to sum and sse
+#define CALC_SUM_AND_SSE                                                   \
+  res_cmp = _mm_cmpgt_epi16(zero_reg, sum_reg);                            \
+  sse_reg_hi = _mm_bsrli_si128(sse_reg, 8);                                \
+  sum_reg_lo = _mm_unpacklo_epi16(sum_reg, res_cmp);                       \
+  sum_reg_hi = _mm_unpackhi_epi16(sum_reg, res_cmp);                       \
+  sse_reg = _mm_add_epi32(sse_reg, sse_reg_hi);                            \
+  sum_reg = _mm_add_epi32(sum_reg_lo, sum_reg_hi);                         \
+                                                                           \
+  sse_reg_hi = _mm_bsrli_si128(sse_reg, 4);                                \
+  sum_reg_hi = _mm_bsrli_si128(sum_reg, 8);                                \
+                                                                           \
+  sse_reg = _mm_add_epi32(sse_reg, sse_reg_hi);                            \
+  sum_reg = _mm_add_epi32(sum_reg, sum_reg_hi);                            \
+  *((int *)sse) = _mm_cvtsi128_si32(sse_reg);                              \
+  sum_reg_hi = _mm_bsrli_si128(sum_reg, 4);                                \
+  sum_reg = _mm_add_epi32(sum_reg, sum_reg_hi);                            \
+  sum = _mm_cvtsi128_si32(sum_reg);
+
+// Functions related to sub pixel variance width 8
+#define LOAD_SRC_DST_INSERT(src_stride, dst_stride)              \
+  /* load source and destination of 2 rows and insert*/          \
+  src_reg = _mm_unpacklo_epi64(                                  \
+      _mm_loadl_epi64((__m128i const *)(src)),                         \
+      _mm_loadl_epi64((__m128i const *)(src + src_stride)));           \
+  dst_reg = _mm_unpacklo_epi64(                                  \
+      _mm_loadl_epi64((__m128i const *)(dst)),                         \
+      _mm_loadl_epi64((__m128i const *)(dst + dst_stride)));
+
+#define AVG_NEXT_SRC_INSERT(src_reg, size_stride)                              \
+  src_next_reg = _mm_unpacklo_epi64(                                           \
+      _mm_loadl_epi64((__m128i const *)(src + size_stride)),                         \
+      _mm_loadl_epi64((__m128i const *)(src + (size_stride << 1))));                 \
+  /* average between current and next stride source */                         \
+  src_reg = _mm_avg_epu8(src_reg, src_next_reg);
+
+#define MERGE_NEXT_SRC_INSERT(src_reg, size_stride)                            \
+  src_next_reg = _mm_unpacklo_epi64(                                           \
+      _mm_loadl_epi64((__m128i const *)(src + size_stride)),                         \
+      _mm_loadl_epi64((__m128i const *)(src + (src_stride + size_stride))));         \
+  MERGE_WITH_SRC(src_reg, src_next_reg)
+
+#define LOAD_SRC_NEXT_BYTE_INSERT                                    \
+  /* load source and another source from next row   */               \
+  src_reg = _mm_unpacklo_epi64(                                      \
+      _mm_loadl_epi64((__m128i const *)(src)),                             \
+      _mm_loadl_epi64((__m128i const *)(src + src_stride)));               \
+  /* load source and next row source from 1 byte onwards   */        \
+  src_next_reg = _mm_unpacklo_epi64(                                 \
+      _mm_loadl_epi64((__m128i const *)(src + 1)),                         \
+      _mm_loadl_epi64((__m128i const *)(src + src_stride + 1)));
+
+#define LOAD_DST_INSERT                                        \
+  dst_reg = _mm_unpacklo_epi64(                                \
+      _mm_loadl_epi64((__m128i const *)(dst)),                       \
+      _mm_loadl_epi64((__m128i const *)(dst + dst_stride)));
+
+#define LOAD_SRC_MERGE_HALF(filter)                          \
+  __m128i src_reg_0 = _mm_loadl_epi64((__m128i const *)(src));     \
+  __m128i src_reg_1 = _mm_loadl_epi64((__m128i const *)(src + 1)); \
+  __m128i src_lo = _mm_unpacklo_epi8(src_reg_0, src_reg_1);
+
+#define FILTER_SRC_HALF(filter)               \
+  /* filter the source */                     \
+  src_lo = _mm_maddubs_epi16(src_lo, filter); \
+                                              \
+  /* add 8 to source */                       \
+  src_lo = _mm_add_epi16(src_lo, pw8);        \
+                                              \
+  /* divide source by 16 */                   \
+  src_lo = _mm_srai_epi16(src_lo, 4);
+
+unsigned int aom_sub_pixel_variance16xh_ssse3(const uint8_t *src, int src_stride,
+                                              int x_offset, int y_offset,
+                                              const uint8_t *dst, int dst_stride,
+                                              int height, unsigned int *sse) {
+  __m128i src_reg, dst_reg, exp_src_lo, exp_src_hi, exp_dst_lo, exp_dst_hi;
+  __m128i sse_reg, sum_reg, sse_reg_hi, res_cmp, sum_reg_lo, sum_reg_hi;
+  __m128i zero_reg;
+  int i, sum;
+  sum_reg = _mm_setzero_si128();
+  sse_reg = _mm_setzero_si128();
+  zero_reg = _mm_setzero_si128();
+
+  // x_offset = 0 and y_offset = 0
+  if (x_offset == 0) {
+    if (y_offset == 0) {
+      for (i = 0; i < height; i++) {
+        LOAD_SRC_DST
+        // expend each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride;
+        dst += dst_stride;
+      }
+      // x_offset = 0 and y_offset = 4
+    } else if (y_offset == 4) {
+      __m128i src_next_reg;
+      for (i = 0; i < height; i++) {
+        LOAD_SRC_DST
+        AVG_NEXT_SRC(src_reg, src_stride)
+        // expend each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride;
+        dst += dst_stride;
+      }
+      // x_offset = 0 and y_offset = bilin interpolation
+    } else {
+      __m128i filter, pw8, src_next_reg;
+
+      y_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + y_offset));
+      pw8 = _mm_set1_epi16(8);
+      for (i = 0; i < height; i++) {
+        LOAD_SRC_DST
+        MERGE_NEXT_SRC(src_reg, src_stride)
+        FILTER_SRC(filter)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride;
+        dst += dst_stride;
+      }
+    }
+    // x_offset = 4  and y_offset = 0
+  } else if (x_offset == 4) {
+    if (y_offset == 0) {
+      __m128i src_next_reg;
+      for (i = 0; i < height; i++) {
+        LOAD_SRC_DST
+        AVG_NEXT_SRC(src_reg, 1)
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride;
+        dst += dst_stride;
+      }
+      // x_offset = 4  and y_offset = 4
+    } else if (y_offset == 4) {
+      __m128i src_next_reg, src_avg;
+      // load source and another source starting from the next
+      // following byte
+      src_reg = _mm_loadu_si128((__m128i const *)(src));
+      AVG_NEXT_SRC(src_reg, 1)
+      for (i = 0; i < height; i++) {
+        src_avg = src_reg;
+        src += src_stride;
+        LOAD_SRC_DST
+        AVG_NEXT_SRC(src_reg, 1)
+        // average between previous average to current average
+        src_avg = _mm_avg_epu8(src_avg, src_reg);
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_avg, zero_reg)
+        // save current source average
+        CALC_SUM_SSE_INSIDE_LOOP
+        dst += dst_stride;
+      }
+      // x_offset = 4  and y_offset = bilin interpolation
+    } else {
+      __m128i filter, pw8, src_next_reg, src_avg;
+      y_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + y_offset));
+      pw8 = _mm_set1_epi16(8);
+      // load source and another source starting from the next
+      // following byte
+      src_reg = _mm_loadu_si128((__m128i const *)(src));
+      AVG_NEXT_SRC(src_reg, 1)
+      for (i = 0; i < height; i++) {
+        // save current source average
+        src_avg = src_reg;
+        src += src_stride;
+        LOAD_SRC_DST
+        AVG_NEXT_SRC(src_reg, 1)
+        MERGE_WITH_SRC(src_avg, src_reg)
+        FILTER_SRC(filter)
+        CALC_SUM_SSE_INSIDE_LOOP
+        dst += dst_stride;
+      }
+    }
+    // x_offset = bilin interpolation and y_offset = 0
+  } else {
+    if (y_offset == 0) {
+      __m128i filter, pw8, src_next_reg;
+      x_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + x_offset));
+      pw8 = _mm_set1_epi16(8);
+      for (i = 0; i < height; i++) {
+        LOAD_SRC_DST
+        MERGE_NEXT_SRC(src_reg, 1)
+        FILTER_SRC(filter)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride;
+        dst += dst_stride;
+      }
+      // x_offset = bilin interpolation and y_offset = 4
+    } else if (y_offset == 4) {
+      __m128i filter, pw8, src_next_reg, src_pack;
+      x_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + x_offset));
+      pw8 = _mm_set1_epi16(8);
+      src_reg = _mm_loadu_si128((__m128i const *)(src));
+      MERGE_NEXT_SRC(src_reg, 1)
+      FILTER_SRC(filter)
+      // convert each 16 bit to 8 bit to each low and high lane source
+      src_pack = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+      for (i = 0; i < height; i++) {
+        src += src_stride;
+        LOAD_SRC_DST
+        MERGE_NEXT_SRC(src_reg, 1)
+        FILTER_SRC(filter)
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        // average between previous pack to the current
+        src_pack = _mm_avg_epu8(src_pack, src_reg);
+        MERGE_WITH_SRC(src_pack, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src_pack = src_reg;
+        dst += dst_stride;
+      }
+      // x_offset = bilin interpolation and y_offset = bilin interpolation
+    } else {
+      __m128i xfilter, yfilter, pw8, src_next_reg, src_pack;
+      x_offset <<= 4;
+      xfilter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + x_offset));
+      y_offset <<= 4;
+      yfilter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + y_offset));
+      pw8 = _mm_set1_epi16(8);
+      // load source and another source starting from the next
+      // following byte
+      src_reg = _mm_loadu_si128((__m128i const *)(src));
+      MERGE_NEXT_SRC(src_reg, 1)
+
+      FILTER_SRC(xfilter)
+      // convert each 16 bit to 8 bit to each low and high lane source
+      src_pack = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+      for (i = 0; i < height; i++) {
+        src += src_stride;
+        LOAD_SRC_DST
+        MERGE_NEXT_SRC(src_reg, 1)
+        FILTER_SRC(xfilter)
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        // merge previous pack to current pack source
+        MERGE_WITH_SRC(src_pack, src_reg)
+        // filter the source
+        FILTER_SRC(yfilter)
+        src_pack = src_reg;
+        CALC_SUM_SSE_INSIDE_LOOP
+        dst += dst_stride;
+      }
+    }
+  }
+  CALC_SUM_AND_SSE
+  return sum;
+}
+
+unsigned int aom_sub_pixel_variance8xh_ssse3(const uint8_t *src, int src_stride,
+                                             int x_offset, int y_offset,
+                                             const uint8_t *dst, int dst_stride,
+                                             int height, unsigned int *sse) {
+  __m128i src_reg, dst_reg, exp_src_lo, exp_src_hi, exp_dst_lo, exp_dst_hi;
+  __m128i sse_reg, sum_reg, sse_reg_hi, res_cmp, sum_reg_lo, sum_reg_hi;
+  __m128i zero_reg;
+  int i, sum;
+  sum_reg = _mm_setzero_si128();
+  sse_reg = _mm_setzero_si128();
+  zero_reg = _mm_setzero_si128();
+
+  // x_offset = 0 and y_offset = 0
+  if (x_offset == 0) {
+    if (y_offset == 0) {
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        // expend each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = 0 and y_offset = 4
+    } else if (y_offset == 4) {
+      __m128i src_next_reg;
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        AVG_NEXT_SRC_INSERT(src_reg, src_stride)
+        // expend each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = 0 and y_offset = bilin interpolation
+    } else {
+      __m128i filter, pw8, src_next_reg;
+      y_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + y_offset));
+      pw8 = _mm_set1_epi16(8);
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        MERGE_NEXT_SRC_INSERT(src_reg, src_stride)
+        FILTER_SRC(filter)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+    }
+    // x_offset = 4  and y_offset = 0
+  } else if (x_offset == 4) {
+    if (y_offset == 0) {
+      __m128i src_next_reg;
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        LOAD_DST_INSERT
+        /* average between current and next stride source */
+        src_reg = _mm_avg_epu8(src_reg, src_next_reg);
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = 4  and y_offset = 4
+    } else if (y_offset == 4) {
+      __m128i src_next_reg, src_avg, src_temp;
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      src_avg = _mm_avg_epu8(src_reg, src_next_reg);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        src_next_reg = _mm_avg_epu8(src_reg, src_next_reg);
+        src_temp = _mm_castpd_si128(_mm_shuffle_pd(
+            _mm_castsi128_pd(src_avg), _mm_castsi128_pd(src_next_reg), 1));
+        src_temp = _mm_avg_epu8(src_avg, src_temp);
+        LOAD_DST_INSERT
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_temp, zero_reg)
+        // save current source average
+        src_avg = src_next_reg;
+        CALC_SUM_SSE_INSIDE_LOOP
+        dst += dst_stride << 1;
+        src += src_stride << 1;
+      }
+      // last 2 rows processing happens here
+      __m128i src_reg_0 = _mm_loadl_epi64((__m128i const *)(src));
+      __m128i src_reg_1 = _mm_loadl_epi64((__m128i const *)(src + 1));
+      src_reg_0 = _mm_avg_epu8(src_reg_0, src_reg_1);
+      src_next_reg = _mm_castpd_si128(_mm_shuffle_pd(
+          _mm_castsi128_pd(src_avg), _mm_castsi128_pd(src_reg_0), 1));
+      LOAD_DST_INSERT
+      src_avg = _mm_avg_epu8(src_avg, src_next_reg);
+      MERGE_WITH_SRC(src_avg, zero_reg)
+      CALC_SUM_SSE_INSIDE_LOOP
+    } else {
+      // x_offset = 4  and y_offset = bilin interpolation
+      __m128i filter, pw8, src_next_reg, src_avg, src_temp;
+      y_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + y_offset));
+      pw8 = _mm_set1_epi16(8);
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      src_avg = _mm_avg_epu8(src_reg, src_next_reg);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        src_next_reg = _mm_avg_epu8(src_reg, src_next_reg);
+        src_temp = _mm_castpd_si128(_mm_shuffle_pd(
+            _mm_castsi128_pd(src_avg), _mm_castsi128_pd(src_next_reg), 1));
+        LOAD_DST_INSERT
+        MERGE_WITH_SRC(src_avg, src_temp)
+        // save current source average
+        src_avg = src_next_reg;
+        FILTER_SRC(filter)
+        CALC_SUM_SSE_INSIDE_LOOP
+        dst += dst_stride << 1;
+        src += src_stride << 1;
+      }
+      // last 2 rows processing happens here
+      __m128i src_reg_0 = _mm_loadl_epi64((__m128i const *)(src));
+      __m128i src_reg_1 = _mm_loadl_epi64((__m128i const *)(src + 1));
+      src_reg_0 = _mm_avg_epu8(src_reg_0, src_reg_1);
+      src_next_reg = _mm_castpd_si128(_mm_shuffle_pd(
+          _mm_castsi128_pd(src_avg), _mm_castsi128_pd(src_reg_0), 1));
+      LOAD_DST_INSERT
+      MERGE_WITH_SRC(src_avg, src_next_reg)
+      FILTER_SRC(filter)
+      CALC_SUM_SSE_INSIDE_LOOP
+    }
+    // x_offset = bilin interpolation and y_offset = 0
+  } else {
+    if (y_offset == 0) {
+      __m128i filter, pw8, src_next_reg;
+      x_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + x_offset));
+      pw8 = _mm_set1_epi16(8);
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        MERGE_NEXT_SRC_INSERT(src_reg, 1)
+        FILTER_SRC(filter)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = bilin interpolation and y_offset = 4
+    } else if (y_offset == 4) {
+      __m128i filter, pw8, src_next_reg, src_pack;
+      x_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + x_offset));
+      pw8 = _mm_set1_epi16(8);
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      MERGE_WITH_SRC(src_reg, src_next_reg)
+      FILTER_SRC(filter)
+      // convert each 16 bit to 8 bit to each low and high lane source
+      src_pack = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        LOAD_DST_INSERT
+        MERGE_WITH_SRC(src_reg, src_next_reg)
+        FILTER_SRC(filter)
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        src_next_reg = _mm_castpd_si128(_mm_shuffle_pd(
+            _mm_castsi128_pd(src_pack), _mm_castsi128_pd(src_reg), 1));
+        // average between previous pack to the current
+        src_pack = _mm_avg_epu8(src_pack, src_next_reg);
+        MERGE_WITH_SRC(src_pack, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src_pack = src_reg;
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // last 2 rows processing happens here
+      LOAD_SRC_MERGE_HALF(filter)
+      LOAD_DST_INSERT
+      FILTER_SRC_HALF(filter)
+      src_reg_0 = _mm_packus_epi16(src_lo, src_lo);
+      src_next_reg = _mm_castpd_si128(_mm_shuffle_pd(
+          _mm_castsi128_pd(src_pack), _mm_castsi128_pd(src_reg_0), 1));
+      // average between previous pack to the current
+      src_pack = _mm_avg_epu8(src_pack, src_next_reg);
+      MERGE_WITH_SRC(src_pack, zero_reg)
+      CALC_SUM_SSE_INSIDE_LOOP
+    } else {
+      // x_offset = bilin interpolation and y_offset = bilin interpolation
+      __m128i xfilter, yfilter, pw8, src_next_reg, src_pack;
+      x_offset <<= 4;
+      xfilter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + x_offset));
+      y_offset <<= 4;
+      yfilter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + y_offset));
+      pw8 = _mm_set1_epi16(8);
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      MERGE_WITH_SRC(src_reg, src_next_reg)
+      FILTER_SRC(xfilter)
+      // convert each 16 bit to 8 bit to each low and high lane source
+      src_pack = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        LOAD_DST_INSERT
+        MERGE_WITH_SRC(src_reg, src_next_reg)
+        FILTER_SRC(xfilter)
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        src_next_reg = _mm_castpd_si128(_mm_shuffle_pd(
+            _mm_castsi128_pd(src_pack), _mm_castsi128_pd(src_reg), 1));
+        // average between previous pack to the current
+        MERGE_WITH_SRC(src_pack, src_next_reg)
+        // filter the source
+        FILTER_SRC(yfilter)
+        src_pack = src_reg;
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // last 2 rows processing happens here
+      LOAD_SRC_MERGE_HALF(xfilter)
+      LOAD_DST_INSERT
+      FILTER_SRC_HALF(xfilter)
+      src_reg_0 = _mm_packus_epi16(src_lo, src_lo);
+      src_next_reg = _mm_castpd_si128(_mm_shuffle_pd(
+          _mm_castsi128_pd(src_pack), _mm_castsi128_pd(src_reg_0), 1));
+      MERGE_WITH_SRC(src_pack, src_next_reg)
+      FILTER_SRC(yfilter)
+      CALC_SUM_SSE_INSIDE_LOOP
+    }
+  }
+  CALC_SUM_AND_SSE
+  return sum;
+}
+
+unsigned int aom_sub_pixel_avg_variance16xh_ssse3(
+    const uint8_t *src, int src_stride, int x_offset, int y_offset,
+    const uint8_t *dst, int dst_stride, const uint8_t *sec, int sec_stride,
+    int height, unsigned int *sse) {
+  __m128i sec_reg;
+  __m128i src_reg, dst_reg, exp_src_lo, exp_src_hi, exp_dst_lo, exp_dst_hi;
+  __m128i sse_reg, sum_reg, sse_reg_hi, res_cmp, sum_reg_lo, sum_reg_hi;
+  __m128i zero_reg;
+  int i, sum;
+  sum_reg = _mm_setzero_si128();
+  sse_reg = _mm_setzero_si128();
+  zero_reg = _mm_setzero_si128();
+
+  // x_offset = 0 and y_offset = 0
+  if (x_offset == 0) {
+    if (y_offset == 0) {
+      for (i = 0; i < height; i++) {
+        LOAD_SRC_DST
+        sec_reg = _mm_loadu_si128((__m128i const *)(sec));
+        src_reg = _mm_avg_epu8(src_reg, sec_reg);
+        sec += sec_stride;
+        // expend each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride;
+        dst += dst_stride;
+      }
+    } else if (y_offset == 4) {
+      __m128i src_next_reg;
+      for (i = 0; i < height; i++) {
+        LOAD_SRC_DST
+        AVG_NEXT_SRC(src_reg, src_stride)
+        sec_reg = _mm_loadu_si128((__m128i const *)(sec));
+        src_reg = _mm_avg_epu8(src_reg, sec_reg);
+        sec += sec_stride;
+        // expend each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride;
+        dst += dst_stride;
+      }
+      // x_offset = 0 and y_offset = bilin interpolation
+    } else {
+      __m128i filter, pw8, src_next_reg;
+
+      y_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + y_offset));
+      pw8 = _mm_set1_epi16(8);
+      for (i = 0; i < height; i++) {
+        LOAD_SRC_DST
+        MERGE_NEXT_SRC(src_reg, src_stride)
+        FILTER_SRC(filter)
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        sec_reg = _mm_loadu_si128((__m128i const *)(sec));
+        src_reg = _mm_avg_epu8(src_reg, sec_reg);
+        sec += sec_stride;
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride;
+        dst += dst_stride;
+      }
+    }
+    // x_offset = 4  and y_offset = 0
+  } else if (x_offset == 4) {
+    if (y_offset == 0) {
+      __m128i src_next_reg;
+      for (i = 0; i < height; i++) {
+        LOAD_SRC_DST
+        AVG_NEXT_SRC(src_reg, 1)
+        sec_reg = _mm_loadu_si128((__m128i const *)(sec));
+        src_reg = _mm_avg_epu8(src_reg, sec_reg);
+        sec += sec_stride;
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride;
+        dst += dst_stride;
+      }
+      // x_offset = 4  and y_offset = 4
+    } else if (y_offset == 4) {
+      __m128i src_next_reg, src_avg;
+      // load source and another source starting from the next
+      // following byte
+      src_reg = _mm_loadu_si128((__m128i const *)(src));
+      AVG_NEXT_SRC(src_reg, 1)
+      for (i = 0; i < height; i++) {
+        // save current source average
+        src_avg = src_reg;
+        src += src_stride;
+        LOAD_SRC_DST
+        AVG_NEXT_SRC(src_reg, 1)
+        // average between previous average to current average
+        src_avg = _mm_avg_epu8(src_avg, src_reg);
+        sec_reg = _mm_loadu_si128((__m128i const *)(sec));
+        src_avg = _mm_avg_epu8(src_avg, sec_reg);
+        sec += sec_stride;
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_avg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        dst += dst_stride;
+      }
+      // x_offset = 4  and y_offset = bilin interpolation
+    } else {
+      __m128i filter, pw8, src_next_reg, src_avg;
+      y_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + y_offset));
+      pw8 = _mm_set1_epi16(8);
+      // load source and another source starting from the next
+      // following byte
+      src_reg = _mm_loadu_si128((__m128i const *)(src));
+      AVG_NEXT_SRC(src_reg, 1)
+      for (i = 0; i < height; i++) {
+        // save current source average
+        src_avg = src_reg;
+        src += src_stride;
+        LOAD_SRC_DST
+        AVG_NEXT_SRC(src_reg, 1)
+        MERGE_WITH_SRC(src_avg, src_reg)
+        FILTER_SRC(filter)
+        src_avg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        sec_reg = _mm_loadu_si128((__m128i const *)(sec));
+        src_avg = _mm_avg_epu8(src_avg, sec_reg);
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_avg, zero_reg)
+        sec += sec_stride;
+        CALC_SUM_SSE_INSIDE_LOOP
+        dst += dst_stride;
+      }
+    }
+    // x_offset = bilin interpolation and y_offset = 0
+  } else {
+    if (y_offset == 0) {
+      __m128i filter, pw8, src_next_reg;
+      x_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + x_offset));
+      pw8 = _mm_set1_epi16(8);
+      for (i = 0; i < height; i++) {
+        LOAD_SRC_DST
+        MERGE_NEXT_SRC(src_reg, 1)
+        FILTER_SRC(filter)
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        sec_reg = _mm_loadu_si128((__m128i const *)(sec));
+        src_reg = _mm_avg_epu8(src_reg, sec_reg);
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        sec += sec_stride;
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride;
+        dst += dst_stride;
+      }
+      // x_offset = bilin interpolation and y_offset = 4
+    } else if (y_offset == 4) {
+      __m128i filter, pw8, src_next_reg, src_pack;
+      x_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + x_offset));
+      pw8 = _mm_set1_epi16(8);
+      src_reg = _mm_loadu_si128((__m128i const *)(src));
+      MERGE_NEXT_SRC(src_reg, 1)
+      FILTER_SRC(filter)
+      // convert each 16 bit to 8 bit to each low and high lane source
+      src_pack = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+      for (i = 0; i < height; i++) {
+        src += src_stride;
+        LOAD_SRC_DST
+        MERGE_NEXT_SRC(src_reg, 1)
+        FILTER_SRC(filter)
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        // average between previous pack to the current
+        src_pack = _mm_avg_epu8(src_pack, src_reg);
+        sec_reg = _mm_loadu_si128((__m128i const *)(sec));
+        src_pack = _mm_avg_epu8(src_pack, sec_reg);
+        sec += sec_stride;
+        MERGE_WITH_SRC(src_pack, zero_reg)
+        src_pack = src_reg;
+        CALC_SUM_SSE_INSIDE_LOOP
+        dst += dst_stride;
+      }
+      // x_offset = bilin interpolation and y_offset = bilin interpolation
+    } else {
+      __m128i xfilter, yfilter, pw8, src_next_reg, src_pack;
+      x_offset <<= 4;
+      xfilter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + x_offset));
+      y_offset <<= 4;
+      yfilter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + y_offset));
+      pw8 = _mm_set1_epi16(8);
+      // load source and another source starting from the next
+      // following byte
+      src_reg = _mm_loadu_si128((__m128i const *)(src));
+      MERGE_NEXT_SRC(src_reg, 1)
+
+      FILTER_SRC(xfilter)
+      // convert each 16 bit to 8 bit to each low and high lane source
+      src_pack = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+      for (i = 0; i < height; i++) {
+        src += src_stride;
+        LOAD_SRC_DST
+        MERGE_NEXT_SRC(src_reg, 1)
+        FILTER_SRC(xfilter)
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        // merge previous pack to current pack source
+        MERGE_WITH_SRC(src_pack, src_reg)
+        // filter the source
+        FILTER_SRC(yfilter)
+        src_pack = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        sec_reg = _mm_loadu_si128((__m128i const *)(sec));
+        src_pack = _mm_avg_epu8(src_pack, sec_reg);
+        MERGE_WITH_SRC(src_pack, zero_reg)
+        src_pack = src_reg;
+        sec += sec_stride;
+        CALC_SUM_SSE_INSIDE_LOOP
+        dst += dst_stride;
+      }
+    }
+  }
+  CALC_SUM_AND_SSE
+  return sum;
+}
+
+unsigned int aom_sub_pixel_avg_variance8xh_ssse3(
+    const uint8_t *src, int src_stride, int x_offset, int y_offset,
+    const uint8_t *dst, int dst_stride, const uint8_t *sec, int sec_stride,
+    int height, unsigned int *sse) {
+  __m128i sec_reg;
+  __m128i src_reg, dst_reg, exp_src_lo, exp_src_hi, exp_dst_lo, exp_dst_hi;
+  __m128i sse_reg, sum_reg, sse_reg_hi, res_cmp, sum_reg_lo, sum_reg_hi;
+  __m128i zero_reg;
+  int i, sum;
+  sum_reg = _mm_setzero_si128();
+  sse_reg = _mm_setzero_si128();
+  zero_reg = _mm_setzero_si128();
+
+  // x_offset = 0 and y_offset = 0
+  if (x_offset == 0) {
+    if (y_offset == 0) {
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        sec_reg = _mm_unpacklo_epi64(
+            _mm_loadl_epi64((__m128i const *)(sec)),
+            _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+        src_reg = _mm_avg_epu8(src_reg, sec_reg);
+        sec += sec_stride << 1;
+        // expend each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = 0 and y_offset = 4
+    } else if (y_offset == 4) {
+      __m128i src_next_reg;
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        AVG_NEXT_SRC_INSERT(src_reg, src_stride)
+        sec_reg = _mm_unpacklo_epi64(
+            _mm_loadl_epi64((__m128i const *)(sec)),
+            _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+        src_reg = _mm_avg_epu8(src_reg, sec_reg);
+        sec += sec_stride << 1;
+        // expend each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = 0 and y_offset = bilin interpolation
+    } else {
+      __m128i filter, pw8, src_next_reg;
+      y_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + y_offset));
+      pw8 = _mm_set1_epi16(8);
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        MERGE_NEXT_SRC_INSERT(src_reg, src_stride)
+        FILTER_SRC(filter)
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        sec_reg = _mm_unpacklo_epi64(
+            _mm_loadl_epi64((__m128i const *)(sec)),
+            _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+        src_reg = _mm_avg_epu8(src_reg, sec_reg);
+        sec += sec_stride << 1;
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+    }
+    // x_offset = 4  and y_offset = 0
+  } else if (x_offset == 4) {
+    if (y_offset == 0) {
+      __m128i src_next_reg;
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        LOAD_DST_INSERT
+        /* average between current and next stride source */
+        src_reg = _mm_avg_epu8(src_reg, src_next_reg);
+        sec_reg = _mm_unpacklo_epi64(
+            _mm_loadl_epi64((__m128i const *)(sec)),
+            _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+        src_reg = _mm_avg_epu8(src_reg, sec_reg);
+        sec += sec_stride << 1;
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = 4  and y_offset = 4
+    } else if (y_offset == 4) {
+      __m128i src_next_reg, src_avg, src_temp;
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      src_avg = _mm_avg_epu8(src_reg, src_next_reg);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        src_next_reg = _mm_avg_epu8(src_reg, src_next_reg);
+        src_temp = _mm_castpd_si128(_mm_shuffle_pd(
+            _mm_castsi128_pd(src_avg), _mm_castsi128_pd(src_next_reg), 1));
+        src_temp = _mm_avg_epu8(src_avg, src_temp);
+        sec_reg = _mm_unpacklo_epi64(
+            _mm_loadl_epi64((__m128i const *)(sec)),
+            _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+        src_temp = _mm_avg_epu8(src_temp, sec_reg);
+        sec += sec_stride << 1;
+        LOAD_DST_INSERT
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_temp, zero_reg)
+        // save current source average
+        src_avg = src_next_reg;
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // last 2 rows processing happens here
+      __m128i src_reg_0 = _mm_loadl_epi64((__m128i const *)(src));
+      __m128i src_reg_1 = _mm_loadl_epi64((__m128i const *)(src + 1));
+      src_reg_0 = _mm_avg_epu8(src_reg_0, src_reg_1);
+      src_next_reg = _mm_castpd_si128(_mm_shuffle_pd(
+          _mm_castsi128_pd(src_avg), _mm_castsi128_pd(src_reg_0), 1));
+      LOAD_DST_INSERT
+      src_avg = _mm_avg_epu8(src_avg, src_next_reg);
+      sec_reg = _mm_unpacklo_epi64(
+          _mm_loadl_epi64((__m128i const *)(sec)),
+          _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+      src_avg = _mm_avg_epu8(src_avg, sec_reg);
+      MERGE_WITH_SRC(src_avg, zero_reg)
+      CALC_SUM_SSE_INSIDE_LOOP
+    } else {
+      // x_offset = 4  and y_offset = bilin interpolation
+      __m128i filter, pw8, src_next_reg, src_avg, src_temp;
+      y_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + y_offset));
+      pw8 = _mm_set1_epi16(8);
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      src_avg = _mm_avg_epu8(src_reg, src_next_reg);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        src_next_reg = _mm_avg_epu8(src_reg, src_next_reg);
+        src_temp = _mm_castpd_si128(_mm_shuffle_pd(
+            _mm_castsi128_pd(src_avg), _mm_castsi128_pd(src_next_reg), 1));
+        LOAD_DST_INSERT
+        MERGE_WITH_SRC(src_avg, src_temp)
+        FILTER_SRC(filter)
+        src_avg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        sec_reg = _mm_unpacklo_epi64(
+            _mm_loadl_epi64((__m128i const *)(sec)),
+            _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+        src_avg = _mm_avg_epu8(src_avg, sec_reg);
+        sec += sec_stride << 1;
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_avg, zero_reg)
+        // save current source average
+        src_avg = src_next_reg;
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // last 2 rows processing happens here
+      __m128i src_reg_0 = _mm_loadl_epi64((__m128i const *)(src));
+      __m128i src_reg_1 = _mm_loadl_epi64((__m128i const *)(src + 1));
+      src_reg_0 = _mm_avg_epu8(src_reg_0, src_reg_1);
+      src_next_reg = _mm_castpd_si128(_mm_shuffle_pd(
+          _mm_castsi128_pd(src_avg), _mm_castsi128_pd(src_reg_0), 1));
+      LOAD_DST_INSERT
+      MERGE_WITH_SRC(src_avg, src_next_reg)
+      FILTER_SRC(filter)
+      src_avg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+      sec_reg = _mm_unpacklo_epi64(
+          _mm_loadl_epi64((__m128i const *)(sec)),
+          _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+      src_avg = _mm_avg_epu8(src_avg, sec_reg);
+      MERGE_WITH_SRC(src_avg, zero_reg)
+      CALC_SUM_SSE_INSIDE_LOOP
+    }
+    // x_offset = bilin interpolation and y_offset = 0
+  } else {
+    if (y_offset == 0) {
+      __m128i filter, pw8, src_next_reg;
+      x_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + x_offset));
+      pw8 = _mm_set1_epi16(8);
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        MERGE_NEXT_SRC_INSERT(src_reg, 1)
+        FILTER_SRC(filter)
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        sec_reg = _mm_unpacklo_epi64(
+            _mm_loadl_epi64((__m128i const *)(sec)),
+            _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+        src_reg = _mm_avg_epu8(src_reg, sec_reg);
+        sec += sec_stride << 1;
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = bilin interpolation and y_offset = 4
+    } else if (y_offset == 4) {
+      __m128i filter, pw8, src_next_reg, src_pack;
+      x_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + x_offset));
+      pw8 = _mm_set1_epi16(8);
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      MERGE_WITH_SRC(src_reg, src_next_reg)
+      FILTER_SRC(filter)
+      // convert each 16 bit to 8 bit to each low and high lane source
+      src_pack = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        LOAD_DST_INSERT
+        MERGE_WITH_SRC(src_reg, src_next_reg)
+        FILTER_SRC(filter)
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        src_next_reg = _mm_castpd_si128(_mm_shuffle_pd(
+            _mm_castsi128_pd(src_pack), _mm_castsi128_pd(src_reg), 1));
+        // average between previous pack to the current
+        src_pack = _mm_avg_epu8(src_pack, src_next_reg);
+        sec_reg = _mm_unpacklo_epi64(
+            _mm_loadl_epi64((__m128i const *)(sec)),
+            _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+        src_pack = _mm_avg_epu8(src_pack, sec_reg);
+        sec += sec_stride << 1;
+        MERGE_WITH_SRC(src_pack, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src_pack = src_reg;
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // last 2 rows processing happens here
+      LOAD_SRC_MERGE_HALF(filter)
+      LOAD_DST_INSERT
+      FILTER_SRC_HALF(filter)
+      src_reg_0 = _mm_packus_epi16(src_lo, src_lo);
+      src_next_reg = _mm_castpd_si128(_mm_shuffle_pd(
+          _mm_castsi128_pd(src_pack), _mm_castsi128_pd(src_reg_0), 1));
+      // average between previous pack to the current
+      src_pack = _mm_avg_epu8(src_pack, src_next_reg);
+      sec_reg = _mm_unpacklo_epi64(
+          _mm_loadl_epi64((__m128i const *)(sec)),
+          _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+      src_pack = _mm_avg_epu8(src_pack, sec_reg);
+      MERGE_WITH_SRC(src_pack, zero_reg)
+      CALC_SUM_SSE_INSIDE_LOOP
+    } else {
+      // x_offset = bilin interpolation and y_offset = bilin interpolation
+      __m128i xfilter, yfilter, pw8, src_next_reg, src_pack;
+      x_offset <<= 4;
+      xfilter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + x_offset));
+      y_offset <<= 4;
+      yfilter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + y_offset));
+      pw8 = _mm_set1_epi16(8);
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      MERGE_WITH_SRC(src_reg, src_next_reg)
+      FILTER_SRC(xfilter)
+      // convert each 16 bit to 8 bit to each low and high lane source
+      src_pack = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        LOAD_DST_INSERT
+        MERGE_WITH_SRC(src_reg, src_next_reg)
+        FILTER_SRC(xfilter)
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        src_next_reg = _mm_castpd_si128(_mm_shuffle_pd(
+            _mm_castsi128_pd(src_pack), _mm_castsi128_pd(src_reg), 1));
+        // average between previous pack to the current
+        MERGE_WITH_SRC(src_pack, src_next_reg)
+        // filter the source
+        FILTER_SRC(yfilter)
+        src_pack = src_reg;
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        sec_reg = _mm_unpacklo_epi64(
+            _mm_loadl_epi64((__m128i const *)(sec)),
+            _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+        src_reg = _mm_avg_epu8(src_reg, sec_reg);
+        sec += sec_stride << 1;
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // last 2 rows processing happens here
+      LOAD_SRC_MERGE_HALF(xfilter)
+      LOAD_DST_INSERT
+      FILTER_SRC_HALF(xfilter)
+      src_reg_0 = _mm_packus_epi16(src_lo, src_lo);
+      src_next_reg = _mm_castpd_si128(_mm_shuffle_pd(
+          _mm_castsi128_pd(src_pack), _mm_castsi128_pd(src_reg_0), 1));
+      MERGE_WITH_SRC(src_pack, src_next_reg)
+      FILTER_SRC(yfilter)
+      src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+      sec_reg = _mm_unpacklo_epi64(
+          _mm_loadl_epi64((__m128i const *)(sec)),
+          _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+      src_reg = _mm_avg_epu8(src_reg, sec_reg);
+      MERGE_WITH_SRC(src_reg, zero_reg)
+      CALC_SUM_SSE_INSIDE_LOOP
+    }
+  }
+  CALC_SUM_AND_SSE
+  return sum;
+}
+
+#undef FILTER_SRC
+#undef MERGE_WITH_SRC
+#undef LOAD_SRC_DST
+#undef AVG_NEXT_SRC
+#undef MERGE_NEXT_SRC
+#undef CALC_SUM_SSE_INSIDE_LOOP
+#undef CALC_SUM_AND_SSE
+#undef LOAD_SRC_DST_INSERT
+#undef AVG_NEXT_SRC_INSERT
+#undef MERGE_NEXT_SRC_INSERT
+#undef LOAD_SRC_NEXT_BYTE_INSERT
+#undef LOAD_DST_INSERT
+#undef LOAD_SRC_MERGE_HALF
+#undef FILTER_SRC_HALF
+
+#define FILTER_SRC(filter)                               \
+  /* filter the source */                                \
+  exp_src_lo = _mm_maddubs_pi16(exp_src_lo, filter);     \
+  exp_src_hi = _mm_maddubs_pi16(exp_src_hi, filter);     \
+                                                         \
+  /* add 8 to source */                                  \
+  exp_src_lo = _mm_add_pi16(exp_src_lo, pw8);            \
+  exp_src_hi = _mm_add_pi16(exp_src_hi, pw8);            \
+                                                         \
+  /* divide source by 16 */                              \
+  exp_src_lo = _mm_srai_pi16(exp_src_lo, 4);             \
+  exp_src_hi = _mm_srai_pi16(exp_src_hi, 4);
+
+#define MERGE_WITH_SRC(src_reg, reg)               \
+  exp_src_lo = _mm_unpacklo_pi8(src_reg, reg);     \
+  exp_src_hi = _mm_unpackhi_pi8(src_reg, reg);
+
+#define CALC_SUM_SSE_INSIDE_LOOP                          \
+  /* expand each byte to 2 bytes */                       \
+  exp_dst_lo = _mm_unpacklo_pi8(dst_reg, zero_reg);       \
+  exp_dst_hi = _mm_unpackhi_pi8(dst_reg, zero_reg);       \
+  /* source - dest */                                     \
+  exp_src_lo = _mm_sub_pi16(exp_src_lo, exp_dst_lo);      \
+  exp_src_hi = _mm_sub_pi16(exp_src_hi, exp_dst_hi);      \
+  /* caculate sum */                                      \
+  sum_reg = _mm_add_pi16(sum_reg, exp_src_lo);            \
+  exp_src_lo = _mm_madd_pi16(exp_src_lo, exp_src_lo);     \
+  sum_reg = _mm_add_pi16(sum_reg, exp_src_hi);            \
+  exp_src_hi = _mm_madd_pi16(exp_src_hi, exp_src_hi);     \
+  /* calculate sse */                                     \
+  sse_reg = _mm_add_pi32(sse_reg, exp_src_lo);            \
+  sse_reg = _mm_add_pi32(sse_reg, exp_src_hi);
+
+// final calculation to sum and sse
+#define CALC_SUM_AND_SSE                                                   \
+  res_cmp = _mm_cmpgt_pi16(zero_reg, sum_reg);                             \
+  sum_reg_lo = _mm_unpacklo_pi16(sum_reg, res_cmp);                        \
+  sum_reg_hi = _mm_unpackhi_pi16(sum_reg, res_cmp);                        \
+  sum_reg = _mm_add_pi32(sum_reg_lo, sum_reg_hi);                          \
+                                                                           \
+  sse_reg_hi = _mm_srli_si64(sse_reg, 32);                                 \
+  sse_reg = _mm_add_pi32(sse_reg, sse_reg_hi);                             \
+  *((int *)sse) = _mm_cvtsi64_si32(sse_reg);                               \
+  sum_reg_hi = _mm_srli_si64(sum_reg, 32);                                 \
+  sum_reg = _mm_add_pi32(sum_reg, sum_reg_hi);                             \
+  sum = _mm_cvtsi64_si32(sum_reg);
+
+// Functions related to sub pixel variance width 8
+#define LOAD_SRC_DST_INSERT(src_stride, dst_stride)              \
+  /* load source and destination of 2 rows and insert*/          \
+  src_reg = _mm_unpacklo_pi32(                                   \
+      _mm_cvtsi32_si64(*(uint32_t const *)(src)),                \
+      _mm_cvtsi32_si64(*(uint32_t const *)(src + src_stride)));  \
+  dst_reg = _mm_unpacklo_pi32(                                   \
+      _mm_cvtsi32_si64(*(uint32_t const *)(dst)),                \
+      _mm_cvtsi32_si64(*(uint32_t const *)(dst + dst_stride)));
+
+#define AVG_NEXT_SRC_INSERT(src_reg, size_stride)                              \
+  src_next_reg = _mm_unpacklo_pi32(                                            \
+      _mm_cvtsi32_si64(*(uint32_t const *)(src + size_stride)),                \
+      _mm_cvtsi32_si64(*(uint32_t const *)(src + (size_stride << 1))));        \
+  /* average between current and next stride source */                         \
+  src_reg = _mm_avg_pu8(src_reg, src_next_reg);
+
+#define MERGE_NEXT_SRC_INSERT(src_reg, size_stride)                            \
+  src_next_reg = _mm_unpacklo_pi32(                                            \
+      _mm_cvtsi32_si64(*(uint32_t const *)(src + size_stride)),                \
+      _mm_cvtsi32_si64(*(uint32_t const *)(src + src_stride + size_stride)));  \
+  MERGE_WITH_SRC(src_reg, src_next_reg)
+
+#define LOAD_SRC_NEXT_BYTE_INSERT                                    \
+  /* load source and another source from next row   */               \
+  src_reg = _mm_unpacklo_pi32(                                       \
+      _mm_cvtsi32_si64(*(uint32_t const *)(src)),                    \
+      _mm_cvtsi32_si64(*(uint32_t const *)(src + src_stride)));      \
+  /* load source and next row source from 1 byte onwards   */        \
+  src_next_reg = _mm_unpacklo_pi32(                                  \
+      _mm_cvtsi32_si64(*(uint32_t const *)(src + 1)),                \
+      _mm_cvtsi32_si64(*(uint32_t const *)(src + src_stride + 1)));
+
+#define LOAD_DST_INSERT                                        \
+  dst_reg = _mm_unpacklo_pi32(                                 \
+      _mm_cvtsi32_si64(*(uint32_t const *)(dst)),              \
+      _mm_cvtsi32_si64(*(uint32_t const *)(dst + dst_stride)));
+
+#define LOAD_SRC_MERGE_HALF(filter)                                 \
+  __m64 src_reg_0 = _mm_cvtsi32_si64(*(uint32_t const *)(src));     \
+  __m64 src_reg_1 = _mm_cvtsi32_si64(*(uint32_t const *)(src + 1)); \
+  __m64 src_lo = _mm_unpacklo_pi8(src_reg_0, src_reg_1);
+
+#define FILTER_SRC_HALF(filter)               \
+  /* filter the source */                     \
+  src_lo = _mm_maddubs_pi16(src_lo, filter);  \
+                                              \
+  /* add 8 to source */                       \
+  src_lo = _mm_add_pi16(src_lo, pw8);         \
+                                              \
+  /* divide source by 16 */                   \
+  src_lo = _mm_srai_pi16(src_lo, 4);
+
+unsigned int aom_sub_pixel_variance4xh_ssse3(const uint8_t *src, int src_stride,
+                                             int x_offset, int y_offset,
+                                             const uint8_t *dst, int dst_stride,
+                                             int height, unsigned int *sse) {
+  __m64 src_reg, dst_reg, exp_src_lo, exp_src_hi, exp_dst_lo, exp_dst_hi;
+  __m64 sse_reg, sum_reg, sse_reg_hi, res_cmp, sum_reg_lo, sum_reg_hi;
+  __m64 zero_reg;
+  int i, sum;
+  sum_reg = _mm_setzero_si64();
+  sse_reg = _mm_setzero_si64();
+  zero_reg = _mm_setzero_si64();
+
+  // x_offset = 0 and y_offset = 0
+  if (x_offset == 0) {
+    if (y_offset == 0) {
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        // expend each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = 0 and y_offset = 4
+    } else if (y_offset == 4) {
+      __m64 src_next_reg;
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        AVG_NEXT_SRC_INSERT(src_reg, src_stride)
+        // expend each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = 0 and y_offset = bilin interpolation
+    } else {
+      __m64 filter, pw8, src_next_reg;
+      y_offset <<= 4;
+      filter = *(__m64 const *)(bilinear_filters_sse2 + y_offset);
+      pw8 = _mm_set1_pi16(8);
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        MERGE_NEXT_SRC_INSERT(src_reg, src_stride)
+        FILTER_SRC(filter)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+    }
+    // x_offset = 4  and y_offset = 0
+  } else if (x_offset == 4) {
+    if (y_offset == 0) {
+      __m64 src_next_reg;
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        LOAD_DST_INSERT
+        /* average between current and next stride source */
+        src_reg = _mm_avg_pu8(src_reg, src_next_reg);
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = 4  and y_offset = 4
+    } else if (y_offset == 4) {
+      __m64 src_next_reg, src_avg, src_temp;
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      src_avg = _mm_avg_pu8(src_reg, src_next_reg);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        src_next_reg = _mm_avg_pu8(src_reg, src_next_reg);
+        src_temp = _mm_alignr_pi8(src_next_reg, src_avg, 4);
+        src_temp = _mm_avg_pu8(src_avg, src_temp);
+        LOAD_DST_INSERT
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_temp, zero_reg)
+        // save current source average
+        src_avg = src_next_reg;
+        CALC_SUM_SSE_INSIDE_LOOP
+        dst += dst_stride << 1;
+        src += src_stride << 1;
+      }
+      // last 2 rows processing happens here
+      __m64 src_reg_0 = _mm_cvtsi32_si64(*(uint32_t const *)(src));
+      __m64 src_reg_1 = _mm_cvtsi32_si64(*(uint32_t const *)(src + 1));
+      src_reg_0 = _mm_avg_pu8(src_reg_0, src_reg_1);
+      src_next_reg = _mm_alignr_pi8(src_reg_0, src_avg, 4);
+      LOAD_DST_INSERT
+      src_avg = _mm_avg_pu8(src_avg, src_next_reg);
+      MERGE_WITH_SRC(src_avg, zero_reg)
+      CALC_SUM_SSE_INSIDE_LOOP
+    } else {
+      // x_offset = 4  and y_offset = bilin interpolation
+      __m64 filter, pw8, src_next_reg, src_avg, src_temp;
+      y_offset <<= 4;
+      filter = *(__m64 const *)(bilinear_filters_sse2 + y_offset);
+      pw8 = _mm_set1_pi16(8);
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      src_avg = _mm_avg_pu8(src_reg, src_next_reg);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        src_next_reg = _mm_avg_pu8(src_reg, src_next_reg);
+        src_temp = _mm_alignr_pi8(src_next_reg, src_avg, 4);
+        LOAD_DST_INSERT
+        MERGE_WITH_SRC(src_avg, src_temp)
+        // save current source average
+        src_avg = src_next_reg;
+        FILTER_SRC(filter)
+        CALC_SUM_SSE_INSIDE_LOOP
+        dst += dst_stride << 1;
+        src += src_stride << 1;
+      }
+      // last 2 rows processing happens here
+      __m64 src_reg_0 = _mm_cvtsi32_si64(*(uint32_t const *)(src));
+      __m64 src_reg_1 = _mm_cvtsi32_si64(*(uint32_t const *)(src + 1));
+      src_reg_0 = _mm_avg_pu8(src_reg_0, src_reg_1);
+      src_next_reg = _mm_alignr_pi8(src_reg_0, src_avg, 4);
+      LOAD_DST_INSERT
+      MERGE_WITH_SRC(src_avg, src_next_reg)
+      FILTER_SRC(filter)
+      CALC_SUM_SSE_INSIDE_LOOP
+    }
+    // x_offset = bilin interpolation and y_offset = 0
+  } else {
+    if (y_offset == 0) {
+      __m64 filter, pw8, src_next_reg;
+      x_offset <<= 4;
+      filter = *(__m64 const *)(bilinear_filters_sse2 + x_offset);
+      pw8 = _mm_set1_pi16(8);
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        MERGE_NEXT_SRC_INSERT(src_reg, 1)
+        FILTER_SRC(filter)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = bilin interpolation and y_offset = 4
+    } else if (y_offset == 4) {
+      __m64 filter, pw8, src_next_reg, src_pack;
+      x_offset <<= 4;
+      filter = *(__m64 const *)(bilinear_filters_sse2 + x_offset);
+      pw8 = _mm_set1_pi16(8);
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      MERGE_WITH_SRC(src_reg, src_next_reg)
+      FILTER_SRC(filter)
+      // convert each 16 bit to 8 bit to each low and high lane source
+      src_pack = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        LOAD_DST_INSERT
+        MERGE_WITH_SRC(src_reg, src_next_reg)
+        FILTER_SRC(filter)
+        src_reg = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+        src_next_reg = _mm_alignr_pi8(src_reg, src_pack, 4);
+        // average between previous pack to the current
+        src_pack = _mm_avg_pu8(src_pack, src_next_reg);
+        MERGE_WITH_SRC(src_pack, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src_pack = src_reg;
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // last 2 rows processing happens here
+      LOAD_SRC_MERGE_HALF(filter)
+      LOAD_DST_INSERT
+      FILTER_SRC_HALF(filter)
+      src_reg_0 = _mm_packs_pu16(src_lo, src_lo);
+      src_next_reg = _mm_alignr_pi8(src_reg_0, src_pack, 4);
+      // average between previous pack to the current
+      src_pack = _mm_avg_pu8(src_pack, src_next_reg);
+      MERGE_WITH_SRC(src_pack, zero_reg)
+      CALC_SUM_SSE_INSIDE_LOOP
+    } else {
+      // x_offset = bilin interpolation and y_offset = bilin interpolation
+      __m64 xfilter, yfilter, pw8, src_next_reg, src_pack;
+      x_offset <<= 4;
+      xfilter = *(__m64 const *)(bilinear_filters_sse2 + x_offset);
+      y_offset <<= 4;
+      yfilter = *(__m64 const *)(bilinear_filters_sse2 + y_offset);
+      pw8 = _mm_set1_pi16(8);
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      MERGE_WITH_SRC(src_reg, src_next_reg)
+      FILTER_SRC(xfilter)
+      // convert each 16 bit to 8 bit to each low and high lane source
+      src_pack = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        LOAD_DST_INSERT
+        MERGE_WITH_SRC(src_reg, src_next_reg)
+        FILTER_SRC(xfilter)
+        src_reg = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+        src_next_reg = _mm_alignr_pi8(src_reg, src_pack, 4);
+        // average between previous pack to the current
+        MERGE_WITH_SRC(src_pack, src_next_reg)
+        // filter the source
+        FILTER_SRC(yfilter)
+        src_pack = src_reg;
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // last 2 rows processing happens here
+      LOAD_SRC_MERGE_HALF(xfilter)
+      LOAD_DST_INSERT
+      FILTER_SRC_HALF(xfilter)
+      src_reg_0 = _mm_packs_pu16(src_lo, src_lo);
+      src_next_reg = _mm_alignr_pi8(src_reg_0, src_pack, 4);
+      MERGE_WITH_SRC(src_pack, src_next_reg)
+      FILTER_SRC(yfilter)
+      CALC_SUM_SSE_INSIDE_LOOP
+    }
+  }
+  CALC_SUM_AND_SSE
+  return sum;
+}
+
+unsigned int aom_sub_pixel_avg_variance4xh_ssse3(
+    const uint8_t *src, int src_stride, int x_offset, int y_offset,
+    const uint8_t *dst, int dst_stride, const uint8_t *sec, int sec_stride,
+    int height, unsigned int *sse) {
+  __m64 sec_reg;
+  __m64 src_reg, dst_reg, exp_src_lo, exp_src_hi, exp_dst_lo, exp_dst_hi;
+  __m64 sse_reg, sum_reg, sse_reg_hi, res_cmp, sum_reg_lo, sum_reg_hi;
+  __m64 zero_reg;
+  int i, sum;
+  sum_reg = _mm_setzero_si64();
+  sse_reg = _mm_setzero_si64();
+  zero_reg = _mm_setzero_si64();
+
+  // x_offset = 0 and y_offset = 0
+  if (x_offset == 0) {
+    if (y_offset == 0) {
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        sec_reg = _mm_unpacklo_pi32(
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+        src_reg = _mm_avg_pu8(src_reg, sec_reg);
+        sec += sec_stride << 1;
+        // expend each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = 0 and y_offset = 4
+    } else if (y_offset == 4) {
+      __m64 src_next_reg;
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        AVG_NEXT_SRC_INSERT(src_reg, src_stride)
+        sec_reg = _mm_unpacklo_pi32(
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+        src_reg = _mm_avg_pu8(src_reg, sec_reg);
+        sec += sec_stride << 1;
+        // expend each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = 0 and y_offset = bilin interpolation
+    } else {
+      __m64 filter, pw8, src_next_reg;
+      y_offset <<= 4;
+      filter = *(__m64 const *)(bilinear_filters_sse2 + y_offset);
+      pw8 = _mm_set1_pi16(8);
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        MERGE_NEXT_SRC_INSERT(src_reg, src_stride)
+        FILTER_SRC(filter)
+        src_reg = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+        sec_reg = _mm_unpacklo_pi32(
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+        src_reg = _mm_avg_pu8(src_reg, sec_reg);
+        sec += sec_stride << 1;
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+    }
+    // x_offset = 4  and y_offset = 0
+  } else if (x_offset == 4) {
+    if (y_offset == 0) {
+      __m64 src_next_reg;
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        LOAD_DST_INSERT
+        /* average between current and next stride source */
+        src_reg = _mm_avg_pu8(src_reg, src_next_reg);
+        sec_reg = _mm_unpacklo_pi32(
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+        src_reg = _mm_avg_pu8(src_reg, sec_reg);
+        sec += sec_stride << 1;
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = 4  and y_offset = 4
+    } else if (y_offset == 4) {
+      __m64 src_next_reg, src_avg, src_temp;
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      src_avg = _mm_avg_pu8(src_reg, src_next_reg);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        src_next_reg = _mm_avg_pu8(src_reg, src_next_reg);
+        src_temp = _mm_alignr_pi8(src_next_reg, src_avg, 4);
+        src_temp = _mm_avg_pu8(src_avg, src_temp);
+        sec_reg = _mm_unpacklo_pi32(
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+        src_temp = _mm_avg_pu8(src_temp, sec_reg);
+        sec += sec_stride << 1;
+        LOAD_DST_INSERT
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_temp, zero_reg)
+        // save current source average
+        src_avg = src_next_reg;
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // last 2 rows processing happens here
+      __m64 src_reg_0 = _mm_cvtsi32_si64(*(uint32_t const *)(src));
+      __m64 src_reg_1 = _mm_cvtsi32_si64(*(uint32_t const *)(src + 1));
+      src_reg_0 = _mm_avg_pu8(src_reg_0, src_reg_1);
+      src_next_reg = _mm_alignr_pi8(src_reg_0, src_avg, 4);
+      LOAD_DST_INSERT
+      src_avg = _mm_avg_pu8(src_avg, src_next_reg);
+      sec_reg = _mm_unpacklo_pi32(
+          _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+          _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+      src_avg = _mm_avg_pu8(src_avg, sec_reg);
+      MERGE_WITH_SRC(src_avg, zero_reg)
+      CALC_SUM_SSE_INSIDE_LOOP
+    } else {
+      // x_offset = 4  and y_offset = bilin interpolation
+      __m64 filter, pw8, src_next_reg, src_avg, src_temp;
+      y_offset <<= 4;
+      filter = *(__m64 const *)(bilinear_filters_sse2 + y_offset);
+      pw8 = _mm_set1_pi16(8);
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      src_avg = _mm_avg_pu8(src_reg, src_next_reg);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        src_next_reg = _mm_avg_pu8(src_reg, src_next_reg);
+        src_temp = _mm_alignr_pi8(src_next_reg, src_avg, 4);
+        LOAD_DST_INSERT
+        MERGE_WITH_SRC(src_avg, src_temp)
+        FILTER_SRC(filter)
+        src_avg = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+        sec_reg = _mm_unpacklo_pi32(
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+        src_avg = _mm_avg_pu8(src_avg, sec_reg);
+        sec += sec_stride << 1;
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_avg, zero_reg)
+        // save current source average
+        src_avg = src_next_reg;
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // last 2 rows processing happens here
+      __m64 src_reg_0 = _mm_cvtsi32_si64(*(uint32_t const *)(src));
+      __m64 src_reg_1 = _mm_cvtsi32_si64(*(uint32_t const *)(src + 1));
+      src_reg_0 = _mm_avg_pu8(src_reg_0, src_reg_1);
+      src_next_reg = _mm_alignr_pi8(src_reg_0, src_avg, 4);
+      LOAD_DST_INSERT
+      MERGE_WITH_SRC(src_avg, src_next_reg)
+      FILTER_SRC(filter)
+      src_avg = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+      sec_reg = _mm_unpacklo_pi32(
+          _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+          _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+      src_avg = _mm_avg_pu8(src_avg, sec_reg);
+      MERGE_WITH_SRC(src_avg, zero_reg)
+      CALC_SUM_SSE_INSIDE_LOOP
+    }
+    // x_offset = bilin interpolation and y_offset = 0
+  } else {
+    if (y_offset == 0) {
+      __m64 filter, pw8, src_next_reg;
+      x_offset <<= 4;
+      filter = *(__m64 const *)(bilinear_filters_sse2 + x_offset);
+      pw8 = _mm_set1_pi16(8);
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        MERGE_NEXT_SRC_INSERT(src_reg, 1)
+        FILTER_SRC(filter)
+        src_reg = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+        sec_reg = _mm_unpacklo_pi32(
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+        src_reg = _mm_avg_pu8(src_reg, sec_reg);
+        sec += sec_stride << 1;
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = bilin interpolation and y_offset = 4
+    } else if (y_offset == 4) {
+      __m64 filter, pw8, src_next_reg, src_pack;
+      x_offset <<= 4;
+      filter = *(__m64 const *)(bilinear_filters_sse2 + x_offset);
+      pw8 = _mm_set1_pi16(8);
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      MERGE_WITH_SRC(src_reg, src_next_reg)
+      FILTER_SRC(filter)
+      // convert each 16 bit to 8 bit to each low and high lane source
+      src_pack = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        LOAD_DST_INSERT
+        MERGE_WITH_SRC(src_reg, src_next_reg)
+        FILTER_SRC(filter)
+        src_reg = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+        src_next_reg = _mm_alignr_pi8(src_reg, src_pack, 4);
+        // average between previous pack to the current
+        src_pack = _mm_avg_pu8(src_pack, src_next_reg);
+        sec_reg = _mm_unpacklo_pi32(
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+        src_pack = _mm_avg_pu8(src_pack, sec_reg);
+        sec += sec_stride << 1;
+        MERGE_WITH_SRC(src_pack, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src_pack = src_reg;
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // last 2 rows processing happens here
+      LOAD_SRC_MERGE_HALF(filter)
+      LOAD_DST_INSERT
+      FILTER_SRC_HALF(filter)
+      src_reg_0 = _mm_packs_pu16(src_lo, src_lo);
+      src_next_reg = _mm_alignr_pi8(src_reg_0, src_pack, 4);
+      // average between previous pack to the current
+      src_pack = _mm_avg_pu8(src_pack, src_next_reg);
+      sec_reg = _mm_unpacklo_pi32(
+          _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+          _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+      src_pack = _mm_avg_pu8(src_pack, sec_reg);
+      MERGE_WITH_SRC(src_pack, zero_reg)
+      CALC_SUM_SSE_INSIDE_LOOP
+    } else {
+      // x_offset = bilin interpolation and y_offset = bilin interpolation
+      __m64 xfilter, yfilter, pw8, src_next_reg, src_pack;
+      x_offset <<= 4;
+      xfilter = *(__m64 const *)(bilinear_filters_sse2 + x_offset);
+      y_offset <<= 4;
+      yfilter = *(__m64 const *)(bilinear_filters_sse2 + y_offset);
+      pw8 = _mm_set1_pi16(8);
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      MERGE_WITH_SRC(src_reg, src_next_reg)
+      FILTER_SRC(xfilter)
+      // convert each 16 bit to 8 bit to each low and high lane source
+      src_pack = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        LOAD_DST_INSERT
+        MERGE_WITH_SRC(src_reg, src_next_reg)
+        FILTER_SRC(xfilter)
+        src_reg = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+        src_next_reg = _mm_alignr_pi8(src_reg, src_pack, 4);
+        // average between previous pack to the current
+        MERGE_WITH_SRC(src_pack, src_next_reg)
+        // filter the source
+        FILTER_SRC(yfilter)
+        src_pack = src_reg;
+        src_reg = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+        sec_reg = _mm_unpacklo_pi32(
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+        src_reg = _mm_avg_pu8(src_reg, sec_reg);
+        sec += sec_stride << 1;
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // last 2 rows processing happens here
+      LOAD_SRC_MERGE_HALF(xfilter)
+      LOAD_DST_INSERT
+      FILTER_SRC_HALF(xfilter)
+      src_reg_0 = _mm_packs_pu16(src_lo, src_lo);
+      src_next_reg = _mm_alignr_pi8(src_reg_0, src_pack, 4);
+      MERGE_WITH_SRC(src_pack, src_next_reg)
+      FILTER_SRC(yfilter)
+      src_reg = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+      sec_reg = _mm_unpacklo_pi32(
+          _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+          _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+      src_reg = _mm_avg_pu8(src_reg, sec_reg);
+      MERGE_WITH_SRC(src_reg, zero_reg)
+      CALC_SUM_SSE_INSIDE_LOOP
+    }
+  }
+  CALC_SUM_AND_SSE
+  return sum;
+}
+
diff --git a/aom_dsp/e2k/subtract_e2k.c b/aom_dsp/e2k/subtract_e2k.c
new file mode 100644
index 0000000..53449fb
--- /dev/null
+++ b/aom_dsp/e2k/subtract_e2k.c
@@ -0,0 +1,111 @@
+/*
+ * Copyright (c) 2021, Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2016, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "config/aom_dsp_rtcd.h"
+
+#define PROCESS16(i) \
+  v2 = _mm_load_si128((const __m128i *)(src + i)); \
+  v3 = _mm_load_si128((const __m128i *)(pred + i)); \
+  v0 = _mm_unpacklo_epi8(v2, vzero); \
+  v1 = _mm_unpackhi_epi8(v2, vzero); \
+  v0 = _mm_sub_epi16(v0, _mm_unpacklo_epi8(v3, vzero)); \
+  v1 = _mm_sub_epi16(v1, _mm_unpackhi_epi8(v3, vzero)); \
+  _mm_store_si128((__m128i *)(diff + i), v0); \
+  _mm_store_si128((__m128i *)(diff + i + 8), v1);
+
+void aom_subtract_block_sse2(int rows, int cols, int16_t *diff,
+                             ptrdiff_t diff_stride, const uint8_t *src,
+                             ptrdiff_t src_stride, const uint8_t *pred,
+                             ptrdiff_t pred_stride) {
+  __m128i v0, v1, v2, v3, vzero = _mm_setzero_si128();
+  __m64 h0, h1, h2, h3, hzero;
+  int i;
+
+  switch (cols) {
+    case 128:
+      PRAGMA_E2K("ivdep")
+      for (i = 0; i < rows; i++) {
+        PROCESS16(0) PROCESS16(16) PROCESS16(32) PROCESS16(48)
+        PROCESS16(64) PROCESS16(80) PROCESS16(96) PROCESS16(112)
+        src += src_stride;
+        pred += pred_stride;
+        diff += diff_stride;
+      }
+      break;
+
+    case 64:
+      PRAGMA_E2K("ivdep")
+      for (i = 0; i < rows; i++) {
+        PROCESS16(0) PROCESS16(16) PROCESS16(32) PROCESS16(48)
+        src += src_stride;
+        pred += pred_stride;
+        diff += diff_stride;
+      }
+      break;
+
+    case 32:
+      PRAGMA_E2K("ivdep")
+      for (i = 0; i < rows; i++) {
+        PROCESS16(0) PROCESS16(16)
+        src += src_stride;
+        pred += pred_stride;
+        diff += diff_stride;
+      }
+      break;
+
+    case 16:
+      PRAGMA_E2K("ivdep")
+      for (i = 0; i < rows; i++) {
+        PROCESS16(0)
+        src += src_stride;
+        pred += pred_stride;
+        diff += diff_stride;
+      }
+      break;
+
+    case 8:
+      hzero = _mm_setzero_si64();
+      PRAGMA_E2K("ivdep")
+      for (i = 0; i < rows; i++) {
+        h2 = *(const __m64 *)src;
+        h3 = *(const __m64 *)pred;
+        h0 = _mm_unpacklo_pi8(h2, hzero);
+        h1 = _mm_unpackhi_pi8(h2, hzero);
+        h0 = _mm_sub_pi16(h0, _mm_unpacklo_pi8(h3, hzero));
+        h1 = _mm_sub_pi16(h1, _mm_unpackhi_pi8(h3, hzero));
+        *(__m64 *)diff = h0;
+        *(__m64 *)(diff + 4) = h1;
+        src += src_stride;
+        pred += pred_stride;
+        diff += diff_stride;
+      }
+      break;
+
+    case 4:
+      hzero = _mm_setzero_si64();
+      PRAGMA_E2K("ivdep")
+      for (i = 0; i < rows; i++) {
+        h2 = _mm_cvtsi32_si64(*(uint32_t const *)src);
+        h3 = _mm_cvtsi32_si64(*(uint32_t const *)pred);
+        h0 = _mm_unpacklo_pi8(h2, hzero);
+        h0 = _mm_sub_pi16(h0, _mm_unpacklo_pi8(h3, hzero));
+        *(__m64 *)diff = h0;
+        src += src_stride;
+        pred += pred_stride;
+        diff += diff_stride;
+      }
+      break;
+  }
+}
+
diff --git a/aom_dsp/simd/v256_intrinsics_c.h b/aom_dsp/simd/v256_intrinsics_c.h
index 8127ee3..6fcb55c 100644
--- a/aom_dsp/simd/v256_intrinsics_c.h
+++ b/aom_dsp/simd/v256_intrinsics_c.h
@@ -78,7 +78,11 @@ SIMD_INLINE c_v256 c_v256_load_unaligned(const void *p) {
 }
 
 SIMD_INLINE c_v256 c_v256_load_aligned(const void *p) {
+#ifdef __e2k__
+  if (SIMD_CHECK && (uintptr_t)p & 15) {
+#else
   if (SIMD_CHECK && (uintptr_t)p & 31) {
+#endif
     fprintf(stderr, "Error: unaligned v256 load at %p\n", p);
     abort();
   }
@@ -93,7 +97,11 @@ SIMD_INLINE void c_v256_store_unaligned(void *p, c_v256 a) {
 }
 
 SIMD_INLINE void c_v256_store_aligned(void *p, c_v256 a) {
+#ifdef __e2k__
+  if (SIMD_CHECK && (uintptr_t)p & 15) {
+#else
   if (SIMD_CHECK && (uintptr_t)p & 31) {
+#endif
     fprintf(stderr, "Error: unaligned v256 store at %p\n", p);
     abort();
   }
diff --git a/aom_dsp/simd/v256_intrinsics_x86.h b/aom_dsp/simd/v256_intrinsics_x86.h
index 5983cb8..3262695 100644
--- a/aom_dsp/simd/v256_intrinsics_x86.h
+++ b/aom_dsp/simd/v256_intrinsics_x86.h
@@ -12,7 +12,7 @@
 #ifndef AOM_AOM_DSP_SIMD_V256_INTRINSICS_X86_H_
 #define AOM_AOM_DSP_SIMD_V256_INTRINSICS_X86_H_
 
-#if !defined(__AVX2__)
+#if !defined(__AVX2__) || defined(__e2k__)
 
 #include "aom_dsp/simd/v256_intrinsics_v128.h"
 
diff --git a/aom_dsp/x86/aom_asm_stubs.c b/aom_dsp/x86/aom_asm_stubs.c
index ce8285e..17aa7b1 100644
--- a/aom_dsp/x86/aom_asm_stubs.c
+++ b/aom_dsp/x86/aom_asm_stubs.c
@@ -46,8 +46,10 @@ filter8_1dfunction aom_filter_block1d4_h2_sse2;
 //                              const int16_t *filter_x, int x_step_q4,
 //                              const int16_t *filter_y, int y_step_q4,
 //                              int w, int h);
+#if !ARCH_E2K
 FUN_CONV_1D(horiz, x_step_q4, filter_x, h, src, , sse2);
 FUN_CONV_1D(vert, y_step_q4, filter_y, v, src - src_stride * 3, , sse2);
+#endif
 
 #if CONFIG_AV1_HIGHBITDEPTH
 highbd_filter8_1dfunction aom_highbd_filter_block1d16_v8_sse2;
diff --git a/aom_dsp/x86/variance_sse2.c b/aom_dsp/x86/variance_sse2.c
index e372a4b..4ebfd46 100644
--- a/aom_dsp/x86/variance_sse2.c
+++ b/aom_dsp/x86/variance_sse2.c
@@ -434,7 +434,9 @@ DECLS(ssse3);
   FN(4, 4, 4, 2, 2, opt, (int32_t), (int32_t));
 #endif
 
+#if !ARCH_E2K
 FNS(sse2);
+#endif
 FNS(ssse3);
 
 #undef FNS
@@ -533,7 +535,9 @@ DECLS(ssse3);
   FN(4, 4, 4, 2, 2, opt, (uint32_t), (int32_t));
 #endif
 
+#if !ARCH_E2K
 FNS(sse2);
+#endif
 FNS(ssse3);
 
 #undef FNS
diff --git a/aom_ports/e2k.h b/aom_ports/e2k.h
new file mode 100644
index 0000000..e3d6107
--- /dev/null
+++ b/aom_ports/e2k.h
@@ -0,0 +1,98 @@
+/*
+ * Copyright (c) 2018, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#ifndef AOM_AOM_PORTS_E2K_H_
+#define AOM_AOM_PORTS_E2K_H_
+#include <stdlib.h>
+
+#include "config/aom_config.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#ifdef __e2k__
+#define PRAGMA_E2K _Pragma
+#else
+#define PRAGMA_E2K(x)
+#endif
+
+#define _mm_extract_pi32(a, b) _mm_extract_epi32(_mm_movpi64_epi64(a), b)
+
+#ifdef RTCD_C
+#define HAS_MMX    1
+#define HAS_SSE    2
+#define HAS_SSE2   2
+#define HAS_SSE3   2
+#define HAS_SSSE3  2
+#define HAS_SSE4_1 2
+#define HAS_AVX    4
+#define HAS_AVX2   4
+
+#define e2k_simd_caps() 3
+#endif
+
+#define aom_convolve8_horiz_sse2 aom_convolve8_horiz_ssse3
+#define aom_convolve8_vert_sse2 aom_convolve8_vert_ssse3
+
+// aom_sub_pixel_variance*_sse2 -> aom_sub_pixel_variance*_ssse3
+#define aom_sub_pixel_variance128x128_sse2 aom_sub_pixel_variance128x128_ssse3
+#define aom_sub_pixel_variance128x64_sse2 aom_sub_pixel_variance128x64_ssse3
+#define aom_sub_pixel_variance64x128_sse2 aom_sub_pixel_variance64x128_ssse3
+#define aom_sub_pixel_variance64x64_sse2 aom_sub_pixel_variance64x64_ssse3
+#define aom_sub_pixel_variance64x32_sse2 aom_sub_pixel_variance64x32_ssse3
+#define aom_sub_pixel_variance64x16_sse2 aom_sub_pixel_variance64x16_ssse3
+#define aom_sub_pixel_variance32x64_sse2 aom_sub_pixel_variance32x64_ssse3
+#define aom_sub_pixel_variance32x32_sse2 aom_sub_pixel_variance32x32_ssse3
+#define aom_sub_pixel_variance32x16_sse2 aom_sub_pixel_variance32x16_ssse3
+#define aom_sub_pixel_variance32x8_sse2 aom_sub_pixel_variance32x8_ssse3
+#define aom_sub_pixel_variance16x64_sse2 aom_sub_pixel_variance16x64_ssse3
+#define aom_sub_pixel_variance16x32_sse2 aom_sub_pixel_variance16x32_ssse3
+#define aom_sub_pixel_variance16x16_sse2 aom_sub_pixel_variance16x16_ssse3
+#define aom_sub_pixel_variance16x8_sse2 aom_sub_pixel_variance16x8_ssse3
+#define aom_sub_pixel_variance16x4_sse2 aom_sub_pixel_variance16x4_ssse3
+#define aom_sub_pixel_variance8x32_sse2 aom_sub_pixel_variance8x32_ssse3
+#define aom_sub_pixel_variance8x16_sse2 aom_sub_pixel_variance8x16_ssse3
+#define aom_sub_pixel_variance8x8_sse2 aom_sub_pixel_variance8x8_ssse3
+#define aom_sub_pixel_variance8x4_sse2 aom_sub_pixel_variance8x4_ssse3
+#define aom_sub_pixel_variance4x16_sse2 aom_sub_pixel_variance4x16_ssse3
+#define aom_sub_pixel_variance4x8_sse2 aom_sub_pixel_variance4x8_ssse3
+#define aom_sub_pixel_variance4x4_sse2 aom_sub_pixel_variance4x4_ssse3
+
+// aom_sub_pixel_avg_variance*_sse2 -> aom_sub_pixel_avg_variance*_ssse3
+#define aom_sub_pixel_avg_variance128x128_sse2 aom_sub_pixel_avg_variance128x128_ssse3
+#define aom_sub_pixel_avg_variance128x64_sse2 aom_sub_pixel_avg_variance128x64_ssse3
+#define aom_sub_pixel_avg_variance64x128_sse2 aom_sub_pixel_avg_variance64x128_ssse3
+#define aom_sub_pixel_avg_variance64x64_sse2 aom_sub_pixel_avg_variance64x64_ssse3
+#define aom_sub_pixel_avg_variance64x32_sse2 aom_sub_pixel_avg_variance64x32_ssse3
+#define aom_sub_pixel_avg_variance64x16_sse2 aom_sub_pixel_avg_variance64x16_ssse3
+#define aom_sub_pixel_avg_variance32x64_sse2 aom_sub_pixel_avg_variance32x64_ssse3
+#define aom_sub_pixel_avg_variance32x32_sse2 aom_sub_pixel_avg_variance32x32_ssse3
+#define aom_sub_pixel_avg_variance32x16_sse2 aom_sub_pixel_avg_variance32x16_ssse3
+#define aom_sub_pixel_avg_variance32x8_sse2 aom_sub_pixel_avg_variance32x8_ssse3
+#define aom_sub_pixel_avg_variance16x64_sse2 aom_sub_pixel_avg_variance16x64_ssse3
+#define aom_sub_pixel_avg_variance16x32_sse2 aom_sub_pixel_avg_variance16x32_ssse3
+#define aom_sub_pixel_avg_variance16x16_sse2 aom_sub_pixel_avg_variance16x16_ssse3
+#define aom_sub_pixel_avg_variance16x8_sse2 aom_sub_pixel_avg_variance16x8_ssse3
+#define aom_sub_pixel_avg_variance16x4_sse2 aom_sub_pixel_avg_variance16x4_ssse3
+#define aom_sub_pixel_avg_variance8x32_sse2 aom_sub_pixel_avg_variance8x32_ssse3
+#define aom_sub_pixel_avg_variance8x16_sse2 aom_sub_pixel_avg_variance8x16_ssse3
+#define aom_sub_pixel_avg_variance8x8_sse2 aom_sub_pixel_avg_variance8x8_ssse3
+#define aom_sub_pixel_avg_variance8x4_sse2 aom_sub_pixel_avg_variance8x4_ssse3
+#define aom_sub_pixel_avg_variance4x16_sse2 aom_sub_pixel_avg_variance4x16_ssse3
+#define aom_sub_pixel_avg_variance4x8_sse2 aom_sub_pixel_avg_variance4x8_ssse3
+#define aom_sub_pixel_avg_variance4x4_sse2 aom_sub_pixel_avg_variance4x4_ssse3
+
+#ifdef __cplusplus
+}  // extern "C"
+#endif
+
+#endif  // AOM_AOM_PORTS_E2K_H_
diff --git a/av1/av1.cmake b/av1/av1.cmake
index b48c614..a4cf999 100644
--- a/av1/av1.cmake
+++ b/av1/av1.cmake
@@ -422,6 +422,11 @@ if(CONFIG_REALTIME_ONLY)
                    "${AOM_ROOT}/av1/encoder/x86/pickrst_sse4.c")
 endif()
 
+if("${AOM_TARGET_CPU}" STREQUAL "e2k")
+  list(APPEND AOM_AV1_ENCODER_INTRIN_SSE4_1
+              "${AOM_ROOT}/av1/encoder/e2k/error_e2k.c")
+endif()
+
 list(APPEND AOM_AV1_ENCODER_INTRIN_AVX2
             "${AOM_ROOT}/av1/encoder/x86/av1_quantize_avx2.c"
             "${AOM_ROOT}/av1/encoder/x86/av1_highbd_quantize_avx2.c"
diff --git a/av1/encoder/e2k/error_e2k.c b/av1/encoder/e2k/error_e2k.c
new file mode 100644
index 0000000..09b4961
--- /dev/null
+++ b/av1/encoder/e2k/error_e2k.c
@@ -0,0 +1,68 @@
+/*
+ * Copyright (c) 2021, Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2016, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "config/av1_rtcd.h"
+
+#include "aom/aom_integer.h"
+
+#define READ_COEFF(coeff, r) \
+  if (sizeof(tran_low_t) == 4) { \
+    __m128i x0 = _mm_loadu_si128((const __m128i *)(coeff + i)); \
+    __m128i x1 = _mm_loadu_si128((const __m128i *)(coeff + i + 4)); \
+    r = _mm_packs_epi32(x0, x1); \
+  } else { \
+    r = _mm_loadu_si128((const __m128i *)(coeff + i)); \
+  } \
+
+int64_t av1_block_error_sse2(const tran_low_t *coeff, const tran_low_t *dqcoeff,
+                             intptr_t block_size, int64_t *ssz) {
+  __m128i sse_reg, ssz_reg, coeff_reg, dqcoeff_reg;
+  __m128i exp_dqcoeff_lo, exp_dqcoeff_hi, exp_coeff_lo, exp_coeff_hi;
+  int i;
+  const __m128i zero_reg = _mm_setzero_si128();
+
+  // init sse and ssz registerd to zero
+  sse_reg = _mm_setzero_si128();
+  ssz_reg = _mm_setzero_si128();
+
+  for (i = 0; i < block_size; i += 8) {
+    // load 16 bytes from coeff and dqcoeff
+    READ_COEFF(coeff, coeff_reg)
+    READ_COEFF(dqcoeff, dqcoeff_reg)
+    // dqcoeff - coeff
+    dqcoeff_reg = _mm_sub_epi16(dqcoeff_reg, coeff_reg);
+    // madd (dqcoeff - coeff)
+    dqcoeff_reg = _mm_madd_epi16(dqcoeff_reg, dqcoeff_reg);
+    // madd coeff
+    coeff_reg = _mm_madd_epi16(coeff_reg, coeff_reg);
+    // expand each double word of madd (dqcoeff - coeff) to quad word
+    exp_dqcoeff_lo = _mm_unpacklo_epi32(dqcoeff_reg, zero_reg);
+    exp_dqcoeff_hi = _mm_unpackhi_epi32(dqcoeff_reg, zero_reg);
+    // expand each double word of madd (coeff) to quad word
+    exp_coeff_lo = _mm_unpacklo_epi32(coeff_reg, zero_reg);
+    exp_coeff_hi = _mm_unpackhi_epi32(coeff_reg, zero_reg);
+    // add each quad word of madd (dqcoeff - coeff) and madd (coeff)
+    sse_reg = _mm_add_epi64(sse_reg, exp_dqcoeff_lo);
+    ssz_reg = _mm_add_epi64(ssz_reg, exp_coeff_lo);
+    sse_reg = _mm_add_epi64(sse_reg, exp_dqcoeff_hi);
+    ssz_reg = _mm_add_epi64(ssz_reg, exp_coeff_hi);
+  }
+  // add the higher 64 bit to the low 64 bit
+  sse_reg = _mm_add_epi64(sse_reg, _mm_bsrli_si128(sse_reg, 8));
+  ssz_reg = _mm_add_epi64(ssz_reg, _mm_bsrli_si128(ssz_reg, 8));
+
+  // store the results
+  _mm_storel_epi64((__m128i *)ssz, ssz_reg);
+  return _mm_cvtsi128_si64(sse_reg);
+}
diff --git a/build/cmake/aom_config_defaults.cmake b/build/cmake/aom_config_defaults.cmake
index f1b1e57..faf2acc 100644
--- a/build/cmake/aom_config_defaults.cmake
+++ b/build/cmake/aom_config_defaults.cmake
@@ -28,6 +28,7 @@ set_aom_detect_var(ARCH_MIPS 0 "Enables MIPS architecture.")
 set_aom_detect_var(ARCH_PPC 0 "Enables PPC architecture.")
 set_aom_detect_var(ARCH_X86 0 "Enables X86 architecture.")
 set_aom_detect_var(ARCH_X86_64 0 "Enables X86_64 architecture.")
+set_aom_detect_var(ARCH_E2K 0 "Enables E2K architecture.")
 
 # ARM feature flags.
 set_aom_detect_var(HAVE_NEON 0 "Enables NEON intrinsics optimizations.")
diff --git a/build/cmake/aom_configure.cmake b/build/cmake/aom_configure.cmake
index 43d60ae..d83b277 100644
--- a/build/cmake/aom_configure.cmake
+++ b/build/cmake/aom_configure.cmake
@@ -73,6 +73,8 @@ if(NOT AOM_TARGET_CPU)
     set(AOM_TARGET_CPU "arm64")
   elseif(cpu_lowercase MATCHES "^ppc")
     set(AOM_TARGET_CPU "ppc")
+  elseif(cpu_lowercase STREQUAL "e2k")
+    set(AOM_TARGET_CPU "e2k")
   else()
     message(WARNING "The architecture ${CMAKE_SYSTEM_PROCESSOR} is not "
                     "supported, falling back to the generic target")
diff --git a/build/cmake/aom_optimization.cmake b/build/cmake/aom_optimization.cmake
index e4b29de..b0bef50 100644
--- a/build/cmake/aom_optimization.cmake
+++ b/build/cmake/aom_optimization.cmake
@@ -130,6 +130,9 @@ endfunction()
 # to ensure that all cmake generators can determine the linker language, and
 # that build tools don't complain that an object exposes no symbols.
 function(add_asm_library lib_name asm_sources)
+  if("${AOM_TARGET_CPU}" STREQUAL "e2k")
+    return()
+  endif()
   if("${${asm_sources}}" STREQUAL "")
     return()
   endif()
diff --git a/build/cmake/cpu.cmake b/build/cmake/cpu.cmake
index ef2d755..5e19edf 100644
--- a/build/cmake/cpu.cmake
+++ b/build/cmake/cpu.cmake
@@ -79,4 +79,20 @@ elseif("${AOM_TARGET_CPU}" MATCHES "^x86")
       set(AOM_RTCD_FLAGS ${AOM_RTCD_FLAGS} --disable-${flavor})
     endif()
   endforeach()
+elseif("${AOM_TARGET_CPU}" STREQUAL "e2k")
+  set(ARCH_E2K 1)
+  set(RTCD_ARCH_E2K "yes")
+
+  set(X86_FLAVORS "MMX;SSE;SSE2;SSE3;SSSE3;SSE4_1")
+  foreach(flavor ${X86_FLAVORS})
+    if(ENABLE_${flavor} AND NOT disable_remaining_flavors)
+      set(HAVE_${flavor} 1)
+      set(RTCD_HAVE_${flavor} "yes")
+    else()
+      set(disable_remaining_flavors 1)
+      set(HAVE_${flavor} 0)
+      string(TOLOWER ${flavor} flavor)
+      set(AOM_RTCD_FLAGS ${AOM_RTCD_FLAGS} --disable-${flavor})
+    endif()
+  endforeach()
 endif()
diff --git a/build/cmake/rtcd.pl b/build/cmake/rtcd.pl
index e9f75dd..0357a14 100755
--- a/build/cmake/rtcd.pl
+++ b/build/cmake/rtcd.pl
@@ -289,6 +289,36 @@ EOF
   common_bottom;
 }
 
+sub e2k() {
+  determine_indirection("c", @ALL_ARCHS);
+
+  # Assign the helper variable for each enabled extension
+  foreach my $opt (@ALL_ARCHS) {
+    my $opt_uc = uc $opt;
+    eval "\$have_${opt}=\"flags & HAS_${opt_uc}\"";
+  }
+
+  common_top;
+  print <<EOF;
+#include "aom_ports/e2k.h"
+#ifdef RTCD_C
+static void setup_rtcd_internal(void)
+{
+    int flags = e2k_simd_caps();
+
+    (void)flags;
+
+EOF
+
+  set_function_pointers("c", @ALL_ARCHS);
+
+  print <<EOF;
+}
+#endif
+EOF
+  common_bottom;
+}
+
 sub arm() {
   determine_indirection("c", @ALL_ARCHS);
 
@@ -420,6 +450,9 @@ if ($opts{arch} eq 'x86') {
   @REQUIRES = filter(qw/mmx sse sse2/);
   &require(@REQUIRES);
   x86;
+} elsif ($opts{arch} eq 'e2k') {
+  @ALL_ARCHS = filter(qw/mmx sse sse2 sse3 ssse3 sse4_1/);
+  e2k;
 } elsif ($opts{arch} eq 'mips32' || $opts{arch} eq 'mips64') {
   @ALL_ARCHS = filter("$opts{arch}");
   if (aom_config("HAVE_DSPR2") eq "yes") {
diff --git a/test/variance_test.cc b/test/variance_test.cc
index fa90305..9c67417 100644
--- a/test/variance_test.cc
+++ b/test/variance_test.cc
@@ -1996,6 +1996,7 @@ const VarianceParams kArrayVariance_sse2[] = {
 INSTANTIATE_TEST_SUITE_P(SSE2, AvxVarianceTest,
                          ::testing::ValuesIn(kArrayVariance_sse2));
 
+#if !ARCH_E2K
 const SubpelVarianceParams kArraySubpelVariance_sse2[] = {
   SubpelVarianceParams(7, 7, &aom_sub_pixel_variance128x128_sse2, 0),
   SubpelVarianceParams(7, 6, &aom_sub_pixel_variance128x64_sse2, 0),
@@ -2053,6 +2054,7 @@ const SubpelAvgVarianceParams kArraySubpelAvgVariance_sse2[] = {
 };
 INSTANTIATE_TEST_SUITE_P(SSE2, AvxSubpelAvgVarianceTest,
                          ::testing::ValuesIn(kArraySubpelAvgVariance_sse2));
+#endif
 
 #if CONFIG_AV1_HIGHBITDEPTH
 #if HAVE_SSE2
-- 
2.17.1

