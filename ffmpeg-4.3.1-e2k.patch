From e554c92765905a0703a180f0f613f9bdb2a55e22 Mon Sep 17 00:00:00 2001
From: Ilya Kurdyukov <jpegqs@gmail.com>
Date: Wed, 17 Mar 2021 16:15:20 +0700
Subject: [PATCH] ffmpeg-4.3.1 e2k support

---
 configure                            |   20 +-
 libavcodec/audiodsp.c                |    2 +
 libavcodec/audiodsp.h                |    1 +
 libavcodec/blockdsp.c                |    2 +
 libavcodec/blockdsp.h                |    1 +
 libavcodec/e2k/Makefile              |   28 +
 libavcodec/e2k/audiodsp.c            |   62 +
 libavcodec/e2k/blockdsp.c            |   72 +
 libavcodec/e2k/dctdsp.h              |   27 +
 libavcodec/e2k/fdctdsp.c             |  389 +++++
 libavcodec/e2k/fft.c                 | 1043 +++++++++++++
 libavcodec/e2k/fft.h                 |   29 +
 libavcodec/e2k/fft_init.c            |  152 ++
 libavcodec/e2k/fmtconvert.c          |   55 +
 libavcodec/e2k/h264chroma_init.c     |   63 +
 libavcodec/e2k/h264chroma_template.c |  139 ++
 libavcodec/e2k/h264dsp.c             |  708 +++++++++
 libavcodec/e2k/h264qpel.c            |  255 ++++
 libavcodec/e2k/h264qpel_template.c   |  354 +++++
 libavcodec/e2k/hevcdsp.c             |   94 ++
 libavcodec/e2k/hpeldsp.c             |  302 ++++
 libavcodec/e2k/hpeldsp.h             |   30 +
 libavcodec/e2k/idctdsp.c             |  237 +++
 libavcodec/e2k/lossless_audiodsp.c   |   75 +
 libavcodec/e2k/lossless_videodsp.c   |   59 +
 libavcodec/e2k/me_cmp.c              |  461 ++++++
 libavcodec/e2k/mpegaudiodsp.c        |  142 ++
 libavcodec/e2k/mpegvideo.c           |  100 ++
 libavcodec/e2k/mpegvideodsp.c        |   86 ++
 libavcodec/e2k/mpegvideoencdsp.c     |   75 +
 libavcodec/e2k/pixblockdsp.c         |   83 ++
 libavcodec/e2k/svq1enc.c             |   68 +
 libavcodec/e2k/vc1dsp.c              |  303 ++++
 libavcodec/e2k/videodsp.c            |   36 +
 libavcodec/e2k/vorbisdsp.c           |   62 +
 libavcodec/e2k/vp3dsp.c              |  169 +++
 libavcodec/e2k/vp8dsp.c              |  428 ++++++
 libavcodec/fdctdsp.c                 |    2 +
 libavcodec/fdctdsp.h                 |    2 +
 libavcodec/fft.h                     |    1 +
 libavcodec/fft_template.c            |    1 +
 libavcodec/fmtconvert.c              |    2 +
 libavcodec/fmtconvert.h              |    1 +
 libavcodec/h264chroma.c              |    2 +
 libavcodec/h264chroma.h              |    1 +
 libavcodec/h264dsp.c                 |    1 +
 libavcodec/h264dsp.h                 |    2 +
 libavcodec/h264qpel.c                |    2 +
 libavcodec/h264qpel.h                |    1 +
 libavcodec/hevcdsp.c                 |    2 +
 libavcodec/hevcdsp.h                 |    1 +
 libavcodec/hpeldsp.c                 |    2 +
 libavcodec/hpeldsp.h                 |    1 +
 libavcodec/idctdsp.c                 |    2 +
 libavcodec/idctdsp.h                 |    2 +
 libavcodec/lossless_audiodsp.c       |    2 +
 libavcodec/lossless_audiodsp.h       |    1 +
 libavcodec/lossless_videodsp.c       |    2 +
 libavcodec/lossless_videodsp.h       |    1 +
 libavcodec/me_cmp.c                  |    2 +
 libavcodec/me_cmp.h                  |    1 +
 libavcodec/mpegaudiodsp.c            |    1 +
 libavcodec/mpegaudiodsp.h            |    1 +
 libavcodec/mpegvideo.c               |    2 +
 libavcodec/mpegvideo.h               |    1 +
 libavcodec/mpegvideodsp.c            |    2 +
 libavcodec/mpegvideodsp.h            |    1 +
 libavcodec/mpegvideoencdsp.c         |    2 +
 libavcodec/mpegvideoencdsp.h         |    2 +
 libavcodec/pixblockdsp.c             |    2 +
 libavcodec/pixblockdsp.h             |    2 +
 libavcodec/svq1enc.c                 |    2 +
 libavcodec/svq1enc.h                 |    1 +
 libavcodec/tests/dct.c               |    2 +
 libavcodec/tests/e2k/dct.c           |   31 +
 libavcodec/vc1dsp.c                  |    2 +
 libavcodec/vc1dsp.h                  |    1 +
 libavcodec/videodsp.c                |    2 +
 libavcodec/videodsp.h                |    1 +
 libavcodec/vorbisdsp.c               |    2 +
 libavcodec/vorbisdsp.h               |    1 +
 libavcodec/vp3dsp.c                  |    2 +
 libavcodec/vp3dsp.h                  |    1 +
 libavcodec/vp8dsp.c                  |    2 +
 libavcodec/vp8dsp.h                  |    1 +
 libavutil/cpu.c                      |    8 +
 libavutil/cpu.h                      |    2 +
 libavutil/cpu_internal.h             |    2 +
 libavutil/e2k/Makefile               |    2 +
 libavutil/e2k/cpu.c                  |   41 +
 libavutil/e2k/cpu.h                  |   27 +
 libavutil/e2k/float_dsp.c            |  120 ++
 libavutil/e2k/intreadwrite.h         |   54 +
 libavutil/e2k/timer.h                |   35 +
 libavutil/e2k/util_e2k.h             |  201 +++
 libavutil/float_dsp.c                |    2 +
 libavutil/float_dsp.h                |    1 +
 libavutil/intreadwrite.h             |    2 +
 libavutil/tests/cpu.c                |    2 +
 libavutil/timer.h                    |    2 +
 libswscale/e2k/Makefile              |    3 +
 libswscale/e2k/swscale.c             | 2046 ++++++++++++++++++++++++++
 libswscale/e2k/yuv2rgb.c             |  248 ++++
 libswscale/e2k/yuv2rgb.h             |   52 +
 libswscale/e2k/yuv2yuv.c             |  146 ++
 libswscale/swscale.c                 |    2 +
 libswscale/swscale_internal.h        |    5 +
 libswscale/swscale_unscaled.c        |    2 +
 libswscale/utils.c                   |   13 +
 libswscale/yuv2rgb.c                 |    2 +
 tests/checkasm/checkasm.c            |    2 +
 tests/checkasm/huffyuvdsp.c          |    8 +-
 112 files changed, 9341 insertions(+), 5 deletions(-)
 create mode 100644 libavcodec/e2k/Makefile
 create mode 100644 libavcodec/e2k/audiodsp.c
 create mode 100644 libavcodec/e2k/blockdsp.c
 create mode 100644 libavcodec/e2k/dctdsp.h
 create mode 100644 libavcodec/e2k/fdctdsp.c
 create mode 100644 libavcodec/e2k/fft.c
 create mode 100644 libavcodec/e2k/fft.h
 create mode 100644 libavcodec/e2k/fft_init.c
 create mode 100644 libavcodec/e2k/fmtconvert.c
 create mode 100644 libavcodec/e2k/h264chroma_init.c
 create mode 100644 libavcodec/e2k/h264chroma_template.c
 create mode 100644 libavcodec/e2k/h264dsp.c
 create mode 100644 libavcodec/e2k/h264qpel.c
 create mode 100644 libavcodec/e2k/h264qpel_template.c
 create mode 100644 libavcodec/e2k/hevcdsp.c
 create mode 100644 libavcodec/e2k/hpeldsp.c
 create mode 100644 libavcodec/e2k/hpeldsp.h
 create mode 100644 libavcodec/e2k/idctdsp.c
 create mode 100644 libavcodec/e2k/lossless_audiodsp.c
 create mode 100644 libavcodec/e2k/lossless_videodsp.c
 create mode 100644 libavcodec/e2k/me_cmp.c
 create mode 100644 libavcodec/e2k/mpegaudiodsp.c
 create mode 100644 libavcodec/e2k/mpegvideo.c
 create mode 100644 libavcodec/e2k/mpegvideodsp.c
 create mode 100644 libavcodec/e2k/mpegvideoencdsp.c
 create mode 100644 libavcodec/e2k/pixblockdsp.c
 create mode 100644 libavcodec/e2k/svq1enc.c
 create mode 100644 libavcodec/e2k/vc1dsp.c
 create mode 100644 libavcodec/e2k/videodsp.c
 create mode 100644 libavcodec/e2k/vorbisdsp.c
 create mode 100644 libavcodec/e2k/vp3dsp.c
 create mode 100644 libavcodec/e2k/vp8dsp.c
 create mode 100644 libavcodec/tests/e2k/dct.c
 create mode 100644 libavutil/e2k/Makefile
 create mode 100644 libavutil/e2k/cpu.c
 create mode 100644 libavutil/e2k/cpu.h
 create mode 100644 libavutil/e2k/float_dsp.c
 create mode 100644 libavutil/e2k/intreadwrite.h
 create mode 100644 libavutil/e2k/timer.h
 create mode 100644 libavutil/e2k/util_e2k.h
 create mode 100644 libswscale/e2k/Makefile
 create mode 100644 libswscale/e2k/swscale.c
 create mode 100644 libswscale/e2k/yuv2rgb.c
 create mode 100644 libswscale/e2k/yuv2rgb.h
 create mode 100644 libswscale/e2k/yuv2yuv.c

diff --git a/configure b/configure
index 19c1865..3c2a9ab 100755
--- a/configure
+++ b/configure
@@ -1989,6 +1989,7 @@ ARCH_LIST="
     parisc
     ppc
     ppc64
+    e2k
     s390
     sh4
     sparc
@@ -2060,6 +2061,10 @@ ARCH_EXT_LIST_PPC="
     vsx
 "
 
+ARCH_EXT_LIST_E2K="
+    e2k
+"
+
 ARCH_EXT_LIST_X86="
     $ARCH_EXT_LIST_X86_SIMD
     cpunop
@@ -2069,6 +2074,7 @@ ARCH_EXT_LIST_X86="
 ARCH_EXT_LIST="
     $ARCH_EXT_LIST_ARM
     $ARCH_EXT_LIST_PPC
+    $ARCH_EXT_LIST_E2K
     $ARCH_EXT_LIST_X86
     $ARCH_EXT_LIST_MIPS
     $ARCH_EXT_LIST_LOONGSON
@@ -2594,10 +2600,10 @@ for ext in $(filter_out mmx $ARCH_EXT_LIST_X86_SIMD); do
 done
 
 aligned_stack_if_any="aarch64 ppc x86"
-fast_64bit_if_any="aarch64 alpha ia64 mips64 parisc64 ppc64 sparc64 x86_64"
-fast_clz_if_any="aarch64 alpha avr32 mips ppc x86"
+fast_64bit_if_any="aarch64 alpha ia64 mips64 parisc64 ppc64 e2k sparc64 x86_64"
+fast_clz_if_any="aarch64 alpha avr32 mips ppc e2k x86"
 fast_unaligned_if_any="aarch64 ppc x86"
-simd_align_16_if_any="altivec neon sse"
+simd_align_16_if_any="altivec e2k neon sse"
 simd_align_32_if_any="avx"
 simd_align_64_if_any="avx512"
 
@@ -4889,6 +4895,9 @@ case "$arch" in
     "Power Macintosh"|ppc*|powerpc*)
         arch="ppc"
     ;;
+    e2k|elbrus)
+        arch="e2k"
+    ;;
     s390|s390x)
         arch="s390"
     ;;
@@ -5177,6 +5186,11 @@ elif enabled ppc; then
         ;;
     esac
 
+elif enabled e2k; then
+
+    cpu="e2k"
+    cpuflags="-msse4.1 -mno-avx"
+
 elif enabled sparc; then
 
     case $cpu in
diff --git a/libavcodec/audiodsp.c b/libavcodec/audiodsp.c
index efcb0a8..36b8528 100644
--- a/libavcodec/audiodsp.c
+++ b/libavcodec/audiodsp.c
@@ -113,6 +113,8 @@ av_cold void ff_audiodsp_init(AudioDSPContext *c)
         ff_audiodsp_init_arm(c);
     if (ARCH_PPC)
         ff_audiodsp_init_ppc(c);
+    if (ARCH_E2K)
+        ff_audiodsp_init_e2k(c);
     if (ARCH_X86)
         ff_audiodsp_init_x86(c);
 }
diff --git a/libavcodec/audiodsp.h b/libavcodec/audiodsp.h
index aa6fa78..9c05e28 100644
--- a/libavcodec/audiodsp.h
+++ b/libavcodec/audiodsp.h
@@ -55,6 +55,7 @@ typedef struct AudioDSPContext {
 void ff_audiodsp_init(AudioDSPContext *c);
 void ff_audiodsp_init_arm(AudioDSPContext *c);
 void ff_audiodsp_init_ppc(AudioDSPContext *c);
+void ff_audiodsp_init_e2k(AudioDSPContext *c);
 void ff_audiodsp_init_x86(AudioDSPContext *c);
 
 #endif /* AVCODEC_AUDIODSP_H */
diff --git a/libavcodec/blockdsp.c b/libavcodec/blockdsp.c
index c7efe7e..704c723 100644
--- a/libavcodec/blockdsp.c
+++ b/libavcodec/blockdsp.c
@@ -71,6 +71,8 @@ av_cold void ff_blockdsp_init(BlockDSPContext *c, AVCodecContext *avctx)
         ff_blockdsp_init_arm(c);
     if (ARCH_PPC)
         ff_blockdsp_init_ppc(c);
+    if (ARCH_E2K)
+        ff_blockdsp_init_e2k(c);
     if (ARCH_X86)
         ff_blockdsp_init_x86(c, avctx);
     if (ARCH_MIPS)
diff --git a/libavcodec/blockdsp.h b/libavcodec/blockdsp.h
index 26fc2ea..8eccb47 100644
--- a/libavcodec/blockdsp.h
+++ b/libavcodec/blockdsp.h
@@ -44,6 +44,7 @@ void ff_blockdsp_init(BlockDSPContext *c, AVCodecContext *avctx);
 void ff_blockdsp_init_alpha(BlockDSPContext *c);
 void ff_blockdsp_init_arm(BlockDSPContext *c);
 void ff_blockdsp_init_ppc(BlockDSPContext *c);
+void ff_blockdsp_init_e2k(BlockDSPContext *c);
 void ff_blockdsp_init_x86(BlockDSPContext *c, AVCodecContext *avctx);
 void ff_blockdsp_init_mips(BlockDSPContext *c);
 
diff --git a/libavcodec/e2k/Makefile b/libavcodec/e2k/Makefile
new file mode 100644
index 0000000..5242f83
--- /dev/null
+++ b/libavcodec/e2k/Makefile
@@ -0,0 +1,28 @@
+# subsystems
+OBJS-$(CONFIG_AUDIODSP)                += e2k/audiodsp.o
+OBJS-$(CONFIG_BLOCKDSP)                += e2k/blockdsp.o
+OBJS-$(CONFIG_FFT)                     += e2k/fft_init.o e2k/fft.o
+OBJS-$(CONFIG_FDCTDSP)                 += e2k/fdctdsp.o
+OBJS-$(CONFIG_FMTCONVERT)              += e2k/fmtconvert.o
+OBJS-$(CONFIG_H264CHROMA)              += e2k/h264chroma_init.o
+OBJS-$(CONFIG_H264DSP)                 += e2k/h264dsp.o e2k/hpeldsp.o
+OBJS-$(CONFIG_H264QPEL)                += e2k/h264qpel.o
+OBJS-$(CONFIG_HPELDSP)                 += e2k/hpeldsp.o
+OBJS-$(CONFIG_IDCTDSP)                 += e2k/idctdsp.o
+OBJS-$(CONFIG_LLVIDDSP)                += e2k/lossless_videodsp.o
+OBJS-$(CONFIG_ME_CMP)                  += e2k/me_cmp.o
+OBJS-$(CONFIG_MPEGAUDIODSP)            += e2k/mpegaudiodsp.o
+OBJS-$(CONFIG_MPEGVIDEO)               += e2k/mpegvideo.o e2k/mpegvideodsp.o
+OBJS-$(CONFIG_MPEGVIDEOENC)            += e2k/mpegvideoencdsp.o
+OBJS-$(CONFIG_PIXBLOCKDSP)             += e2k/pixblockdsp.o
+OBJS-$(CONFIG_VC1DSP)                  += e2k/vc1dsp.o
+OBJS-$(CONFIG_VIDEODSP)                += e2k/videodsp.o
+OBJS-$(CONFIG_VP3DSP)                  += e2k/vp3dsp.o
+OBJS-$(CONFIG_VP8DSP)                  += e2k/vp8dsp.o
+
+# decoders/encoders
+OBJS-$(CONFIG_HEVC_DECODER)            += e2k/hevcdsp.o
+OBJS-$(CONFIG_LLAUDDSP)                += e2k/lossless_audiodsp.o
+OBJS-$(CONFIG_SVQ1_ENCODER)            += e2k/svq1enc.o
+OBJS-$(CONFIG_VORBIS_DECODER)          += e2k/vorbisdsp.o
+OBJS-$(CONFIG_VP7_DECODER)             += e2k/vp8dsp.o
diff --git a/libavcodec/e2k/audiodsp.c b/libavcodec/e2k/audiodsp.c
new file mode 100644
index 0000000..c2e4433
--- /dev/null
+++ b/libavcodec/e2k/audiodsp.c
@@ -0,0 +1,62 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2007 Luca Barbato <lu_zero@gentoo.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * miscellaneous audio operations
+ */
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/audiodsp.h"
+
+static int32_t scalarproduct_int16_e2k(const int16_t *v1, const int16_t *v2, int order)
+{
+    int i;
+    vec_s16 vec1, vec2;
+    vec_s32 res = _mm_setzero_si128(), tmp;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < order; i += 8) {
+        vec1 = VEC_LD(v1);
+        vec2 = VEC_LD(v2);
+        tmp = _mm_madd_epi16(vec1, vec2);
+        res = _mm_add_epi32(res, tmp);
+        v1 += 8;
+        v2 += 8;
+    }
+
+    res = _mm_hadd_epi32(res, res);
+    return _mm_extract_epi32(res, 0) + _mm_extract_epi32(res, 1);
+}
+
+av_cold void ff_audiodsp_init_e2k(AudioDSPContext *c)
+{
+    if (!E2K_BASE(av_get_cpu_flags()))
+        return;
+
+    c->scalarproduct_int16 = scalarproduct_int16_e2k;
+}
diff --git a/libavcodec/e2k/blockdsp.c b/libavcodec/e2k/blockdsp.c
new file mode 100644
index 0000000..f85dce1
--- /dev/null
+++ b/libavcodec/e2k/blockdsp.c
@@ -0,0 +1,72 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2002 Brian Foley
+ * Copyright (c) 2002 Dieter Shirley
+ * Copyright (c) 2003-2004 Romain Dolbeau <romain@dolbeau.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include <string.h>
+
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include "libavutil/mem.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/blockdsp.h"
+
+static void clear_block_e2k(int16_t *block)
+{
+    LOAD_ZERO;
+    VEC_ST(block, zerov);
+    VEC_ST(block + 8, zerov);
+    VEC_ST(block + 8 * 2, zerov);
+    VEC_ST(block + 8 * 3, zerov);
+    VEC_ST(block + 8 * 4, zerov);
+    VEC_ST(block + 8 * 5, zerov);
+    VEC_ST(block + 8 * 6, zerov);
+    VEC_ST(block + 8 * 7, zerov);
+}
+
+static void clear_blocks_e2k(int16_t *blocks)
+{
+    int i;
+    LOAD_ZERO;
+    for (i = 0; i < 6; i++, blocks += 64) {
+        VEC_ST(blocks, zerov);
+        VEC_ST(blocks + 8, zerov);
+        VEC_ST(blocks + 8 * 2, zerov);
+        VEC_ST(blocks + 8 * 3, zerov);
+        VEC_ST(blocks + 8 * 4, zerov);
+        VEC_ST(blocks + 8 * 5, zerov);
+        VEC_ST(blocks + 8 * 6, zerov);
+        VEC_ST(blocks + 8 * 7, zerov);
+    }
+}
+
+av_cold void ff_blockdsp_init_e2k(BlockDSPContext *c)
+{
+    if (!E2K_BASE(av_get_cpu_flags()))
+        return;
+
+    c->clear_block = clear_block_e2k;
+    c->clear_blocks = clear_blocks_e2k;
+}
diff --git a/libavcodec/e2k/dctdsp.h b/libavcodec/e2k/dctdsp.h
new file mode 100644
index 0000000..1281dc7
--- /dev/null
+++ b/libavcodec/e2k/dctdsp.h
@@ -0,0 +1,27 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_E2K_DCTDSP_H
+#define AVCODEC_E2K_DCTDSP_H
+
+#include <stdint.h>
+
+void ff_fdct_e2k(int16_t *block);
+void ff_idct_e2k(int16_t *block);
+
+#endif /* AVCODEC_E2K_DCTDSP_H */
diff --git a/libavcodec/e2k/fdctdsp.c b/libavcodec/e2k/fdctdsp.c
new file mode 100644
index 0000000..568a67f
--- /dev/null
+++ b/libavcodec/e2k/fdctdsp.c
@@ -0,0 +1,389 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (C) 2003  James Klicman <james@klicman.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/fdctdsp.h"
+
+#include "dctdsp.h"
+
+#define C1     0.98078528040323044912618224 /* cos(1 * PI / 16) */
+#define C2     0.92387953251128675612818319 /* cos(2 * PI / 16) */
+#define C3     0.83146961230254523707878838 /* cos(3 * PI / 16) */
+#define C4     0.70710678118654752440084436 /* cos(4 * PI / 16) */
+#define C5     0.55557023301960222474283081 /* cos(5 * PI / 16) */
+#define C6     0.38268343236508977172845998 /* cos(6 * PI / 16) */
+#define C7     0.19509032201612826784828487 /* cos(7 * PI / 16) */
+
+#define W0 -(2 * C2)
+#define W1  (2 * C6)
+#define W2 (M_SQRT2 * C6)
+#define W3 (M_SQRT2 * C3)
+#define W4 (M_SQRT2 * (-C1 + C3 + C5 - C7))
+#define W5 (M_SQRT2 *  (C1 + C3 - C5 + C7))
+#define W6 (M_SQRT2 *  (C1 + C3 + C5 - C7))
+#define W7 (M_SQRT2 *  (C1 + C3 - C5 - C7))
+#define W8 (M_SQRT2 *  (C7 - C3))
+#define W9 (M_SQRT2 * (-C1 - C3))
+#define WA (M_SQRT2 * (-C3 - C5))
+#define WB (M_SQRT2 *  (C5 - C3))
+
+#define LD_W0 _mm_set1_ps(W0)
+#define LD_W1 _mm_set1_ps(W1)
+#define LD_W2 _mm_set1_ps(W2)
+#define LD_W3 _mm_set1_ps(W3)
+#define LD_W4 _mm_set1_ps(W4)
+#define LD_W5 _mm_set1_ps(W5)
+#define LD_W6 _mm_set1_ps(W6)
+#define LD_W7 _mm_set1_ps(W7)
+#define LD_W8 _mm_set1_ps(W8)
+#define LD_W9 _mm_set1_ps(W9)
+#define LD_WA _mm_set1_ps(WA)
+#define LD_WB _mm_set1_ps(WB)
+
+#define _mm_madd_ps(a, b, c) _mm_add_ps(_mm_mul_ps(a, b), c)
+
+#define FDCTROW(b0, b1, b2, b3, b4, b5, b6, b7) /* {{{ */           \
+    x0 = _mm_add_ps(b0, b7);            /* x0 = b0 + b7; */         \
+    x7 = _mm_sub_ps(b0, b7);            /* x7 = b0 - b7; */         \
+    x1 = _mm_add_ps(b1, b6);            /* x1 = b1 + b6; */         \
+    x6 = _mm_sub_ps(b1, b6);            /* x6 = b1 - b6; */         \
+    x2 = _mm_add_ps(b2, b5);            /* x2 = b2 + b5; */         \
+    x5 = _mm_sub_ps(b2, b5);            /* x5 = b2 - b5; */         \
+    x3 = _mm_add_ps(b3, b4);            /* x3 = b3 + b4; */         \
+    x4 = _mm_sub_ps(b3, b4);            /* x4 = b3 - b4; */         \
+                                                                    \
+    b7 = _mm_add_ps(x0, x3);            /* b7 = x0 + x3; */         \
+    b1 = _mm_add_ps(x1, x2);            /* b1 = x1 + x2; */         \
+    b0 = _mm_add_ps(b7, b1);            /* b0 = b7 + b1; */         \
+    b4 = _mm_sub_ps(b7, b1);            /* b4 = b7 - b1; */         \
+                                                                    \
+    b2   = _mm_sub_ps(x0, x3);          /* b2 = x0 - x3; */         \
+    b6   = _mm_sub_ps(x1, x2);          /* b6 = x1 - x2; */         \
+    b5   = _mm_add_ps(b6, b2);          /* b5 = b6 + b2; */         \
+    cnst = LD_W2;                                                   \
+    b5   = _mm_mul_ps(cnst, b5);        /* b5 = b5 * W2; */         \
+    cnst = LD_W1;                                                   \
+    b2   = _mm_madd_ps(cnst, b2, b5);   /* b2 = b5 + b2 * W1; */    \
+    cnst = LD_W0;                                                   \
+    b6   = _mm_madd_ps(cnst, b6, b5);   /* b6 = b5 + b6 * W0; */    \
+                                                                    \
+    x0   = _mm_add_ps(x4, x7);          /* x0 = x4 + x7; */         \
+    x1   = _mm_add_ps(x5, x6);          /* x1 = x5 + x6; */         \
+    x2   = _mm_add_ps(x4, x6);          /* x2 = x4 + x6; */         \
+    x3   = _mm_add_ps(x5, x7);          /* x3 = x5 + x7; */         \
+    x8   = _mm_add_ps(x2, x3);          /* x8 = x2 + x3; */         \
+    cnst = LD_W3;                                                   \
+    x8   = _mm_mul_ps(cnst, x8);        /* x8 = x8 * W3; */         \
+                                                                    \
+    cnst = LD_W8;                                                   \
+    x0   = _mm_mul_ps(cnst, x0);        /* x0 *= W8; */             \
+    cnst = LD_W9;                                                   \
+    x1   = _mm_mul_ps(cnst, x1);        /* x1 *= W9; */             \
+    cnst = LD_WA;                                                   \
+    x2   = _mm_madd_ps(cnst, x2, x8);   /* x2 = x2 * WA + x8; */    \
+    cnst = LD_WB;                                                   \
+    x3   = _mm_madd_ps(cnst, x3, x8);   /* x3 = x3 * WB + x8; */    \
+                                                                    \
+    cnst = LD_W4;                                                   \
+    b7   = _mm_madd_ps(cnst, x4, x0);   /* b7 = x4 * W4 + x0; */    \
+    cnst = LD_W5;                                                   \
+    b5   = _mm_madd_ps(cnst, x5, x1);   /* b5 = x5 * W5 + x1; */    \
+    cnst = LD_W6;                                                   \
+    b3   = _mm_madd_ps(cnst, x6, x1);   /* b3 = x6 * W6 + x1; */    \
+    cnst = LD_W7;                                                   \
+    b1   = _mm_madd_ps(cnst, x7, x0);   /* b1 = x7 * W7 + x0; */    \
+                                                                    \
+    b7 = _mm_add_ps(b7, x2);            /* b7 = b7 + x2; */         \
+    b5 = _mm_add_ps(b5, x3);            /* b5 = b5 + x3; */         \
+    b3 = _mm_add_ps(b3, x2);            /* b3 = b3 + x2; */         \
+    b1 = _mm_add_ps(b1, x3)             /* b1 = b1 + x3; */         \
+    /* }}} */
+
+#define FDCTCOL(b0, b1, b2, b3, b4, b5, b6, b7) /* {{{ */           \
+    x0 = _mm_add_ps(b0, b7);            /* x0 = b0 + b7; */         \
+    x7 = _mm_sub_ps(b0, b7);            /* x7 = b0 - b7; */         \
+    x1 = _mm_add_ps(b1, b6);            /* x1 = b1 + b6; */         \
+    x6 = _mm_sub_ps(b1, b6);            /* x6 = b1 - b6; */         \
+    x2 = _mm_add_ps(b2, b5);            /* x2 = b2 + b5; */         \
+    x5 = _mm_sub_ps(b2, b5);            /* x5 = b2 - b5; */         \
+    x3 = _mm_add_ps(b3, b4);            /* x3 = b3 + b4; */         \
+    x4 = _mm_sub_ps(b3, b4);            /* x4 = b3 - b4; */         \
+                                                                    \
+    b7 = _mm_add_ps(x0, x3);            /* b7 = x0 + x3; */         \
+    b1 = _mm_add_ps(x1, x2);            /* b1 = x1 + x2; */         \
+    b0 = _mm_add_ps(b7, b1);            /* b0 = b7 + b1; */         \
+    b4 = _mm_sub_ps(b7, b1);            /* b4 = b7 - b1; */         \
+                                                                    \
+    b2   = _mm_sub_ps(x0, x3);          /* b2 = x0 - x3; */         \
+    b6   = _mm_sub_ps(x1, x2);          /* b6 = x1 - x2; */         \
+    b5   = _mm_add_ps(b6, b2);          /* b5 = b6 + b2; */         \
+    cnst = LD_W2;                                                   \
+    b5   = _mm_mul_ps(cnst, b5);        /* b5 = b5 * W2; */         \
+    cnst = LD_W1;                                                   \
+    b2   = _mm_madd_ps(cnst, b2, b5);   /* b2 = b5 + b2 * W1; */    \
+    cnst = LD_W0;                                                   \
+    b6   = _mm_madd_ps(cnst, b6, b5);   /* b6 = b5 + b6 * W0; */    \
+                                                                    \
+    x0   = _mm_add_ps(x4, x7);          /* x0 = x4 + x7; */         \
+    x1   = _mm_add_ps(x5, x6);          /* x1 = x5 + x6; */         \
+    x2   = _mm_add_ps(x4, x6);          /* x2 = x4 + x6; */         \
+    x3   = _mm_add_ps(x5, x7);          /* x3 = x5 + x7; */         \
+    x8   = _mm_add_ps(x2, x3);          /* x8 = x2 + x3; */         \
+    cnst = LD_W3;                                                   \
+    x8   = _mm_mul_ps(cnst, x8);        /* x8 = x8 * W3; */         \
+                                                                    \
+    cnst = LD_W8;                                                   \
+    x0   = _mm_mul_ps(cnst, x0);        /* x0 *= W8; */             \
+    cnst = LD_W9;                                                   \
+    x1   = _mm_mul_ps(cnst, x1);        /* x1 *= W9; */             \
+    cnst = LD_WA;                                                   \
+    x2   = _mm_madd_ps(cnst, x2, x8);   /* x2 = x2 * WA + x8; */    \
+    cnst = LD_WB;                                                   \
+    x3   = _mm_madd_ps(cnst, x3, x8);   /* x3 = x3 * WB + x8; */    \
+                                                                    \
+    cnst = LD_W4;                                                   \
+    b7   = _mm_madd_ps(cnst, x4, x0);   /* b7 = x4 * W4 + x0; */    \
+    cnst = LD_W5;                                                   \
+    b5   = _mm_madd_ps(cnst, x5, x1);   /* b5 = x5 * W5 + x1; */    \
+    cnst = LD_W6;                                                   \
+    b3   = _mm_madd_ps(cnst, x6, x1);   /* b3 = x6 * W6 + x1; */    \
+    cnst = LD_W7;                                                   \
+    b1   = _mm_madd_ps(cnst, x7, x0);   /* b1 = x7 * W7 + x0; */    \
+                                                                    \
+    b7 = _mm_add_ps(b7, x2);            /* b7 += x2; */             \
+    b5 = _mm_add_ps(b5, x3);            /* b5 += x3; */             \
+    b3 = _mm_add_ps(b3, x2);            /* b3 += x2; */             \
+    b1 = _mm_add_ps(b1, x3)             /* b1 += x3; */             \
+    /* }}} */
+
+/* two dimensional discrete cosine transform */
+void ff_fdct_e2k(int16_t *block)
+{
+    vec_f b00, b10, b20, b30, b40, b50, b60, b70;
+    vec_f b01, b11, b21, b31, b41, b51, b61, b71;
+    vec_f cnst;
+    vec_f x0, x1, x2, x3, x4, x5, x6, x7, x8;
+    vec_s16 a0, a1, a2, a3, a4, a5, a6, a7;
+    vec_s16 z0, z1, z2, z3, z4, z5, z6, z7;
+
+    a0 = VEC_LD(block + 8 * 0);
+    a4 = VEC_LD(block + 8 * 4);
+    a1 = VEC_LD(block + 8 * 1);
+    a5 = VEC_LD(block + 8 * 5);
+    a2 = VEC_LD(block + 8 * 2);
+    a6 = VEC_LD(block + 8 * 6);
+    a3 = VEC_LD(block + 8 * 3);
+    a7 = VEC_LD(block + 8 * 7);
+
+    TRANSPOSE8(a0, a1, a2, a3, a4, a5, a6, a7);
+
+    /* Some of the initial calculations can be done as vector short
+     * before conversion to vec_f.  The following code section
+     * takes advantage of this. */
+
+    /* fdct rows {{{ */
+    z0 = _mm_add_epi16(a0, a7);
+    z7 = _mm_sub_epi16(a0, a7);
+    z1 = _mm_add_epi16(a1, a6);
+    z6 = _mm_sub_epi16(a1, a6);
+    z2 = _mm_add_epi16(a2, a5);
+    z5 = _mm_sub_epi16(a2, a5);
+    z3 = _mm_add_epi16(a3, a4);
+    z4 = _mm_sub_epi16(a3, a4);
+
+#define CTF0(n) \
+    b##n##0 = _mm_cvtepi32_ps(_mm_srai_epi32(_mm_unpacklo_epi16(a##n, a##n), 16));\
+    b##n##1 = _mm_cvtepi32_ps(_mm_srai_epi32(_mm_unpackhi_epi16(a##n, a##n), 16));
+
+    a7 = _mm_add_epi16(z0, z3);
+    a1 = _mm_add_epi16(z1, z2);
+    a0 = _mm_add_epi16(a7, a1);
+    a4 = _mm_sub_epi16(a7, a1);
+    CTF0(0);
+    CTF0(4);
+
+    a2 = _mm_sub_epi16(z0, z3);
+    a6 = _mm_sub_epi16(z1, z2);
+    CTF0(2);
+    CTF0(6);
+
+#undef CTF0
+
+    x0 = _mm_add_ps(b60, b20);
+    x1 = _mm_add_ps(b61, b21);
+
+    cnst = LD_W2;
+    x0   = _mm_mul_ps(cnst, x0);
+    x1   = _mm_mul_ps(cnst, x1);
+    cnst = LD_W1;
+    b20  = _mm_madd_ps(cnst, b20, x0);
+    b21  = _mm_madd_ps(cnst, b21, x1);
+    cnst = LD_W0;
+    b60  = _mm_madd_ps(cnst, b60, x0);
+    b61  = _mm_madd_ps(cnst, b61, x1);
+
+#define CTFX(x, b) \
+    b##0 = _mm_cvtepi32_ps(_mm_srai_epi32(_mm_unpacklo_epi16(x, x), 16)); \
+    b##1 = _mm_cvtepi32_ps(_mm_srai_epi32(_mm_unpackhi_epi16(x, x), 16));
+
+    CTFX(z4, b7);
+    CTFX(z5, b5);
+    CTFX(z6, b3);
+    CTFX(z7, b1);
+
+#undef CTFX
+
+    x0   = _mm_add_ps(b70, b10);
+    x1   = _mm_add_ps(b50, b30);
+    x2   = _mm_add_ps(b70, b30);
+    x3   = _mm_add_ps(b50, b10);
+    x8   = _mm_add_ps(x2, x3);
+    cnst = LD_W3;
+    x8   = _mm_mul_ps(cnst, x8);
+
+    cnst = LD_W8;
+    x0   = _mm_mul_ps(cnst, x0);
+    cnst = LD_W9;
+    x1   = _mm_mul_ps(cnst, x1);
+    cnst = LD_WA;
+    x2   = _mm_madd_ps(cnst, x2, x8);
+    cnst = LD_WB;
+    x3   = _mm_madd_ps(cnst, x3, x8);
+
+    cnst = LD_W4;
+    b70  = _mm_madd_ps(cnst, b70, x0);
+    cnst = LD_W5;
+    b50  = _mm_madd_ps(cnst, b50, x1);
+    cnst = LD_W6;
+    b30  = _mm_madd_ps(cnst, b30, x1);
+    cnst = LD_W7;
+    b10  = _mm_madd_ps(cnst, b10, x0);
+
+    b70 = _mm_add_ps(b70, x2);
+    b50 = _mm_add_ps(b50, x3);
+    b30 = _mm_add_ps(b30, x2);
+    b10 = _mm_add_ps(b10, x3);
+
+    x0   = _mm_add_ps(b71, b11);
+    x1   = _mm_add_ps(b51, b31);
+    x2   = _mm_add_ps(b71, b31);
+    x3   = _mm_add_ps(b51, b11);
+    x8   = _mm_add_ps(x2, x3);
+    cnst = LD_W3;
+    x8   = _mm_mul_ps(cnst, x8);
+
+    cnst = LD_W8;
+    x0   = _mm_mul_ps(cnst, x0);
+    cnst = LD_W9;
+    x1   = _mm_mul_ps(cnst, x1);
+    cnst = LD_WA;
+    x2   = _mm_madd_ps(cnst, x2, x8);
+    cnst = LD_WB;
+    x3   = _mm_madd_ps(cnst, x3, x8);
+
+    cnst = LD_W4;
+    b71  = _mm_madd_ps(cnst, b71, x0);
+    cnst = LD_W5;
+    b51  = _mm_madd_ps(cnst, b51, x1);
+    cnst = LD_W6;
+    b31  = _mm_madd_ps(cnst, b31, x1);
+    cnst = LD_W7;
+    b11  = _mm_madd_ps(cnst, b11, x0);
+
+    b71 = _mm_add_ps(b71, x2);
+    b51 = _mm_add_ps(b51, x3);
+    b31 = _mm_add_ps(b31, x2);
+    b11 = _mm_add_ps(b11, x3);
+    /* }}} */
+
+    /* 8x8 matrix transpose (vec_f[8][2]) {{{ */
+    x0 = _mm_unpacklo_ps(b00, b10);
+    x1 = _mm_unpackhi_ps(b00, b10);
+    x2 = _mm_unpacklo_ps(b20, b30);
+    x3 = _mm_unpackhi_ps(b20, b30);
+    b00 = _mm_unpacklo_ps2(x0, x2);
+    b10 = _mm_unpackhi_ps2(x0, x2);
+    b20 = _mm_unpacklo_ps2(x1, x3);
+    b30 = _mm_unpackhi_ps2(x1, x3);
+
+    x4 = _mm_unpacklo_ps(b41, b51);
+    x5 = _mm_unpackhi_ps(b41, b51);
+    x6 = _mm_unpacklo_ps(b61, b71);
+    x7 = _mm_unpackhi_ps(b61, b71);
+    b41 = _mm_unpacklo_ps2(x4, x6);
+    b51 = _mm_unpackhi_ps2(x4, x6);
+    b61 = _mm_unpacklo_ps2(x5, x7);
+    b71 = _mm_unpackhi_ps2(x5, x7);
+
+    x0 = _mm_unpacklo_ps(b01, b11);
+    x1 = _mm_unpackhi_ps(b01, b11);
+    x2 = _mm_unpacklo_ps(b21, b31);
+    x3 = _mm_unpackhi_ps(b21, b31);
+    x4 = _mm_unpacklo_ps(b40, b50);
+    x5 = _mm_unpackhi_ps(b40, b50);
+    x6 = _mm_unpacklo_ps(b60, b70);
+    x7 = _mm_unpackhi_ps(b60, b70);
+    b40 = _mm_unpacklo_ps2(x0, x2);
+    b50 = _mm_unpackhi_ps2(x0, x2);
+    b60 = _mm_unpacklo_ps2(x1, x3);
+    b70 = _mm_unpackhi_ps2(x1, x3);
+    b01 = _mm_unpacklo_ps2(x4, x6);
+    b11 = _mm_unpackhi_ps2(x4, x6);
+    b21 = _mm_unpacklo_ps2(x5, x7);
+    b31 = _mm_unpackhi_ps2(x5, x7);
+    /* }}} */
+
+    FDCTCOL(b00, b10, b20, b30, b40, b50, b60, b70);
+    FDCTCOL(b01, b11, b21, b31, b41, b51, b61, b71);
+
+    /* round, convert back to short */
+#define CTS(n) \
+    a##n = _mm_packs_epi32(_mm_cvtps_epi32(b##n##0), _mm_cvtps_epi32(b##n##1)); \
+    VEC_ST(block + 8 * n, a##n)
+
+    CTS(0); CTS(1); CTS(2); CTS(3);
+    CTS(4); CTS(5); CTS(6); CTS(7);
+
+#undef CTS
+}
+
+av_cold void ff_fdctdsp_init_e2k(FDCTDSPContext *c, AVCodecContext *avctx,
+                                 unsigned high_bit_depth)
+{
+    if (!E2K_BASE(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+    // libavcodec/tests/dct
+
+    if (!high_bit_depth) {
+        if (avctx->dct_algo == FF_DCT_AUTO ||
+            avctx->dct_algo == FF_DCT_ALTIVEC) {
+            c->fdct = ff_fdct_e2k;
+        }
+    }
+}
diff --git a/libavcodec/e2k/fft.c b/libavcodec/e2k/fft.c
new file mode 100644
index 0000000..5b58202
--- /dev/null
+++ b/libavcodec/e2k/fft.c
@@ -0,0 +1,1043 @@
+/*
+ * FFT transform
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2014 Rong Yan
+ * Copyright (c) 2009 Loren Merritt
+ *
+ * This algorithm (though not any of the implementation details) is
+ * based on libdjbfft by D. J. Bernstein.
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+
+#include "config.h"
+#include "libavutil/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+#include "libavcodec/fft.h"
+#include "libavcodec/fft-internal.h"
+#include "fft.h"
+
+#define _mm_madd_ps(a, b, c) _mm_add_ps(_mm_mul_ps(a, b), c)
+#define _mm_msub_ps(a, b, c) _mm_sub_ps(_mm_mul_ps(a, b), c)
+#define _mm_nmsub_ps(a, b, c) _mm_sub_ps(c, _mm_mul_ps(a, b))
+
+static av_always_inline
+void pass_e2k_interleave(FFTComplex *z, const FFTSample *wre, unsigned int n)
+{
+    int i1 = n * 4, i2 = n * 8, i3 = n * 12;
+    FFTSample *out = (FFTSample*)z;
+    const FFTSample *wim = wre + n * 2;
+    vec_f vz0, vzo1, vzo2, vzo3;
+    vec_f x0, x1, x2, x3;
+    vec_f x4, x5, x6, x7;
+    vec_f x8, x9, x10, x11;
+    vec_f x12, x13, x14, x15;
+    vec_f x16, x17, x18, x19;
+    vec_f x20, x21, x22, x23;
+    vec_f vz0plus1, vzo1plus1, vzo2plus1, vzo3plus1;
+    vec_f y0, y1, y2, y3;
+    vec_f y4, y5, y8, y9;
+    vec_f y10, y13, y14, y15;
+    vec_f y16, y17, y18, y19;
+    vec_f y20, y21, y22, y23;
+    vec_f wr1, wi1, wr0, wi0;
+    vec_f wr2, wi2, wr3, wi3;
+    vec_f xmulwi0, xmulwi1, ymulwi2, ymulwi3;
+
+    n = n - 2;
+    vzo2 = _mm_loadu_ps(out + i2);  // zo2.r  zo2.i  z(o2+1).r  z(o2+1).i
+    vzo2plus1 = _mm_loadu_ps(out + i2 + 4);
+    vzo3 = _mm_loadu_ps(out + i3);  // zo3.r  zo3.i  z(o3+1).r  z(o3+1).i
+    vzo3plus1 = _mm_loadu_ps(out + i3 + 4);
+    vz0 = _mm_loadu_ps(out);    // z0.r  z0.i  z1.r  z1.i
+    vz0plus1 = _mm_loadu_ps(out + 4);
+    vzo1 = _mm_loadu_ps(out + i1);  // zo1.r  zo1.i  z(o1+1).r  z(o1+1).i
+    vzo1plus1 = _mm_loadu_ps(out + i1 + 4);
+
+    x0 = _mm_add_ps(vzo2, vzo3);
+    x1 = _mm_sub_ps(vzo2, vzo3);
+    y0 = _mm_add_ps(vzo2plus1, vzo3plus1);
+    y1 = _mm_sub_ps(vzo2plus1, vzo3plus1);
+
+    wr1 = _mm_set1_ps(wre[1]);
+    wi1 = _mm_set1_ps(wim[-1]);
+    wi2 = _mm_set1_ps(wim[-2]);
+    wi3 = _mm_set1_ps(wim[-3]);
+    wr2 = _mm_set1_ps(wre[2]);
+    wr3 = _mm_set1_ps(wre[3]);
+
+    x2 = _mm_unpackhi_ps(x0, x1);
+    x3 = _mm_shuffle_ps(x2, x2, 0x1b);
+
+    y2 = _mm_unpacklo_ps(y0, y1);
+    y3 = _mm_unpackhi_ps(y0, y1);
+    y4 = _mm_shuffle_ps(y2, y2, 0x1b);
+    y5 = _mm_shuffle_ps(y3, y3, 0x1b);
+
+    ymulwi2 = _mm_mul_ps(y4, wi2);
+    ymulwi3 = _mm_mul_ps(y5, wi3);
+    x4 = _mm_mul_ps(x2, wr1);
+    x5 = _mm_mul_ps(x3, wi1);
+    y8 = _mm_madd_ps(y2, wr2, ymulwi2);
+    y9 = _mm_msub_ps(y2, wr2, ymulwi2);
+    x6 = _mm_add_ps(x4, x5);
+    x7 = _mm_sub_ps(x4, x5);
+    y13 = _mm_madd_ps(y3, wr3, ymulwi3);
+    y14 = _mm_msub_ps(y3, wr3, ymulwi3);
+
+    x8 = _mm_shuffle_ps(x6, x7, 0xe4);
+    y10 = _mm_shuffle_ps(y8, y9, 0xe4);
+    y15 = _mm_shuffle_ps(y13, y14, 0xe4);
+
+    x9 = _mm_shuffle_ps(x0, x8, 0x84);
+    x10 = _mm_shuffle_ps(x1, x8, 0x71);
+
+    y16 = _mm_shuffle_ps(y10, y15, 0x88);
+    y17 = _mm_shuffle_ps(y10, y15, 0x77);
+
+    x11 = _mm_add_ps(vz0, x9);
+    x12 = _mm_sub_ps(vz0, x9);
+    x13 = _mm_add_ps(vzo1, x10);
+    x14 = _mm_sub_ps(vzo1, x10);
+
+    y18 = _mm_add_ps(vz0plus1, y16);
+    y19 = _mm_sub_ps(vz0plus1, y16);
+    y20 = _mm_add_ps(vzo1plus1, y17);
+    y21 = _mm_sub_ps(vzo1plus1, y17);
+
+    x15 = _mm_blend_ps(x13, x14, 0xa);
+    x16 = _mm_blend_ps(x14, x13, 0xa);
+    y22 = _mm_blend_ps(y20, y21, 0xa);
+    y23 = _mm_blend_ps(y21, y20, 0xa);
+
+    _mm_storeu_ps(out, x11);
+    _mm_storeu_ps(out + 4, y18);
+    _mm_storeu_ps(out + i1, x15);
+    _mm_storeu_ps(out + i1 + 4, y22);
+    _mm_storeu_ps(out + i2, x12);
+    _mm_storeu_ps(out + i2 + 4, y19);
+    _mm_storeu_ps(out + i3, x16);
+    _mm_storeu_ps(out + i3 + 4, y23);
+
+    do {
+        out += 8;
+        wre += 4;
+        wim -= 4;
+        wr0 = _mm_set1_ps(wre[0]);
+        wr1 = _mm_set1_ps(wre[1]);
+        wi0 = _mm_set1_ps(wim[0]);
+        wi1 = _mm_set1_ps(wim[-1]);
+
+        wr2 = _mm_set1_ps(wre[2]);
+        wr3 = _mm_set1_ps(wre[3]);
+        wi2 = _mm_set1_ps(wim[-2]);
+        wi3 = _mm_set1_ps(wim[-3]);
+
+        vzo2 = _mm_loadu_ps(out + i2);  // zo2.r  zo2.i  z(o2+1).r  z(o2+1).i
+        vzo2plus1 = _mm_loadu_ps(out + i2 + 4);
+        vzo3 = _mm_loadu_ps(out + i3);  // zo3.r  zo3.i  z(o3+1).r  z(o3+1).i
+        vzo3plus1 = _mm_loadu_ps(out + i3 + 4);
+        vz0 = _mm_loadu_ps(out + 0);    // z0.r  z0.i  z1.r  z1.i
+        vz0plus1 = _mm_loadu_ps(out + 4);
+        vzo1 = _mm_loadu_ps(out + i1); // zo1.r  zo1.i  z(o1+1).r  z(o1+1).i
+        vzo1plus1 = _mm_loadu_ps(out + i1 + 4);
+
+        x0 = _mm_add_ps(vzo2, vzo3);
+        x1 = _mm_sub_ps(vzo2, vzo3);
+
+        y0 = _mm_add_ps(vzo2plus1, vzo3plus1);
+        y1 = _mm_sub_ps(vzo2plus1, vzo3plus1);
+
+        x2 = _mm_unpacklo_ps(x0, x1);
+        x3 = _mm_unpackhi_ps(x0, x1);
+        x4 = _mm_shuffle_ps(x2, x2, 0x1b);
+        x5 = _mm_shuffle_ps(x3, x3, 0x1b);
+
+        y2 = _mm_unpacklo_ps(y0, y1);
+        y3 = _mm_unpackhi_ps(y0, y1);
+        y4 = _mm_shuffle_ps(y2, y2, 0x1b);
+        y5 = _mm_shuffle_ps(y3, y3, 0x1b);
+
+        xmulwi0 = _mm_mul_ps(x4, wi0);
+        xmulwi1 = _mm_mul_ps(x5, wi1);
+        x8 = _mm_madd_ps(x2, wr0, xmulwi0);
+        x9 = _mm_msub_ps(x2, wr0, xmulwi0);
+
+        ymulwi2 = _mm_mul_ps(y4, wi2);
+        ymulwi3 = _mm_mul_ps(y5, wi3);
+        x13 = _mm_madd_ps(x3, wr1, xmulwi1);
+        x14 = _mm_msub_ps(x3, wr1, xmulwi1);
+
+        y8 = _mm_madd_ps(y2, wr2, ymulwi2);
+        y9 = _mm_msub_ps(y2, wr2, ymulwi2);
+        y13 = _mm_madd_ps(y3, wr3, ymulwi3);
+        y14 = _mm_msub_ps(y3, wr3, ymulwi3);
+
+        x10 = _mm_shuffle_ps(x8, x9, 0xe4);
+        x15 = _mm_shuffle_ps(x13, x14, 0xe4);
+
+        y10 = _mm_shuffle_ps(y8, y9, 0xe4);
+        y15 = _mm_shuffle_ps(y13, y14, 0xe4);
+
+        x16 = _mm_shuffle_ps(x10, x15, 0x88);
+        x17 = _mm_shuffle_ps(x10, x15, 0x77);
+
+        y16 = _mm_shuffle_ps(y10, y15, 0x88);
+        y17 = _mm_shuffle_ps(y10, y15, 0x77);
+
+        x18 = _mm_add_ps(vz0, x16);
+        x19 = _mm_sub_ps(vz0, x16);
+        x20 = _mm_add_ps(vzo1, x17);
+        x21 = _mm_sub_ps(vzo1, x17);
+
+        y18 = _mm_add_ps(vz0plus1, y16);
+        y19 = _mm_sub_ps(vz0plus1, y16);
+        y20 = _mm_add_ps(vzo1plus1, y17);
+        y21 = _mm_sub_ps(vzo1plus1, y17);
+
+        x22 = _mm_blend_ps(x20, x21, 0xa);
+        x23 = _mm_blend_ps(x21, x20, 0xa);
+        y22 = _mm_blend_ps(y20, y21, 0xa);
+        y23 = _mm_blend_ps(y21, y20, 0xa);
+
+        _mm_storeu_ps(out, x18);
+        _mm_storeu_ps(out + 4, y18);
+        _mm_storeu_ps(out + i1, x22);
+        _mm_storeu_ps(out + i1 + 4, y22);
+        _mm_storeu_ps(out + i2, x19);
+        _mm_storeu_ps(out + i2 + 4, y19);
+        _mm_storeu_ps(out + i3, x23);
+        _mm_storeu_ps(out + i3 + 4, y23);
+    } while (n -= 2);
+}
+
+static av_always_inline
+void fft2_e2k_interleave(FFTComplex *z)
+{
+#if 1
+    vec_f a, b, c;
+    float *out = (float*)z;
+    vec_f sign = _mm_castsi128_ps(_mm_set1_epi32(-1 << 31));
+
+    a = _mm_loadu_ps(out);
+    b = _mm_unpacklo_ps2(a, a);
+    c = _mm_unpackhi_ps2(a, _mm_xor_ps(a, sign));
+    a = _mm_add_ps(b, c);
+    _mm_storeu_ps(out, a);
+#else
+    FFTSample r0, i0, r1, i1;
+    r0 = z[0].re; i0 = z[0].im;
+    r1 = z[1].re; i1 = z[1].im;
+    z[0].re = r0 + r1;
+    z[0].im = i0 + i1;
+    z[1].re = r0 - r1;
+    z[1].im = i0 - i1;
+#endif
+}
+
+static av_always_inline
+void fft4_e2k_interleave(FFTComplex *z)
+{
+    vec_f a, b, c, d;
+    float *out = (float*)z;
+    a = _mm_loadu_ps(out);
+    b = _mm_loadu_ps(out + 4);
+
+    c = _mm_shuffle_ps(a, b, 0x64);
+    d = _mm_shuffle_ps(a, b, 0xce);
+    a = _mm_add_ps(c, d);
+    b = _mm_sub_ps(c, d);
+
+    c = _mm_unpacklo_ps2(a, b);
+    d = _mm_shuffle_ps(a, b, 0xbe);
+
+    a = _mm_add_ps(c, d);
+    b = _mm_sub_ps(c, d);
+    _mm_storeu_ps(out, a);
+    _mm_storeu_ps(out + 4, b);
+}
+
+static av_always_inline
+void fft8_e2k_interleave(FFTComplex *z)
+{
+    vec_f vz0, vz1, vz2, vz3;
+    vec_f x0, x1, x2, x3;
+    vec_f x4, x5, x6, x7;
+    vec_f x8, x9, x10, x11;
+    vec_f x12, x13, x14, x15;
+    vec_f x16, x17, x18, x19;
+    vec_f x20, x21, x22, x23;
+    vec_f x24, x25, x26, x27;
+    vec_f x28, x29, x30, x31;
+    vec_f x32, x33, x34;
+
+    float *out = (float*)z;
+    vec_f vc1 = _mm_set1_ps(sqrthalf);
+
+    vz0 = _mm_loadu_ps(out);
+    vz1 = _mm_loadu_ps(out + 4);
+    vz2 = _mm_loadu_ps(out + 8);
+    vz3 = _mm_loadu_ps(out + 12);
+
+    x0 = _mm_shuffle_ps(vz0, vz1, 0x64);
+    x1 = _mm_shuffle_ps(vz0, vz1, 0xce);
+    x2 = _mm_shuffle_ps(vz2, vz3, 0x46);
+    x3 = _mm_shuffle_ps(vz2, vz3, 0xec);
+
+    x4 = _mm_add_ps(x0, x1);
+    x5 = _mm_sub_ps(x0, x1);
+    x6 = _mm_add_ps(x2, x3);
+    x7 = _mm_sub_ps(x2, x3);
+
+    x8 = _mm_unpacklo_ps2(x4, x5);
+    x9 = _mm_shuffle_ps(x4, x5, 0xbe);
+    x10 = _mm_shuffle_ps(x6, x7, 0x66);
+    x11 = _mm_shuffle_ps(x6, x7, 0xcc);
+
+    x12 = _mm_add_ps(x8, x9);
+    x13 = _mm_sub_ps(x8, x9);
+    x14 = _mm_add_ps(x10, x11);
+    x15 = _mm_sub_ps(x10, x11);
+    x16 = _mm_unpacklo_ps(x12, x13);
+    x17 = _mm_unpacklo_ps(x14, x15);
+    x18 = _mm_shuffle_ps(x17, x17, 0x6c);
+    x19 = _mm_add_ps(x16, x18); // z0.r  z2.r  z0.i  z2.i
+    x20 = _mm_sub_ps(x16, x18); // z4.r  z6.r  z4.i  z6.i
+
+    x21 = _mm_unpackhi_ps(x12, x13);
+    x22 = _mm_unpackhi_ps(x14, x15);
+    x23 = _mm_unpackhi_ps2(x22, x22);
+    x24 = _mm_add_ps(x22, x23);
+    x25 = _mm_sub_ps(x22, x23);
+
+    x26 = _mm_unpacklo_ps2(x24, x25);
+    x26 = _mm_mul_ps(_mm_shuffle_ps(x26, x26, 0x8d), vc1); // 1,s1,0,s0
+
+    x27 = _mm_add_ps(x21, x26); // z1.r  z7.r  z1.i  z3.i
+    x28 = _mm_sub_ps(x21, x26); // z5.r  z3.r  z5.i  z7.i
+
+    x29 = _mm_shuffle_ps(x19, x27, 0x88); // z0.r  z0.i  z1.r  z1.i
+    x30 = _mm_shuffle_ps(x19, x27, 0xdd); // z2.r  z2.i  z7.r  z3.i
+    x31 = _mm_shuffle_ps(x20, x28, 0x88); // z4.r  z4.i  z5.r  z5.i
+    x32 = _mm_shuffle_ps(x20, x28, 0xdd); // z6.r  z6.i  z3.r  z7.i
+    x33 = _mm_blend_ps(x30, x32, 0x4); // z2.r  z2.i  z3.r  z3.i
+    x34 = _mm_blend_ps(x32, x30, 0x4); // z6.r  z6.i  z7.r  z7.i
+
+    _mm_storeu_ps(out, x29);
+    _mm_storeu_ps(out + 4, x33);
+    _mm_storeu_ps(out + 8, x31);
+    _mm_storeu_ps(out + 12, x34);
+}
+
+static av_always_inline
+void fft16_e2k_interleave(FFTComplex *z)
+{
+    float *out = (float*)z;
+    vec_f vc0 = _mm_set1_ps(sqrthalf);
+    vec_f vc1 = _mm_set1_ps(ff_cos_16[1]);
+    vec_f vc2 = _mm_set1_ps(ff_cos_16[3]);
+    vec_f vz0, vz1, vz2, vz3;
+    vec_f vz4, vz5, vz6, vz7;
+    vec_f x0, x1, x2, x3;
+    vec_f x4, x5, x6, x7;
+    vec_f x8, x9, x10, x11;
+    vec_f x12, x13, x14, x15;
+    vec_f x16, x17, x18, x19;
+    vec_f x20, x21, x22, x23;
+    vec_f x24, x25, x26, x27;
+    vec_f x28, x29, x30, x31;
+    vec_f x32, x33, x34, x35;
+    vec_f x36, x37, x38, x39;
+    vec_f x40, x41, x42, x43;
+    vec_f x44, x45, x46, x47;
+    vec_f x48, x49, x50, x51;
+    vec_f x52, x53, x54, x55;
+    vec_f x56, x57, x58, x59;
+    vec_f x60, x61, x62, x63;
+    vec_f x64, x65, x66, x67;
+    vec_f x68, x69, x70, x71;
+    vec_f x72, x73, x74, x75;
+    vec_f x76, x77, x78, x79;
+    vec_f x80, x81, x82, x83;
+    vec_f x84, x85, x86;
+
+    vz0 = _mm_loadu_ps(out);
+    vz1 = _mm_loadu_ps(out + 4);
+    vz2 = _mm_loadu_ps(out + 8);
+    vz3 = _mm_loadu_ps(out + 12);
+    vz4 = _mm_loadu_ps(out + 16);
+    vz5 = _mm_loadu_ps(out + 20);
+    vz6 = _mm_loadu_ps(out + 24);
+    vz7 = _mm_loadu_ps(out + 28);
+
+    x0 = _mm_shuffle_ps(vz0, vz1, 0x64);
+    x1 = _mm_shuffle_ps(vz0, vz1, 0xce);
+    x2 = _mm_unpacklo_ps2(vz2, vz3);
+    x3 = _mm_unpackhi_ps2(vz2, vz3);
+
+    x4 = _mm_shuffle_ps(vz4, vz5, 0x64);
+    x5 = _mm_shuffle_ps(vz4, vz5, 0xce);
+    x6 = _mm_shuffle_ps(vz6, vz7, 0x64);
+    x7 = _mm_shuffle_ps(vz6, vz7, 0xce);
+
+    x8 = _mm_add_ps(x0, x1);
+    x9 = _mm_sub_ps(x0, x1);
+    x10 = _mm_add_ps(x2, x3);
+    x11 = _mm_sub_ps(x2, x3);
+
+    x12 = _mm_add_ps(x4, x5);
+    x13 = _mm_sub_ps(x4, x5);
+    x14 = _mm_add_ps(x6, x7);
+    x15 = _mm_sub_ps(x6, x7);
+
+    x16 = _mm_unpacklo_ps2(x8, x9);
+    x17 = _mm_shuffle_ps(x8, x9, 0xbe);
+    x18 = _mm_shuffle_ps(x10, x11, 0x96);
+    x19 = _mm_shuffle_ps(x10, x11, 0xcc);
+    x20 = _mm_unpacklo_ps2(x12, x14);
+    x21 = _mm_unpackhi_ps2(x12, x14);
+    x22 = _mm_unpacklo_ps2(x13, x15);
+    x23 = _mm_shuffle_ps(x13, x15, 0xbb);
+
+    x24 = _mm_add_ps(x16, x17);
+    x25 = _mm_sub_ps(x16, x17);
+    x26 = _mm_add_ps(x18, x19);
+    x27 = _mm_sub_ps(x18, x19);
+    x28 = _mm_add_ps(x20, x21);
+    x29 = _mm_sub_ps(x20, x21);
+    x30 = _mm_add_ps(x22, x23);
+    x31 = _mm_sub_ps(x22, x23);
+
+    x32 = _mm_add_ps(x24, x26);
+    x33 = _mm_sub_ps(x24, x26);
+    x34 = _mm_unpacklo_ps2(x32, x33);
+
+    x35 = _mm_shuffle_ps(x28, x29, 0x96);
+    x36 = _mm_shuffle_ps(x28, x29, 0xcc);
+    x37 = _mm_add_ps(x35, x36);
+    x38 = _mm_sub_ps(x35, x36);
+    x39 = _mm_shuffle_ps(x37, x38, 0x14);
+
+    x40 = _mm_shuffle_ps(x27, x38, 0xeb);
+    x41 = _mm_shuffle_ps(x26, x37, 0xbe);
+    x42 = _mm_add_ps(x40, x41);
+    x43 = _mm_sub_ps(x40, x41);
+    x44 = _mm_mul_ps(x42, vc0);
+    x45 = _mm_mul_ps(x43, vc0);
+
+    x46 = _mm_add_ps(x34, x39);  // z0.r  z0.i  z4.r  z4.i
+    x47 = _mm_sub_ps(x34, x39);  // z8.r  z8.i  z12.r  z12.i
+
+    x48 = _mm_shuffle_ps(x30, x31, 0x96);
+    x49 = _mm_shuffle_ps(x30, x31, 0x3c);
+    x50 = _mm_add_ps(x48, x49);
+    x51 = _mm_sub_ps(x48, x49);
+    x52 = _mm_mul_ps(x50, vc1);
+    x53 = _mm_mul_ps(x50, vc2);
+    x54 = _mm_mul_ps(x51, vc1);
+    x55 = _mm_mul_ps(x51, vc2);
+
+    x56 = _mm_unpackhi_ps2(x24, x25);
+    x57 = _mm_shuffle_ps(x44, x45, 0x14);
+    x58 = _mm_add_ps(x56, x57);
+    x59 = _mm_sub_ps(x56, x57);
+
+    x60 = _mm_shuffle_ps(x54, x54, 0xb1);
+    x61 = _mm_shuffle_ps(x55, x55, 0xb1);
+    x62 = _mm_add_ps(x52, x61);
+    x63 = _mm_sub_ps(x52, x61);
+    x64 = _mm_add_ps(x60, x53);
+    x65 = _mm_sub_ps(x60, x53);
+    x66 = _mm_shuffle_ps(x62, x64, 0xb4);
+    x67 = _mm_shuffle_ps(x65, x63, 0xb4);
+
+    x68 = _mm_add_ps(x58, x66); // z1.r    z1.i  z3.r    z3.i
+    x69 = _mm_sub_ps(x58, x66); // z9.r    z9.i  z11.r  z11.i
+    x70 = _mm_add_ps(x59, x67); // z5.r    z5.i  z15.r  z15.i
+    x71 = _mm_sub_ps(x59, x67); // z13.r  z13.i z7.r   z7.i
+
+    x72 = _mm_shuffle_ps(x27, x27, 0xe1);
+    x73 = _mm_add_ps(x25, x72);
+    x74 = _mm_sub_ps(x25, x72);
+    x75 = _mm_unpacklo_ps2(x73, x74);
+    x76 = _mm_shuffle_ps(x44, x45, 0xeb);
+    x77 = _mm_add_ps(x75, x76); // z2.r   z2.i    z6.r    z6.i
+    x78 = _mm_sub_ps(x75, x76); // z10.r  z10.i  z14.r  z14.i
+
+    x79 = _mm_unpacklo_ps2(x46, x68); // z0.r  z0.i  z1.r  z1.i
+    x80 = _mm_shuffle_ps(x77, x68, 0xe4); // z2.r  z2.i  z3.r  z3.i
+    x81 = _mm_shuffle_ps(x46, x70, 0x4e); // z4.r  z4.i  z5.r  z5.i
+    x82 = _mm_unpackhi_ps2(x77, x71); // z6.r  z6.i  z7.r  z7.i
+    _mm_storeu_ps(out, x79);
+    _mm_storeu_ps(out + 4, x80);
+    _mm_storeu_ps(out + 8, x81);
+    _mm_storeu_ps(out + 12, x82);
+    x83 = _mm_unpacklo_ps2(x47, x69); // z8.r  z8.i  z9.r  z9.i
+    x84 = _mm_shuffle_ps(x78, x69, 0xe4); // z10.r  z10.i  z11.r  z11.i
+    x85 = _mm_shuffle_ps(x47, x71, 0x4e); // z12.r  z12.i  z13.r  z13.i
+    x86 = _mm_unpackhi_ps2(x78, x70); // z14.r  z14.i  z15.r  z15.i
+    _mm_storeu_ps(out + 16, x83);
+    _mm_storeu_ps(out + 20, x84);
+    _mm_storeu_ps(out + 24, x85);
+    _mm_storeu_ps(out + 28, x86);
+}
+
+static av_always_inline
+void fft4_e2k(FFTComplex *z)
+{
+    vec_f a, b, c, d;
+    float *out = (float*)z;
+    a = _mm_loadu_ps(out);
+    b = _mm_loadu_ps(out + 4);
+
+    c = _mm_shuffle_ps(a, b, 0x64);
+    d = _mm_shuffle_ps(a, b, 0xce);
+    a = _mm_add_ps(c, d);
+    b = _mm_sub_ps(c, d);
+
+    c = _mm_unpacklo_ps(a, b);
+    d = _mm_unpackhi_ps(a, _mm_shuffle_ps(b, b, 0xb1));
+
+    a = _mm_add_ps(c, d);
+    b = _mm_sub_ps(c, d);
+
+    c = _mm_unpacklo_ps2(a, b);
+    d = _mm_unpackhi_ps2(a, b);
+
+    _mm_storeu_ps(out, c);
+    _mm_storeu_ps(out + 4, d);
+}
+
+static av_always_inline
+void fft8_e2k(FFTComplex *z)
+{
+    vec_f vz0, vz1, vz2, vz3;
+    vec_f vz4, vz5, vz6, vz7, vz8;
+
+    float *out = (float*)z;
+    vec_f vc0 = _mm_setzero_ps();
+    vec_f vc1 = _mm_setr_ps(-sqrthalf, sqrthalf, sqrthalf, -sqrthalf);
+    vec_f vc2 = _mm_set1_ps(sqrthalf);
+
+    vz0 = _mm_loadu_ps(out);
+    vz1 = _mm_loadu_ps(out + 4);
+    vz2 = _mm_loadu_ps(out + 8);
+    vz3 = _mm_loadu_ps(out + 12);
+
+    vz6 = _mm_unpacklo_ps(vz2, vz3);
+    vz7 = _mm_unpackhi_ps(vz2, vz3);
+    vz4 = _mm_shuffle_ps(vz0, vz1, 0x64);
+    vz5 = _mm_shuffle_ps(vz0, vz1, 0xce);
+
+    vz2 = _mm_add_ps(vz6, vz7);
+    vz3 = _mm_sub_ps(vz6, vz7);
+    vz8 = _mm_shuffle_ps(vz3, vz3, 0x4e);
+
+    vz0 = _mm_add_ps(vz4, vz5);
+    vz1 = _mm_sub_ps(vz4, vz5);
+
+    vz3 = _mm_madd_ps(vz3, vc1, vc0);
+    vz3 = _mm_madd_ps(vz8, vc2, vz3);
+
+    vz4 = _mm_unpacklo_ps(vz0, vz1);
+    vz5 = _mm_unpackhi_ps(vz0, _mm_shuffle_ps(vz1, vz1, 0xb1));
+    vz6 = _mm_shuffle_ps(vz2, vz3, 0x39);
+    vz7 = _mm_shuffle_ps(vz2, vz3, 0x6c);
+
+    vz0 = _mm_add_ps(vz4, vz5);
+    vz1 = _mm_sub_ps(vz4, vz5);
+    vz2 = _mm_add_ps(vz6, vz7);
+    vz3 = _mm_sub_ps(vz6, vz7);
+
+    vz4 = _mm_unpacklo_ps2(vz0, vz1);
+    vz5 = _mm_unpackhi_ps2(vz0, vz1);
+    vz6 = _mm_shuffle_ps(vz2, vz3, 0xd8);
+    vz7 = _mm_shuffle_ps(vz2, vz3, 0x8d);
+
+    vz2 = _mm_sub_ps(vz4, vz6);
+    vz3 = _mm_sub_ps(vz5, vz7);
+
+    vz0 = _mm_add_ps(vz4, vz6);
+    vz1 = _mm_add_ps(vz5, vz7);
+
+    _mm_storeu_ps(out, vz0);
+    _mm_storeu_ps(out + 4, vz1);
+    _mm_storeu_ps(out + 8, vz2);
+    _mm_storeu_ps(out + 12, vz3);
+}
+
+static av_always_inline
+void fft16_e2k(FFTComplex *z)
+{
+    float *out = (float*)z;
+    vec_f vc0 = _mm_setzero_ps();
+    vec_f vc1 = _mm_setr_ps(-sqrthalf, sqrthalf, sqrthalf, -sqrthalf);
+    vec_f vc2 = _mm_set1_ps(sqrthalf);
+    vec_f vc3 = _mm_setr_ps(1.0, 0.92387953, sqrthalf, 0.38268343);
+    vec_f vc4 = _mm_setr_ps(0.0, 0.38268343, sqrthalf, 0.92387953);
+    vec_f vc5 = _mm_setr_ps(-0.0, -0.38268343, -sqrthalf, -0.92387953);
+
+    vec_f vz0, vz1, vz2, vz3;
+    vec_f vz4, vz5, vz6, vz7;
+    vec_f vz8, vz9, vz10, vz11;
+    vec_f vz12, vz13;
+
+    vz0 = _mm_loadu_ps(out + 16);
+    vz1 = _mm_loadu_ps(out + 20);
+    vz2 = _mm_loadu_ps(out + 24);
+    vz3 = _mm_loadu_ps(out + 28);
+
+    vz4 = _mm_shuffle_ps(vz0, vz1, 0x64);
+    vz5 = _mm_shuffle_ps(vz0, vz1, 0xce);
+    vz6 = _mm_shuffle_ps(vz2, vz3, 0x64);
+    vz7 = _mm_shuffle_ps(vz2, vz3, 0xce);
+
+    vz0 = _mm_add_ps(vz4, vz5);
+    vz1= _mm_sub_ps(vz4, vz5);
+    vz2 = _mm_add_ps(vz6, vz7);
+    vz3 = _mm_sub_ps(vz6, vz7);
+
+    vz4 = _mm_unpacklo_ps(vz0, vz1);
+    vz5 = _mm_unpackhi_ps(vz0, _mm_shuffle_ps(vz1, vz1, 0xb1));
+    vz6 = _mm_unpacklo_ps(vz2, vz3);
+    vz7 = _mm_unpackhi_ps(vz2, _mm_shuffle_ps(vz3, vz3, 0xb1));
+
+    vz0 = _mm_add_ps(vz4, vz5);
+    vz1 = _mm_sub_ps(vz4, vz5);
+    vz2 = _mm_add_ps(vz6, vz7);
+    vz3 = _mm_sub_ps(vz6, vz7);
+
+    vz4 = _mm_unpacklo_ps2(vz0, vz1);
+    vz5 = _mm_unpackhi_ps2(vz0, vz1);
+
+    vz6 = _mm_unpacklo_ps2(vz2, vz3);
+    vz7 = _mm_unpackhi_ps2(vz2, vz3);
+
+    vz0 = _mm_loadu_ps(out);
+    vz1 = _mm_loadu_ps(out + 4);
+    vz2 = _mm_loadu_ps(out + 8);
+    vz3 = _mm_loadu_ps(out + 12);
+    vz10 = _mm_unpacklo_ps(vz2, vz3);
+    vz11 = _mm_unpackhi_ps(vz2, vz3);
+    vz8 = _mm_shuffle_ps(vz0, vz1, 0x64);
+    vz9 = _mm_shuffle_ps(vz0, vz1, 0xce);
+
+    vz2 = _mm_add_ps(vz10, vz11);
+    vz3 = _mm_sub_ps(vz10, vz11);
+    vz12 = _mm_shuffle_ps(vz3, vz3, 0x4e);
+    vz0 = _mm_add_ps(vz8, vz9);
+    vz1 = _mm_sub_ps(vz8, vz9);
+
+    vz3 = _mm_madd_ps(vz3, vc1, vc0);
+    vz3 = _mm_madd_ps(vz12, vc2, vz3);
+    vz8 = _mm_unpacklo_ps(vz0, vz1);
+    vz9 = _mm_unpackhi_ps(vz0, _mm_shuffle_ps(vz1, vz1, 0xb1));
+    vz10 = _mm_shuffle_ps(vz2, vz3, 0x39);
+    vz11 = _mm_shuffle_ps(vz2, vz3, 0x6c);
+
+    vz0 = _mm_add_ps(vz8, vz9);
+    vz1 = _mm_sub_ps(vz8, vz9);
+    vz2 = _mm_add_ps(vz10, vz11);
+    vz3 = _mm_sub_ps(vz10, vz11);
+
+    vz8 = _mm_unpacklo_ps2(vz0, vz1);
+    vz9 = _mm_unpackhi_ps2(vz0, vz1);
+    vz10 = _mm_shuffle_ps(vz2, vz3, 0xd8);
+    vz11 = _mm_shuffle_ps(vz2, vz3, 0x8d);
+
+    vz2 = _mm_sub_ps(vz8, vz10);
+    vz3 = _mm_sub_ps(vz9, vz11);
+    vz0 = _mm_add_ps(vz8, vz10);
+    vz1 = _mm_add_ps(vz9, vz11);
+
+    vz8 = _mm_madd_ps(vz4, vc3, vc0);
+    vz9 = _mm_madd_ps(vz5, vc3, vc0);
+    vz10 = _mm_madd_ps(vz6, vc3, vc0);
+    vz11 = _mm_madd_ps(vz7, vc3, vc0);
+
+    vz8 = _mm_madd_ps(vz5, vc4, vz8);
+    vz9 = _mm_madd_ps(vz4, vc5, vz9);
+    vz10 = _mm_madd_ps(vz7, vc5, vz10);
+    vz11 = _mm_madd_ps(vz6, vc4, vz11);
+
+    vz12 = _mm_sub_ps(vz10, vz8);
+    vz10 = _mm_add_ps(vz10, vz8);
+
+    vz13 = _mm_sub_ps(vz9, vz11);
+    vz11 = _mm_add_ps(vz9, vz11);
+
+    vz4 = _mm_sub_ps(vz0, vz10);
+    vz0 = _mm_add_ps(vz0, vz10);
+
+    vz7 = _mm_sub_ps(vz3, vz12);
+    vz3 = _mm_add_ps(vz3, vz12);
+
+    vz5 = _mm_sub_ps(vz1, vz11);
+    vz1 = _mm_add_ps(vz1, vz11);
+
+    vz6 = _mm_sub_ps(vz2, vz13);
+    vz2 = _mm_add_ps(vz2, vz13);
+
+    _mm_storeu_ps(out, vz0);
+    _mm_storeu_ps(out + 4, vz1);
+    _mm_storeu_ps(out + 8, vz2);
+    _mm_storeu_ps(out + 12, vz3);
+    _mm_storeu_ps(out + 16, vz4);
+    _mm_storeu_ps(out + 20, vz5);
+    _mm_storeu_ps(out + 24, vz6);
+    _mm_storeu_ps(out + 28, vz7);
+}
+
+static av_always_inline
+void pass_e2k(FFTComplex *z, const FFTSample *wre, unsigned int n)
+{
+    int i1 = n * 4, i2 = n * 8, i3 = n * 12;
+    FFTSample *out = (FFTSample*)z;
+    const FFTSample *wim = wre + n * 2;
+    vec_f v0, v1, v2, v3;
+    vec_f v4, v5, v6, v7;
+    vec_f v8, v9, v10, v11;
+    vec_f v12, v13;
+
+    n = n - 2;
+
+    v8 = _mm_loadu_ps(wre);
+#if 0
+    v9 = _mm_loadu_ps(wim - 3);
+#else
+    v10 = _mm_loadu_ps(wim);
+    v9 = _mm_loadu_ps(wim - 4);
+    v9 = _mm_castsi128_ps(_mm_alignr_epi8(_mm_castps_si128(v10), _mm_castps_si128(v9), 4));
+#endif
+    v9 = _mm_shuffle_ps(v9, v9, 0x1b);
+
+    v4 = _mm_loadu_ps(out + i2);
+    v5 = _mm_loadu_ps(out + i2 + 4);
+    v6 = _mm_loadu_ps(out + i3);
+    v7 = _mm_loadu_ps(out + i3 + 4);
+    v10 = _mm_mul_ps(v4, v8); // r2*wre
+    v11 = _mm_mul_ps(v5, v8); // i2*wre
+    v12 = _mm_mul_ps(v6, v8); // r3*wre
+    v13 = _mm_mul_ps(v7, v8); // i3*wre
+
+    v0 = _mm_loadu_ps(out); // r0
+    v3 = _mm_loadu_ps(out + i1 + 4); // i1
+    v10 = _mm_madd_ps(v5, v9, v10); // r2*wim
+    v11 = _mm_nmsub_ps(v4, v9, v11); // i2*wim
+    v12 = _mm_nmsub_ps(v7, v9, v12); // r3*wim
+    v13 = _mm_madd_ps(v6, v9, v13); // i3*wim
+
+    v1 = _mm_loadu_ps(out + 4); // i0
+    v2 = _mm_loadu_ps(out + i1); // r1
+    v8 = _mm_sub_ps(v12, v10);
+    v12 = _mm_add_ps(v12, v10);
+    v9 = _mm_sub_ps(v11, v13);
+    v13 = _mm_add_ps(v11, v13);
+    v4 = _mm_sub_ps(v0, v12);
+    v0 = _mm_add_ps(v0, v12);
+    v7 = _mm_sub_ps(v3, v8);
+    v3 = _mm_add_ps(v3, v8);
+
+    _mm_storeu_ps(out, v0); // r0
+    _mm_storeu_ps(out + i1 + 4, v3); // i1
+    _mm_storeu_ps(out + i2, v4); // r2
+    _mm_storeu_ps(out + i3 + 4, v7);// i3
+
+    v5 = _mm_sub_ps(v1, v13);
+    v1 = _mm_add_ps(v1, v13);
+    v6 = _mm_sub_ps(v2, v9);
+    v2 = _mm_add_ps(v2, v9);
+
+    _mm_storeu_ps(out + 4, v1); // i0
+    _mm_storeu_ps(out + i1, v2); // r1
+    _mm_storeu_ps(out + i2 + 4, v5); // i2
+    _mm_storeu_ps(out + i3, v6); // r3
+
+    do {
+        out += 8;
+        wre += 4;
+        wim -= 4;
+
+        v8 = _mm_loadu_ps(wre);
+#if 0
+        v9 = _mm_loadu_ps(wim - 3);
+#else
+        v10 = _mm_loadu_ps(wim);
+        v9 = _mm_loadu_ps(wim - 4);
+        v9 = _mm_castsi128_ps(_mm_alignr_epi8(_mm_castps_si128(v10), _mm_castps_si128(v9), 4));
+#endif
+        v9 = _mm_shuffle_ps(v9, v9, 0x1b);
+
+        v4 = _mm_loadu_ps(out + i2); // r2
+        v5 = _mm_loadu_ps(out + i2 + 4); // i2
+        v6 = _mm_loadu_ps(out + i3); // r3
+        v7 = _mm_loadu_ps(out + i3 + 4);// i3
+        v10 = _mm_mul_ps(v4, v8); // r2*wre
+        v11 = _mm_mul_ps(v5, v8); // i2*wre
+        v12 = _mm_mul_ps(v6, v8); // r3*wre
+        v13 = _mm_mul_ps(v7, v8); // i3*wre
+
+        v0 = _mm_loadu_ps(out); // r0
+        v3 = _mm_loadu_ps(out + i1 + 4); // i1
+        v10 = _mm_madd_ps(v5, v9, v10); // r2*wim
+        v11 = _mm_nmsub_ps(v4, v9, v11); // i2*wim
+        v12 = _mm_nmsub_ps(v7, v9, v12); // r3*wim
+        v13 = _mm_madd_ps(v6, v9, v13); // i3*wim
+
+        v1 = _mm_loadu_ps(out + 4); // i0
+        v2 = _mm_loadu_ps(out + i1); // r1
+        v8 = _mm_sub_ps(v12, v10);
+        v12 = _mm_add_ps(v12, v10);
+        v9 = _mm_sub_ps(v11, v13);
+        v13 = _mm_add_ps(v11, v13);
+        v4 = _mm_sub_ps(v0, v12);
+        v0 = _mm_add_ps(v0, v12);
+        v7 = _mm_sub_ps(v3, v8);
+        v3 = _mm_add_ps(v3, v8);
+
+        _mm_storeu_ps(out, v0); // r0
+        _mm_storeu_ps(out + i1 + 4, v3); // i1
+        _mm_storeu_ps(out + i2, v4); // r2
+        _mm_storeu_ps(out + i3 + 4, v7); // i3
+
+        v5 = _mm_sub_ps(v1, v13);
+        v1 = _mm_add_ps(v1, v13);
+        v6 = _mm_sub_ps(v2, v9);
+        v2 = _mm_add_ps(v2, v9);
+
+        _mm_storeu_ps(out + 4, v1); // i0
+        _mm_storeu_ps(out + i1, v2); // r1
+        _mm_storeu_ps(out + i2 + 4, v5); // i2
+        _mm_storeu_ps(out + i3, v6); // r3
+    } while (n -= 2);
+}
+
+static void fft32_e2k_interleave(FFTComplex *z)
+{
+    fft16_e2k_interleave(z);
+    fft8_e2k_interleave(z+16);
+    fft8_e2k_interleave(z+24);
+    pass_e2k_interleave(z,ff_cos_32,4);
+}
+
+static void fft64_e2k_interleave(FFTComplex *z)
+{
+    fft32_e2k_interleave(z);
+    fft16_e2k_interleave(z+32);
+    fft16_e2k_interleave(z+48);
+    pass_e2k_interleave(z,ff_cos_64, 8);
+}
+
+static void fft128_e2k_interleave(FFTComplex *z)
+{
+    fft64_e2k_interleave(z);
+    fft32_e2k_interleave(z+64);
+    fft32_e2k_interleave(z+96);
+    pass_e2k_interleave(z,ff_cos_128,16);
+}
+
+static void fft256_e2k_interleave(FFTComplex *z)
+{
+    fft128_e2k_interleave(z);
+    fft64_e2k_interleave(z+128);
+    fft64_e2k_interleave(z+192);
+    pass_e2k_interleave(z,ff_cos_256,32);
+}
+
+static void fft512_e2k_interleave(FFTComplex *z)
+{
+    fft256_e2k_interleave(z);
+    fft128_e2k_interleave(z+256);
+    fft128_e2k_interleave(z+384);
+    pass_e2k_interleave(z,ff_cos_512,64);
+}
+
+static void fft1024_e2k_interleave(FFTComplex *z)
+{
+    fft512_e2k_interleave(z);
+    fft256_e2k_interleave(z+512);
+    fft256_e2k_interleave(z+768);
+    pass_e2k_interleave(z,ff_cos_1024,128);
+}
+
+static void fft2048_e2k_interleave(FFTComplex *z)
+{
+    fft1024_e2k_interleave(z);
+    fft512_e2k_interleave(z+1024);
+    fft512_e2k_interleave(z+1536);
+    pass_e2k_interleave(z,ff_cos_2048,256);
+}
+
+static void fft4096_e2k_interleave(FFTComplex *z)
+{
+    fft2048_e2k_interleave(z);
+    fft1024_e2k_interleave(z+2048);
+    fft1024_e2k_interleave(z+3072);
+    pass_e2k_interleave(z,ff_cos_4096, 512);
+}
+
+static void fft8192_e2k_interleave(FFTComplex *z)
+{
+    fft4096_e2k_interleave(z);
+    fft2048_e2k_interleave(z+4096);
+    fft2048_e2k_interleave(z+6144);
+    pass_e2k_interleave(z,ff_cos_8192,1024);
+}
+
+static void fft16384_e2k_interleave(FFTComplex *z)
+{
+    fft8192_e2k_interleave(z);
+    fft4096_e2k_interleave(z+8192);
+    fft4096_e2k_interleave(z+12288);
+    pass_e2k_interleave(z,ff_cos_16384,2048);
+}
+
+static void fft32768_e2k_interleave(FFTComplex *z)
+{
+    fft16384_e2k_interleave(z);
+    fft8192_e2k_interleave(z+16384);
+    fft8192_e2k_interleave(z+24576);
+    pass_e2k_interleave(z,ff_cos_32768,4096);
+}
+
+static void fft65536_e2k_interleave(FFTComplex *z)
+{
+    fft32768_e2k_interleave(z);
+    fft16384_e2k_interleave(z+32768);
+    fft16384_e2k_interleave(z+49152);
+    pass_e2k_interleave(z,ff_cos_65536,8192);
+}
+
+static void fft32_e2k(FFTComplex *z)
+{
+    fft16_e2k(z);
+    fft8_e2k(z+16);
+    fft8_e2k(z+24);
+    pass_e2k(z,ff_cos_32,4);
+}
+
+static void fft64_e2k(FFTComplex *z)
+{
+    fft32_e2k(z);
+    fft16_e2k(z+32);
+    fft16_e2k(z+48);
+    pass_e2k(z,ff_cos_64, 8);
+}
+
+static void fft128_e2k(FFTComplex *z)
+{
+    fft64_e2k(z);
+    fft32_e2k(z+64);
+    fft32_e2k(z+96);
+    pass_e2k(z,ff_cos_128,16);
+}
+
+static void fft256_e2k(FFTComplex *z)
+{
+    fft128_e2k(z);
+    fft64_e2k(z+128);
+    fft64_e2k(z+192);
+    pass_e2k(z,ff_cos_256,32);
+}
+
+static void fft512_e2k(FFTComplex *z)
+{
+    fft256_e2k(z);
+    fft128_e2k(z+256);
+    fft128_e2k(z+384);
+    pass_e2k(z,ff_cos_512,64);
+}
+
+static void fft1024_e2k(FFTComplex *z)
+{
+    fft512_e2k(z);
+    fft256_e2k(z+512);
+    fft256_e2k(z+768);
+    pass_e2k(z,ff_cos_1024,128);
+
+}
+
+static void fft2048_e2k(FFTComplex *z)
+{
+    fft1024_e2k(z);
+    fft512_e2k(z+1024);
+    fft512_e2k(z+1536);
+    pass_e2k(z,ff_cos_2048,256);
+}
+
+static void fft4096_e2k(FFTComplex *z)
+{
+    fft2048_e2k(z);
+    fft1024_e2k(z+2048);
+    fft1024_e2k(z+3072);
+    pass_e2k(z,ff_cos_4096, 512);
+}
+
+static void fft8192_e2k(FFTComplex *z)
+{
+    fft4096_e2k(z);
+    fft2048_e2k(z+4096);
+    fft2048_e2k(z+6144);
+    pass_e2k(z,ff_cos_8192,1024);
+}
+
+static void fft16384_e2k(FFTComplex *z)
+{
+    fft8192_e2k(z);
+    fft4096_e2k(z+8192);
+    fft4096_e2k(z+12288);
+    pass_e2k(z,ff_cos_16384,2048);
+}
+
+static void fft32768_e2k(FFTComplex *z)
+{
+    fft16384_e2k(z);
+    fft8192_e2k(z+16384);
+    fft8192_e2k(z+24576);
+    pass_e2k(z,ff_cos_32768,4096);
+}
+
+static void fft65536_e2k(FFTComplex *z)
+{
+    fft32768_e2k(z);
+    fft16384_e2k(z+32768);
+    fft16384_e2k(z+49152);
+    pass_e2k(z,ff_cos_65536,8192);
+}
+
+static void (* const fft_dispatch_e2k[])(FFTComplex*) = {
+    fft4_e2k, fft8_e2k, fft16_e2k, fft32_e2k, fft64_e2k, fft128_e2k, fft256_e2k, fft512_e2k, fft1024_e2k,
+    fft2048_e2k, fft4096_e2k, fft8192_e2k, fft16384_e2k, fft32768_e2k, fft65536_e2k,
+};
+
+static void (* const fft_dispatch_e2k_interleave[])(FFTComplex*) = {
+    fft4_e2k_interleave, fft8_e2k_interleave, fft16_e2k_interleave, fft32_e2k_interleave, fft64_e2k_interleave,
+    fft128_e2k_interleave, fft256_e2k_interleave, fft512_e2k_interleave, fft1024_e2k_interleave,
+    fft2048_e2k_interleave, fft4096_e2k_interleave, fft8192_e2k_interleave, fft16384_e2k_interleave, fft32768_e2k_interleave, fft65536_e2k_interleave,
+};
+
+void ff_fft_calc_interleave_e2k(FFTContext *s, FFTComplex *z)
+{
+     fft_dispatch_e2k_interleave[s->nbits-2](z);
+}
+
+void ff_fft_calc_e2k(FFTContext *s, FFTComplex *z)
+{
+     fft_dispatch_e2k[s->nbits-2](z);
+}
+
diff --git a/libavcodec/e2k/fft.h b/libavcodec/e2k/fft.h
new file mode 100644
index 0000000..62ae2f3
--- /dev/null
+++ b/libavcodec/e2k/fft.h
@@ -0,0 +1,29 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_E2K_FFT_H
+#define AVCODEC_E2K_FFT_H
+
+#include "config.h"
+#include "libavcodec/fft.h"
+#include "libavcodec/fft-internal.h"
+
+void ff_fft_calc_interleave_e2k(FFTContext *s, FFTComplex *z);
+void ff_fft_calc_e2k(FFTContext *s, FFTComplex *z);
+
+#endif /* AVCODEC_E2K_FFT_H */
diff --git a/libavcodec/e2k/fft_init.c b/libavcodec/e2k/fft_init.c
new file mode 100644
index 0000000..116236d
--- /dev/null
+++ b/libavcodec/e2k/fft_init.c
@@ -0,0 +1,152 @@
+/*
+ * FFT/IFFT transforms
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2009 Loren Merritt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+#include "libavutil/cpu.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+#include "libavcodec/fft.h"
+
+#include "fft.h"
+
+/**
+ * Do a complex FFT with the parameters defined in ff_fft_init().
+ * The input data must be permuted before with s->revtab table.
+ * No 1.0 / sqrt(n) normalization is done.
+ *
+ * This code assumes that the 'z' pointer is 16 bytes-aligned.
+ * It also assumes all FFTComplex are 8 bytes-aligned pairs of floats.
+ */
+
+static void imdct_half_e2k(FFTContext *s, FFTSample *output, const FFTSample *input)
+{
+    int j, k;
+    int n = 1 << s->mdct_bits;
+    int n4 = n >> 2;
+    int n8 = n >> 3;
+    int n32 = n >> 5;
+    const uint16_t *revtabj = s->revtab;
+    const uint16_t *revtabk = s->revtab + n4;
+    const vec_f *tcos = (const vec_f*)(s->tcos + n8);
+    const vec_f *tsin = (const vec_f*)(s->tsin + n8);
+    const vec_f *pin = (const vec_f*)(input + n4);
+    vec_f *pout = (vec_f*)(output + n4);
+
+    /* pre rotation */
+    k = n32 - 1;
+    do {
+        vec_f cos, sin, cos0, sin0, cos1, sin1;
+        vec_f re, im, r0, i0, r1, i1, a, b;
+#define CMULA(p, perm) \
+    a = pin[ k*2+p];                 /* { z[k].re,    z[k].im,    z[k+1].re,  z[k+1].im  } */ \
+    b = pin[-k*2-p-1];               /* { z[-k-2].re, z[-k-2].im, z[-k-1].re, z[-k-1].im } */ \
+    re = _mm_shuffle_ps(a, b, 0x88); /* { z[k].re,    z[k+1].re,  z[-k-2].re, z[-k-1].re } */ \
+    im = _mm_shuffle_ps(b, a, 0x77); /* { z[-k-1].im, z[-k-2].im, z[k+1].im,  z[k].im    } */ \
+    cos = _mm_shuffle_ps(cos0, cos1, perm);  /* { cos[k], cos[k+1], cos[-k-2], cos[-k-1] } */ \
+    sin = _mm_shuffle_ps(sin0, sin1, perm); \
+    r##p = _mm_sub_ps(_mm_mul_ps(im, cos), _mm_mul_ps(re, sin)); \
+    i##p = _mm_add_ps(_mm_mul_ps(re, cos), _mm_mul_ps(im, sin));
+#define STORE2(L, v, dst) \
+    VEC_ST##L(output + dst * 2, _mm_castps_si128(v));
+#define STORE8(p) \
+    a = _mm_unpacklo_ps(r##p, i##p); \
+    b = _mm_unpackhi_ps(r##p, i##p); \
+    STORE2(L, a, revtabk[ p*2-4]); \
+    STORE2(H, a, revtabk[ p*2-3]); \
+    STORE2(L, b, revtabj[-p*2+2]); \
+    STORE2(H, b, revtabj[-p*2+3]);
+
+        cos0 = tcos[k];
+        sin0 = tsin[k];
+        cos1 = tcos[-k-1];
+        sin1 = tsin[-k-1];
+        CMULA(0, 0xe4);
+        CMULA(1, 0x4e);
+        STORE8(0);
+        STORE8(1);
+        revtabj += 4;
+        revtabk -= 4;
+        k--;
+    } while (k >= 0);
+
+    ff_fft_calc_e2k(s, (FFTComplex*)output);
+
+    /* post rotation + reordering */
+    j = -n32;
+    k = n32 - 1;
+    do {
+        vec_f cos, sin, re, im, a, b, c, d;
+#define CMULB(d0, d1, o)                  \
+    re = pout[o*2]; im = pout[o*2+1]; \
+    cos = tcos[o];  sin = tsin[o];    \
+    d0 = _mm_sub_ps(_mm_mul_ps(im, sin), _mm_mul_ps(re, cos)); \
+    d1 = _mm_add_ps(_mm_mul_ps(re, sin), _mm_mul_ps(im, cos));
+
+        CMULB(a, b, j);
+        CMULB(c, d, k);
+        d = _mm_shuffle_ps(d, d, 0x1b);
+        b = _mm_shuffle_ps(b, b, 0x1b);
+        pout[2*j]   = _mm_unpacklo_ps(a, d);
+        pout[2*j+1] = _mm_unpackhi_ps(a, d);
+        pout[2*k]   = _mm_unpacklo_ps(c, b);
+        pout[2*k+1] = _mm_unpackhi_ps(c, b);
+        j++;
+        k--;
+    } while (k >= 0);
+}
+
+static void imdct_calc_e2k(FFTContext *s, FFTSample *output, const FFTSample *input)
+{
+    int k;
+    int n = 1 << s->mdct_bits;
+    int n4 = n >> 2;
+    int n16 = n >> 4;
+    vec_u32 sign = _mm_set1_epi32(-1 << 31);
+    vec_u32 *p0 = (vec_u32*)(output + n4);
+    vec_u32 *p1 = (vec_u32*)(output + n4 * 3);
+
+    imdct_half_e2k(s, output + n4, input);
+
+    for (k = 0; k < n16; k++) {
+        vec_u32 a = p0[k] ^ sign;
+        vec_u32 b = p1[-1 - k];
+        p0[-1 - k] = _mm_shuffle_epi32(a, 0x1b);
+        p1[k]      = _mm_shuffle_epi32(b, 0x1b);
+    }
+}
+
+av_cold void ff_fft_init_e2k(FFTContext *s)
+{
+    if (!E2K_BASE(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+    // libavcodec/tests/fft -n 2..14 [-i]
+    // libavcodec/tests/fft -{m|d|r} -n 4..14 [-i]
+
+    s->fft_calc = ff_fft_calc_interleave_e2k;
+
+    if (s->mdct_bits >= 5) {
+        s->imdct_calc = imdct_calc_e2k;
+        s->imdct_half = imdct_half_e2k;
+    }
+}
diff --git a/libavcodec/e2k/fmtconvert.c b/libavcodec/e2k/fmtconvert.c
new file mode 100644
index 0000000..bfd9cb5
--- /dev/null
+++ b/libavcodec/e2k/fmtconvert.c
@@ -0,0 +1,55 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2006 Luca Barbato <lu_zero@gentoo.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include "libavutil/mem.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+#include "libavcodec/fmtconvert.h"
+
+static void int32_to_float_fmul_scalar_e2k(float *dst, const int32_t *src,
+                                           float mul, int len)
+{
+    int i;
+    __m128 src1, src2, dst1, dst2, mul_v;
+    mul_v = _mm_set1_ps(mul);
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < len; i += 8) {
+        src1 = _mm_cvtepi32_ps(VEC_LD(src + i));
+        src2 = _mm_cvtepi32_ps(VEC_LD(src + i + 4));
+        dst1 = _mm_mul_ps(src1, mul_v);
+        dst2 = _mm_mul_ps(src2, mul_v);
+        _mm_storeu_ps(dst + i, dst1);
+        _mm_storeu_ps(dst + i + 4, dst2);
+    }
+}
+
+av_cold void ff_fmt_convert_init_e2k(FmtConvertContext *c,
+                                     AVCodecContext *avctx)
+{
+    if (!E2K_BASE(av_get_cpu_flags()))
+        return;
+
+    c->int32_to_float_fmul_scalar = int32_to_float_fmul_scalar_e2k;
+}
diff --git a/libavcodec/e2k/h264chroma_init.c b/libavcodec/e2k/h264chroma_init.c
new file mode 100644
index 0000000..802a26c
--- /dev/null
+++ b/libavcodec/e2k/h264chroma_init.c
@@ -0,0 +1,63 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2004 Romain Dolbeau <romain@dolbeau.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include "libavutil/intreadwrite.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/h264chroma.h"
+
+#define PUT_OP_U8_E2K(d, s, dst) d = s
+#define AVG_OP_U8_E2K(d, s, dst) d = _mm_avg_epu8(dst, s)
+
+#define OP_U8_E2K                          PUT_OP_U8_E2K
+#define PREFIX_h264_chroma_mc8_e2k         put_h264_chroma_mc8_e2k
+#define PREFIX_h264_chroma_mc8_num         e2k_put_h264_chroma_mc8_num
+#include "h264chroma_template.c"
+#undef OP_U8_E2K
+#undef PREFIX_h264_chroma_mc8_e2k
+#undef PREFIX_h264_chroma_mc8_num
+
+#define OP_U8_E2K                          AVG_OP_U8_E2K
+#define PREFIX_h264_chroma_mc8_e2k         avg_h264_chroma_mc8_e2k
+#define PREFIX_h264_chroma_mc8_num         e2k_avg_h264_chroma_mc8_num
+#include "h264chroma_template.c"
+#undef OP_U8_E2K
+#undef PREFIX_h264_chroma_mc8_e2k
+#undef PREFIX_h264_chroma_mc8_num
+
+av_cold void ff_h264chroma_init_e2k(H264ChromaContext *c, int bit_depth)
+{
+    const int high_bit_depth = bit_depth > 8;
+
+    if (!E2K_BASE(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+    if (!high_bit_depth) {
+        c->put_h264_chroma_pixels_tab[0] = put_h264_chroma_mc8_e2k;
+        c->avg_h264_chroma_pixels_tab[0] = avg_h264_chroma_mc8_e2k;
+    }
+}
diff --git a/libavcodec/e2k/h264chroma_template.c b/libavcodec/e2k/h264chroma_template.c
new file mode 100644
index 0000000..cc0d3f1
--- /dev/null
+++ b/libavcodec/e2k/h264chroma_template.c
@@ -0,0 +1,139 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2004 Romain Dolbeau <romain@dolbeau.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/mem.h"
+#include "libavutil/e2k/util_e2k.h"
+
+/* this code assume that stride % 16 == 0 */
+
+#define CHROMA_MC8_E2K_CORE() \
+    vsrc2ssH = _mm_unpacklo_epi8(vsrc2uc, zerov); \
+    vsrc3ssH = _mm_unpacklo_epi8(vsrc3uc, zerov); \
+    \
+    psum = _mm_mullo_epi16(vA, vsrc0ssH); \
+    psum = _mm_add_epi16(_mm_mullo_epi16(vB, vsrc1ssH), psum); \
+    psum1 = _mm_mullo_epi16(vC, vsrc2ssH); \
+    psum1 = _mm_add_epi16(_mm_mullo_epi16(vD, vsrc3ssH), psum1); \
+    psum = _mm_add_epi16(_mm_add_epi16(psum, psum1), bias); \
+    psum = _mm_srai_epi16(psum, 6);\
+    vfdst = _mm_packus_epi16(psum, psum); \
+    OP_U8_E2K(fsum, vfdst, VEC_LD8(dst)); \
+    VEC_STL(dst, fsum); \
+    vsrc0ssH = vsrc2ssH; \
+    vsrc1ssH = vsrc3ssH; \
+    dst += stride; \
+    src += stride;
+
+#define CHROMA_MC8_E2K_CORE_SIMPLE(vA, vB) \
+    vsrc0ssH = _mm_unpacklo_epi8(vsrc0uc, zerov); \
+    vsrc1ssH = _mm_unpacklo_epi8(vsrc1uc, zerov); \
+    \
+    psum = _mm_mullo_epi16(vA, vsrc0ssH); \
+    psum = _mm_add_epi16(_mm_mullo_epi16(vB, vsrc1ssH), psum); \
+    psum = _mm_add_epi16(psum, bias); \
+    psum = _mm_srai_epi16(psum, 6); \
+    vfdst = _mm_packus_epi16(psum, psum); \
+    OP_U8_E2K(fsum, vfdst, VEC_LD8(dst)); \
+    VEC_STL(dst, fsum); \
+    dst += stride; \
+    src += stride;
+
+#define GET_VSRC1(vs0, s)     \
+    vs0 = VEC_LD8(s);
+
+#define GET_VSRC(vs0, vs1, s) \
+    vs0 = VEC_LD8(s);         \
+    vs1 = VEC_LD8(s + 1);       
+
+#ifdef PREFIX_h264_chroma_mc8_e2k
+static void PREFIX_h264_chroma_mc8_e2k(uint8_t *dst, uint8_t *src,
+                                       ptrdiff_t stride, int h,
+                                       int x, int y)
+{
+    int i;
+    LOAD_ZERO;
+    const vec_s16 vA = _mm_set1_epi16((8 - x) * (8 - y));
+    const vec_s16 vB = _mm_set1_epi16((    x) * (8 - y));
+    const vec_s16 vC = _mm_set1_epi16((8 - x) * y);
+    const vec_s16 vD = _mm_set1_epi16((    x) * y);
+    const vec_s16 bias = _mm_set1_epi16(32);
+
+    vec_u8 vsrc0uc, vsrc1uc, vsrc2uc, vsrc3uc;
+    vec_s16 vsrc0ssH, vsrc1ssH, vsrc2ssH, vsrc3ssH, psum, psum1;
+    vec_u8 vfdst, fsum;
+
+    if (y) {
+        GET_VSRC(vsrc0uc, vsrc1uc, src);
+        vsrc0ssH = _mm_unpacklo_epi8(vsrc0uc, zerov);
+        vsrc1ssH = _mm_unpacklo_epi8(vsrc1uc, zerov);
+        src += stride;
+        if (x) {
+            for (i = 0; i < h; i++) {
+                GET_VSRC(vsrc2uc, vsrc3uc, src);
+                CHROMA_MC8_E2K_CORE();
+            }
+        } else {
+            for (i = 0; i < h; i++) {
+                GET_VSRC1(vsrc1uc, src);
+                CHROMA_MC8_E2K_CORE_SIMPLE(vA, vC);
+                vsrc0uc = vsrc1uc;
+            }
+        }
+    } else {
+        for (i = 0; i < h; i++) {
+            GET_VSRC(vsrc0uc, vsrc1uc, src);
+            CHROMA_MC8_E2K_CORE_SIMPLE(vA, vB);
+        }
+    }
+}
+#endif
+
+/* this code assume that stride % 16 == 0 */
+#ifdef PREFIX_no_rnd_vc1_chroma_mc8_e2k
+static void PREFIX_no_rnd_vc1_chroma_mc8_e2k(uint8_t *dst, uint8_t *src,
+                                             ptrdiff_t stride, int h,
+                                             int x, int y)
+{
+    int i;
+    LOAD_ZERO;
+    const vec_s16 vA = _mm_set1_epi16((8 - x) * (8 - y));
+    const vec_s16 vB = _mm_set1_epi16((    x) * (8 - y));
+    const vec_s16 vC = _mm_set1_epi16((8 - x) * y);
+    const vec_s16 vD = _mm_set1_epi16((    x) * y);
+    const vec_s16 bias = _mm_set1_epi16(28);
+
+    vec_u8 vsrc0uc, vsrc1uc, vsrc2uc, vsrc3uc;
+    vec_s16 vsrc0ssH, vsrc1ssH, vsrc2ssH, vsrc3ssH, psum, psum1;
+    vec_u8 vfdst, fsum;
+
+    GET_VSRC(vsrc0uc, vsrc1uc, src);
+    src += stride;
+
+    vsrc0ssH = _mm_unpacklo_epi8(vsrc0uc, zerov);
+    vsrc1ssH = _mm_unpacklo_epi8(vsrc1uc, zerov);
+
+    for (i = 0; i < h; i++) {
+        GET_VSRC(vsrc2uc, vsrc3uc, src);
+        CHROMA_MC8_E2K_CORE();
+    }
+}
+#endif
+
diff --git a/libavcodec/e2k/h264dsp.c b/libavcodec/e2k/h264dsp.c
new file mode 100644
index 0000000..23d0e4b
--- /dev/null
+++ b/libavcodec/e2k/h264dsp.c
@@ -0,0 +1,708 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2004 Romain Dolbeau <romain@dolbeau.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include <stdint.h>
+#include <string.h>
+
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include "libavutil/intreadwrite.h"
+#include "libavutil/mem.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/h264dec.h"
+#include "libavcodec/h264dsp.h"
+
+/****************************************************************************
+ * IDCT transform:
+ ****************************************************************************/
+
+#define VEC_1D_DCT(vb0, vb1, vb2, vb3, va0, va1, va2, va3)         \
+    /* 1st stage */                                                \
+    vz0 = _mm_add_epi16(vb0, vb2); /* temp[0] = Y[0] + Y[2] */     \
+    vz1 = _mm_sub_epi16(vb0, vb2); /* temp[1] = Y[0] - Y[2] */     \
+    vz2 = _mm_srai_epi16(vb1, 1);                                  \
+    vz2 = _mm_sub_epi16(vz2, vb3); /* temp[2] = Y[1].1/2 - Y[3] */ \
+    vz3 = _mm_srai_epi16(vb3, 1);                                  \
+    vz3 = _mm_add_epi16(vb1, vz3); /* temp[3] = Y[1] + Y[3].1/2 */ \
+    /* 2nd stage: output */                                        \
+    va0 = _mm_add_epi16(vz0, vz3); /* x[0] = temp[0] + temp[3] */  \
+    va1 = _mm_add_epi16(vz1, vz2); /* x[1] = temp[1] + temp[2] */  \
+    va2 = _mm_sub_epi16(vz1, vz2); /* x[2] = temp[1] - temp[2] */  \
+    va3 = _mm_sub_epi16(vz0, vz3)  /* x[3] = temp[0] - temp[3] */
+
+#define VEC_TRANSPOSE_4(a0, a1, a2, a3, b0, b1, b2, b3) \
+    b0 = _mm_unpacklo_epi16(a0, a0); \
+    b1 = _mm_unpacklo_epi16(a1, a0); \
+    b2 = _mm_unpacklo_epi16(a2, a0); \
+    b3 = _mm_unpacklo_epi16(a3, a0); \
+    a0 = _mm_unpacklo_epi16(b0, b2); \
+    a1 = _mm_unpackhi_epi16(b0, b2); \
+    a2 = _mm_unpacklo_epi16(b1, b3); \
+    a3 = _mm_unpackhi_epi16(b1, b3); \
+    b0 = _mm_unpacklo_epi16(a0, a2); \
+    b1 = _mm_unpackhi_epi16(a0, a2); \
+    b2 = _mm_unpacklo_epi16(a1, a3); \
+    b3 = _mm_unpackhi_epi16(a1, a3)
+
+#define VEC_LOAD_U8_ADD_S16_STORE_U8(va)               \
+    va = _mm_srai_epi16(va, 6);                        \
+    vdst = _mm_cvtsi32_si128(*(uint32_t*)dst);         \
+    vdst = _mm_unpacklo_epi8(vdst, zerov);             \
+    va = _mm_add_epi16(va, vdst);                      \
+    va = _mm_packus_epi16(va, va);                     \
+    *(uint32_t*)dst = _mm_extract_epi32(va, 0);        \
+    dst += stride;
+
+static void h264_idct_add_e2k(uint8_t *dst, int16_t *block, int stride)
+{
+    vec_s16 va0, va1, va2, va3;
+    vec_s16 vz0, vz1, vz2, vz3;
+    vec_s16 vtmp0, vtmp1, vtmp2, vtmp3;
+    vec_u8 vdst;
+    LOAD_ZERO;
+
+    block[0] += 32;  /* add 32 as a DC-level for rounding */
+
+    vtmp0 = VEC_LD(block);
+    vtmp1 = VEC_ALIGNR8(vtmp0, vtmp0);
+    vtmp2 = VEC_LD(block + 8);
+    vtmp3 = VEC_ALIGNR8(vtmp2, vtmp2);
+    VEC_ST(block, zerov);
+    VEC_ST(block + 8, zerov);
+
+    VEC_1D_DCT(vtmp0, vtmp1, vtmp2, vtmp3, va0, va1, va2, va3);
+    VEC_TRANSPOSE_4(va0, va1, va2, va3, vtmp0, vtmp1, vtmp2, vtmp3);
+    VEC_1D_DCT(vtmp0, vtmp1, vtmp2, vtmp3, va0, va1, va2, va3);
+
+    VEC_LOAD_U8_ADD_S16_STORE_U8(va0);
+    VEC_LOAD_U8_ADD_S16_STORE_U8(va1);
+    VEC_LOAD_U8_ADD_S16_STORE_U8(va2);
+    VEC_LOAD_U8_ADD_S16_STORE_U8(va3);
+}
+
+#define IDCT8_1D_E2K(s0, s1, s2, s3, s4, s5, s6, s7, d0, d1, d2, d3, d4, d5, d6, d7) { \
+    /* a0 = SRC(0) + SRC(4); */      \
+    /* a2 = SRC(0) - SRC(4); */      \
+    /* a4 = (SRC(2)>>1) - SRC(6); */ \
+    /* a6 = (SRC(6)>>1) + SRC(2); */ \
+    vec_s16 a0v = _mm_add_epi16(s0, s4); \
+    vec_s16 a2v = _mm_sub_epi16(s0, s4); \
+    vec_s16 a4v = _mm_sub_epi16(_mm_srai_epi16(s2, 1), s6); \
+    vec_s16 a6v = _mm_add_epi16(_mm_srai_epi16(s6, 1), s2); \
+    /* b0 = a0 + a6; */ \
+    /* b2 = a2 + a4; */ \
+    /* b4 = a2 - a4; */ \
+    /* b6 = a0 - a6; */ \
+    vec_s16 b0v = _mm_add_epi16(a0v, a6v);  \
+    vec_s16 b2v = _mm_add_epi16(a2v, a4v);  \
+    vec_s16 b4v = _mm_sub_epi16(a2v, a4v);  \
+    vec_s16 b6v = _mm_sub_epi16(a0v, a6v);  \
+    /* a1 = (SRC(5)-SRC(3)) - (SRC(7) + (SRC(7)>>1)); */ \
+    /* a3 = (SRC(7)+SRC(1)) - (SRC(3) + (SRC(3)>>1)); */ \
+    /* a5 = (SRC(7)-SRC(1)) + (SRC(5) + (SRC(5)>>1)); */ \
+    /* a7 = (SRC(5)+SRC(3)) + (SRC(1) + (SRC(1)>>1)); */ \
+    vec_s16 a1v = _mm_sub_epi16(_mm_sub_epi16(s5, s3), _mm_add_epi16(s7, _mm_srai_epi16(s7, 1))); \
+    vec_s16 a3v = _mm_sub_epi16(_mm_add_epi16(s7, s1), _mm_add_epi16(s3, _mm_srai_epi16(s3, 1))); \
+    vec_s16 a5v = _mm_add_epi16(_mm_sub_epi16(s7, s1), _mm_add_epi16(s5, _mm_srai_epi16(s5, 1))); \
+    vec_s16 a7v = _mm_add_epi16(_mm_add_epi16(s5, s3), _mm_add_epi16(s1, _mm_srai_epi16(s1, 1))); \
+    /* b1 = (a7>>2) + a1; */ \
+    /* b3 = a3 + (a5>>2); */ \
+    /* b5 = (a3>>2) - a5; */ \
+    /* b7 = a7 - (a1>>2); */ \
+    vec_s16 b1v = _mm_add_epi16(_mm_srai_epi16(a7v, 2), a1v); \
+    vec_s16 b3v = _mm_add_epi16(a3v, _mm_srai_epi16(a5v, 2)); \
+    vec_s16 b5v = _mm_sub_epi16(_mm_srai_epi16(a3v, 2), a5v); \
+    vec_s16 b7v = _mm_sub_epi16(a7v, _mm_srai_epi16(a1v, 2)); \
+    /* DST(0, b0 + b7); */ \
+    /* DST(1, b2 + b5); */ \
+    /* DST(2, b4 + b3); */ \
+    /* DST(3, b6 + b1); */ \
+    /* DST(4, b6 - b1); */ \
+    /* DST(5, b4 - b3); */ \
+    /* DST(6, b2 - b5); */ \
+    /* DST(7, b0 - b7); */ \
+    d0 = _mm_add_epi16(b0v, b7v); \
+    d1 = _mm_add_epi16(b2v, b5v); \
+    d2 = _mm_add_epi16(b4v, b3v); \
+    d3 = _mm_add_epi16(b6v, b1v); \
+    d4 = _mm_sub_epi16(b6v, b1v); \
+    d5 = _mm_sub_epi16(b4v, b3v); \
+    d6 = _mm_sub_epi16(b2v, b5v); \
+    d7 = _mm_sub_epi16(b0v, b7v); \
+}
+
+#define E2K_STORE_SUM_CLIP(dest, idctv) {     \
+    /* unaligned load */                      \
+    __m128i dstv = VEC_LD8(dest);             \
+    dstv = _mm_unpacklo_epi8(dstv, zerov);    \
+    idctv = _mm_srai_epi16(idctv, 6);         \
+    dstv = _mm_add_epi16(dstv, idctv);        \
+    dstv = _mm_packus_epi16(dstv, dstv);      \
+    /* unaligned store */                     \
+    VEC_STL(dest, dstv);                      \
+}
+
+static void h264_idct8_add_e2k(uint8_t *dst, int16_t *dct, int stride)
+{
+    vec_s16 s0, s1, s2, s3, s4, s5, s6, s7;
+    vec_s16 d0, d1, d2, d3, d4, d5, d6, d7;
+    vec_s16 idct0, idct1, idct2, idct3, idct4, idct5, idct6, idct7;
+
+    LOAD_ZERO;
+
+    dct[0] += 32; // rounding for the >>6 at the end
+
+    s0 = VEC_LD(dct + 8 * 0);
+    s1 = VEC_LD(dct + 8 * 1);
+    s2 = VEC_LD(dct + 8 * 2);
+    s3 = VEC_LD(dct + 8 * 3);
+    s4 = VEC_LD(dct + 8 * 4);
+    s5 = VEC_LD(dct + 8 * 5);
+    s6 = VEC_LD(dct + 8 * 6);
+    s7 = VEC_LD(dct + 8 * 7);
+    VEC_ST(dct + 8 * 0, zerov);
+    VEC_ST(dct + 8 * 1, zerov);
+    VEC_ST(dct + 8 * 2, zerov);
+    VEC_ST(dct + 8 * 3, zerov);
+    VEC_ST(dct + 8 * 4, zerov);
+    VEC_ST(dct + 8 * 5, zerov);
+    VEC_ST(dct + 8 * 6, zerov);
+    VEC_ST(dct + 8 * 7, zerov);
+
+    IDCT8_1D_E2K(s0, s1, s2, s3, s4, s5, s6, s7,
+                     d0, d1, d2, d3, d4, d5, d6, d7);
+
+    TRANSPOSE8(d0, d1, d2, d3, d4, d5, d6, d7);
+
+    IDCT8_1D_E2K(d0, d1, d2, d3, d4, d5, d6, d7,
+                     idct0, idct1, idct2, idct3, idct4, idct5, idct6, idct7);
+
+    E2K_STORE_SUM_CLIP(&dst[0*stride], idct0);
+    E2K_STORE_SUM_CLIP(&dst[1*stride], idct1);
+    E2K_STORE_SUM_CLIP(&dst[2*stride], idct2);
+    E2K_STORE_SUM_CLIP(&dst[3*stride], idct3);
+    E2K_STORE_SUM_CLIP(&dst[4*stride], idct4);
+    E2K_STORE_SUM_CLIP(&dst[5*stride], idct5);
+    E2K_STORE_SUM_CLIP(&dst[6*stride], idct6);
+    E2K_STORE_SUM_CLIP(&dst[7*stride], idct7);
+}
+
+static void h264_idct_dc_add_e2k(uint8_t *dst, int16_t *block, int stride)
+{
+    __m64 dc16, zerov = _mm_setzero_si64();
+    __m64 dcplus, dcminus, v0, v1, v2, v3;
+    int i, dc;
+
+    dc = (block[0] + 32) >> 6;
+    block[0] = 0;
+    dc16 = _mm_set1_pi16(dc);
+    dcplus = _mm_packs_pu16(dc16, dc16);
+    dc16 = _mm_sub_pi16(zerov, dc16);
+    dcminus = _mm_packs_pu16(dc16, dc16);
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < 4; i += 4) {
+        v0 = _mm_cvtsi32_si64(*(uint32_t*)(dst + 0 * stride));
+        v1 = _mm_cvtsi32_si64(*(uint32_t*)(dst + 1 * stride));
+        v2 = _mm_cvtsi32_si64(*(uint32_t*)(dst + 2 * stride));
+        v3 = _mm_cvtsi32_si64(*(uint32_t*)(dst + 3 * stride));
+
+        v0 = _mm_unpacklo_pi32(v0, v1);
+        v2 = _mm_unpacklo_pi32(v2, v3);
+        v0 = _mm_adds_pu8(v0, dcplus);
+        v2 = _mm_adds_pu8(v2, dcplus);
+        v0 = _mm_subs_pu8(v0, dcminus);
+        v2 = _mm_subs_pu8(v2, dcminus);
+
+        *(uint32_t*)(dst + 0 * stride) = _mm_extract_pi32(v0, 0);
+        *(uint32_t*)(dst + 1 * stride) = _mm_extract_pi32(v0, 1);
+        *(uint32_t*)(dst + 2 * stride) = _mm_extract_pi32(v2, 0);
+        *(uint32_t*)(dst + 3 * stride) = _mm_extract_pi32(v2, 1);
+        dst += 4 * stride;
+    }
+}
+
+static void h264_idct8_dc_add_e2k(uint8_t *dst, int16_t *block, int stride)
+{
+    vec_s16 dc16;
+    vec_u8 dcplus, dcminus, v0, v1, v2, v3;
+    LOAD_ZERO;
+    int i, dc;
+
+    dc = (block[0] + 32) >> 6;
+    block[0] = 0;
+    dc16 = _mm_set1_epi16(dc);
+    dcplus = _mm_packus_epi16(dc16, dc16);
+    dc16 = _mm_sub_epi16(zerov, dc16);
+    dcminus = _mm_packus_epi16(dc16, dc16);
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < 8; i += 4) {
+        v0 = VEC_LD8(dst + 0 * stride);
+        v1 = VEC_LD8(dst + 1 * stride);
+        v2 = VEC_LD8(dst + 2 * stride);
+        v3 = VEC_LD8(dst + 3 * stride);
+
+        v0 = _mm_unpacklo_epi64(v0, v1);
+        v2 = _mm_unpacklo_epi64(v2, v3);
+        v0 = _mm_adds_epu8(v0, dcplus);
+        v2 = _mm_adds_epu8(v2, dcplus);
+        v0 = _mm_subs_epu8(v0, dcminus);
+        v2 = _mm_subs_epu8(v2, dcminus);
+
+        VEC_STL(dst + 0 * stride, v0);
+        VEC_STH(dst + 1 * stride, v0);
+        VEC_STL(dst + 2 * stride, v2);
+        VEC_STH(dst + 3 * stride, v2);
+        dst += 4 * stride;
+    }
+}
+
+static void h264_idct_add16_e2k(uint8_t *dst, const int *block_offset,
+                                    int16_t *block, int stride,
+                                    const uint8_t nnzc[15 * 8])
+{
+    int i;
+    for (i = 0; i < 16; i++) {
+        int nnz = nnzc[scan8[i]];
+        if (nnz) {
+            if (nnz == 1 && block[i * 16])
+                h264_idct_dc_add_e2k(dst + block_offset[i], block + i * 16, stride);
+            else                           
+                h264_idct_add_e2k(dst + block_offset[i], block + i * 16, stride);
+        }
+    }
+}
+
+static void h264_idct_add16intra_e2k(uint8_t *dst, const int *block_offset,
+                                         int16_t *block, int stride,
+                                         const uint8_t nnzc[15 * 8])
+{
+    int i;
+    for (i = 0; i < 16; i++) {
+        if (nnzc[scan8[i]])
+            h264_idct_add_e2k(dst + block_offset[i], block + i * 16, stride);
+        else if(block[i * 16])
+            h264_idct_dc_add_e2k(dst + block_offset[i], block + i * 16, stride);
+    }
+}
+
+static void h264_idct8_add4_e2k(uint8_t *dst, const int *block_offset,
+                                    int16_t *block, int stride,
+                                    const uint8_t nnzc[15 * 8])
+{
+    int i;
+    for (i = 0; i < 16; i += 4){
+        int nnz = nnzc[scan8[i]];
+        if (nnz) {
+            if (nnz == 1 && block[i * 16]) 
+                h264_idct8_dc_add_e2k(dst + block_offset[i], block + i * 16, stride);
+            else
+                h264_idct8_add_e2k(dst + block_offset[i], block + i * 16, stride);
+        }
+    }
+}
+
+static void h264_idct_add8_e2k(uint8_t **dest, const int *block_offset,
+                                   int16_t *block, int stride,
+                                   const uint8_t nnzc[15 * 8])
+{
+    int i, j;
+    for (j = 1; j < 3; j++) {
+        for (i = j * 16; i < j * 16 + 4; i++) {
+            if (nnzc[scan8[i]])
+                h264_idct_add_e2k(dest[j - 1] + block_offset[i], block + i * 16, stride);
+            else if (block[i * 16])
+                h264_idct_dc_add_e2k(dest[j - 1] + block_offset[i], block + i * 16, stride);
+        }
+    }
+}
+
+#define transpose4x16(r0, r1, r2, r3) {             \
+    vec_u8 r4, r5, r6, r7;                          \
+                                                    \
+    r4 = _mm_unpacklo_epi8(r0, r2);  /*0, 2 set 0*/ \
+    r5 = _mm_unpackhi_epi8(r0, r2);  /*0, 2 set 1*/ \
+    r6 = _mm_unpacklo_epi8(r1, r3);  /*1, 3 set 0*/ \
+    r7 = _mm_unpackhi_epi8(r1, r3);  /*1, 3 set 1*/ \
+                                                    \
+    r0 = _mm_unpacklo_epi8(r4, r6);  /*all set 0*/  \
+    r1 = _mm_unpackhi_epi8(r4, r6);  /*all set 1*/  \
+    r2 = _mm_unpacklo_epi8(r5, r7);  /*all set 2*/  \
+    r3 = _mm_unpackhi_epi8(r5, r7);  /*all set 3*/  \
+}
+
+#define WRITE4(i, j) ((uint32_t*)dst)[(i * 4 + j) * (dst_stride >> 2)] = _mm_extract_epi32(r##i, j);
+
+static av_always_inline void write16x4(uint8_t *dst, int dst_stride,
+                                       vec_u8 r0, vec_u8 r1,
+                                       vec_u8 r2, vec_u8 r3) {
+
+    WRITE4(0, 0) WRITE4(0, 1) WRITE4(0, 2) WRITE4(0, 3)
+    WRITE4(1, 0) WRITE4(1, 1) WRITE4(1, 2) WRITE4(1, 3)
+    WRITE4(2, 0) WRITE4(2, 1) WRITE4(2, 2) WRITE4(2, 3)
+    WRITE4(3, 0) WRITE4(3, 1) WRITE4(3, 2) WRITE4(3, 3)
+}
+
+/** \brief performs a 6x16 transpose of data in src, and stores it to dst */
+#define read_and_transpose16x6(src, st, r8, r9, r10, r11, r12, r13) { \
+    vec_u8 r0, r1, r2, r3, r4, r5, r6, r7, r14, r15;                \
+    r0 = VEC_LD8(src);                                                \
+    r1 = VEC_LD8(src + st);                                           \
+    r2 = VEC_LD8(src + st * 2);                                       \
+    r3 = VEC_LD8(src + st * 3);                                       \
+    r4 = VEC_LD8(src + st * 4);                                       \
+    r5 = VEC_LD8(src + st * 5);                                       \
+    r6 = VEC_LD8(src + st * 6);                                       \
+    r7 = VEC_LD8(src + st * 7);                                       \
+    r8 = VEC_LD8(src + st * 8);                                       \
+    r9 = VEC_LD8(src + st * 9);                                       \
+    r10 = VEC_LD8(src + st * 10);                                     \
+    r11 = VEC_LD8(src + st * 11);                                     \
+    r12 = VEC_LD8(src + st * 12);                                     \
+    r13 = VEC_LD8(src + st * 13);                                     \
+    r14 = VEC_LD8(src + st * 14);                                     \
+    r15 = VEC_LD8(src + st * 15);                                     \
+                                                                      \
+    /*Merge first pairs*/                                             \
+    r0 = _mm_unpacklo_epi8(r0, r4);    /*  0, 4 */                    \
+    r1 = _mm_unpacklo_epi8(r1, r5);    /*  1, 5 */                    \
+    r2 = _mm_unpacklo_epi8(r2, r6);    /*  2, 6 */                    \
+    r3 = _mm_unpacklo_epi8(r3, r7);    /*  3, 7 */                    \
+    r4 = _mm_unpacklo_epi8(r8, r12);   /*  8,12 */                    \
+    r5 = _mm_unpacklo_epi8(r9, r13);   /*  9,13 */                    \
+    r6 = _mm_unpacklo_epi8(r10, r14);  /* 10,14 */                    \
+    r7 = _mm_unpacklo_epi8(r11, r15);  /* 11,15 */                    \
+                                                                      \
+    /*Merge second pairs*/                                            \
+    r8  = _mm_unpacklo_epi8(r0, r2);   /* 0, 2, 4, 6 set 0 */         \
+    r9  = _mm_unpackhi_epi8(r0, r2);   /* 0, 2, 4, 6 set 1 */         \
+    r10 = _mm_unpacklo_epi8(r1, r3);   /* 1, 3, 5, 7 set 0 */         \
+    r11 = _mm_unpackhi_epi8(r1, r3);   /* 1, 3, 5, 7 set 1 */         \
+    r12 = _mm_unpacklo_epi8(r4, r6);   /* 8,10,12,14 set 0 */         \
+    r13 = _mm_unpackhi_epi8(r4, r6);   /* 8,10,12,14 set 1 */         \
+    r14 = _mm_unpacklo_epi8(r5, r7);   /* 9,11,13,15 set 0 */         \
+    r15 = _mm_unpackhi_epi8(r5, r7);   /* 9,11,13,15 set 1 */         \
+                                                                      \
+    /*Third merge*/                                                   \
+    r0 = _mm_unpacklo_epi8(r8, r10);   /* 0..7 set 0  */              \
+    r1 = _mm_unpackhi_epi8(r8, r10);   /* 0..7 set 1  */              \
+    r2 = _mm_unpacklo_epi8(r9, r11);   /* 0..7 set 2  */              \
+    r4 = _mm_unpacklo_epi8(r12, r14);  /* 8..15 set 0 */              \
+    r5 = _mm_unpackhi_epi8(r12, r14);  /* 8..15 set 1 */              \
+    r6 = _mm_unpacklo_epi8(r13, r15);  /* 8..15 set 2 */              \
+    /* Don't need to compute 3 and 7*/                                \
+                                                                      \
+    /*Final merge*/                                                   \
+    r8  = _mm_unpacklo_epi64(r0, r4);   /* all set 0 */               \
+    r9  = _mm_unpackhi_epi64(r0, r4);   /* all set 1 */               \
+    r10 = _mm_unpacklo_epi64(r1, r5);   /* all set 2 */               \
+    r11 = _mm_unpackhi_epi64(r1, r5);   /* all set 3 */               \
+    r12 = _mm_unpacklo_epi64(r2, r6);   /* all set 4 */               \
+    r13 = _mm_unpackhi_epi64(r2, r6);   /* all set 5 */               \
+    /* Don't need to compute 14 and 15*/                              \
+}
+
+// out: o = |x-y| < a
+static av_always_inline vec_u8 diff_lt_e2k(vec_u8 x, vec_u8 y, vec_u8 a) {
+    vec_u8 o = _mm_or_si128(_mm_subs_epu8(x, y), _mm_subs_epu8(y, x)); /* |x-y| */
+    return _mm_cmpgt_epi8(a, _mm_xor_si128(o, _mm_set1_epi8(-128)));
+}
+
+static av_always_inline vec_u8 h264_deblock_mask(vec_u8 p0, vec_u8 p1, vec_u8 q0,
+                                                 vec_u8 q1, vec_u8 alpha, vec_u8 beta) {
+    vec_u8 mask, tempmask;
+
+    mask = diff_lt_e2k(p0, q0, alpha);
+    tempmask = diff_lt_e2k(p1, p0, beta);
+    mask = _mm_and_si128(mask, tempmask);
+    tempmask = diff_lt_e2k(q1, q0, beta);
+    mask = _mm_and_si128(mask, tempmask);
+
+    return mask;
+}
+
+// out: newp1 = clip((p2 + ((p0 + q0 + 1) >> 1)) >> 1, p1-tc0, p1+tc0)
+static av_always_inline vec_u8 h264_deblock_q1(vec_u8 p0, vec_u8 p1, vec_u8 p2,
+                                               vec_u8 q0, vec_u8 tc0) {
+
+    vec_u8 average = _mm_avg_epu8(p0, q0);
+    vec_u8 temp, uncliped;
+    vec_u8 ones = _mm_set1_epi8(1), maxv, minv;
+
+    temp = _mm_xor_si128(average, p2);
+    average = _mm_avg_epu8(average, p2);     /* avg(p2, avg(p0, q0))   */
+    temp = _mm_and_si128(temp, ones);        /* (p2^avg(p0, q0)) & 1   */
+    uncliped = _mm_subs_epu8(average, temp); /* (p2+((p0+q0+1)>>1))>>1 */
+    maxv = _mm_adds_epu8(p1, tc0);
+    minv = _mm_subs_epu8(p1, tc0);
+    return _mm_min_epu8(maxv, _mm_max_epu8(minv, uncliped));
+}
+
+#define h264_deblock_p0_q0(p0, p1, q0, q1, tc0masked) {                                              \
+    vec_u8 pq0bit = _mm_xor_si128(p0, q0);                                                           \
+    vec_u8 q1minus, p0minus, stage1, stage2, c127 = _mm_set1_epi8(127);                              \
+    vec_u8 vec160 = _mm_set1_epi8(160), delta, deltaneg, notv = _mm_set1_epi8(-1);                   \
+                                                                                                     \
+    q1minus = _mm_xor_si128(q1, notv);            /* 255 - q1 */                                     \
+    stage1 = _mm_avg_epu8(p1, q1minus);           /* (p1 - q1 + 256)>>1 */                           \
+    stage2 = _mm_srli_epi16(stage1, 1);                                                              \
+    stage2 = _mm_and_si128(stage2, c127);         /* (p1 - q1 + 256)>>2 = 64 + (p1 - q1) >> 2 */     \
+    p0minus = _mm_xor_si128(p0, notv);            /* 255 - p0 */                                     \
+    stage1 = _mm_avg_epu8(q0, p0minus);           /* (q0 - p0 + 256)>>1 */                           \
+    pq0bit = _mm_and_si128(pq0bit, _mm_set1_epi8(1));                                                \
+    stage2 = _mm_avg_epu8(stage2, pq0bit);        /* 32 + ((q0 - p0)&1 + (p1 - q1) >> 2 + 1) >> 1 */ \
+    stage2 = _mm_adds_epu8(stage2, stage1);       /* 160 + ((p0 - q0) + (p1 - q1) >> 2 + 1) >> 1 */  \
+    deltaneg = _mm_subs_epu8(vec160, stage2);     /* -d */                                           \
+    delta = _mm_subs_epu8(stage2, vec160);        /*  d */                                           \
+    deltaneg = _mm_min_epu8(tc0masked, deltaneg);                                                    \
+    delta = _mm_min_epu8(tc0masked, delta);                                                          \
+    p0 = _mm_subs_epu8(p0, deltaneg);                                                                \
+    q0 = _mm_subs_epu8(q0, delta);                                                                   \
+    p0 = _mm_adds_epu8(p0, delta);                                                                   \
+    q0 = _mm_adds_epu8(q0, deltaneg);                                                                \
+}
+
+#define h264_loop_filter_luma_e2k(p2, p1, p0, q0, q1, q2, alpha, beta, tc0) {                \
+    vec_u8 alphavec, betavec, mask, p1mask, q1mask;                                          \
+    vec_s8 tc0vec;                                                                           \
+    vec_u8 finaltc0, tc0masked, newp1, newq1;                                                \
+                                                                                             \
+    betavec = _mm_set1_epi8(beta - 128);                                                     \
+    alphavec = _mm_set1_epi8(alpha - 128);                                                   \
+    mask = h264_deblock_mask(p0, p1, q0, q1, alphavec, betavec); /* if in block */           \
+                                                                                             \
+    tc0vec = _mm_cvtsi32_si128(*(uint32_t*)tc0);                                             \
+    tc0vec = _mm_unpacklo_epi8(tc0vec, tc0vec);                                              \
+    tc0vec = _mm_unpacklo_epi8(tc0vec, tc0vec);                                              \
+    mask = _mm_blendv_epi8(mask, _mm_setzero_si128(), tc0vec);                               \
+    finaltc0 = _mm_and_si128(tc0vec, mask);                /* if (tc0[i] >= 0) tc = tc0 */   \
+                                                                                             \
+    p1mask = diff_lt_e2k(p2, p0, betavec);                                                   \
+    p1mask = _mm_and_si128(p1mask, mask);                       /* if(|p2 - p0| < beta) */   \
+    tc0masked = _mm_and_si128(p1mask, tc0vec);                                               \
+    finaltc0 = _mm_sub_epi8(finaltc0, p1mask);                  /* tc++ */                   \
+    newp1 = h264_deblock_q1(p0, p1, p2, q0, tc0masked);                                      \
+    /*end if*/                                                                               \
+                                                                                             \
+    q1mask = diff_lt_e2k(q2, q0, betavec);                                                   \
+    q1mask = _mm_and_si128(q1mask, mask);                       /* if(|q2 - q0| < beta) */   \
+    tc0masked = _mm_and_si128(q1mask, tc0vec);                                               \
+    finaltc0 = _mm_sub_epi8(finaltc0, q1mask);                  /* tc++ */                   \
+    newq1 = h264_deblock_q1(p0, q1, q2, q0, tc0masked);                                      \
+    /*end if*/                                                                               \
+                                                                                             \
+    h264_deblock_p0_q0(p0, p1, q0, q1, finaltc0);                                            \
+    p1 = newp1;                                                                              \
+    q1 = newq1;                                                                              \
+}
+
+static void h264_v_loop_filter_luma_e2k(uint8_t *pix, ptrdiff_t stride, int alpha, int beta, int8_t *tc0) {
+    if ((tc0[0] & tc0[1] & tc0[2] & tc0[3]) >= 0) {
+        vec_u8 p2, p1, p0, q0, q1, q2;
+        p2 = VEC_LD(pix - 3 * stride);
+        p1 = VEC_LD(pix - 2 * stride);
+        p0 = VEC_LD(pix - stride);
+        q0 = VEC_LD(pix);
+        q1 = VEC_LD(pix + stride);
+        q2 = VEC_LD(pix + 2 * stride);
+        h264_loop_filter_luma_e2k(p2, p1, p0, q0, q1, q2, alpha, beta, tc0);
+        VEC_ST(pix - 2 * stride, p1);
+        VEC_ST(pix - 1 * stride, p0);
+        VEC_ST(pix, q0);
+        VEC_ST(pix + stride, q1);
+    }
+}
+
+static void h264_h_loop_filter_luma_e2k(uint8_t *pix, ptrdiff_t stride, int alpha, int beta, int8_t *tc0) {
+    vec_u8 line0, line1, line2, line3, line4, line5;
+    if ((tc0[0] & tc0[1] & tc0[2] & tc0[3]) < 0) return;
+    read_and_transpose16x6(pix-3, stride, line0, line1, line2, line3, line4, line5);
+    h264_loop_filter_luma_e2k(line0, line1, line2, line3, line4, line5, alpha, beta, tc0);
+    transpose4x16(line1, line2, line3, line4);
+    write16x4(pix-2, stride, line1, line2, line3, line4);
+}
+
+static void weight_h264_pixels16_e2k(uint8_t *block, ptrdiff_t stride, int height,
+                                     int log2_denom, int weight, int offset)
+{
+    int y;
+    vec_u8 vblock;
+    vec_s16 vweight, voffset, v0, v1;
+    LOAD_ZERO;
+
+    offset <<= log2_denom;
+    if (log2_denom) offset += 1 << (log2_denom - 1);
+
+    vweight = _mm_set1_epi16(weight);
+    voffset = _mm_set1_epi16(offset);
+
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < height; y++) {
+        vblock = VEC_LD(block);
+        v0 = _mm_unpacklo_epi8(vblock, zerov);
+        v1 = _mm_unpackhi_epi8(vblock, zerov);
+
+        v0 = _mm_mullo_epi16(v0, vweight);
+        v1 = _mm_mullo_epi16(v1, vweight);
+        v0 = _mm_adds_epi16(v0, voffset);
+        v1 = _mm_adds_epi16(v1, voffset);
+        v0 = _mm_srai_epi16(v0, log2_denom);
+        v1 = _mm_srai_epi16(v1, log2_denom);
+
+        vblock = _mm_packus_epi16(v0, v1);
+        VEC_ST(block, vblock);
+        block += stride;
+    }
+}
+
+static void weight_h264_pixels8_e2k(uint8_t *block, ptrdiff_t stride, int height,
+                                    int log2_denom, int weight, int offset)
+{
+    int y;
+    vec_u8 vblock;
+    vec_s16 vweight, voffset, v0;
+    LOAD_ZERO;
+
+    offset <<= log2_denom;
+    if (log2_denom) offset += 1 << (log2_denom - 1);
+
+    vweight = _mm_set1_epi16(weight);
+    voffset = _mm_set1_epi16(offset);
+
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < height; y++) {
+        vblock = VEC_LD8(block);
+        v0 = _mm_unpacklo_epi8(vblock, zerov);
+
+        v0 = _mm_mullo_epi16(v0, vweight);
+        v0 = _mm_adds_epi16(v0, voffset);
+        v0 = _mm_srai_epi16(v0, log2_denom);
+
+        vblock = _mm_packus_epi16(v0, v0);
+        VEC_STL(block, vblock);
+        block += stride;
+    }
+}
+
+static void biweight_h264_pixels16_e2k(uint8_t *dst, uint8_t *src, ptrdiff_t stride, int height,
+                                       int log2_denom, int weightd, int weights, int offset)
+{
+    int y;
+    vec_u8 vsrc, vdst;
+    vec_s16 vweights, vweightd, voffset, v0, v1, v2, v3;
+    LOAD_ZERO;
+
+    offset = ((offset + 1) | 1) << log2_denom;
+    vweights = _mm_set1_epi16(weights);
+    vweightd = _mm_set1_epi16(weightd);
+    voffset = _mm_set1_epi16(offset);
+
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < height; y++) {
+        vdst = VEC_LD(dst);
+        vsrc = VEC_LD(src);
+        v0 = _mm_unpacklo_epi8(vdst, zerov);
+        v1 = _mm_unpackhi_epi8(vdst, zerov);
+        v2 = _mm_unpacklo_epi8(vsrc, zerov);
+        v3 = _mm_unpackhi_epi8(vsrc, zerov);
+
+        v0 = _mm_mullo_epi16(v0, vweightd);
+        v1 = _mm_mullo_epi16(v1, vweightd);
+        v2 = _mm_mullo_epi16(v2, vweights);
+        v3 = _mm_mullo_epi16(v3, vweights);
+        v0 = _mm_adds_epi16(v0, voffset);
+        v1 = _mm_adds_epi16(v1, voffset);
+        v0 = _mm_adds_epi16(v0, v2);
+        v1 = _mm_adds_epi16(v1, v3);
+        v0 = _mm_srai_epi16(v0, log2_denom + 1);
+        v1 = _mm_srai_epi16(v1, log2_denom + 1);
+
+        vdst = _mm_packus_epi16(v0, v1);
+        VEC_ST(dst, vdst);
+        dst += stride;
+        src += stride;
+    }
+}
+
+static void biweight_h264_pixels8_e2k(uint8_t *dst, uint8_t *src, ptrdiff_t stride, int height,
+                                      int log2_denom, int weightd, int weights, int offset)
+{
+    int y;
+    vec_u8 vsrc, vdst;
+    vec_s16 vweights, vweightd, voffset, v0, v2;
+    LOAD_ZERO;
+
+    offset = ((offset + 1) | 1) << log2_denom;
+    vweights = _mm_set1_epi16(weights);
+    vweightd = _mm_set1_epi16(weightd);
+    voffset = _mm_set1_epi16(offset);
+
+    PRAGMA_E2K("ivdep")
+    for (y = 0; y < height; y++) {
+        vdst = VEC_LD(dst);
+        vsrc = VEC_LD(src);
+        v0 = _mm_unpacklo_epi8(vdst, zerov);
+        v2 = _mm_unpacklo_epi8(vsrc, zerov);
+
+        v0 = _mm_mullo_epi16(v0, vweightd);
+        v2 = _mm_mullo_epi16(v2, vweights);
+        v0 = _mm_adds_epi16(v0, voffset);
+        v0 = _mm_adds_epi16(v0, v2);
+        v0 = _mm_srai_epi16(v0, log2_denom + 1);
+
+        vdst = _mm_packus_epi16(v0, v0);
+        VEC_STL(dst, vdst);
+        dst += stride;
+        src += stride;
+    }
+}
+
+av_cold void ff_h264dsp_init_e2k(H264DSPContext *c, const int bit_depth,
+                                 const int chroma_format_idc)
+{
+    if (!E2K_BASE(av_get_cpu_flags()))
+        return;
+
+    if (bit_depth == 8) {
+        c->h264_idct_add = h264_idct_add_e2k;
+        if (chroma_format_idc <= 1)
+            c->h264_idct_add8 = h264_idct_add8_e2k; // !checkasm
+
+        c->h264_idct_add16      = h264_idct_add16_e2k;
+        c->h264_idct_add16intra = h264_idct_add16intra_e2k;
+        c->h264_idct_dc_add     = h264_idct_dc_add_e2k;
+        c->h264_idct8_dc_add    = h264_idct8_dc_add_e2k;
+        c->h264_idct8_add       = h264_idct8_add_e2k;
+        c->h264_idct8_add4      = h264_idct8_add4_e2k;
+        c->h264_v_loop_filter_luma = h264_v_loop_filter_luma_e2k;
+        c->h264_h_loop_filter_luma = h264_h_loop_filter_luma_e2k;
+
+        c->weight_h264_pixels_tab[0]   = weight_h264_pixels16_e2k;   // !checkasm
+        c->weight_h264_pixels_tab[1]   = weight_h264_pixels8_e2k;    // !checkasm
+        c->biweight_h264_pixels_tab[0] = biweight_h264_pixels16_e2k; // !checkasm
+        c->biweight_h264_pixels_tab[1] = biweight_h264_pixels8_e2k;  // !checkasm
+    }
+}
diff --git a/libavcodec/e2k/h264qpel.c b/libavcodec/e2k/h264qpel.c
new file mode 100644
index 0000000..f8fe094
--- /dev/null
+++ b/libavcodec/e2k/h264qpel.c
@@ -0,0 +1,255 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2004 Romain Dolbeau <romain@dolbeau.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include "libavutil/intreadwrite.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/h264qpel.h"
+
+#include "hpeldsp.h"
+
+#define PUT_OP_U8_E2K(d, s, dst) d = s
+#define AVG_OP_U8_E2K(d, s, dst) d = _mm_avg_epu8(dst, s)
+
+#define OP_U8_E2K                          PUT_OP_U8_E2K
+#define PREFIX_h264_qpel16_h_lowpass_e2k   put_h264_qpel16_h_lowpass_e2k
+#define PREFIX_h264_qpel16_v_lowpass_e2k   put_h264_qpel16_v_lowpass_e2k
+#define PREFIX_h264_qpel16_hv_lowpass_e2k  put_h264_qpel16_hv_lowpass_e2k
+#include "h264qpel_template.c"
+#undef OP_U8_E2K
+#undef PREFIX_h264_qpel16_h_lowpass_e2k
+#undef PREFIX_h264_qpel16_v_lowpass_e2k
+#undef PREFIX_h264_qpel16_hv_lowpass_e2k
+
+#define OP_U8_E2K                          AVG_OP_U8_E2K
+#define PREFIX_h264_qpel16_h_lowpass_e2k   avg_h264_qpel16_h_lowpass_e2k
+#define PREFIX_h264_qpel16_v_lowpass_e2k   avg_h264_qpel16_v_lowpass_e2k
+#define PREFIX_h264_qpel16_hv_lowpass_e2k  avg_h264_qpel16_hv_lowpass_e2k
+#include "h264qpel_template.c"
+#undef OP_U8_E2K
+#undef PREFIX_h264_qpel16_h_lowpass_e2k
+#undef PREFIX_h264_qpel16_v_lowpass_e2k
+#undef PREFIX_h264_qpel16_hv_lowpass_e2k
+
+#define H264_MC(OPNAME, SIZE, CODETYPE) \
+static void OPNAME##h264_qpel##SIZE##_mc00_##CODETYPE(uint8_t *dst, const uint8_t *src, ptrdiff_t stride)\
+{\
+    ff_##OPNAME##pixels##SIZE##_##CODETYPE(dst, src, stride, SIZE);\
+}\
+\
+static void OPNAME##h264_qpel##SIZE##_mc10_##CODETYPE(uint8_t *dst, const uint8_t *src, ptrdiff_t stride)\
+{ \
+    DECLARE_ALIGNED(16, uint8_t, half)[SIZE*SIZE];\
+    put_h264_qpel##SIZE##_h_lowpass_##CODETYPE(half, src, SIZE, stride);\
+    OPNAME##pixels##SIZE##_l2_##CODETYPE(dst, src, half, stride, stride, SIZE);\
+}\
+\
+static void OPNAME##h264_qpel##SIZE##_mc20_##CODETYPE(uint8_t *dst, const uint8_t *src, ptrdiff_t stride)\
+{\
+    OPNAME##h264_qpel##SIZE##_h_lowpass_##CODETYPE(dst, src, stride, stride);\
+}\
+\
+static void OPNAME##h264_qpel##SIZE##_mc30_##CODETYPE(uint8_t *dst, const uint8_t *src, ptrdiff_t stride)\
+{\
+    DECLARE_ALIGNED(16, uint8_t, half)[SIZE*SIZE];\
+    put_h264_qpel##SIZE##_h_lowpass_##CODETYPE(half, src, SIZE, stride);\
+    OPNAME##pixels##SIZE##_l2_##CODETYPE(dst, src+1, half, stride, stride, SIZE);\
+}\
+\
+static void OPNAME##h264_qpel##SIZE##_mc01_##CODETYPE(uint8_t *dst, const uint8_t *src, ptrdiff_t stride)\
+{\
+    DECLARE_ALIGNED(16, uint8_t, half)[SIZE*SIZE];\
+    put_h264_qpel##SIZE##_v_lowpass_##CODETYPE(half, src, SIZE, stride);\
+    OPNAME##pixels##SIZE##_l2_##CODETYPE(dst, src, half, stride, stride, SIZE);\
+}\
+\
+static void OPNAME##h264_qpel##SIZE##_mc02_##CODETYPE(uint8_t *dst, const uint8_t *src, ptrdiff_t stride)\
+{\
+    OPNAME##h264_qpel##SIZE##_v_lowpass_##CODETYPE(dst, src, stride, stride);\
+}\
+\
+static void OPNAME##h264_qpel##SIZE##_mc03_##CODETYPE(uint8_t *dst, const uint8_t *src, ptrdiff_t stride)\
+{\
+    DECLARE_ALIGNED(16, uint8_t, half)[SIZE*SIZE];\
+    put_h264_qpel##SIZE##_v_lowpass_##CODETYPE(half, src, SIZE, stride);\
+    OPNAME##pixels##SIZE##_l2_##CODETYPE(dst, src+stride, half, stride, stride, SIZE);\
+}\
+\
+static void OPNAME##h264_qpel##SIZE##_mc11_##CODETYPE(uint8_t *dst, const uint8_t *src, ptrdiff_t stride)\
+{\
+    DECLARE_ALIGNED(16, uint8_t, halfH)[SIZE*SIZE];\
+    DECLARE_ALIGNED(16, uint8_t, halfV)[SIZE*SIZE];\
+    put_h264_qpel##SIZE##_h_lowpass_##CODETYPE(halfH, src, SIZE, stride);\
+    put_h264_qpel##SIZE##_v_lowpass_##CODETYPE(halfV, src, SIZE, stride);\
+    OPNAME##pixels##SIZE##_l2_##CODETYPE(dst, halfH, halfV, stride, SIZE, SIZE);\
+}\
+\
+static void OPNAME##h264_qpel##SIZE##_mc31_##CODETYPE(uint8_t *dst, const uint8_t *src, ptrdiff_t stride)\
+{\
+    DECLARE_ALIGNED(16, uint8_t, halfH)[SIZE*SIZE];\
+    DECLARE_ALIGNED(16, uint8_t, halfV)[SIZE*SIZE];\
+    put_h264_qpel##SIZE##_h_lowpass_##CODETYPE(halfH, src, SIZE, stride);\
+    put_h264_qpel##SIZE##_v_lowpass_##CODETYPE(halfV, src+1, SIZE, stride);\
+    OPNAME##pixels##SIZE##_l2_##CODETYPE(dst, halfH, halfV, stride, SIZE, SIZE);\
+}\
+\
+static void OPNAME##h264_qpel##SIZE##_mc13_##CODETYPE(uint8_t *dst, const uint8_t *src, ptrdiff_t stride)\
+{\
+    DECLARE_ALIGNED(16, uint8_t, halfH)[SIZE*SIZE];\
+    DECLARE_ALIGNED(16, uint8_t, halfV)[SIZE*SIZE];\
+    put_h264_qpel##SIZE##_h_lowpass_##CODETYPE(halfH, src + stride, SIZE, stride);\
+    put_h264_qpel##SIZE##_v_lowpass_##CODETYPE(halfV, src, SIZE, stride);\
+    OPNAME##pixels##SIZE##_l2_##CODETYPE(dst, halfH, halfV, stride, SIZE, SIZE);\
+}\
+\
+static void OPNAME##h264_qpel##SIZE##_mc33_##CODETYPE(uint8_t *dst, const uint8_t *src, ptrdiff_t stride)\
+{\
+    DECLARE_ALIGNED(16, uint8_t, halfH)[SIZE*SIZE];\
+    DECLARE_ALIGNED(16, uint8_t, halfV)[SIZE*SIZE];\
+    put_h264_qpel##SIZE##_h_lowpass_##CODETYPE(halfH, src + stride, SIZE, stride);\
+    put_h264_qpel##SIZE##_v_lowpass_##CODETYPE(halfV, src+1, SIZE, stride);\
+    OPNAME##pixels##SIZE##_l2_##CODETYPE(dst, halfH, halfV, stride, SIZE, SIZE);\
+}\
+\
+static void OPNAME##h264_qpel##SIZE##_mc22_##CODETYPE(uint8_t *dst, const uint8_t *src, ptrdiff_t stride)\
+{\
+    DECLARE_ALIGNED(16, int16_t, tmp)[SIZE*(SIZE+8)];\
+    OPNAME##h264_qpel##SIZE##_hv_lowpass_##CODETYPE(dst, tmp, src, stride, SIZE, stride);\
+}\
+\
+static void OPNAME##h264_qpel##SIZE##_mc21_##CODETYPE(uint8_t *dst, const uint8_t *src, ptrdiff_t stride)\
+{\
+    DECLARE_ALIGNED(16, uint8_t, halfH)[SIZE*SIZE];\
+    DECLARE_ALIGNED(16, uint8_t, halfHV)[SIZE*SIZE];\
+    DECLARE_ALIGNED(16, int16_t, tmp)[SIZE*(SIZE+8)];\
+    put_h264_qpel##SIZE##_h_lowpass_##CODETYPE(halfH, src, SIZE, stride);\
+    put_h264_qpel##SIZE##_hv_lowpass_##CODETYPE(halfHV, tmp, src, SIZE, SIZE, stride);\
+    OPNAME##pixels##SIZE##_l2_##CODETYPE(dst, halfH, halfHV, stride, SIZE, SIZE);\
+}\
+\
+static void OPNAME##h264_qpel##SIZE##_mc23_##CODETYPE(uint8_t *dst, const uint8_t *src, ptrdiff_t stride)\
+{\
+    DECLARE_ALIGNED(16, uint8_t, halfH)[SIZE*SIZE];\
+    DECLARE_ALIGNED(16, uint8_t, halfHV)[SIZE*SIZE];\
+    DECLARE_ALIGNED(16, int16_t, tmp)[SIZE*(SIZE+8)];\
+    put_h264_qpel##SIZE##_h_lowpass_##CODETYPE(halfH, src + stride, SIZE, stride);\
+    put_h264_qpel##SIZE##_hv_lowpass_##CODETYPE(halfHV, tmp, src, SIZE, SIZE, stride);\
+    OPNAME##pixels##SIZE##_l2_##CODETYPE(dst, halfH, halfHV, stride, SIZE, SIZE);\
+}\
+\
+static void OPNAME##h264_qpel##SIZE##_mc12_##CODETYPE(uint8_t *dst, const uint8_t *src, ptrdiff_t stride)\
+{\
+    DECLARE_ALIGNED(16, uint8_t, halfV)[SIZE*SIZE];\
+    DECLARE_ALIGNED(16, uint8_t, halfHV)[SIZE*SIZE];\
+    DECLARE_ALIGNED(16, int16_t, tmp)[SIZE*(SIZE+8)];\
+    put_h264_qpel##SIZE##_v_lowpass_##CODETYPE(halfV, src, SIZE, stride);\
+    put_h264_qpel##SIZE##_hv_lowpass_##CODETYPE(halfHV, tmp, src, SIZE, SIZE, stride);\
+    OPNAME##pixels##SIZE##_l2_##CODETYPE(dst, halfV, halfHV, stride, SIZE, SIZE);\
+}\
+\
+static void OPNAME##h264_qpel##SIZE##_mc32_##CODETYPE(uint8_t *dst, const uint8_t *src, ptrdiff_t stride)\
+{\
+    DECLARE_ALIGNED(16, uint8_t, halfV)[SIZE*SIZE];\
+    DECLARE_ALIGNED(16, uint8_t, halfHV)[SIZE*SIZE];\
+    DECLARE_ALIGNED(16, int16_t, tmp)[SIZE*(SIZE+8)];\
+    put_h264_qpel##SIZE##_v_lowpass_##CODETYPE(halfV, src+1, SIZE, stride);\
+    put_h264_qpel##SIZE##_hv_lowpass_##CODETYPE(halfHV, tmp, src, SIZE, SIZE, stride);\
+    OPNAME##pixels##SIZE##_l2_##CODETYPE(dst, halfV, halfHV, stride, SIZE, SIZE);\
+}\
+
+#if 1
+static av_always_inline void put_pixels16_l2_e2k(uint8_t *dst, const uint8_t *src1,
+                                                 const uint8_t *src2, int dst_stride,
+                                                 int src_stride1, int h)
+{
+    int i;
+    vec_u8 a, b, d;
+
+    for (i = 0; i < h; i++) {
+        a = VEC_LD(src1 + i * src_stride1);
+        b = VEC_LD(src2 + i * 16);
+        d = _mm_avg_epu8(a, b);
+        VEC_ST(dst, d);
+        dst += dst_stride;
+    }
+}
+
+static av_always_inline void avg_pixels16_l2_e2k(uint8_t *dst, const uint8_t *src1,
+                                                 const uint8_t *src2, int dst_stride,
+                                                 int src_stride1, int h)
+{
+    int i;
+    vec_u8 a, b, d;
+
+    for (i = 0; i < h; i++) {
+        a = VEC_LD(src1 + i * src_stride1);
+        b = VEC_LD(src2 + i * 16);
+        d = _mm_avg_epu8(a, b);
+        a = _mm_avg_epu8(VEC_LD(dst), d);
+        VEC_ST(dst, a);
+        dst += dst_stride;
+    }
+}
+
+#else // Implemented but could be faster
+#define put_pixels16_l2_e2k(d,s1,s2,ds,s1s,h) put_pixels16_l2(d,s1,s2,ds,s1s,16,h)
+#define avg_pixels16_l2_e2k(d,s1,s2,ds,s1s,h) avg_pixels16_l2(d,s1,s2,ds,s1s,16,h)
+#endif
+
+H264_MC(put_, 16, e2k)
+H264_MC(avg_, 16, e2k)
+
+av_cold void ff_h264qpel_init_e2k(H264QpelContext *c, int bit_depth)
+{
+    const int high_bit_depth = bit_depth > 8;
+
+    if (!E2K_BASE(av_get_cpu_flags()))
+        return;
+
+    if (!high_bit_depth) {
+#define dspfunc(PFX, IDX, NUM) \
+        c->PFX##_pixels_tab[IDX][ 0] = PFX##NUM##_mc00_e2k; \
+        c->PFX##_pixels_tab[IDX][ 1] = PFX##NUM##_mc10_e2k; \
+        c->PFX##_pixels_tab[IDX][ 2] = PFX##NUM##_mc20_e2k; \
+        c->PFX##_pixels_tab[IDX][ 3] = PFX##NUM##_mc30_e2k; \
+        c->PFX##_pixels_tab[IDX][ 4] = PFX##NUM##_mc01_e2k; \
+        c->PFX##_pixels_tab[IDX][ 5] = PFX##NUM##_mc11_e2k; \
+        c->PFX##_pixels_tab[IDX][ 6] = PFX##NUM##_mc21_e2k; \
+        c->PFX##_pixels_tab[IDX][ 7] = PFX##NUM##_mc31_e2k; \
+        c->PFX##_pixels_tab[IDX][ 8] = PFX##NUM##_mc02_e2k; \
+        c->PFX##_pixels_tab[IDX][ 9] = PFX##NUM##_mc12_e2k; \
+        c->PFX##_pixels_tab[IDX][10] = PFX##NUM##_mc22_e2k; \
+        c->PFX##_pixels_tab[IDX][11] = PFX##NUM##_mc32_e2k; \
+        c->PFX##_pixels_tab[IDX][12] = PFX##NUM##_mc03_e2k; \
+        c->PFX##_pixels_tab[IDX][13] = PFX##NUM##_mc13_e2k; \
+        c->PFX##_pixels_tab[IDX][14] = PFX##NUM##_mc23_e2k; \
+        c->PFX##_pixels_tab[IDX][15] = PFX##NUM##_mc33_e2k
+
+        dspfunc(put_h264_qpel, 0, 16);
+        dspfunc(avg_h264_qpel, 0, 16);
+#undef dspfunc
+    }
+}
diff --git a/libavcodec/e2k/h264qpel_template.c b/libavcodec/e2k/h264qpel_template.c
new file mode 100644
index 0000000..bbd6516
--- /dev/null
+++ b/libavcodec/e2k/h264qpel_template.c
@@ -0,0 +1,354 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2004 Romain Dolbeau <romain@dolbeau.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+#if HAVE_UNISTD_H
+#include <unistd.h>
+#endif
+
+#include "libavutil/avassert.h"
+#include "libavutil/mem.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#define load_alignment() { \
+    srcM2 = VEC_LD(src - 2); \
+    srcM1 = VEC_LD(src - 1); \
+    srcP0 = VEC_LD(src); \
+    srcP1 = VEC_LD(src + 1); \
+    srcP2 = VEC_LD(src + 2); \
+    srcP3 = VEC_LD(src + 3); \
+}
+
+/* this code assume stride % 16 == 0 */
+#ifdef PREFIX_h264_qpel16_h_lowpass_e2k
+static void PREFIX_h264_qpel16_h_lowpass_e2k(uint8_t *dst,
+                                             const uint8_t *src,
+                                             int dstStride, int srcStride)
+{
+    int i;
+
+    LOAD_ZERO;
+    const vec_s16 v5ss = _mm_set1_epi16(5);
+    const vec_s16 v20ss = _mm_set1_epi16(20);
+    const vec_s16 v16ss = _mm_set1_epi16(16);
+
+    vec_u8 srcM2, srcM1, srcP0, srcP1, srcP2, srcP3;
+    vec_s16 srcP0A, srcP0B, srcP1A, srcP1B,
+              srcP2A, srcP2B, srcP3A, srcP3B,
+              srcM1A, srcM1B, srcM2A, srcM2B,
+              sum1A, sum1B, sum2A, sum2B, sum3A, sum3B,
+              pp1A, pp1B, pp2A, pp2B, pp3A, pp3B,
+              sumA, sumB;
+    vec_u8 sum, fsum;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < 16; i++) {
+        load_alignment();
+
+        srcP0A = _mm_unpacklo_epi8(srcP0, zerov);
+        srcP0B = _mm_unpackhi_epi8(srcP0, zerov);
+        srcP1A = _mm_unpacklo_epi8(srcP1, zerov);
+        srcP1B = _mm_unpackhi_epi8(srcP1, zerov);
+
+        srcP2A = _mm_unpacklo_epi8(srcP2, zerov);
+        srcP2B = _mm_unpackhi_epi8(srcP2, zerov);
+        srcP3A = _mm_unpacklo_epi8(srcP3, zerov);
+        srcP3B = _mm_unpackhi_epi8(srcP3, zerov);
+
+        srcM1A = _mm_unpacklo_epi8(srcM1, zerov);
+        srcM1B = _mm_unpackhi_epi8(srcM1, zerov);
+        srcM2A = _mm_unpacklo_epi8(srcM2, zerov);
+        srcM2B = _mm_unpackhi_epi8(srcM2, zerov);
+
+        sum1A = _mm_adds_epi16(srcP0A, srcP1A);
+        sum1B = _mm_adds_epi16(srcP0B, srcP1B);
+        sum2A = _mm_adds_epi16(srcM1A, srcP2A);
+        sum2B = _mm_adds_epi16(srcM1B, srcP2B);
+        sum3A = _mm_adds_epi16(srcM2A, srcP3A);
+        sum3B = _mm_adds_epi16(srcM2B, srcP3B);
+
+        pp1A = _mm_add_epi16(_mm_mullo_epi16(sum1A, v20ss), v16ss);
+        pp1B = _mm_add_epi16(_mm_mullo_epi16(sum1B, v20ss), v16ss);
+        pp2A = _mm_mullo_epi16(sum2A, v5ss);
+        pp2B = _mm_mullo_epi16(sum2B, v5ss);
+        pp3A = _mm_add_epi16(sum3A, pp1A);
+        pp3B = _mm_add_epi16(sum3B, pp1B);
+        sumA = _mm_sub_epi16(pp3A, pp2A);
+        sumB = _mm_sub_epi16(pp3B, pp2B);
+        sumA = _mm_srai_epi16(sumA, 5);
+        sumB = _mm_srai_epi16(sumB, 5);
+        sum = _mm_packus_epi16(sumA, sumB);
+
+        OP_U8_E2K(fsum, sum, VEC_LD(dst));
+        VEC_ST(dst, fsum);
+
+        src += srcStride;
+        dst += dstStride;
+    }
+}
+#endif /* PREFIX_h264_qpel16_h_lowpass_e2k */
+
+/* this code assume stride % 16 == 0 */
+#ifdef PREFIX_h264_qpel16_v_lowpass_e2k
+static void PREFIX_h264_qpel16_v_lowpass_e2k(uint8_t *dst,
+                                             const uint8_t *src,
+                                             int dstStride, int srcStride)
+{
+    int i;
+
+    LOAD_ZERO;
+    const vec_s16 v20ss = _mm_set1_epi16(20);
+    const vec_s16 v5ss = _mm_set1_epi16(5);
+    const vec_s16 v16ss = _mm_set1_epi16(16);
+
+    const vec_u8 srcM2 = VEC_LD(src - srcStride * 2);
+    const vec_u8 srcM1 = VEC_LD(src - srcStride);
+    const vec_u8 srcP0 = VEC_LD(src);
+    const vec_u8 srcP1 = VEC_LD(src + srcStride);
+    const vec_u8 srcP2 = VEC_LD(src + srcStride * 2);
+
+    vec_s16 srcM2ssA = _mm_unpacklo_epi8(srcM2, zerov);
+    vec_s16 srcM2ssB = _mm_unpackhi_epi8(srcM2, zerov);
+    vec_s16 srcM1ssA = _mm_unpacklo_epi8(srcM1, zerov);
+    vec_s16 srcM1ssB = _mm_unpackhi_epi8(srcM1, zerov);
+    vec_s16 srcP0ssA = _mm_unpacklo_epi8(srcP0, zerov);
+    vec_s16 srcP0ssB = _mm_unpackhi_epi8(srcP0, zerov);
+    vec_s16 srcP1ssA = _mm_unpacklo_epi8(srcP1, zerov);
+    vec_s16 srcP1ssB = _mm_unpackhi_epi8(srcP1, zerov);
+    vec_s16 srcP2ssA = _mm_unpacklo_epi8(srcP2, zerov);
+    vec_s16 srcP2ssB = _mm_unpackhi_epi8(srcP2, zerov);
+
+    vec_s16 pp1A, pp1B, pp2A, pp2B, pp3A, pp3B,
+              sumA, sumB, srcP3ssA, srcP3ssB,
+              sum1A, sum1B, sum2A, sum2B, sum3A, sum3B;
+    vec_u8 sum, fsum, srcP3;
+
+    src += srcStride * 3;
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < 16; i++) {
+        srcP3 = VEC_LD(src);
+        src += srcStride;
+
+        srcP3ssA = _mm_unpacklo_epi8(srcP3, zerov);
+        srcP3ssB = _mm_unpackhi_epi8(srcP3, zerov);
+
+        sum1A = _mm_adds_epi16(srcP0ssA, srcP1ssA);
+        sum1B = _mm_adds_epi16(srcP0ssB, srcP1ssB);
+        sum2A = _mm_adds_epi16(srcM1ssA, srcP2ssA);
+        sum2B = _mm_adds_epi16(srcM1ssB, srcP2ssB);
+        sum3A = _mm_adds_epi16(srcM2ssA, srcP3ssA);
+        sum3B = _mm_adds_epi16(srcM2ssB, srcP3ssB);
+
+        srcM2ssA = srcM1ssA;
+        srcM2ssB = srcM1ssB;
+        srcM1ssA = srcP0ssA;
+        srcM1ssB = srcP0ssB;
+        srcP0ssA = srcP1ssA;
+        srcP0ssB = srcP1ssB;
+        srcP1ssA = srcP2ssA;
+        srcP1ssB = srcP2ssB;
+        srcP2ssA = srcP3ssA;
+        srcP2ssB = srcP3ssB;
+
+        pp1A = _mm_add_epi16(_mm_mullo_epi16(sum1A, v20ss), v16ss);
+        pp1B = _mm_add_epi16(_mm_mullo_epi16(sum1B, v20ss), v16ss);
+        pp2A = _mm_mullo_epi16(sum2A, v5ss);
+        pp2B = _mm_mullo_epi16(sum2B, v5ss);
+        pp3A = _mm_add_epi16(sum3A, pp1A);
+        pp3B = _mm_add_epi16(sum3B, pp1B);
+        sumA = _mm_sub_epi16(pp3A, pp2A);
+        sumB = _mm_sub_epi16(pp3B, pp2B);
+        sumA = _mm_srai_epi16(sumA, 5);
+        sumB = _mm_srai_epi16(sumB, 5);
+        sum = _mm_packus_epi16(sumA, sumB);
+
+        OP_U8_E2K(fsum, sum, VEC_LD(dst));
+        VEC_ST(dst, fsum);
+        dst += dstStride;
+    }
+}
+#endif /* PREFIX_h264_qpel16_v_lowpass_e2k */
+
+/* this code assume stride % 16 == 0 *and* tmp is properly aligned */
+#ifdef PREFIX_h264_qpel16_hv_lowpass_e2k
+static void PREFIX_h264_qpel16_hv_lowpass_e2k(uint8_t *dst, int16_t *tmp,
+                                              const uint8_t *src,
+                                              int dstStride, int tmpStride,
+                                              int srcStride)
+{
+    int i;
+    LOAD_ZERO;
+    const vec_s16 v20ss = _mm_set1_epi16(20);
+    const vec_s16 v5ss = _mm_set1_epi16(5);
+    const vec_s32 v512si = _mm_set1_epi32(512);
+
+    vec_s16 srcP0A, srcP0B, srcP1A, srcP1B,
+              srcP2A, srcP2B, srcP3A, srcP3B,
+              srcM1A, srcM1B, srcM2A, srcM2B,
+              sum1A, sum1B, sum2A, sum2B, sum3A, sum3B,
+              pp1A, pp1B, pp2A, pp2B, sumA, sumB;
+    int16_t *tmpbis = tmp;
+
+    vec_s16 tmpM1ssA, tmpM1ssB, tmpM2ssA, tmpM2ssB,
+              tmpP0ssA, tmpP0ssB, tmpP1ssA, tmpP1ssB,
+              tmpP2ssA, tmpP2ssB;
+
+    vec_s32 pp1Al, pp1Ah, pp1Bl, pp1Bh, pp2Al, pp2Ah, pp2Bl, pp2Bh,
+              pp3Al, pp3Ah, pp3Bl, pp3Bh, sumAl, sumAh, sumBl, sumBh;
+    vec_u8 fsum, sum;
+
+    src -= 2 * srcStride;
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < 21; i ++) {
+        vec_u8 srcM2, srcM1, srcP0, srcP1, srcP2, srcP3;
+
+        load_alignment();
+
+        srcP0A = _mm_unpacklo_epi8(srcP0, zerov);
+        srcP0B = _mm_unpackhi_epi8(srcP0, zerov);
+        srcP1A = _mm_unpacklo_epi8(srcP1, zerov);
+        srcP1B = _mm_unpackhi_epi8(srcP1, zerov);
+
+        srcP2A = _mm_unpacklo_epi8(srcP2, zerov);
+        srcP2B = _mm_unpackhi_epi8(srcP2, zerov);
+        srcP3A = _mm_unpacklo_epi8(srcP3, zerov);
+        srcP3B = _mm_unpackhi_epi8(srcP3, zerov);
+
+        srcM1A = _mm_unpacklo_epi8(srcM1, zerov);
+        srcM1B = _mm_unpackhi_epi8(srcM1, zerov);
+        srcM2A = _mm_unpacklo_epi8(srcM2, zerov);
+        srcM2B = _mm_unpackhi_epi8(srcM2, zerov);
+
+        sum1A = _mm_adds_epi16(srcP0A, srcP1A);
+        sum1B = _mm_adds_epi16(srcP0B, srcP1B);
+        sum2A = _mm_adds_epi16(srcM1A, srcP2A);
+        sum2B = _mm_adds_epi16(srcM1B, srcP2B);
+        sum3A = _mm_adds_epi16(srcM2A, srcP3A);
+        sum3B = _mm_adds_epi16(srcM2B, srcP3B);
+
+        pp1A = _mm_add_epi16(_mm_mullo_epi16(sum1A, v20ss), sum3A);
+        pp1B = _mm_add_epi16(_mm_mullo_epi16(sum1B, v20ss), sum3B);
+        pp2A = _mm_mullo_epi16(sum2A, v5ss);
+        pp2B = _mm_mullo_epi16(sum2B, v5ss);
+        sumA = _mm_sub_epi16(pp1A, pp2A);
+        sumB = _mm_sub_epi16(pp1B, pp2B);
+
+        VEC_ST(tmp, sumA);
+        VEC_ST(tmp + 8, sumB);
+
+        src += srcStride;
+        tmp += tmpStride; /* int16_t*, and stride is 16, so it's OK here */
+    }
+
+    tmpM2ssA = VEC_LD(tmpbis);
+    tmpM2ssB = VEC_LD(tmpbis + 8);
+    tmpbis += tmpStride;
+    tmpM1ssA = VEC_LD(tmpbis);
+    tmpM1ssB = VEC_LD(tmpbis + 8);
+    tmpbis += tmpStride;
+    tmpP0ssA = VEC_LD(tmpbis);
+    tmpP0ssB = VEC_LD(tmpbis + 8);
+    tmpbis += tmpStride;
+    tmpP1ssA = VEC_LD(tmpbis);
+    tmpP1ssB = VEC_LD(tmpbis + 8);
+    tmpbis += tmpStride;
+    tmpP2ssA = VEC_LD(tmpbis);
+    tmpP2ssB = VEC_LD(tmpbis + 8);
+    tmpbis += tmpStride;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < 16; i++) {
+        vec_s16 tmp0, tmp1;
+        const vec_s16 tmpP3ssA = VEC_LD(tmpbis);
+        const vec_s16 tmpP3ssB = VEC_LD(tmpbis + 8);
+
+        const vec_s16 sum1A = _mm_adds_epi16(tmpP0ssA, tmpP1ssA);
+        const vec_s16 sum1B = _mm_adds_epi16(tmpP0ssB, tmpP1ssB);
+        const vec_s16 sum2A = _mm_adds_epi16(tmpM1ssA, tmpP2ssA);
+        const vec_s16 sum2B = _mm_adds_epi16(tmpM1ssB, tmpP2ssB);
+        vec_s16 sum3A = _mm_adds_epi16(tmpM2ssA, tmpP3ssA);
+        vec_s16 sum3B = _mm_adds_epi16(tmpM2ssB, tmpP3ssB);
+
+        tmpbis += tmpStride;
+
+        tmpM2ssA = tmpM1ssA;
+        tmpM2ssB = tmpM1ssB;
+        tmpM1ssA = tmpP0ssA;
+        tmpM1ssB = tmpP0ssB;
+        tmpP0ssA = tmpP1ssA;
+        tmpP0ssB = tmpP1ssB;
+        tmpP1ssA = tmpP2ssA;
+        tmpP1ssB = tmpP2ssB;
+        tmpP2ssA = tmpP3ssA;
+        tmpP2ssB = tmpP3ssB;
+
+        tmp0 = _mm_mullo_epi16(sum1A, v20ss);
+        tmp1 = _mm_mulhi_epi16(sum1A, v20ss);
+        pp1Al = _mm_unpacklo_epi16(tmp0, tmp1);
+        pp1Ah = _mm_unpackhi_epi16(tmp0, tmp1);
+        tmp0 = _mm_mullo_epi16(sum1B, v20ss);
+        tmp1 = _mm_mulhi_epi16(sum1B, v20ss);
+        pp1Bl = _mm_unpacklo_epi16(tmp0, tmp1);
+        pp1Bh = _mm_unpackhi_epi16(tmp0, tmp1);
+
+        pp1Al = _mm_add_epi32(pp1Al, v512si);
+        pp1Ah = _mm_add_epi32(pp1Ah, v512si);
+        pp1Bl = _mm_add_epi32(pp1Bl, v512si);
+        pp1Bh = _mm_add_epi32(pp1Bh, v512si);
+
+        tmp0 = _mm_mullo_epi16(sum2A, v5ss);
+        tmp1 = _mm_mulhi_epi16(sum2A, v5ss);
+        pp2Al = _mm_unpacklo_epi16(tmp0, tmp1);
+        pp2Ah = _mm_unpackhi_epi16(tmp0, tmp1);
+        tmp0 = _mm_mullo_epi16(sum2B, v5ss);
+        tmp1 = _mm_mulhi_epi16(sum2B, v5ss);
+        pp2Bl = _mm_unpacklo_epi16(tmp0, tmp1);
+        pp2Bh = _mm_unpackhi_epi16(tmp0, tmp1);
+
+        tmp0 = _mm_srai_epi32(_mm_unpacklo_epi16(sum3A, sum3A), 16);
+        tmp1 = _mm_srai_epi32(_mm_unpackhi_epi16(sum3A, sum3A), 16);
+        pp3Al = _mm_add_epi32(tmp0, pp1Al);
+        pp3Ah = _mm_add_epi32(tmp1, pp1Ah);
+        tmp0 = _mm_srai_epi32(_mm_unpacklo_epi16(sum3B, sum3B), 16);
+        tmp1 = _mm_srai_epi32(_mm_unpackhi_epi16(sum3B, sum3B), 16);
+        pp3Bl = _mm_add_epi32(tmp0, pp1Bl);
+        pp3Bh = _mm_add_epi32(tmp1, pp1Bh);
+
+        sumAl = _mm_sub_epi32(pp3Al, pp2Al);
+        sumAh = _mm_sub_epi32(pp3Ah, pp2Ah);
+        sumBl = _mm_sub_epi32(pp3Bl, pp2Bl);
+        sumBh = _mm_sub_epi32(pp3Bh, pp2Bh);
+
+        sumAl = _mm_srai_epi32(sumAl, 10);
+        sumAh = _mm_srai_epi32(sumAh, 10);
+        sumBl = _mm_srai_epi32(sumBl, 10);
+        sumBh = _mm_srai_epi32(sumBh, 10);
+
+        sumA = _mm_packs_epi32(sumAl, sumAh);
+        sumB = _mm_packs_epi32(sumBl, sumBh);
+        sum = _mm_packus_epi16(sumA, sumB);
+
+        OP_U8_E2K(fsum, sum, VEC_LD(dst));
+        VEC_ST(dst, fsum);
+        dst += dstStride;
+    }
+}
+#endif /* PREFIX_h264_qpel16_hv_lowpass_e2k */
diff --git a/libavcodec/e2k/hevcdsp.c b/libavcodec/e2k/hevcdsp.c
new file mode 100644
index 0000000..74004d7
--- /dev/null
+++ b/libavcodec/e2k/hevcdsp.c
@@ -0,0 +1,94 @@
+/*
+ * SIMD-optimized IDCT functions for HEVC decoding
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) Alexandra Hajkova
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/hevcdsp.h"
+
+#define transform4x4(shift) { \
+    vec_s16 src_02, src_13; \
+    vec_s32 e0, o0, e1, o1, add; \
+    src_02 = _mm_unpacklo_epi16(src_01, src_23); \
+    src_13 = _mm_unpackhi_epi16(src_01, src_23); \
+    e0 = _mm_madd_epi16(src_02, trans0); \
+    o0 = _mm_madd_epi16(src_13, trans1); \
+    e1 = _mm_madd_epi16(src_02, trans2); \
+    o1 = _mm_madd_epi16(src_13, trans3); \
+    add = _mm_set1_epi32(1 << (shift - 1)); \
+    e0 = _mm_add_epi32(e0, add); \
+    e1 = _mm_add_epi32(e1, add); \
+    res0 = _mm_add_epi32(e0, o0); \
+    res1 = _mm_add_epi32(e1, o1); \
+    res2 = _mm_sub_epi32(e1, o1); \
+    res3 = _mm_sub_epi32(e0, o0); \
+    res0 = _mm_srai_epi32(res0, shift); \
+    res1 = _mm_srai_epi32(res1, shift); \
+    res2 = _mm_srai_epi32(res2, shift); \
+    res3 = _mm_srai_epi32(res3, shift); \
+    packed0 = _mm_packs_epi32(res0, res1); \
+    packed1 = _mm_packs_epi32(res2, res3); \
+    \
+    res0 = _mm_unpacklo_epi16(packed0, packed1); \
+    res1 = _mm_unpackhi_epi16(packed0, packed1); \
+    src_01 = _mm_unpacklo_epi16(res0, res1); \
+    src_23 = _mm_unpackhi_epi16(res0, res1); \
+}
+
+#define HEVC_IDCT4X4_E2K(depth) \
+static void ff_hevc_idct_4x4##_##depth##_e2k(int16_t *coeffs, int col_limit) \
+{ \
+    const int shift = 7; \
+    const int shift2 = 20 - depth; \
+    vec_s16 src_01, src_23; \
+    vec_s32 res0, res1, res2, res3; \
+    vec_s16 packed0, packed1; \
+    vec_s16 trans0 = _mm_set1_epi32(64 | 64 << 16); \
+    vec_s16 trans1 = _mm_set1_epi32(83 | 36 << 16); \
+    vec_s16 trans2 = _mm_set1_epi32(64 | -64 << 16); \
+    vec_s16 trans3 = _mm_set1_epi32(36 | -83 << 16); \
+    \
+    src_01 = VEC_LD(coeffs); \
+    src_23 = VEC_LD(coeffs + 8); \
+    transform4x4(shift); \
+    transform4x4(shift2); \
+    VEC_ST(coeffs, src_01); \
+    VEC_ST(coeffs + 8, src_23); \
+}
+
+HEVC_IDCT4X4_E2K(8)
+HEVC_IDCT4X4_E2K(10)
+
+av_cold void ff_hevc_dsp_init_e2k(HEVCDSPContext *c, const int bit_depth)
+{
+    if (!E2K_BASE(av_get_cpu_flags()))
+        return;
+
+    if (bit_depth == 8)
+        c->idct[0] = ff_hevc_idct_4x4_8_e2k;
+    if (bit_depth == 10)
+        c->idct[0] = ff_hevc_idct_4x4_10_e2k;
+}
diff --git a/libavcodec/e2k/hpeldsp.c b/libavcodec/e2k/hpeldsp.c
new file mode 100644
index 0000000..9ff59bb
--- /dev/null
+++ b/libavcodec/e2k/hpeldsp.c
@@ -0,0 +1,302 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2002 Brian Foley
+ * Copyright (c) 2002 Dieter Shirley
+ * Copyright (c) 2003-2004 Romain Dolbeau <romain@dolbeau.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/hpeldsp.h"
+
+#include "hpeldsp.h"
+
+/* next one assumes that ((line_size % 16) == 0) */
+void ff_put_pixels16_e2k(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
+{
+    vec_u8 v0, v1, v2, v3;
+    int i;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i += 4) {
+        v0 = VEC_LD(pixels);
+        v1 = VEC_LD(pixels + line_size);
+        v2 = VEC_LD(pixels + line_size * 2);
+        v3 = VEC_LD(pixels + line_size * 3);
+        VEC_ST(block, v0);
+        VEC_ST(block + line_size, v1);
+        VEC_ST(block + line_size * 2, v2);
+        VEC_ST(block + line_size * 3, v3);
+        pixels += line_size * 4;
+        block += line_size * 4;
+    }
+}
+
+/* next one assumes that ((line_size % 16) == 0) */
+#define op_avg(a,b)  a = ( ((a)|(b)) - ((((a)^(b))&0xFEFEFEFEUL)>>1) )
+void ff_avg_pixels16_e2k(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
+{
+    vec_u8 pixelsv, blockv;
+    int i;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        blockv = VEC_LD(block);
+        pixelsv = VEC_LD(pixels);
+        blockv = _mm_avg_epu8(blockv, pixelsv);
+        VEC_ST(block, blockv);
+        pixels += line_size;
+        block += line_size;
+    }
+}
+
+/* next one assumes that ((line_size % 8) == 0) */
+static void avg_pixels8_e2k(uint8_t * block, const uint8_t * pixels, ptrdiff_t line_size, int h)
+{
+    __m64 pixelsv, blockv;
+    int i;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        blockv = *(__m64*)block;
+        pixelsv = *(__m64*)pixels;
+        blockv = _mm_avg_pu8(blockv, pixelsv);
+        *(__m64*)block = blockv;
+        pixels += line_size;
+        block += line_size;
+    }
+}
+
+/* next one assumes that ((line_size % 8) == 0) */
+static void put_pixels8_xy2_e2k(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
+{
+    int i;
+    vec_u8 pixelsv1, pixelsv2, blockv;
+    vec_u16 pixelssum1, pixelssum2, temp3;
+    LOAD_ZERO;
+    const vec_u16 vctwo = _mm_set1_epi16(2);
+
+    pixelsv1 = VEC_LD8(pixels);
+    pixelsv2 = VEC_LD8(pixels + 1);
+    pixelsv1 = _mm_unpacklo_epi8(pixelsv1, zerov);
+    pixelsv2 = _mm_unpacklo_epi8(pixelsv2, zerov);
+    pixelssum1 = _mm_add_epi16(pixelsv1, pixelsv2);
+    pixelssum1 = _mm_add_epi16(pixelssum1, vctwo);
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        pixels += line_size;
+        blockv = VEC_LD8(block);
+        pixelsv1 = VEC_LD8(pixels);
+        pixelsv2 = VEC_LD8(pixels + 1);
+        pixelsv1 = _mm_unpacklo_epi8(pixelsv1, zerov);
+        pixelsv2 = _mm_unpacklo_epi8(pixelsv2, zerov);
+        pixelssum2 = _mm_add_epi16(pixelsv1, pixelsv2);
+        temp3 = _mm_add_epi16(pixelssum1, pixelssum2);
+        temp3 = _mm_srai_epi16(temp3, 2);
+        pixelssum1 = _mm_add_epi16(pixelssum2, vctwo);
+
+        blockv = _mm_packus_epi16(temp3, temp3);
+        VEC_STL(block, blockv);
+        block += line_size;
+    }
+}
+
+/* next one assumes that ((line_size % 8) == 0) */
+static void put_no_rnd_pixels8_xy2_e2k(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
+{
+    int i;
+    vec_u8 pixelsv1, pixelsv2, blockv;
+    vec_u16 pixelssum1, pixelssum2, temp3;
+    LOAD_ZERO;
+    const vec_u16 vcone = _mm_set1_epi16(1);
+
+    pixelsv1 = VEC_LD8(pixels);
+    pixelsv2 = VEC_LD8(pixels + 1);
+    pixelsv1 = _mm_unpacklo_epi8(pixelsv1, zerov);
+    pixelsv2 = _mm_unpacklo_epi8(pixelsv2, zerov);
+    pixelssum1 = _mm_add_epi16(pixelsv1, pixelsv2);
+    pixelssum1 = _mm_add_epi16(pixelssum1, vcone);
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        pixels += line_size;
+        blockv = VEC_LD8(block);
+        pixelsv1 = VEC_LD8(pixels);
+        pixelsv2 = VEC_LD8(pixels + 1);
+        pixelsv1 = _mm_unpacklo_epi8(pixelsv1, zerov);
+        pixelsv2 = _mm_unpacklo_epi8(pixelsv2, zerov);
+        pixelssum2 = _mm_add_epi16(pixelsv1, pixelsv2);
+        temp3 = _mm_add_epi16(pixelssum1, pixelssum2);
+        temp3 = _mm_srai_epi16(temp3, 2);
+        pixelssum1 = _mm_add_epi16(pixelssum2, vcone);
+
+        blockv = _mm_packus_epi16(temp3, temp3);
+        VEC_STL(block, blockv);
+        block += line_size;
+    }
+}
+
+/* next one assumes that ((line_size % 16) == 0) */
+static void put_pixels16_xy2_e2k(uint8_t * block, const uint8_t * pixels, ptrdiff_t line_size, int h)
+{
+    int i;
+    vec_u8 pixelsv1, pixelsv2, pixelsv3, pixelsv4, blockv;
+    vec_u16 temp3, temp4, pixelssum1, pixelssum2, pixelssum3, pixelssum4;
+    LOAD_ZERO;
+    const vec_u16 vctwo = _mm_set1_epi16(2);
+
+    pixelsv1 = VEC_LD(pixels);
+    pixelsv2 = VEC_LD(pixels + 1);
+    pixelsv3 = _mm_unpackhi_epi8(pixelsv1, zerov);
+    pixelsv4 = _mm_unpackhi_epi8(pixelsv2, zerov);
+    pixelsv1 = _mm_unpacklo_epi8(pixelsv1, zerov);
+    pixelsv2 = _mm_unpacklo_epi8(pixelsv2, zerov);
+    pixelssum3 = _mm_add_epi16(pixelsv3, pixelsv4);
+    pixelssum3 = _mm_add_epi16(pixelssum3, vctwo);
+    pixelssum1 = _mm_add_epi16(pixelsv1, pixelsv2);
+    pixelssum1 = _mm_add_epi16(pixelssum1, vctwo);
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        pixels += line_size;
+        blockv = VEC_LD(block);
+        pixelsv1 = VEC_LD(pixels);
+        pixelsv2 = VEC_LD(pixels + 1);
+        pixelsv3 = _mm_unpackhi_epi8(pixelsv1, zerov);
+        pixelsv4 = _mm_unpackhi_epi8(pixelsv2, zerov);
+        pixelsv1 = _mm_unpacklo_epi8(pixelsv1, zerov);
+        pixelsv2 = _mm_unpacklo_epi8(pixelsv2, zerov);
+        pixelssum4 = _mm_add_epi16(pixelsv3, pixelsv4);
+        pixelssum2 = _mm_add_epi16(pixelsv1, pixelsv2);
+        temp4 = _mm_add_epi16(pixelssum3, pixelssum4);
+        temp4 = _mm_srai_epi16(temp4, 2);
+        temp3 = _mm_add_epi16(pixelssum1, pixelssum2);
+        temp3 = _mm_srai_epi16(temp3, 2);
+        pixelssum3 = _mm_add_epi16(pixelssum4, vctwo);
+        pixelssum1 = _mm_add_epi16(pixelssum2, vctwo);
+        blockv = _mm_packus_epi16(temp3, temp4);
+        VEC_ST(block, blockv);
+        block += line_size;
+    }
+}
+
+/* next one assumes that ((line_size % 16) == 0) */
+static void put_no_rnd_pixels16_xy2_e2k(uint8_t * block, const uint8_t * pixels, ptrdiff_t line_size, int h)
+{
+    int i;
+    vec_u8 pixelsv1, pixelsv2, pixelsv3, pixelsv4, blockv;
+    vec_u16 temp3, temp4, pixelssum1, pixelssum2, pixelssum3, pixelssum4;
+    LOAD_ZERO;
+    const vec_u16 vcone = _mm_set1_epi16(1);
+
+    pixelsv1 = VEC_LD(pixels);
+    pixelsv2 = VEC_LD(pixels + 1);
+    pixelsv3 = _mm_unpackhi_epi8(pixelsv1, zerov);
+    pixelsv4 = _mm_unpackhi_epi8(pixelsv2, zerov);
+    pixelsv1 = _mm_unpacklo_epi8(pixelsv1, zerov);
+    pixelsv2 = _mm_unpacklo_epi8(pixelsv2, zerov);
+    pixelssum3 = _mm_add_epi16(pixelsv3, pixelsv4);
+    pixelssum3 = _mm_add_epi16(pixelssum3, vcone);
+    pixelssum1 = _mm_add_epi16(pixelsv1, pixelsv2);
+    pixelssum1 = _mm_add_epi16(pixelssum1, vcone);
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        pixels += line_size;
+        blockv = VEC_LD(block);
+        pixelsv1 = VEC_LD(pixels);
+        pixelsv2 = VEC_LD(pixels + 1);
+        pixelsv3 = _mm_unpackhi_epi8(pixelsv1, zerov);
+        pixelsv4 = _mm_unpackhi_epi8(pixelsv2, zerov);
+        pixelsv1 = _mm_unpacklo_epi8(pixelsv1, zerov);
+        pixelsv2 = _mm_unpacklo_epi8(pixelsv2, zerov);
+        pixelssum4 = _mm_add_epi16(pixelsv3, pixelsv4);
+        pixelssum2 = _mm_add_epi16(pixelsv1, pixelsv2);
+        temp4 = _mm_add_epi16(pixelssum3, pixelssum4);
+        temp4 = _mm_srai_epi16(temp4, 2);
+        temp3 = _mm_add_epi16(pixelssum1, pixelssum2);
+        temp3 = _mm_srai_epi16(temp3, 2);
+        pixelssum3 = _mm_add_epi16(pixelssum4, vcone);
+        pixelssum1 = _mm_add_epi16(pixelssum2, vcone);
+        blockv = _mm_packus_epi16(temp3, temp4);
+        VEC_ST(block, blockv);
+        block += line_size;
+    }
+}
+
+/* next one assumes that ((line_size % 8) == 0) */
+static void avg_pixels8_xy2_e2k(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
+{
+    int i;
+    vec_u8 pixelsv1, pixelsv2, blockv, blocktemp;
+    vec_u16 pixelssum1, pixelssum2, temp3;
+    LOAD_ZERO;
+    const vec_u16 vctwo = _mm_set1_epi16(2);
+
+    pixelsv1 = VEC_LD8(pixels);
+    pixelsv2 = VEC_LD8(pixels + 1);
+    pixelsv1 = _mm_unpacklo_epi8(pixelsv1, zerov);
+    pixelsv2 = _mm_unpacklo_epi8(pixelsv2, zerov);
+    pixelssum1 = _mm_add_epi16(pixelsv1, pixelsv2);
+    pixelssum1 = _mm_add_epi16(pixelssum1, vctwo);
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        pixels += line_size;
+        blockv = VEC_LD8(block);
+        pixelsv1 = VEC_LD8(pixels);
+        pixelsv2 = VEC_LD8(pixels + 1);
+        pixelsv1 = _mm_unpacklo_epi8(pixelsv1, zerov);
+        pixelsv2 = _mm_unpacklo_epi8(pixelsv2, zerov);
+        pixelssum2 = _mm_add_epi16(pixelsv1, pixelsv2);
+        temp3 = _mm_add_epi16(pixelssum1, pixelssum2);
+        temp3 = _mm_srai_epi16(temp3, 2);
+        pixelssum1 = _mm_add_epi16(pixelssum2, vctwo);
+        blocktemp = _mm_packus_epi16(temp3, temp3);
+        blockv = _mm_avg_epu8(blocktemp, blockv);
+        VEC_STL(block, blockv);
+        block += line_size;
+    }
+}
+
+av_cold void ff_hpeldsp_init_e2k(HpelDSPContext *c, int flags)
+{
+    if (!E2K_BASE(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+
+    c->avg_pixels_tab[0][0]        = ff_avg_pixels16_e2k;
+    c->avg_pixels_tab[1][0]        = avg_pixels8_e2k;
+    c->avg_pixels_tab[1][3]        = avg_pixels8_xy2_e2k; // fate vsynth1-mpeg2-422
+
+    c->put_pixels_tab[0][0]        = ff_put_pixels16_e2k;
+    c->put_pixels_tab[1][3]        = put_pixels8_xy2_e2k;
+    c->put_pixels_tab[0][3]        = put_pixels16_xy2_e2k;
+
+    c->put_no_rnd_pixels_tab[0][0] = ff_put_pixels16_e2k;
+    c->put_no_rnd_pixels_tab[1][3] = put_no_rnd_pixels8_xy2_e2k;
+    c->put_no_rnd_pixels_tab[0][3] = put_no_rnd_pixels16_xy2_e2k;
+}
diff --git a/libavcodec/e2k/hpeldsp.h b/libavcodec/e2k/hpeldsp.h
new file mode 100644
index 0000000..0ade264
--- /dev/null
+++ b/libavcodec/e2k/hpeldsp.h
@@ -0,0 +1,30 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_E2K_HPELDSP_H
+#define AVCODEC_E2K_HPELDSP_H
+
+#include <stddef.h>
+#include <stdint.h>
+
+void ff_avg_pixels16_e2k(uint8_t *block, const uint8_t *pixels,
+                         ptrdiff_t line_size, int h);
+void ff_put_pixels16_e2k(uint8_t *block, const uint8_t *pixels,
+                         ptrdiff_t line_size, int h);
+
+#endif /* AVCODEC_E2K_HPELDSP_H */
diff --git a/libavcodec/e2k/idctdsp.c b/libavcodec/e2k/idctdsp.c
new file mode 100644
index 0000000..db9d2ca
--- /dev/null
+++ b/libavcodec/e2k/idctdsp.c
@@ -0,0 +1,237 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2001 Michel Lespinasse
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/* NOTE: This code is based on GPL code from the libmpeg2 project.  The
+ * author, Michel Lespinasses, has given explicit permission to release
+ * under LGPL as part of FFmpeg.
+ *
+ * FFmpeg integration by Dieter Shirley
+ *
+ * This file is a direct copy of the AltiVec IDCT module from the libmpeg2
+ * project.  I've deleted all of the libmpeg2-specific code, renamed the
+ * functions and reordered the function parameters.  The only change to the
+ * IDCT function itself was to factor out the partial transposition, and to
+ * perform a full transpose at the end of the function. */
+
+#include "config.h"
+
+#include <stdlib.h>
+#include <string.h>
+
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/idctdsp.h"
+
+#include "dctdsp.h"
+
+#define IDCT_HALF                                         \
+    /* 1st stage */                                       \
+    t1 = _mm_adds_epi16(_mm_mulhrs_epi16(a1, vx7), vx1);  \
+    t8 = _mm_adds_epi16(_mm_mulhrs_epi16(a1, vx1),        \
+                        _mm_subs_epi16(zero, vx7));       \
+    t7 = _mm_adds_epi16(_mm_mulhrs_epi16(a2, vx5), vx3);  \
+    t3 = _mm_adds_epi16(_mm_mulhrs_epi16(ma2, vx3), vx5); \
+                                                          \
+    /* 2nd stage */                                       \
+    t5 = _mm_adds_epi16(vx0, vx4);                        \
+    t0 = _mm_subs_epi16(vx0, vx4);                        \
+    t2 = _mm_adds_epi16(_mm_mulhrs_epi16(a0, vx6), vx2);  \
+    t4 = _mm_adds_epi16(_mm_mulhrs_epi16(a0, vx2),        \
+                        _mm_subs_epi16(zero, vx6));       \
+    t6 = _mm_adds_epi16(t8, t3);                          \
+    t3 = _mm_subs_epi16(t8, t3);                          \
+    t8 = _mm_subs_epi16(t1, t7);                          \
+    t1 = _mm_adds_epi16(t1, t7);                          \
+                                                          \
+    /* 3rd stage */                                       \
+    t7 = _mm_adds_epi16(t5, t2);                          \
+    t2 = _mm_subs_epi16(t5, t2);                          \
+    t5 = _mm_adds_epi16(t0, t4);                          \
+    t0 = _mm_subs_epi16(t0, t4);                          \
+    t4 = _mm_subs_epi16(t8, t3);                          \
+    t3 = _mm_adds_epi16(t8, t3);                          \
+                                                          \
+    /* 4th stage */                                       \
+    vy0 = _mm_adds_epi16(t7, t1);                         \
+    vy7 = _mm_subs_epi16(t7, t1);                         \
+    vy1 = _mm_adds_epi16(_mm_mulhrs_epi16(c4, t3), t5);   \
+    vy6 = _mm_adds_epi16(_mm_mulhrs_epi16(mc4, t3), t5);  \
+    vy2 = _mm_adds_epi16(_mm_mulhrs_epi16(c4, t4), t0);   \
+    vy5 = _mm_adds_epi16(_mm_mulhrs_epi16(mc4, t4), t0);  \
+    vy3 = _mm_adds_epi16(t2, t6);                         \
+    vy4 = _mm_subs_epi16(t2, t6)
+
+#define IDCT                                                                \
+    vec_s16 vx0, vx1, vx2, vx3, vx4, vx5, vx6, vx7;                         \
+    vec_s16 vy0, vy1, vy2, vy3, vy4, vy5, vy6, vy7;                         \
+    vec_s16 t0, t1, t2, t3, t4, t5, t6, t7, t8;                             \
+                                                                            \
+    vec_s16 c4   = _mm_set1_epi16(23170);                                   \
+    vec_s16 a0   = _mm_set1_epi16(13573);                                   \
+    vec_s16 a1   = _mm_set1_epi16(6518);                                    \
+    vec_s16 a2   = _mm_set1_epi16(21895);                                   \
+    vec_s16 mc4  = _mm_set1_epi16(-23170);                                  \
+    vec_s16 ma2  = _mm_set1_epi16(-21895);                                  \
+    vec_s16 bias = _mm_set1_epi32(32 | 31 << 16);                           \
+                                                                            \
+    vec_s16 zero  = _mm_setzero_si128();                                    \
+                                                                            \
+    t0 = VEC_LD(constants[0]);                                              \
+    t1 = VEC_LD(constants[1]);                                              \
+    t2 = VEC_LD(constants[2]);                                              \
+    t3 = VEC_LD(constants[3]);                                              \
+                                                                            \
+    vx0 = _mm_mulhrs_epi16(_mm_slli_epi16(block[0], 4), t0);                \
+    vx1 = _mm_mulhrs_epi16(_mm_slli_epi16(block[1], 4), t1);                \
+    vx2 = _mm_mulhrs_epi16(_mm_slli_epi16(block[2], 4), t2);                \
+    vx3 = _mm_mulhrs_epi16(_mm_slli_epi16(block[3], 4), t3);                \
+    vx4 = _mm_mulhrs_epi16(_mm_slli_epi16(block[4], 4), t0);                \
+    vx5 = _mm_mulhrs_epi16(_mm_slli_epi16(block[5], 4), t3);                \
+    vx6 = _mm_mulhrs_epi16(_mm_slli_epi16(block[6], 4), t2);                \
+    vx7 = _mm_mulhrs_epi16(_mm_slli_epi16(block[7], 4), t1);                \
+                                                                            \
+    IDCT_HALF;                                                              \
+                                                                            \
+    vx0 = _mm_unpacklo_epi16(vy0, vy4);                                     \
+    vx1 = _mm_unpackhi_epi16(vy0, vy4);                                     \
+    vx2 = _mm_unpacklo_epi16(vy1, vy5);                                     \
+    vx3 = _mm_unpackhi_epi16(vy1, vy5);                                     \
+    vx4 = _mm_unpacklo_epi16(vy2, vy6);                                     \
+    vx5 = _mm_unpackhi_epi16(vy2, vy6);                                     \
+    vx6 = _mm_unpacklo_epi16(vy3, vy7);                                     \
+    vx7 = _mm_unpackhi_epi16(vy3, vy7);                                     \
+                                                                            \
+    vy0 = _mm_unpacklo_epi16(vx0, vx4);                                     \
+    vy1 = _mm_unpackhi_epi16(vx0, vx4);                                     \
+    vy2 = _mm_unpacklo_epi16(vx1, vx5);                                     \
+    vy3 = _mm_unpackhi_epi16(vx1, vx5);                                     \
+    vy4 = _mm_unpacklo_epi16(vx2, vx6);                                     \
+    vy5 = _mm_unpackhi_epi16(vx2, vx6);                                     \
+    vy6 = _mm_unpacklo_epi16(vx3, vx7);                                     \
+    vy7 = _mm_unpackhi_epi16(vx3, vx7);                                     \
+                                                                            \
+    vx0 = _mm_adds_epi16(_mm_unpacklo_epi16(vy0, vy4), bias);               \
+    vx1 = _mm_unpackhi_epi16(vy0, vy4);                                     \
+    vx2 = _mm_unpacklo_epi16(vy1, vy5);                                     \
+    vx3 = _mm_unpackhi_epi16(vy1, vy5);                                     \
+    vx4 = _mm_unpacklo_epi16(vy2, vy6);                                     \
+    vx5 = _mm_unpackhi_epi16(vy2, vy6);                                     \
+    vx6 = _mm_unpacklo_epi16(vy3, vy7);                                     \
+    vx7 = _mm_unpackhi_epi16(vy3, vy7);                                     \
+                                                                            \
+    IDCT_HALF;                                                              \
+                                                                            \
+    vx0 = _mm_srai_epi16(vy0, 6);                                           \
+    vx1 = _mm_srai_epi16(vy1, 6);                                           \
+    vx2 = _mm_srai_epi16(vy2, 6);                                           \
+    vx3 = _mm_srai_epi16(vy3, 6);                                           \
+    vx4 = _mm_srai_epi16(vy4, 6);                                           \
+    vx5 = _mm_srai_epi16(vy5, 6);                                           \
+    vx6 = _mm_srai_epi16(vy6, 6);                                           \
+    vx7 = _mm_srai_epi16(vy7, 6)
+
+static const int16_t ALIGNED(16) constants[4][8] = {
+    { 16384, 22725, 21407, 19266, 16384, 19266, 21407, 22725 },
+    { 22725, 31521, 29692, 26722, 22725, 26722, 29692, 31521 },
+    { 21407, 29692, 27969, 25172, 21407, 25172, 27969, 29692 },
+    { 19266, 26722, 25172, 22654, 19266, 22654, 25172, 26722 }
+};
+
+void ff_idct_e2k(int16_t *blk)
+{
+    vec_s16 *block = (vec_s16*)blk;
+
+    IDCT;
+
+    block[0] = vx0;
+    block[1] = vx1;
+    block[2] = vx2;
+    block[3] = vx3;
+    block[4] = vx4;
+    block[5] = vx5;
+    block[6] = vx6;
+    block[7] = vx7;
+}
+
+#define COPY(vx0, vx1, i)                 \
+    tmp = _mm_packus_epi16(vx0, vx1);     \
+    VEC_STL(dest, tmp); dest += stride;   \
+    VEC_STH(dest, tmp); dest += stride
+
+static void idct_put_e2k(uint8_t *dest, ptrdiff_t stride, int16_t *blk)
+{
+    vec_s16 *block = (vec_s16*)blk;
+    vec_u8 tmp;
+
+    IDCT;
+
+    COPY(vx0, vx1, 0);
+    COPY(vx2, vx3, 2);
+    COPY(vx4, vx5, 4);
+    COPY(vx6, vx7, 6);
+}
+
+#define ADD(vx0, vx1, i)                  \
+    tmp = VEC_LD8(dest);                  \
+    t0 = _mm_unpacklo_epi8(tmp, zero);    \
+    tmp = VEC_LD8(dest + stride);         \
+    t1 = _mm_unpacklo_epi8(tmp, zero);    \
+    t0 = _mm_adds_epi16(t0, vx0);         \
+    t1 = _mm_adds_epi16(t1, vx1);         \
+    tmp = _mm_packus_epi16(t0, t1);       \
+    VEC_STL(dest, tmp); dest += stride;   \
+    VEC_STH(dest, tmp); dest += stride
+
+static void idct_add_e2k(uint8_t *dest, ptrdiff_t stride, int16_t *blk)
+{
+    vec_s16 *block = (vec_s16*)blk;
+    vec_u8 tmp;
+
+    IDCT;
+
+    ADD(vx0, vx1, 0);
+    ADD(vx2, vx3, 2);
+    ADD(vx4, vx5, 4);
+    ADD(vx6, vx7, 6);
+}
+
+av_cold void ff_idctdsp_init_e2k(IDCTDSPContext *c, AVCodecContext *avctx,
+                                 unsigned high_bit_depth)
+{
+    if (!E2K_BASE(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+    // libavcodec/tests/dct -i
+
+    if (!high_bit_depth && avctx->lowres == 0) {
+        if ((avctx->idct_algo == FF_IDCT_AUTO && !(avctx->flags & AV_CODEC_FLAG_BITEXACT)) ||
+            (avctx->idct_algo == FF_IDCT_ALTIVEC)) {
+            c->idct      = ff_idct_e2k;
+            c->idct_add  = idct_add_e2k; // untested
+            c->idct_put  = idct_put_e2k; // untested
+            c->perm_type = FF_IDCT_PERM_TRANSPOSE;
+        }
+    }
+}
diff --git a/libavcodec/e2k/lossless_audiodsp.c b/libavcodec/e2k/lossless_audiodsp.c
new file mode 100644
index 0000000..1bb7c45
--- /dev/null
+++ b/libavcodec/e2k/lossless_audiodsp.c
@@ -0,0 +1,75 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2007 Luca Barbato <lu_zero@gentoo.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/lossless_audiodsp.h"
+
+#define GET_T(tt0, tt1, src, a, b) { \
+    tt0 = VEC_LD(src);               \
+    tt1 = VEC_LD(src + 8);           \
+}
+
+static int32_t scalarproduct_and_madd_int16_e2k(int16_t *v1,
+                                                const int16_t *v2,
+                                                const int16_t *v3,
+                                                int order, int mul)
+{
+    int i;
+    LOAD_ZERO;
+    vec_s16 *pv1 = (vec_s16*)v1;
+    vec_s16 muls = _mm_set1_epi16(mul);
+    vec_s16 t0, t1, i0, i1;
+    vec_s32 res = zerov;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < order; i += 16) {
+        GET_T(t0, t1, v2, i1, i2);
+        i0 = pv1[0];
+        i1 = pv1[1];
+        t0 = _mm_madd_epi16(t0, i0);
+        t1 = _mm_madd_epi16(t1, i1);
+        res = _mm_add_epi32(res, _mm_add_epi32(t0, t1));
+        GET_T(t0, t1, v3, i4, i3);
+        pv1[0] = _mm_add_epi16(_mm_mullo_epi16(t0, muls), i0);
+        pv1[1] = _mm_add_epi16(_mm_mullo_epi16(t1, muls), i1);
+        pv1 += 2;
+        v2 += 16;
+        v3 += 16;
+    }
+
+    res = _mm_hadd_epi32(res, res);
+    return _mm_extract_epi32(res, 0) + _mm_extract_epi32(res, 1);
+}
+
+av_cold void ff_llauddsp_init_e2k(LLAudDSPContext *c)
+{
+    if (!E2K_BASE(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+    c->scalarproduct_and_madd_int16 = scalarproduct_and_madd_int16_e2k;
+}
diff --git a/libavcodec/e2k/lossless_videodsp.c b/libavcodec/e2k/lossless_videodsp.c
new file mode 100644
index 0000000..a055ac7
--- /dev/null
+++ b/libavcodec/e2k/lossless_videodsp.c
@@ -0,0 +1,59 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2002 Brian Foley
+ * Copyright (c) 2002 Dieter Shirley
+ * Copyright (c) 2003-2004 Romain Dolbeau <romain@dolbeau.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/lossless_videodsp.h"
+
+static void add_bytes_e2k(uint8_t *dst, uint8_t *src, ptrdiff_t w)
+{
+    int i;
+    __m128i vdst, vsrc;
+
+    /* dst and src are 16 bytes-aligned (guaranteed). */
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i + 15 < w; i += 16) {
+        vdst = _mm_load_si128((const __m128i*)(dst + i));
+        vsrc = _mm_load_si128((const __m128i*)(src + i));
+        vdst = _mm_add_epi8(vsrc, vdst);
+        _mm_store_si128((__m128i*)(dst + i), vdst);
+    }
+    /* If w is not a multiple of 16. */
+    PRAGMA_E2K("ivdep")
+    for (; i < w; i++)
+        dst[i] = dst[i] + src[i];
+}
+
+av_cold void ff_llviddsp_init_e2k(LLVidDSPContext *c)
+{
+    if (!E2K_BASE(av_get_cpu_flags()))
+        return;
+
+    // checkasm
+    c->add_bytes = add_bytes_e2k;
+}
diff --git a/libavcodec/e2k/me_cmp.c b/libavcodec/e2k/me_cmp.c
new file mode 100644
index 0000000..e6eda38
--- /dev/null
+++ b/libavcodec/e2k/me_cmp.c
@@ -0,0 +1,461 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2002 Brian Foley
+ * Copyright (c) 2002 Dieter Shirley
+ * Copyright (c) 2003-2004 Romain Dolbeau <romain@dolbeau.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/avcodec.h"
+#include "libavcodec/mpegvideo.h"
+#include "libavcodec/me_cmp.h"
+
+#define LOAD_PIX(v1, v2, pix) { \
+    v1 = VEC_LD(pix);           \
+    v2 = VEC_LD(pix + 1);       \
+}
+
+static int sad16_x2_e2k(MpegEncContext *v, uint8_t *pix1, uint8_t *pix2,
+                        ptrdiff_t stride, int h)
+{
+    int i;
+    __m128i v0, v1, v2, sum = _mm_setzero_si128();
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        LOAD_PIX(v1, v2, pix2);
+        v0 = VEC_LD(pix1);
+        v1 = _mm_avg_epu8(v1, v2);
+        sum = _mm_add_epi32(sum, _mm_sad_epu8(v0, v1));
+
+        pix1 += stride;
+        pix2 += stride;
+    }
+    return _mm_extract_epi32(sum, 0) + _mm_extract_epi32(sum, 2);
+}
+
+static int sad8_x2_e2k(MpegEncContext *v, uint8_t *pix1, uint8_t *pix2,
+                        ptrdiff_t stride, int h)
+{
+    int i;
+    __m64 v0, v1, v2, sum = _mm_setzero_si64();
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        v1 = *(__m64*)pix2;
+        v2 = *(__m64*)(pix2 + 1);
+        v0 = *(__m64*)pix1;
+        v1 = _mm_avg_pu8(v1, v2);
+        sum = _mm_add_pi32(sum, _mm_sad_pu8(v0, v1));
+
+        pix1 += stride;
+        pix2 += stride;
+    }
+    return _mm_extract_pi32(sum, 0);
+}
+
+static int sad16_y2_e2k(MpegEncContext *v, uint8_t *pix1, uint8_t *pix2,
+                        ptrdiff_t stride, int h)
+{
+    int i;
+    __m128i v0, v1, v2, sum = _mm_setzero_si128();
+
+    v2 = VEC_LD(pix2);
+    pix2 += stride;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        v1 = v2;
+        v2 = VEC_LD(pix2);
+        v0 = VEC_LD(pix1);
+        v1 = _mm_avg_epu8(v1, v2);
+        sum = _mm_add_epi32(sum, _mm_sad_epu8(v0, v1));
+        pix1 += stride;
+        pix2 += stride;
+    }
+    return _mm_extract_epi32(sum, 0) + _mm_extract_epi32(sum, 2);
+}
+
+static int sad8_y2_e2k(MpegEncContext *v, uint8_t *pix1, uint8_t *pix2,
+                        ptrdiff_t stride, int h)
+{
+    int i;
+    __m64 v0, v1, v2, sum = _mm_setzero_si64();
+
+    v2 = *(__m64*)pix2;
+    pix2 += stride;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        v1 = v2;
+        v2 = *(__m64*)pix2;
+        v0 = *(__m64*)pix1;
+        v1 = _mm_avg_pu8(v1, v2);
+        sum = _mm_add_pi32(sum, _mm_sad_pu8(v0, v1));
+        pix1 += stride;
+        pix2 += stride;
+    }
+    return _mm_extract_pi32(sum, 0);
+}
+
+static int sad16_xy2_e2k(MpegEncContext *v, uint8_t *pix1, uint8_t *pix2,
+                         ptrdiff_t stride, int h)
+{
+    int i;
+    LOAD_ZERO;
+    __m128i v0, v1, v2, sum = zerov;
+    __m128i t0, t1, t2, t3, t4, t5;
+    __m128i c2 = _mm_set1_epi16(2);
+
+    LOAD_PIX(v1, v2, pix2);
+    t2 = _mm_unpacklo_epi8(v1, zerov);
+    t3 = _mm_unpackhi_epi8(v1, zerov);
+    t4 = _mm_unpacklo_epi8(v2, zerov);
+    t5 = _mm_unpackhi_epi8(v2, zerov);
+    t2 = _mm_add_epi16(t2, t4);
+    t3 = _mm_add_epi16(t3, t5);
+    pix2 += stride;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        t0 = t2; t1 = t3;
+        LOAD_PIX(v1, v2, pix2);
+        v0 = VEC_LD(pix1);
+        t2 = _mm_unpacklo_epi8(v1, zerov);
+        t3 = _mm_unpackhi_epi8(v1, zerov);
+        t4 = _mm_unpacklo_epi8(v2, zerov);
+        t5 = _mm_unpackhi_epi8(v2, zerov);
+        t2 = _mm_add_epi16(t2, t4);
+        t3 = _mm_add_epi16(t3, t5);
+
+        v1 = _mm_srai_epi16(_mm_add_epi16(_mm_add_epi16(t0, t2), c2), 2);
+        v2 = _mm_srai_epi16(_mm_add_epi16(_mm_add_epi16(t1, t3), c2), 2);
+        v1 = _mm_packus_epi16(v1, v2);
+
+        sum = _mm_add_epi32(sum, _mm_sad_epu8(v0, v1));
+        pix1 += stride;
+        pix2 += stride;
+    }
+    return _mm_extract_epi32(sum, 0) + _mm_extract_epi32(sum, 2);
+}
+
+static int sad8_xy2_e2k(MpegEncContext *v, uint8_t *pix1, uint8_t *pix2,
+                         ptrdiff_t stride, int h)
+{
+    int i;
+    LOAD_ZERO;
+    __m64 v0, sum = _mm_movepi64_pi64(zerov);
+    __m128i v1, v2, t0, t1, t2, c2 = _mm_set1_epi16(2);
+
+    v1 = VEC_LD8(pix2);
+    v2 = VEC_LD8(pix2 + 1);
+    t1 = _mm_unpacklo_epi8(v1, zerov);
+    t2 = _mm_unpacklo_epi8(v2, zerov);
+    t1 = _mm_add_epi16(t1, t2);
+    pix2 += stride;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        t0 = t1;
+        v1 = VEC_LD8(pix2);
+        v2 = VEC_LD8(pix2 + 1);
+        v0 = *(__m64*)pix1;
+        t1 = _mm_unpacklo_epi8(v1, zerov);
+        t2 = _mm_unpacklo_epi8(v2, zerov);
+        t1 = _mm_add_epi16(t1, t2);
+
+        v1 = _mm_srai_epi16(_mm_add_epi16(_mm_add_epi16(t0, t1), c2), 2);
+        v1 = _mm_packus_epi16(v1, v1);
+
+        sum = _mm_add_pi32(sum, _mm_sad_pu8(v0, _mm_movepi64_pi64(v1)));
+        pix1 += stride;
+        pix2 += stride;
+    }
+    return _mm_extract_pi32(sum, 0);
+}
+
+static int sad16_e2k(MpegEncContext *v, uint8_t *pix1, uint8_t *pix2,
+                     ptrdiff_t stride, int h)
+{
+    int i;
+    __m128i v0, v1, sum = _mm_setzero_si128();
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        v0 = VEC_LD(pix1);
+        v1 = VEC_LD(pix2);
+        sum = _mm_add_epi32(sum, _mm_sad_epu8(v0, v1));
+        pix1 += stride;
+        pix2 += stride;
+    }
+    return _mm_extract_epi32(sum, 0) + _mm_extract_epi32(sum, 2);
+}
+
+static int sad8_e2k(MpegEncContext *v, uint8_t *pix1, uint8_t *pix2,
+                    ptrdiff_t stride, int h)
+{
+    int i;
+    __m64 v0, v1, sum = _mm_setzero_si64();
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        v0 = *(__m64*)pix1;
+        v1 = *(__m64*)pix2;
+        sum = _mm_add_pi32(sum, _mm_sad_pu8(v0, v1));
+        pix1 += stride;
+        pix2 += stride;
+    }
+    return _mm_extract_pi32(sum, 0);
+}
+
+/* Sum of Squared Errors for an 8x8 block. */
+static int sse8_e2k(MpegEncContext *v, uint8_t *pix1, uint8_t *pix2,
+                    ptrdiff_t stride, int h)
+{
+    int i;
+    LOAD_ZERO; 
+    __m128i v0, v1, sum = zerov;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        v0 = VEC_LD8(pix1);
+        v1 = VEC_LD8(pix2);
+        v0 = _mm_unpacklo_epi8(v0, zerov);
+        v1 = _mm_unpacklo_epi8(v1, zerov);
+        v0 = _mm_sub_epi16(v0, v1);
+        sum = _mm_add_epi32(sum, _mm_madd_epi16(v0, v0));
+        pix1 += stride;
+        pix2 += stride;
+    }
+    sum = _mm_hadd_epi32(sum, sum);
+    return _mm_extract_epi32(sum, 0) + _mm_extract_epi32(sum, 1);
+}
+
+/* Sum of Squared Errors for a 16x16 block. */
+static int sse16_e2k(MpegEncContext *v, uint8_t *pix1, uint8_t *pix2,
+                     ptrdiff_t stride, int h)
+{
+    int i;
+    LOAD_ZERO; 
+    __m128i v0, v1, v2, v3, sum = zerov;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        v2 = VEC_LD(pix1);
+        v3 = VEC_LD(pix2);
+        v0 = _mm_unpacklo_epi8(v2, zerov);
+        v1 = _mm_unpacklo_epi8(v3, zerov);
+        v2 = _mm_unpackhi_epi8(v2, zerov);
+        v3 = _mm_unpackhi_epi8(v3, zerov);
+        v0 = _mm_sub_epi16(v0, v1);
+        v2 = _mm_sub_epi16(v2, v3);
+        sum = _mm_add_epi32(sum, _mm_madd_epi16(v0, v0));
+        sum = _mm_add_epi32(sum, _mm_madd_epi16(v2, v2));
+        pix1 += stride;
+        pix2 += stride;
+    }
+    sum = _mm_hadd_epi32(sum, sum);
+    return _mm_extract_epi32(sum, 0) + _mm_extract_epi32(sum, 1);
+}
+
+#define HADAMARD8_FIN(t, sum) \
+    v0 = _mm_add_epi16(t##0, t##1); \
+    v1 = _mm_sub_epi16(t##0, t##1); \
+    v2 = _mm_add_epi16(t##2, t##3); \
+    v3 = _mm_sub_epi16(t##2, t##3); \
+    v4 = _mm_add_epi16(t##4, t##5); \
+    v5 = _mm_sub_epi16(t##4, t##5); \
+    v6 = _mm_add_epi16(t##6, t##7); \
+    v7 = _mm_sub_epi16(t##6, t##7); \
+    \
+    t0 = _mm_add_epi16(v0, v2); \
+    t2 = _mm_sub_epi16(v0, v2); \
+    t1 = _mm_add_epi16(v1, v3); \
+    t3 = _mm_sub_epi16(v1, v3); \
+    t4 = _mm_add_epi16(v4, v6); \
+    t6 = _mm_sub_epi16(v4, v6); \
+    t5 = _mm_add_epi16(v5, v7); \
+    t7 = _mm_sub_epi16(v5, v7); \
+    \
+    v0 = _mm_add_epi16(t0, t4); \
+    v4 = _mm_sub_epi16(t0, t4); \
+    v1 = _mm_add_epi16(t1, t5); \
+    v5 = _mm_sub_epi16(t1, t5); \
+    v2 = _mm_add_epi16(t2, t6); \
+    v6 = _mm_sub_epi16(t2, t6); \
+    v3 = _mm_add_epi16(t3, t7); \
+    v7 = _mm_sub_epi16(t3, t7); \
+    \
+    v0 = _mm_madd_epi16(_mm_abs_epi16(v0), onev); \
+    v1 = _mm_madd_epi16(_mm_abs_epi16(v1), onev); \
+    v2 = _mm_madd_epi16(_mm_abs_epi16(v2), onev); \
+    v3 = _mm_madd_epi16(_mm_abs_epi16(v3), onev); \
+    v4 = _mm_madd_epi16(_mm_abs_epi16(v4), onev); \
+    v5 = _mm_madd_epi16(_mm_abs_epi16(v5), onev); \
+    v6 = _mm_madd_epi16(_mm_abs_epi16(v6), onev); \
+    v7 = _mm_madd_epi16(_mm_abs_epi16(v7), onev); \
+    \
+    v0 = _mm_add_epi32(v0, v1); \
+    v2 = _mm_add_epi32(v2, v3); \
+    v4 = _mm_add_epi32(v4, v5); \
+    v6 = _mm_add_epi32(v6, v7); \
+    v0 = _mm_add_epi32(v0, v2); \
+    v4 = _mm_add_epi32(v4, v6); \
+    sum = _mm_add_epi32(v0, v4);
+
+static int hadamard8_diff_e2k(MpegEncContext *s, uint8_t *dst,
+                              uint8_t *src, ptrdiff_t stride, int h)
+{
+    LOAD_ZERO;
+    vec_s16 v0, v1, v2, v3, v4, v5, v6, v7;
+    vec_s16 t0, t1, t2, t3, t4, t5, t6, t7, sum;
+    const vec_s16 onev = _mm_set1_epi16(1);
+    const vec_s16 vprod1 = _mm_setr_epi16(1, -1, 1, -1, 1, -1, 1, -1);
+    const vec_s16 vprod2 = _mm_setr_epi16(1, 1, -1, -1, 1, 1, -1, -1);
+    const vec_s16 vprod3 = _mm_setr_epi16(1, 1, 1, 1, -1, -1, -1, -1);
+    const vec_u8 perm1 = _mm_setr_epi8(
+          0x02, 0x03, 0x00, 0x01, 0x06, 0x07, 0x04, 0x05,
+          0x0A, 0x0B, 0x08, 0x09, 0x0E, 0x0F, 0x0C, 0x0D);
+
+#define ITER(i) {                                         \
+    v0 = VEC_LD8(src + stride * i);                       \
+    v1 = VEC_LD8(dst + stride * i);                       \
+    v0 = _mm_unpacklo_epi8(v0, zerov);                    \
+    v1 = _mm_unpacklo_epi8(v1, zerov);                    \
+    v0 = _mm_sub_epi16(v0, v1);                           \
+    v1 = _mm_shuffle_epi8(v0, perm1);                     \
+    v0 = _mm_add_epi16(_mm_sign_epi16(v0, vprod1), v1);   \
+    v1 = _mm_shuffle_epi32(v0, 0xb1);                     \
+    v0 = _mm_add_epi16(_mm_sign_epi16(v0, vprod2), v1);   \
+    v1 = _mm_shuffle_epi32(v0, 0x4e);                     \
+    t##i = _mm_add_epi16(_mm_sign_epi16(v0, vprod3), v1); \
+}
+    ITER(0); ITER(1); ITER(2); ITER(3);
+    ITER(4); ITER(5); ITER(6); ITER(7);
+#undef ITER
+
+    HADAMARD8_FIN(t, sum)
+
+    sum = _mm_hadd_epi32(sum, sum);
+    return _mm_extract_epi32(sum, 0) + _mm_extract_epi32(sum, 1);
+}
+
+#if 1
+static int hadamard8_diff16_e2k(MpegEncContext *s, uint8_t *dst,
+                                uint8_t *src, ptrdiff_t stride, int h)
+{
+    LOAD_ZERO;
+    vec_s16 v0, v1, v2, v3, v4, v5, v6, v7;
+    vec_s16 x0, x1, x2, x3, x4, x5, x6, x7;
+    vec_s16 t0, t1, t2, t3, t4, t5, t6, t7, sum = zerov;
+    const vec_s16 onev = _mm_set1_epi16(1);
+    const vec_s16 vprod1 = _mm_setr_epi16(1, -1, 1, -1, 1, -1, 1, -1);
+    const vec_s16 vprod2 = _mm_setr_epi16(1, 1, -1, -1, 1, 1, -1, -1);
+    const vec_s16 vprod3 = _mm_setr_epi16(1, 1, 1, 1, -1, -1, -1, -1);
+    const vec_u8 perm1 = _mm_setr_epi8(
+          0x02, 0x03, 0x00, 0x01, 0x06, 0x07, 0x04, 0x05,
+          0x0A, 0x0B, 0x08, 0x09, 0x0E, 0x0F, 0x0C, 0x0D);
+    int i;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i += 8) {
+
+#define ITER(i) {                                         \
+    v2 = VEC_LD(src + stride * i);                        \
+    v3 = VEC_LD(dst + stride * i);                        \
+    v0 = _mm_unpacklo_epi8(v2, zerov);                    \
+    v1 = _mm_unpacklo_epi8(v3, zerov);                    \
+    v2 = _mm_unpackhi_epi8(v2, zerov);                    \
+    v3 = _mm_unpackhi_epi8(v3, zerov);                    \
+    v0 = _mm_sub_epi16(v0, v1);                           \
+    v2 = _mm_sub_epi16(v2, v3);                           \
+    v1 = _mm_shuffle_epi8(v0, perm1);                     \
+    v3 = _mm_shuffle_epi8(v2, perm1);                     \
+    v0 = _mm_add_epi16(_mm_sign_epi16(v0, vprod1), v1);   \
+    v2 = _mm_add_epi16(_mm_sign_epi16(v2, vprod1), v3);   \
+    v1 = _mm_shuffle_epi32(v0, 0xb1);                     \
+    v3 = _mm_shuffle_epi32(v2, 0xb1);                     \
+    v0 = _mm_add_epi16(_mm_sign_epi16(v0, vprod2), v1);   \
+    v2 = _mm_add_epi16(_mm_sign_epi16(v2, vprod2), v3);   \
+    v1 = _mm_shuffle_epi32(v0, 0x4e);                     \
+    v3 = _mm_shuffle_epi32(v2, 0x4e);                     \
+    t##i = _mm_add_epi16(_mm_sign_epi16(v0, vprod3), v1); \
+    x##i = _mm_add_epi16(_mm_sign_epi16(v2, vprod3), v3); \
+}
+      ITER(0); ITER(1); ITER(2); ITER(3);
+      ITER(4); ITER(5); ITER(6); ITER(7);
+#undef ITER
+
+      HADAMARD8_FIN(t, v0)
+      sum = _mm_add_epi32(sum, v0);
+      HADAMARD8_FIN(x, v0)
+      sum = _mm_add_epi32(sum, v0);
+      dst += 8 * stride;
+      src += 8 * stride;
+    }
+    sum = _mm_hadd_epi32(sum, sum);
+    return _mm_extract_epi32(sum, 0) + _mm_extract_epi32(sum, 1);
+}
+#else
+static int hadamard8_diff16_e2k(MpegEncContext *s, uint8_t *dst,
+                                uint8_t *src, ptrdiff_t stride, int h)
+{
+    int i, score = 0;
+    for (i = 0; i < h; i += 8) {
+        score += hadamard8_diff_e2k(s, dst, src, stride, 8);
+        score += hadamard8_diff_e2k(s, dst + 8, src + 8, stride, 8);
+        dst += 8 * stride;
+        src += 8 * stride;
+    }
+    return score;
+}
+#endif
+
+av_cold void ff_me_cmp_init_e2k(MECmpContext *c, AVCodecContext *avctx)
+{
+    if (!E2K_BASE(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+
+    // fate lavf-mxf
+    c->pix_abs[0][0] = sad16_e2k;
+    c->pix_abs[0][1] = sad16_x2_e2k;
+    c->pix_abs[0][2] = sad16_y2_e2k;
+    c->pix_abs[0][3] = sad16_xy2_e2k;
+    c->pix_abs[1][0] = sad8_e2k;
+    c->pix_abs[1][1] = sad8_x2_e2k;
+    c->pix_abs[1][2] = sad8_y2_e2k;
+    c->pix_abs[1][3] = sad8_xy2_e2k;
+
+    c->sad[0] = sad16_e2k;
+    c->sad[1] = sad8_e2k;
+    c->sse[0] = sse16_e2k;
+    c->sse[1] = sse8_e2k;
+
+    // fate vsynth1-mpeg4-qprd
+    c->hadamard8_diff[0] = hadamard8_diff16_e2k;
+    c->hadamard8_diff[1] = hadamard8_diff_e2k;
+}
diff --git a/libavcodec/e2k/mpegaudiodsp.c b/libavcodec/e2k/mpegaudiodsp.c
new file mode 100644
index 0000000..2ccdba8
--- /dev/null
+++ b/libavcodec/e2k/mpegaudiodsp.c
@@ -0,0 +1,142 @@
+/*
+ * Elbrus optimized MP3 decoding functions
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2010 Vitor Sessak
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include "libavutil/internal.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+#include "libavcodec/mpegaudiodsp.h"
+
+#define MACS(rt, ra, rb) rt += (ra) * (rb)
+#define MLSS(rt, ra, rb) rt -= (ra) * (rb)
+
+#define SUM8(op, sum, w, p) {             \
+    op(sum, (w)[0 * 64], (p)[0 * 64]);    \
+    op(sum, (w)[1 * 64], (p)[1 * 64]);    \
+    op(sum, (w)[2 * 64], (p)[2 * 64]);    \
+    op(sum, (w)[3 * 64], (p)[3 * 64]);    \
+    op(sum, (w)[4 * 64], (p)[4 * 64]);    \
+    op(sum, (w)[5 * 64], (p)[5 * 64]);    \
+    op(sum, (w)[6 * 64], (p)[6 * 64]);    \
+    op(sum, (w)[7 * 64], (p)[7 * 64]);    \
+}
+
+static av_always_inline
+void apply_window(const float *buf, const float *win1,
+                  const float *win2, float *sum1, float *sum2, int len)
+{
+    vec_f v0, v1, v2, v3, v4, v5;
+    int i;
+
+#define MULT(j)                                   \
+    v1 = _mm_load_ps(win1 + j * 64);              \
+    v2 = _mm_load_ps(win2 + j * 16);              \
+    v3 = _mm_load_ps(buf + j * 64);               \
+    v0 = _mm_sub_ps(v0, _mm_mul_ps(v3, v1));      \
+    v4 = _mm_sub_ps(v4, _mm_mul_ps(v2, v3))
+
+    v0 = _mm_setzero_ps();
+    v4 = _mm_setzero_ps();
+    MULT(0); MULT(1); MULT(2); MULT(3);
+    MULT(4); MULT(5); MULT(6); MULT(7);
+
+    PRAGMA_E2K("ivdep")
+    for (i = 4; i < len; i += 4) {
+        win1 += 4; win2 += 4; buf += 4;
+        _mm_store_ps(sum1, v0); v5 = v4;
+
+        v0 = _mm_setzero_ps();
+        v4 = _mm_setzero_ps();
+        MULT(0); MULT(1); MULT(2); MULT(3);
+        MULT(4); MULT(5); MULT(6); MULT(7);
+        _mm_store_ps(sum2, _mm_alignr_ps(v4, v5, 1));
+        sum1 += 4; sum2 += 4;
+    }
+    _mm_store_ps(sum1, v0);
+    _mm_store_ps(sum2, _mm_bsrli_ps(v4, 1));
+
+#undef MULT
+}
+
+static void apply_window_mp3(float *in, float *win, int *unused, float *out,
+                             ptrdiff_t incr)
+{
+    float ALIGNED(16) suma[16];
+    float ALIGNED(16) sumb[16];
+    float ALIGNED(16) sumc[16];
+    float ALIGNED(16) sumd[16];
+    float sum;
+
+    /* copy to avoid wrap */
+    memcpy(in + 512, in, 32 * sizeof(*in));
+
+    apply_window(in + 16, win     , win + 512, suma, sumc, 16);
+    apply_window(in + 32, win + 48, win + 640, sumb, sumd, 16);
+
+    sum = suma[0];
+    SUM8(MACS, sum, win + 32, in + 48);
+    suma[0] = sum;
+
+#define SUMS(a, b)                              \
+    v0 = _mm_load_ps(sumd + b);                 \
+    v1 = _mm_load_ps(sumc + a);                 \
+    v0 = _mm_shuffle_ps(v0, v0, 0x1b);          \
+    v1 = _mm_shuffle_ps(v1, v1, 0x1b);          \
+    v0 = _mm_sub_ps(v0, _mm_load_ps(suma + a)); \
+    v1 = _mm_add_ps(v1, _mm_load_ps(sumb + b)); \
+    _mm_storeu_ps(out + a, v0);                 \
+    _mm_storeu_ps(out + b + 16, v1)
+
+    if (incr == 1) {
+        vec_f v0, v1;
+        SUMS(0, 12); SUMS(4, 8); SUMS(8, 4); SUMS(12, 0);
+        out += 16 * incr;
+    } else {
+        int j;
+        float *out2 = out + 32 * incr;
+        out[0] = -suma[0];
+        out += incr;
+        out2 -= incr;
+        PRAGMA_E2K("ivdep")
+        for (j = 1; j < 16; j++) {
+            *out  = sumd[15 - j] - suma[j];
+            *out2 = sumb[16 - j] + sumc[j - 1];
+            out  += incr;
+            out2 -= incr;
+        }
+    }
+
+    sum = 0;
+    SUM8(MLSS, sum, win + 16 + 32, in + 32);
+    *out = sum;
+}
+
+av_cold void ff_mpadsp_init_e2k(MPADSPContext *s)
+{
+    if (!E2K_BASE(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+    s->apply_window_float = apply_window_mp3; // fate audiomatch-square-mp3
+}
diff --git a/libavcodec/e2k/mpegvideo.c b/libavcodec/e2k/mpegvideo.c
new file mode 100644
index 0000000..36bf975
--- /dev/null
+++ b/libavcodec/e2k/mpegvideo.c
@@ -0,0 +1,100 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2002 Dieter Shirley
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include <stdlib.h>
+#include <stdio.h>
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/mpegvideo.h"
+
+/* this code assumes `block' is 16 bytes-aligned */
+static void dct_unquantize_h263_intra_e2k(MpegEncContext *s,
+                                          int16_t *block, int n, int qscale)
+{
+    int level, qmul, qadd = 0, nCoeffs = 63, j;
+    __m128i qmulv, qaddv, v0, v1;
+
+    qmul = qscale << 1;
+    level = block[0];
+
+    if (!s->h263_aic) {
+        level *= n < 4 ? s->y_dc_scale : s->c_dc_scale;
+        qadd = (qscale - 1) | 1;
+    } else {
+        av_assert2(s->block_last_index[n] >= 0);
+    }
+    if (!s->ac_pred) {
+        nCoeffs = s->intra_scantable.raster_end[s->block_last_index[n]];
+    }
+
+    qmulv = _mm_set1_epi16(qmul);
+    qaddv = _mm_set1_epi16(qadd);
+    PRAGMA_E2K("ivdep")
+    for (j = 0; j <= nCoeffs; j += 8) {
+        v0 = _mm_load_si128((const __m128i*)(block + j));
+        v1 = _mm_mullo_epi16(v0, qmulv);
+        v1 = _mm_add_epi16(v1, _mm_sign_epi16(qaddv, v0));
+        _mm_store_si128((__m128i*)(block + j), v1);
+    }
+
+    block[0] = level;
+}
+
+static void dct_unquantize_h263_inter_e2k(MpegEncContext *s,
+                                          int16_t *block, int n, int qscale)
+{
+    int qmul, qadd, nCoeffs, j;
+    __m128i qmulv, qaddv, v0, v1;
+
+    qmul = qscale << 1;
+    qadd = (qscale - 1) | 1;
+
+    av_assert2(s->block_last_index[n] >= 0 || s->h263_aic);
+    nCoeffs = s->inter_scantable.raster_end[s->block_last_index[n]];
+
+    qmulv = _mm_set1_epi16(qmul);
+    qaddv = _mm_set1_epi16(qadd);
+    PRAGMA_E2K("ivdep")
+    for (j = 0; j <= nCoeffs; j += 8) {
+        v0 = _mm_load_si128((const __m128i*)(block + j));
+        v1 = _mm_mullo_epi16(v0, qmulv);
+        v1 = _mm_add_epi16(v1, _mm_sign_epi16(qaddv, v0));
+        _mm_store_si128((__m128i*)(block + j), v1);
+    }
+}
+
+av_cold void ff_mpv_common_init_e2k(MpegEncContext *s)
+{
+    if (!E2K_BASE(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+    // fate flv-add_keyframe_index
+    s->dct_unquantize_h263_intra = dct_unquantize_h263_intra_e2k;
+    s->dct_unquantize_h263_inter = dct_unquantize_h263_inter_e2k;
+}
+
diff --git a/libavcodec/e2k/mpegvideodsp.c b/libavcodec/e2k/mpegvideodsp.c
new file mode 100644
index 0000000..3d44735
--- /dev/null
+++ b/libavcodec/e2k/mpegvideodsp.c
@@ -0,0 +1,86 @@
+/*
+ * GMC (Global Motion Compensation)
+ *
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2003 Romain Dolbeau <romain@dolbeau.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/cpu.h"
+#include "libavutil/mem.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/mpegvideodsp.h"
+
+/* ATM this code assumes stride is a multiple of 8
+ * to preserve proper dst alignment. */
+static void gmc1_e2k(uint8_t *dst /* align 8 */, uint8_t *src /* align1 */,
+                     int stride, int h, int x16, int y16, int rounder)
+{
+    int i;
+    LOAD_ZERO;
+    vec_u8 dstv, srcvA, srcvB;
+    vec_u16 t0, t1, t2, t3;
+
+    vec_u16 Av = _mm_set1_epi16((16 - x16) * (16 - y16));
+    vec_u16 Bv = _mm_set1_epi16(      x16  * (16 - y16));
+    vec_u16 Cv = _mm_set1_epi16((16 - x16) * y16);
+    vec_u16 Dv = _mm_set1_epi16(      x16  * y16);
+    vec_u16 rounderV = _mm_set1_epi16(rounder);
+
+    vec_u8 srcvC = VEC_LD8(src);
+    vec_u8 srcvD = VEC_LD8(src + 1);
+    srcvC = _mm_unpacklo_epi8(srcvC, zerov);
+    srcvD = _mm_unpacklo_epi8(srcvD, zerov);
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        src += stride;
+
+        srcvA = srcvC;
+        srcvB = srcvD;
+        srcvC = VEC_LD8(src);
+        srcvD = VEC_LD8(src + 1);
+        srcvC = _mm_unpacklo_epi8(srcvC, zerov);
+        srcvD = _mm_unpacklo_epi8(srcvD, zerov);
+
+        t0 = _mm_mullo_epi16(srcvA, Av);
+        t1 = _mm_mullo_epi16(srcvB, Bv);
+        t0 = _mm_add_epi16(t0, t1);
+        t2 = _mm_mullo_epi16(srcvC, Cv);
+        t3 = _mm_mullo_epi16(srcvD, Dv);
+        t0 = _mm_add_epi16(t0, rounderV);
+        t2 = _mm_add_epi16(t2, t3);
+        t0 = _mm_add_epi16(t0, t2);
+        t0 = _mm_srli_epi16(t0, 8);
+        dstv = _mm_packus_epi16(t0, t0);
+
+        VEC_STL(dst, dstv);
+        dst += stride;
+    }
+}
+
+av_cold void ff_mpegvideodsp_init_e2k(MpegVideoDSPContext *c)
+{
+    if (!E2K_BASE(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+    c->gmc1 = gmc1_e2k;
+}
diff --git a/libavcodec/e2k/mpegvideoencdsp.c b/libavcodec/e2k/mpegvideoencdsp.c
new file mode 100644
index 0000000..c5d3e4d
--- /dev/null
+++ b/libavcodec/e2k/mpegvideoencdsp.c
@@ -0,0 +1,75 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include <stdint.h>
+
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/mpegvideoencdsp.h"
+
+static int pix_norm1_e2k(uint8_t *pix, int line_size)
+{
+    int i;
+    LOAD_ZERO; 
+    __m128i v0, v1, sum = zerov;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < 16; i++) {
+        v1 = VEC_LD(pix);
+        v0 = _mm_unpacklo_epi8(v1, zerov);
+        v1 = _mm_unpackhi_epi8(v1, zerov);
+        sum = _mm_add_epi32(sum, _mm_madd_epi16(v0, v0));
+        sum = _mm_add_epi32(sum, _mm_madd_epi16(v1, v1));
+        pix += line_size;
+    }
+    sum = _mm_hadd_epi32(sum, sum);
+    return _mm_extract_epi32(sum, 0) + _mm_extract_epi32(sum, 1);
+}
+
+static int pix_sum_e2k(uint8_t *pix, int line_size)
+{
+    int i;
+    LOAD_ZERO;
+    __m128i v0, sum = zerov;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < 16; i++) {
+        v0 = VEC_LD(pix);
+        sum = _mm_add_epi32(sum, _mm_sad_epu8(v0, zerov));
+        pix += line_size;
+    }
+    return _mm_extract_epi32(sum, 0) + _mm_extract_epi32(sum, 2);
+}
+
+av_cold void ff_mpegvideoencdsp_init_e2k(MpegvideoEncDSPContext *c,
+                                         AVCodecContext *avctx)
+{
+    if (!E2K_BASE(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+    c->pix_norm1 = pix_norm1_e2k;
+    c->pix_sum   = pix_sum_e2k;
+}
diff --git a/libavcodec/e2k/pixblockdsp.c b/libavcodec/e2k/pixblockdsp.c
new file mode 100644
index 0000000..f5a5060
--- /dev/null
+++ b/libavcodec/e2k/pixblockdsp.c
@@ -0,0 +1,83 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2002 Brian Foley
+ * Copyright (c) 2002 Dieter Shirley
+ * Copyright (c) 2003-2004 Romain Dolbeau <romain@dolbeau.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/avcodec.h"
+#include "libavcodec/pixblockdsp.h"
+
+static void get_pixels_e2k(int16_t * restrict block, const uint8_t *pixels,
+                           ptrdiff_t stride)
+{
+    LOAD_ZERO;
+    __m128i v0;
+    int i;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < 8; i++) {
+        v0 = VEC_LD8(pixels);
+        v0 = _mm_unpacklo_epi8(v0, zerov);
+        VEC_ST(block + i * 8, v0);
+        pixels += stride;
+    }
+}
+
+static void diff_pixels_e2k(int16_t * restrict block, const uint8_t *s1,
+                            const uint8_t *s2, ptrdiff_t stride)
+{
+    LOAD_ZERO;
+    __m128i v0, v1;
+    int i;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < 8; i++) {
+        v0 = VEC_LD8(s1);
+        v1 = VEC_LD8(s2);
+        v0 = _mm_unpacklo_epi8(v0, zerov);
+        v1 = _mm_unpacklo_epi8(v1, zerov);
+        v0 = _mm_sub_epi16(v0, v1);
+        VEC_ST(block + i * 8, v0);
+        s1 += stride;
+        s2 += stride;
+    }
+}
+
+av_cold void ff_pixblockdsp_init_e2k(PixblockDSPContext *c,
+                                     AVCodecContext *avctx,
+                                     unsigned high_bit_depth)
+{
+    if (!E2K_BASE(av_get_cpu_flags()))
+        return;
+
+    // checkasm
+
+    c->diff_pixels = diff_pixels_e2k;
+
+    if (!high_bit_depth)
+        c->get_pixels = get_pixels_e2k;
+}
diff --git a/libavcodec/e2k/svq1enc.c b/libavcodec/e2k/svq1enc.c
new file mode 100644
index 0000000..263ac60
--- /dev/null
+++ b/libavcodec/e2k/svq1enc.c
@@ -0,0 +1,68 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2007 Luca Barbato <lu_zero@gentoo.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include <stdint.h>
+
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/svq1enc.h"
+
+static int ssd_int8_vs_int16_e2k(const int8_t *pix1, const int16_t *pix2,
+                                 intptr_t size)
+{
+    int i, res;
+    __m128i v0, v1, v2, v3, sum = _mm_setzero_si128();
+
+    for (i = 0; i + 15 < size; i += 16) {
+        v1 = VEC_LD(pix1);
+        v0 = _mm_srai_epi16(_mm_unpacklo_epi8(v1, v1), 8);
+        v1 = _mm_srai_epi16(_mm_unpackhi_epi8(v1, v1), 8);
+        v2 = VEC_LD(pix2);
+        v3 = VEC_LD(pix2 + 8);
+        v0 = _mm_sub_epi16(v0, v2);
+        v1 = _mm_sub_epi16(v1, v3);
+        sum = _mm_add_epi32(sum, _mm_madd_epi16(v0, v0));
+        sum = _mm_add_epi32(sum, _mm_madd_epi16(v1, v1));
+        pix1 += 16;
+        pix2 += 16;
+    }
+    sum = _mm_hadd_epi32(sum, sum);
+    res = _mm_extract_epi32(sum, 0) + _mm_extract_epi32(sum, 1);
+
+    for (; i < size; i++)
+        res += (pix1[i] - pix2[i]) * (pix1[i] - pix2[i]);
+
+    return res;
+}
+
+av_cold void ff_svq1enc_init_e2k(SVQ1EncContext *c)
+{
+    if (!E2K_BASE(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+    c->ssd_int8_vs_int16 = ssd_int8_vs_int16_e2k;
+}
diff --git a/libavcodec/e2k/vc1dsp.c b/libavcodec/e2k/vc1dsp.c
new file mode 100644
index 0000000..91307a9
--- /dev/null
+++ b/libavcodec/e2k/vc1dsp.c
@@ -0,0 +1,303 @@
+/*
+ * VC-1 and WMV3 decoder - DSP functions
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2006 Konstantin Shishkov
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/vc1dsp.h"
+
+// main steps of 8x8 transform
+#define STEP8(s0, s1, s2, s3, s4, s5, s6, s7, vec_rnd) do { \
+    t0 = _mm_slli_epi32(_mm_add_epi32(s0, s4), 2); \
+    t0 = _mm_add_epi32(_mm_slli_epi32(t0, 1), t0); \
+    t0 = _mm_add_epi32(t0, vec_rnd); \
+    t1 = _mm_slli_epi32(_mm_sub_epi32(s0, s4), 2); \
+    t1 = _mm_add_epi32(_mm_slli_epi32(t1, 1), t1); \
+    t1 = _mm_add_epi32(t1, vec_rnd); \
+    t2 = _mm_add_epi32(_mm_slli_epi32(s6, 2), _mm_slli_epi32(s6, 1)); \
+    t2 = _mm_add_epi32(t2, _mm_slli_epi32(s2, 4)); \
+    t3 = _mm_add_epi32(_mm_slli_epi32(s2, 2), _mm_slli_epi32(s2, 1)); \
+    t3 = _mm_sub_epi32(t3, _mm_slli_epi32(s6, 4)); \
+    t4 = _mm_add_epi32(t0, t2); \
+    t5 = _mm_add_epi32(t1, t3); \
+    t6 = _mm_sub_epi32(t1, t3); \
+    t7 = _mm_sub_epi32(t0, t2); \
+\
+    t0 = _mm_slli_epi32(_mm_add_epi32(s1, s3), 4); \
+    t0 = _mm_add_epi32(t0, _mm_slli_epi32(s5, 3)); \
+    t0 = _mm_add_epi32(t0, _mm_slli_epi32(s7, 2)); \
+    t0 = _mm_add_epi32(t0, _mm_sub_epi32(s5, s3)); \
+\
+    t1 = _mm_slli_epi32(_mm_sub_epi32(s1, s5), 4); \
+    t1 = _mm_sub_epi32(t1, _mm_slli_epi32(s7, 3)); \
+    t1 = _mm_sub_epi32(t1, _mm_slli_epi32(s3, 2)); \
+    t1 = _mm_sub_epi32(t1, _mm_add_epi32(s1, s7)); \
+\
+    t2 = _mm_slli_epi32(_mm_sub_epi32(s7, s3), 4); \
+    t2 = _mm_add_epi32(t2, _mm_slli_epi32(s1, 3)); \
+    t2 = _mm_add_epi32(t2, _mm_slli_epi32(s5, 2)); \
+    t2 = _mm_add_epi32(t2, _mm_sub_epi32(s1, s7)); \
+\
+    t3 = _mm_slli_epi32(_mm_sub_epi32(s5, s7), 4); \
+    t3 = _mm_sub_epi32(t3, _mm_slli_epi32(s3, 3)); \
+    t3 = _mm_add_epi32(t3, _mm_slli_epi32(s1, 2)); \
+    t3 = _mm_sub_epi32(t3, _mm_add_epi32(s3, s5)); \
+\
+    s0 = _mm_add_epi32(t4, t0); \
+    s1 = _mm_add_epi32(t5, t1); \
+    s2 = _mm_add_epi32(t6, t2); \
+    s3 = _mm_add_epi32(t7, t3); \
+    s4 = _mm_sub_epi32(t7, t3); \
+    s5 = _mm_sub_epi32(t6, t2); \
+    s6 = _mm_sub_epi32(t5, t1); \
+    s7 = _mm_sub_epi32(t4, t0); \
+}while(0)
+
+#define SHIFT_HOR8(s0, s1, s2, s3, s4, s5, s6, s7) do { \
+    s0 = _mm_srai_epi32(s0, 3); \
+    s1 = _mm_srai_epi32(s1, 3); \
+    s2 = _mm_srai_epi32(s2, 3); \
+    s3 = _mm_srai_epi32(s3, 3); \
+    s4 = _mm_srai_epi32(s4, 3); \
+    s5 = _mm_srai_epi32(s5, 3); \
+    s6 = _mm_srai_epi32(s6, 3); \
+    s7 = _mm_srai_epi32(s7, 3); \
+} while(0)
+
+#define SHIFT_VERT8(s0, s1, s2, s3, s4, s5, s6, s7) do { \
+    s0 = _mm_srai_epi32(s0, 7); \
+    s1 = _mm_srai_epi32(s1, 7); \
+    s2 = _mm_srai_epi32(s2, 7); \
+    s3 = _mm_srai_epi32(s3, 7); \
+    s4 = _mm_srai_epi32(_mm_add_epi32(s4, c1), 7); \
+    s5 = _mm_srai_epi32(_mm_add_epi32(s5, c1), 7); \
+    s6 = _mm_srai_epi32(_mm_add_epi32(s6, c1), 7); \
+    s7 = _mm_srai_epi32(_mm_add_epi32(s7, c1), 7); \
+} while(0)
+
+/* main steps of 4x4 transform */
+#define STEP4(s0, s1, s2, s3, vec_rnd) do { \
+    t1 = _mm_add_epi32(_mm_slli_epi32(s0, 4), s0); \
+    t1 = _mm_add_epi32(t1, vec_rnd); \
+    t2 = _mm_add_epi32(_mm_slli_epi32(s2, 4), s2); \
+    t0 = _mm_add_epi32(t1, t2); \
+    t1 = _mm_sub_epi32(t1, t2); \
+    t3 = _mm_slli_epi32(_mm_sub_epi32(s3, s1), 1); \
+    t3 = _mm_add_epi32(t3, _mm_slli_epi32(t3, 2)); \
+    t2 = _mm_add_epi32(t3, _mm_slli_epi32(s1, 5)); \
+    t3 = _mm_add_epi32(t3, _mm_slli_epi32(s3, 3)); \
+    t3 = _mm_add_epi32(t3, _mm_slli_epi32(s3, 2)); \
+    s0 = _mm_add_epi32(t0, t2); \
+    s1 = _mm_sub_epi32(t1, t3); \
+    s2 = _mm_add_epi32(t1, t3); \
+    s3 = _mm_sub_epi32(t0, t2); \
+} while (0)
+
+#define SHIFT_HOR4(s0, s1, s2, s3) \
+    s0 = _mm_srai_epi32(s0, 3); \
+    s1 = _mm_srai_epi32(s1, 3); \
+    s2 = _mm_srai_epi32(s2, 3); \
+    s3 = _mm_srai_epi32(s3, 3)
+
+#define SHIFT_VERT4(s0, s1, s2, s3) \
+    s0 = _mm_srai_epi32(s0, 7); \
+    s1 = _mm_srai_epi32(s1, 7); \
+    s2 = _mm_srai_epi32(s2, 7); \
+    s3 = _mm_srai_epi32(s3, 7)
+
+#define _mm_unpacklo1_epi16(v) _mm_srai_epi32(_mm_unpacklo_epi16(v, v), 16)
+#define _mm_unpackhi1_epi16(v) _mm_srai_epi32(_mm_unpackhi_epi16(v, v), 16)
+
+#define TRANSPOSE4_32(s0, s1, s2, s3) \
+    t0 = _mm_unpacklo_epi32(s0, s1); \
+    t1 = _mm_unpackhi_epi32(s0, s1); \
+    t2 = _mm_unpacklo_epi32(s2, s3); \
+    t3 = _mm_unpackhi_epi32(s2, s3); \
+    s0 = _mm_unpacklo_epi64(t0, t2); \
+    s1 = _mm_unpackhi_epi64(t0, t2); \
+    s2 = _mm_unpacklo_epi64(t1, t3); \
+    s3 = _mm_unpackhi_epi64(t1, t3);
+
+/* Do inverse transform on 8x8 block */
+static void vc1_inv_trans_8x8_e2k(int16_t block[64])
+{
+    vec_s16 src0, src1, src2, src3, src4, src5, src6, src7;
+    vec_s32 s0, s1, s2, s3, s4, s5, s6, s7;
+    vec_s32 s8, s9, sA, sB, sC, sD, sE, sF;
+    vec_s32 t0, t1, t2, t3, t4, t5, t6, t7;
+    const vec_s32 c64 = _mm_set1_epi32(64);
+    const vec_s32 c4 = _mm_set1_epi32(4);
+    const vec_s32 c1 = _mm_set1_epi32(1);
+
+    src0 = VEC_LD(block + 8 * 0);
+    src1 = VEC_LD(block + 8 * 1);
+    src2 = VEC_LD(block + 8 * 2);
+    src3 = VEC_LD(block + 8 * 3);
+    src4 = VEC_LD(block + 8 * 4);
+    src5 = VEC_LD(block + 8 * 5);
+    src6 = VEC_LD(block + 8 * 6);
+    src7 = VEC_LD(block + 8 * 7);
+
+    s0 = _mm_unpacklo1_epi16(src0);
+    s1 = _mm_unpacklo1_epi16(src1);
+    s2 = _mm_unpacklo1_epi16(src2);
+    s3 = _mm_unpacklo1_epi16(src3);
+    s4 = _mm_unpacklo1_epi16(src4);
+    s5 = _mm_unpacklo1_epi16(src5);
+    s6 = _mm_unpacklo1_epi16(src6);
+    s7 = _mm_unpacklo1_epi16(src7);
+    s8 = _mm_unpackhi1_epi16(src0);
+    s9 = _mm_unpackhi1_epi16(src1);
+    sA = _mm_unpackhi1_epi16(src2);
+    sB = _mm_unpackhi1_epi16(src3);
+    sC = _mm_unpackhi1_epi16(src4);
+    sD = _mm_unpackhi1_epi16(src5);
+    sE = _mm_unpackhi1_epi16(src6);
+    sF = _mm_unpackhi1_epi16(src7);
+    STEP8(s0, s1, s2, s3, s4, s5, s6, s7, c4);
+    SHIFT_HOR8(s0, s1, s2, s3, s4, s5, s6, s7);
+    STEP8(s8, s9, sA, sB, sC, sD, sE, sF, c4);
+    SHIFT_HOR8(s8, s9, sA, sB, sC, sD, sE, sF);
+
+    TRANSPOSE4_32(s0, s1, s2, s3)
+    TRANSPOSE4_32(s4, s5, s6, s7)
+    TRANSPOSE4_32(s8, s9, sA, sB)
+    TRANSPOSE4_32(sC, sD, sE, sF)
+
+    STEP8(s0, s1, s2, s3, s8, s9, sA, sB, c64);
+    SHIFT_VERT8(s0, s1, s2, s3, s8, s9, sA, sB);
+    STEP8(s4, s5, s6, s7, sC, sD, sE, sF, c64);
+    SHIFT_VERT8(s4, s5, s6, s7, sC, sD, sE, sF);
+    src0 = _mm_packs_epi32(s0, s4);
+    src1 = _mm_packs_epi32(s1, s5);
+    src2 = _mm_packs_epi32(s2, s6);
+    src3 = _mm_packs_epi32(s3, s7);
+    src4 = _mm_packs_epi32(s8, sC);
+    src5 = _mm_packs_epi32(s9, sD);
+    src6 = _mm_packs_epi32(sA, sE);
+    src7 = _mm_packs_epi32(sB, sF);
+
+    VEC_ST(block + 8 * 0, src0);
+    VEC_ST(block + 8 * 1, src1);
+    VEC_ST(block + 8 * 2, src2);
+    VEC_ST(block + 8 * 3, src3);
+    VEC_ST(block + 8 * 4, src4);
+    VEC_ST(block + 8 * 5, src5);
+    VEC_ST(block + 8 * 6, src6);
+    VEC_ST(block + 8 * 7, src7);
+}
+
+/* Do inverse transform on 8x4 part of block */
+static void vc1_inv_trans_8x4_e2k(uint8_t *dest, ptrdiff_t stride,
+                                  int16_t *block)
+{
+    LOAD_ZERO;
+    vec_s16 src0, src1, src2, src3;
+    vec_s32 s0, s1, s2, s3, s4, s5, s6, s7;
+    vec_s32 t0, t1, t2, t3, t4, t5, t6, t7;
+    const vec_s32 c64 = _mm_set1_epi32(64);
+    const vec_s32 c4 = _mm_set1_epi32(4);
+    __m128i tmp;
+
+    src0 = VEC_LD(block + 8 * 0);
+    src1 = VEC_LD(block + 8 * 1);
+    src2 = VEC_LD(block + 8 * 2);
+    src3 = VEC_LD(block + 8 * 3);
+
+    t0 = _mm_unpacklo_epi16(src0, src1);
+    t1 = _mm_unpackhi_epi16(src0, src1);
+    t2 = _mm_unpacklo_epi16(src2, src3);
+    t3 = _mm_unpackhi_epi16(src2, src3);
+
+    t4 = _mm_unpacklo_epi32(t0, t2);
+    t5 = _mm_unpackhi_epi32(t0, t2);
+    t6 = _mm_unpacklo_epi32(t1, t3);
+    t7 = _mm_unpackhi_epi32(t1, t3);
+
+    s0 = _mm_unpacklo1_epi16(t4);
+    s1 = _mm_unpackhi1_epi16(t4);
+    s2 = _mm_unpacklo1_epi16(t5);
+    s3 = _mm_unpackhi1_epi16(t5);
+    s4 = _mm_unpacklo1_epi16(t6);
+    s5 = _mm_unpackhi1_epi16(t6);
+    s6 = _mm_unpacklo1_epi16(t7);
+    s7 = _mm_unpackhi1_epi16(t7);
+
+    STEP8(s0, s1, s2, s3, s4, s5, s6, s7, c4);
+    SHIFT_HOR8(s0, s1, s2, s3, s4, s5, s6, s7);
+
+    TRANSPOSE4_32(s0, s1, s2, s3)
+    TRANSPOSE4_32(s4, s5, s6, s7)
+
+    STEP4(s0, s1, s2, s3, c64);
+    SHIFT_VERT4(s0, s1, s2, s3);
+    STEP4(s4, s5, s6, s7, c64);
+    SHIFT_VERT4(s4, s5, s6, s7);
+    src0 = _mm_packs_epi32(s0, s4);
+    src1 = _mm_packs_epi32(s1, s5);
+    src2 = _mm_packs_epi32(s2, s6);
+    src3 = _mm_packs_epi32(s3, s7);
+
+#define ADD(dest, src)                   \
+    tmp = VEC_LD8(dest);                 \
+    tmp = _mm_unpacklo_epi8(tmp, zerov); \
+    tmp = _mm_adds_epi16(tmp, src);      \
+    tmp = _mm_packus_epi16(tmp, tmp);    \
+    VEC_STL(dest, tmp)
+
+    ADD(dest, src0); dest += stride;
+    ADD(dest, src1); dest += stride;
+    ADD(dest, src2); dest += stride;
+    ADD(dest, src3);
+}
+
+#define PUT_OP_U8_E2K(d, s, dst) d = s
+#define AVG_OP_U8_E2K(d, s, dst) d = _mm_avg_epu8(dst, s)
+
+#define OP_U8_E2K                          PUT_OP_U8_E2K
+#define PREFIX_no_rnd_vc1_chroma_mc8_e2k   put_no_rnd_vc1_chroma_mc8_e2k
+#include "h264chroma_template.c"
+#undef OP_U8_E2K
+#undef PREFIX_no_rnd_vc1_chroma_mc8_e2k
+
+#define OP_U8_E2K                          AVG_OP_U8_E2K
+#define PREFIX_no_rnd_vc1_chroma_mc8_e2k   avg_no_rnd_vc1_chroma_mc8_e2k
+#include "h264chroma_template.c"
+#undef OP_U8_E2K
+#undef PREFIX_no_rnd_vc1_chroma_mc8_e2k
+
+av_cold void ff_vc1dsp_init_e2k(VC1DSPContext *dsp)
+{
+    if (!E2K_BASE(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+
+    dsp->vc1_inv_trans_8x8 = vc1_inv_trans_8x8_e2k; // fate mss2-wmv
+    dsp->vc1_inv_trans_8x4 = vc1_inv_trans_8x4_e2k; // fate wmv3-drm-dec
+    dsp->put_no_rnd_vc1_chroma_pixels_tab[0] = put_no_rnd_vc1_chroma_mc8_e2k;
+    dsp->avg_no_rnd_vc1_chroma_pixels_tab[0] = avg_no_rnd_vc1_chroma_mc8_e2k;
+}
diff --git a/libavcodec/e2k/videodsp.c b/libavcodec/e2k/videodsp.c
new file mode 100644
index 0000000..d831d68
--- /dev/null
+++ b/libavcodec/e2k/videodsp.c
@@ -0,0 +1,36 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2003-2004 Romain Dolbeau
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/attributes.h"
+#include "libavcodec/videodsp.h"
+
+static void prefetch_e2k(uint8_t *mem, ptrdiff_t stride, int h)
+{
+    do {
+        __builtin_prefetch(mem);
+        mem += stride;
+    } while (--h);
+}
+
+av_cold void ff_videodsp_init_e2k(VideoDSPContext *ctx, int bpc)
+{
+    ctx->prefetch = prefetch_e2k;
+}
diff --git a/libavcodec/e2k/vorbisdsp.c b/libavcodec/e2k/vorbisdsp.c
new file mode 100644
index 0000000..7a7619e
--- /dev/null
+++ b/libavcodec/e2k/vorbisdsp.c
@@ -0,0 +1,62 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2006 Luca Barbato <lu_zero@gentoo.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/vorbisdsp.h"
+
+static void vorbis_inverse_coupling_e2k(float *mag, float *ang,
+                                        intptr_t blocksize)
+{
+    int i;
+    vec_f m, a, t0, t1, zerov = _mm_setzero_ps();
+    vec_f sign = _mm_castsi128_ps(_mm_set1_epi32(1 << 31));
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < blocksize; i += 4) {
+        m = _mm_load_ps(mag + i);
+        a = _mm_load_ps(ang + i);
+        t0 = _mm_cmple_ps(m, zerov);
+        t1 = _mm_cmple_ps(a, zerov);
+        a = _mm_xor_ps(a, _mm_and_ps(t0, sign));
+        t0 = _mm_andnot_ps(t1, a);
+        t1 = _mm_and_ps(t1, a);
+        a = _mm_sub_ps(m, t0);
+        m = _mm_add_ps(m, t1);
+        _mm_store_ps(ang + i, a);
+        _mm_store_ps(mag + i, m);
+    }
+}
+
+av_cold void ff_vorbisdsp_init_e2k(VorbisDSPContext *c)
+{
+    if (!E2K_BASE(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+    // fate vorbis-encode
+    c->vorbis_inverse_coupling = vorbis_inverse_coupling_e2k;
+}
diff --git a/libavcodec/e2k/vp3dsp.c b/libavcodec/e2k/vp3dsp.c
new file mode 100644
index 0000000..f086096
--- /dev/null
+++ b/libavcodec/e2k/vp3dsp.c
@@ -0,0 +1,169 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (C) 2009 David Conrad
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include <string.h>
+
+#include "config.h"
+
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/vp3dsp.h"
+
+#define IDCT_START(extra) \
+    vec_s16 A, B, C, D, Ad, Bd, Cd, Dd, E, F, G, H; \
+    vec_s16 Ed, Gd, Add, Bdd, Fd, Hd; \
+    vec_s16 addv = _mm_set1_epi16(extra + 8); \
+    \
+    vec_s16 C1 = _mm_set1_epi16(64277); \
+    vec_s16 C2 = _mm_set1_epi16(60547); \
+    vec_s16 C3 = _mm_set1_epi16(54491); \
+    vec_s16 C4 = _mm_set1_epi16(46341); \
+    vec_s16 C5 = _mm_set1_epi16(36410); \
+    vec_s16 C6 = _mm_set1_epi16(25080); \
+    vec_s16 C7 = _mm_set1_epi16(12785); \
+    \
+    vec_s16 b0 = VEC_LD(block + 8 * 0); \
+    vec_s16 b1 = VEC_LD(block + 8 * 1); \
+    vec_s16 b2 = VEC_LD(block + 8 * 2); \
+    vec_s16 b3 = VEC_LD(block + 8 * 3); \
+    vec_s16 b4 = VEC_LD(block + 8 * 4); \
+    vec_s16 b5 = VEC_LD(block + 8 * 5); \
+    vec_s16 b6 = VEC_LD(block + 8 * 6); \
+    vec_s16 b7 = VEC_LD(block + 8 * 7);
+
+// these functions do (a*C)>>16
+// things are tricky because a is signed, but C unsigned.
+// M15 is used if C fits in 15 bit unsigned (C6,C7)
+// M16 is used if C requires 16 bits unsigned
+#define M15(a, C) _mm_mulhi_epi16(a, C)
+#define M16(a, C) _mm_add_epi16(a, M15(a, C))
+
+#define IDCT_1D(ADD, SHIFT)\
+    A = _mm_add_epi16(M16(b1, C1), M15(b7, C7)); \
+    B = _mm_sub_epi16(M15(b1, C7), M16(b7, C1)); \
+    C = _mm_add_epi16(M16(b3, C3), M16(b5, C5)); \
+    D = _mm_sub_epi16(M16(b5, C3), M16(b3, C5)); \
+    \
+    Ad = M16(_mm_sub_epi16(A, C), C4); \
+    Bd = M16(_mm_sub_epi16(B, D), C4); \
+    \
+    Cd = _mm_add_epi16(A, C); \
+    Dd = _mm_add_epi16(B, D); \
+    \
+    E = ADD(M16(_mm_add_epi16(b0, b4), C4)); \
+    F = ADD(M16(_mm_sub_epi16(b0, b4), C4)); \
+    \
+    G = _mm_add_epi16(M16(b2, C2), M15(b6, C6)); \
+    H = _mm_sub_epi16(M15(b2, C6), M16(b6, C2)); \
+    \
+    Ed = _mm_sub_epi16(E, G); \
+    Gd = _mm_add_epi16(E, G); \
+    \
+    Add = _mm_add_epi16(F, Ad); \
+    Bdd = _mm_sub_epi16(Bd, H); \
+    \
+    Fd = _mm_sub_epi16(F, Ad); \
+    Hd = _mm_add_epi16(Bd, H); \
+    \
+    b0 = SHIFT(_mm_add_epi16(Gd, Cd)); \
+    b7 = SHIFT(_mm_sub_epi16(Gd, Cd)); \
+    \
+    b1 = SHIFT(_mm_add_epi16(Add, Hd)); \
+    b2 = SHIFT(_mm_sub_epi16(Add, Hd)); \
+    \
+    b3 = SHIFT(_mm_add_epi16(Ed, Dd)); \
+    b4 = SHIFT(_mm_sub_epi16(Ed, Dd)); \
+    \
+    b5 = SHIFT(_mm_add_epi16(Fd, Bdd)); \
+    b6 = SHIFT(_mm_sub_epi16(Fd, Bdd));
+
+#define NOP(a) a
+#define ADD8(a) _mm_add_epi16(a, addv)
+#define SHIFT4(a) _mm_srai_epi16(a, 4)
+
+static void vp3_idct_put_e2k(uint8_t *dst, ptrdiff_t stride, int16_t block[64])
+{
+    vec_u8 vdst;
+    IDCT_START(2048)
+
+    IDCT_1D(NOP, NOP)
+    TRANSPOSE8(b0, b1, b2, b3, b4, b5, b6, b7);
+    IDCT_1D(ADD8, SHIFT4)
+
+#define PUT(a) \
+    vdst = _mm_packus_epi16(a, a); \
+    VEC_STL(dst, vdst);
+
+    PUT(b0)     dst += stride;
+    PUT(b1)     dst += stride;
+    PUT(b2)     dst += stride;
+    PUT(b3)     dst += stride;
+    PUT(b4)     dst += stride;
+    PUT(b5)     dst += stride;
+    PUT(b6)     dst += stride;
+    PUT(b7)
+    memset(block, 0, sizeof(*block) * 64);
+}
+
+static void vp3_idct_add_e2k(uint8_t *dst, ptrdiff_t stride, int16_t block[64])
+{
+    LOAD_ZERO;
+    vec_u8 vdst;
+    vec_s16 vdst_16;
+
+    IDCT_START(0)
+
+    IDCT_1D(NOP, NOP)
+    TRANSPOSE8(b0, b1, b2, b3, b4, b5, b6, b7);
+    IDCT_1D(ADD8, SHIFT4)
+
+#define ADD(a) \
+    vdst = VEC_LD8(dst); \
+    vdst_16 = _mm_unpacklo_epi8(vdst, zerov); \
+    vdst_16 = _mm_adds_epi16(a, vdst_16); \
+    vdst = _mm_packus_epi16(vdst_16, vdst_16); \
+    VEC_STL(dst, vdst);
+
+    ADD(b0)     dst += stride;
+    ADD(b1)     dst += stride;
+    ADD(b2)     dst += stride;
+    ADD(b3)     dst += stride;
+    ADD(b4)     dst += stride;
+    ADD(b5)     dst += stride;
+    ADD(b6)     dst += stride;
+    ADD(b7)
+    memset(block, 0, sizeof(*block) * 64);
+}
+
+av_cold void ff_vp3dsp_init_e2k(VP3DSPContext *c, int flags)
+{
+    if (!E2K_BASE(av_get_cpu_flags()))
+        return;
+
+    // !checkasm
+    // fate theora-coeff-level64
+
+    c->idct_put = vp3_idct_put_e2k;
+    c->idct_add = vp3_idct_add_e2k;
+}
diff --git a/libavcodec/e2k/vp8dsp.c b/libavcodec/e2k/vp8dsp.c
new file mode 100644
index 0000000..61b46b3
--- /dev/null
+++ b/libavcodec/e2k/vp8dsp.c
@@ -0,0 +1,428 @@
+/*
+ * VP8 compatible video decoder
+ *
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (C) 2010 David Conrad
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavutil/cpu.h"
+#include "libavutil/mem.h"
+#include "libavutil/e2k/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "libavcodec/vp8dsp.h"
+
+#include "hpeldsp.h"
+
+#define REPT4(a, b, c, d) { a, b, c, d,  a, b, c, d,  a, b, c, d,  a, b, c, d }
+
+// h subpel filter uses msum to multiply+add 4 pixel taps at once
+static const uint8_t ALIGNED(16) h_subpel_filters_inner[7][16] =
+{
+    REPT4( -6, 123,  12,  -1),
+    REPT4(-11, 108,  36,  -8),
+    REPT4( -9,  93,  50,  -6),
+    REPT4(-16,  77,  77, -16),
+    REPT4( -6,  50,  93,  -9),
+    REPT4( -8,  36, 108, -11),
+    REPT4( -1,  12, 123,  -6)
+};
+
+// for 6tap filters, these are the outer two taps
+// The zeros mask off pixels 4-7 when filtering 0-3
+// and vice-versa
+static const uint8_t ALIGNED(16) h_subpel_filters_outer[3][16] =
+{
+    REPT4(2, 1, 2, 1),
+    REPT4(3, 3, 3, 3),
+    REPT4(1, 2, 1, 2)
+};
+
+#define INNER_PERM(x) x, x+1, x+2, x+3, x+1, x+2, x+3, x+4
+
+#define INIT_H_SUBPEL_FILTER(j, n, is6tap) \
+    vec_s8 filter_inner = *(__m128i*)h_subpel_filters_inner[j]; \
+    is6tap( \
+        vec_s8 filter_outer = *(__m128i*)h_subpel_filters_outer[(j) >> 1]; \
+        vec_u8 perm_outer = _mm_setr_epi8(0,5, 1,6, 2,7, 3,8, 4,9, 5,10, 6,11, 7,12); \
+    ) \
+    vec_s32 c64 = _mm_set1_epi16(64); \
+    vec_u8 perm_inner_l = _mm_setr_epi8(INNER_PERM(n), INNER_PERM(n + 2)); \
+    vec_u8 perm_inner_h = _mm_setr_epi8(INNER_PERM(n + 4), INNER_PERM(n + 6)); \
+    __m128i v0, v1; \
+    int i
+
+#define FILTER_H(a, is6tap) \
+    v0 = _mm_shuffle_epi8(a, perm_inner_l); \
+    v1 = _mm_shuffle_epi8(a, perm_inner_h); \
+    v0 = _mm_maddubs_epi16(v0, filter_inner); \
+    v1 = _mm_maddubs_epi16(v1, filter_inner); \
+    v0 = _mm_hadds_epi16(v0, v1); \
+    is6tap( \
+        a = _mm_shuffle_epi8(a, perm_outer); \
+        v0 = _mm_adds_epi16(v0, _mm_maddubs_epi16(a, filter_outer)); \
+    ) \
+    v0 = _mm_adds_epi16(v0, c64); \
+    a = _mm_srai_epi16(v0, 7)
+
+#define INIT_H_SUBPEL_FILTER4(j, n, is6tap) \
+    __m64 filter_inner = *(__m64*)h_subpel_filters_inner[j]; \
+    is6tap( \
+        __m64 filter_outer = *(__m64*)h_subpel_filters_outer[(j) >> 1]; \
+        __m64 perm_outer = _mm_setr_pi8(0,5, 1,6, 2,7, 3,8); \
+        __m64 a1; \
+    ) \
+    __m64 c64 = _mm_set1_pi16(64); \
+    __m64 perm_inner_l = _mm_setr_pi8(INNER_PERM(n)); \
+    __m64 perm_inner_h = _mm_setr_pi8(INNER_PERM(n + 2)); \
+    __m64 v0, v1, a0; \
+    int i
+
+#define FILTER_H4(is6tap) \
+    v0 = _mm_shuffle_pi8(a0, perm_inner_l); \
+    v1 = _mm_shuffle_pi8(a0, perm_inner_h); \
+    v0 = _mm_maddubs_pi16(v0, filter_inner); \
+    v1 = _mm_maddubs_pi16(v1, filter_inner); \
+    v0 = _mm_hadds_pi16(v0, v1); \
+    is6tap( \
+        a0 = _mm_shuffle2_pi8(a0, a1, perm_outer); \
+        v0 = _mm_adds_pi16(v0, _mm_maddubs_pi16(a0, filter_outer)); \
+    ) \
+    v0 = _mm_adds_pi16(v0, c64); \
+    a0 = _mm_srai_pi16(v0, 7); \
+    a0 = _mm_packs_pu16(a0, a0); \
+    *(uint32_t*)dst = _mm_cvtsi64_si32(a0)
+
+#define COPY(code) code
+#define NOP(code)
+#define IF6TAP(code) code
+
+static void put_vp8_epel16_h6_e2k(uint8_t *dst, ptrdiff_t dst_stride, uint8_t *src,
+                                  ptrdiff_t src_stride, int h, int mx, int my)
+{
+    INIT_H_SUBPEL_FILTER(mx - 1, 1, IF6TAP);
+    __m128i a0, a1;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        a0 = VEC_LD(src - 2);
+        a1 = VEC_LD(src - 2 + 8);
+        FILTER_H(a0, IF6TAP);
+        FILTER_H(a1, IF6TAP);
+        a0 = _mm_packus_epi16(a0, a1);
+        VEC_ST(dst, a0);
+        src += src_stride;
+        dst += dst_stride;
+    }
+}
+
+static void put_vp8_epel8_h6_e2k(uint8_t *dst, ptrdiff_t dst_stride, uint8_t *src,
+                                  ptrdiff_t src_stride, int h, int mx, int my)
+{
+    INIT_H_SUBPEL_FILTER(mx - 1, 1, IF6TAP);
+    __m128i a0;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        a0 = VEC_LD(src - 2);
+        FILTER_H(a0, IF6TAP);
+        a0 = _mm_packus_epi16(a0, a0);
+        VEC_STL(dst, a0);
+        src += src_stride;
+        dst += dst_stride;
+    }
+}
+
+static void put_vp8_epel8_h4_e2k(uint8_t *dst, ptrdiff_t dst_stride, uint8_t *src,
+                                  ptrdiff_t src_stride, int h, int mx, int my)
+{
+    INIT_H_SUBPEL_FILTER(mx - 1, 0, NOP);
+    __m128i a0;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        a0 = VEC_LD(src - 1);
+        FILTER_H(a0, NOP);
+        a0 = _mm_packus_epi16(a0, a0);
+        VEC_STL(dst, a0);
+        src += src_stride;
+        dst += dst_stride;
+    }
+}
+
+static void put_vp8_epel4_h6_e2k(uint8_t *dst, ptrdiff_t dst_stride, uint8_t *src,
+                                  ptrdiff_t src_stride, int h, int mx, int my)
+{
+    INIT_H_SUBPEL_FILTER4(mx - 1, 1, IF6TAP);
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        a0 = *(__m64*)(src - 2);
+        a1 = _mm_cvtsi32_si64(src[8 - 2]);
+        FILTER_H4(IF6TAP);
+        src += src_stride;
+        dst += dst_stride;
+    }
+}
+
+static void put_vp8_epel4_h4_e2k(uint8_t *dst, ptrdiff_t dst_stride, uint8_t *src,
+                                  ptrdiff_t src_stride, int h, int mx, int my)
+{
+    INIT_H_SUBPEL_FILTER4(mx - 1, 0, NOP);
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        a0 = *(__m64*)(src - 1);
+        FILTER_H4(NOP);
+        src += src_stride;
+        dst += dst_stride;
+    }
+}
+
+#define PAIR_8X2(a, b) (a & 255) | b * 256
+static const int16_t v_subpel_filters[7][3] =
+{
+    { PAIR_8X2( -6, 123), PAIR_8X2( 12,  -1), PAIR_8X2(0, 0) },
+    { PAIR_8X2(-11, 108), PAIR_8X2( 36,  -8), PAIR_8X2(2, 1) },
+    { PAIR_8X2( -9,  93), PAIR_8X2( 50,  -6), PAIR_8X2(0, 0) },
+    { PAIR_8X2(-16,  77), PAIR_8X2( 77, -16), PAIR_8X2(3, 3) },
+    { PAIR_8X2( -6,  50), PAIR_8X2( 93,  -9), PAIR_8X2(0, 0) },
+    { PAIR_8X2( -8,  36), PAIR_8X2(108, -11), PAIR_8X2(1, 2) },
+    { PAIR_8X2( -1,  12), PAIR_8X2(123,  -6), PAIR_8X2(0, 0) }
+};
+
+#define INIT_V_SUBPEL_FILTER(p, type, j, is6tap) \
+    type v0, v1, r0; \
+    type c64 = _mm_set1_##p(64); \
+    type f0 = _mm_set1_##p(v_subpel_filters[j][0]); \
+    type f1 = _mm_set1_##p(v_subpel_filters[j][1]); \
+    is6tap(type f2 = _mm_set1_##p(v_subpel_filters[j][2]);) \
+    int i
+
+#define FILTER_V(p, dstv, lo, CVT, is6tap) \
+    v0 = _mm_maddubs_##p(_mm_unpack##lo(CVT(s1), CVT(s2)), f0); \
+    v1 = _mm_maddubs_##p(_mm_unpack##lo(CVT(s3), CVT(s4)), f1); \
+    v0 = _mm_adds_##p(v0, v1); \
+    is6tap( \
+        v1 = _mm_maddubs_##p(_mm_unpack##lo(CVT(s0), CVT(s5)), f2); \
+        v0 = _mm_adds_##p(v0, v1); \
+    ) \
+    v0 = _mm_adds_##p(v0, c64); \
+    dstv = _mm_srai_##p(v0, 7)
+
+static void put_vp8_epel16_v6_e2k(uint8_t *dst, ptrdiff_t dst_stride, uint8_t *src, 
+                                  ptrdiff_t src_stride, int h, int mx, int my)
+{
+    INIT_V_SUBPEL_FILTER(epi16, __m128i, my - 1, IF6TAP);
+    __m128i s0, s1, s2, s3, s4, s5;
+
+    s0 = VEC_LD(src - 2 * src_stride);
+    s1 = VEC_LD(src - 1 * src_stride);
+    s2 = VEC_LD(src);
+    s3 = VEC_LD(src + 1 * src_stride);
+    s4 = VEC_LD(src + 2 * src_stride);
+    src += src_stride * 3;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        s5 = VEC_LD(src);
+        FILTER_V(epi16, r0, lo_epi8, COPY, IF6TAP);
+        FILTER_V(epi16, v0, hi_epi8, COPY, IF6TAP);
+        r0 = _mm_packus_epi16(r0, v0);
+        VEC_ST(dst, r0);
+        s0 = s1; s1 = s2; s2 = s3; s3 = s4; s4 = s5;
+        dst += dst_stride;
+        src += src_stride;
+    }
+}
+
+static void put_vp8_epel8_v6_e2k(uint8_t *dst, ptrdiff_t dst_stride, uint8_t *src, 
+                                 ptrdiff_t src_stride, int h, int mx, int my)
+{
+    INIT_V_SUBPEL_FILTER(epi16, __m128i, my - 1, IF6TAP);
+    __m64 s0, s1, s2, s3, s4, s5;
+
+    s0 = *(__m64*)(src - 2 * src_stride);
+    s1 = *(__m64*)(src - 1 * src_stride);
+    s2 = *(__m64*)src;
+    s3 = *(__m64*)(src + 1 * src_stride);
+    s4 = *(__m64*)(src + 2 * src_stride);
+    src += src_stride * 3;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        s5 = *(__m64*)src;
+        FILTER_V(epi16, r0, lo_epi8, _mm_movpi64_epi64, IF6TAP);
+        r0 = _mm_packus_epi16(r0, r0);
+        VEC_STL(dst, r0);
+        s0 = s1; s1 = s2; s2 = s3; s3 = s4; s4 = s5;
+        dst += dst_stride;
+        src += src_stride;
+    }
+}
+
+static void put_vp8_epel4_v6_e2k(uint8_t *dst, ptrdiff_t dst_stride, uint8_t *src, 
+                                 ptrdiff_t src_stride, int h, int mx, int my)
+{
+    INIT_V_SUBPEL_FILTER(pi16, __m64, my - 1, IF6TAP);
+    __m64 s0, s1, s2, s3, s4, s5;
+
+    s0 = _mm_cvtsi32_si64(*(uint32_t*)(src - 2 * src_stride));
+    s1 = _mm_cvtsi32_si64(*(uint32_t*)(src - 1 * src_stride));
+    s2 = _mm_cvtsi32_si64(*(uint32_t*)src);
+    s3 = _mm_cvtsi32_si64(*(uint32_t*)(src + 1 * src_stride));
+    s4 = _mm_cvtsi32_si64(*(uint32_t*)(src + 2 * src_stride));
+    src += src_stride * 3;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        s5 = _mm_cvtsi32_si64(*(uint32_t*)src);
+        FILTER_V(pi16, r0, lo_pi8, COPY, IF6TAP);
+        r0 = _mm_packs_pu16(r0, r0);
+        *(uint32_t*)dst = _mm_cvtsi64_si32(r0);
+        s0 = s1; s1 = s2; s2 = s3; s3 = s4; s4 = s5;
+        dst += dst_stride;
+        src += src_stride;
+    }
+}
+
+static void put_vp8_epel8_v4_e2k(uint8_t *dst, ptrdiff_t dst_stride, uint8_t *src, 
+                                 ptrdiff_t src_stride, int h, int mx, int my)
+{
+    INIT_V_SUBPEL_FILTER(epi16, __m128i, my - 1, NOP);
+    __m64 s1, s2, s3, s4;
+
+    s1 = *(__m64*)(src - 1 * src_stride);
+    s2 = *(__m64*)src;
+    s3 = *(__m64*)(src + 1 * src_stride);
+    src += src_stride * 2;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        s4 = *(__m64*)src;
+        FILTER_V(epi16, r0, lo_epi8, _mm_movpi64_epi64, NOP);
+        r0 = _mm_packus_epi16(r0, r0);
+        VEC_STL(dst, r0);
+        s1 = s2; s2 = s3; s3 = s4;
+        dst += dst_stride;
+        src += src_stride;
+    }
+}
+
+static void put_vp8_epel4_v4_e2k(uint8_t *dst, ptrdiff_t dst_stride, uint8_t *src, 
+                                 ptrdiff_t src_stride, int h, int mx, int my)
+{
+    INIT_V_SUBPEL_FILTER(pi16, __m64, my - 1, NOP);
+    __m64 s1, s2, s3, s4;
+
+    s1 = _mm_cvtsi32_si64(*(uint32_t*)(src - 1 * src_stride));
+    s2 = _mm_cvtsi32_si64(*(uint32_t*)src);
+    s3 = _mm_cvtsi32_si64(*(uint32_t*)(src + 1 * src_stride));
+    src += src_stride * 2;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i++) {
+        s4 = _mm_cvtsi32_si64(*(uint32_t*)src);
+        FILTER_V(pi16, r0, lo_pi8, COPY, NOP);
+        r0 = _mm_packs_pu16(r0, r0);
+        *(uint32_t*)dst = _mm_cvtsi64_si32(r0);
+        s1 = s2; s2 = s3; s3 = s4;
+        dst += dst_stride;
+        src += src_stride;
+    }
+}
+
+#define EPEL_HV(WIDTH, HTAPS, VTAPS) \
+static void put_vp8_epel##WIDTH##_h##HTAPS##v##VTAPS##_e2k(uint8_t *dst, ptrdiff_t dstride, uint8_t *src, ptrdiff_t sstride, int h, int mx, int my) \
+{ \
+    DECLARE_ALIGNED(16, uint8_t, tmp)[(2*WIDTH+5)*16]; \
+    if (VTAPS == 6) { \
+        put_vp8_epel##WIDTH##_h##HTAPS##_e2k(tmp, 16,      src-2*sstride, sstride, h+5, mx, my); \
+        put_vp8_epel##WIDTH##_v##VTAPS##_e2k(dst, dstride, tmp+2*16,      16,      h,   mx, my); \
+    } else { \
+        put_vp8_epel##WIDTH##_h##HTAPS##_e2k(tmp, 16,      src-sstride, sstride, h+4, mx, my); \
+        put_vp8_epel##WIDTH##_v##VTAPS##_e2k(dst, dstride, tmp+16,      16,      h,   mx, my); \
+    } \
+}
+
+EPEL_HV(16, 6,6)
+EPEL_HV(8,  6,6)
+EPEL_HV(8,  4,6)
+EPEL_HV(8,  6,4)
+EPEL_HV(8,  4,4)
+EPEL_HV(4,  6,6)
+EPEL_HV(4,  4,6)
+EPEL_HV(4,  6,4)
+EPEL_HV(4,  4,4)
+
+static void put_vp8_pixels16_e2k(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
+                                 ptrdiff_t sstride, int h, int mx, int my)
+{
+    __m128i v0, v1, v2, v3;
+    int i;
+
+    PRAGMA_E2K("ivdep")
+    for (i = 0; i < h; i += 4) {
+        v0 = VEC_LD(src);
+        v1 = VEC_LD(src + sstride);
+        v2 = VEC_LD(src + sstride * 2);
+        v3 = VEC_LD(src + sstride * 3);
+        VEC_ST(dst, v0);
+        VEC_ST(dst + dstride, v1);
+        VEC_ST(dst + dstride * 2, v2);
+        VEC_ST(dst + dstride * 3, v3);
+        src += sstride * 4;
+        dst += dstride * 4;
+    }
+}
+
+
+av_cold void ff_vp78dsp_init_e2k(VP8DSPContext *c)
+{
+    if (!E2K_BASE(av_get_cpu_flags()))
+        return;
+
+    // checkasm
+    c->put_vp8_epel_pixels_tab[0][0][0] = put_vp8_pixels16_e2k;
+    c->put_vp8_epel_pixels_tab[0][0][2] = put_vp8_epel16_h6_e2k;
+    c->put_vp8_epel_pixels_tab[0][2][0] = put_vp8_epel16_v6_e2k;
+    c->put_vp8_epel_pixels_tab[0][2][2] = put_vp8_epel16_h6v6_e2k;
+
+    c->put_vp8_epel_pixels_tab[1][0][2] = put_vp8_epel8_h6_e2k;
+    c->put_vp8_epel_pixels_tab[1][2][0] = put_vp8_epel8_v6_e2k;
+    c->put_vp8_epel_pixels_tab[1][0][1] = put_vp8_epel8_h4_e2k;
+    c->put_vp8_epel_pixels_tab[1][1][0] = put_vp8_epel8_v4_e2k;
+
+    c->put_vp8_epel_pixels_tab[1][2][2] = put_vp8_epel8_h6v6_e2k;
+    c->put_vp8_epel_pixels_tab[1][1][1] = put_vp8_epel8_h4v4_e2k;
+    c->put_vp8_epel_pixels_tab[1][1][2] = put_vp8_epel8_h6v4_e2k;
+    c->put_vp8_epel_pixels_tab[1][2][1] = put_vp8_epel8_h4v6_e2k;
+
+    c->put_vp8_epel_pixels_tab[2][0][2] = put_vp8_epel4_h6_e2k;
+    c->put_vp8_epel_pixels_tab[2][2][0] = put_vp8_epel4_v6_e2k;
+    c->put_vp8_epel_pixels_tab[2][0][1] = put_vp8_epel4_h4_e2k;
+    c->put_vp8_epel_pixels_tab[2][1][0] = put_vp8_epel4_v4_e2k;
+
+    c->put_vp8_epel_pixels_tab[2][2][2] = put_vp8_epel4_h6v6_e2k;
+    c->put_vp8_epel_pixels_tab[2][1][1] = put_vp8_epel4_h4v4_e2k;
+    c->put_vp8_epel_pixels_tab[2][1][2] = put_vp8_epel4_h6v4_e2k;
+    c->put_vp8_epel_pixels_tab[2][2][1] = put_vp8_epel4_h4v6_e2k;
+}
diff --git a/libavcodec/fdctdsp.c b/libavcodec/fdctdsp.c
index b9c2c86..69e6302 100644
--- a/libavcodec/fdctdsp.c
+++ b/libavcodec/fdctdsp.c
@@ -45,6 +45,8 @@ av_cold void ff_fdctdsp_init(FDCTDSPContext *c, AVCodecContext *avctx)
 
     if (ARCH_PPC)
         ff_fdctdsp_init_ppc(c, avctx, high_bit_depth);
+    if (ARCH_E2K)
+        ff_fdctdsp_init_e2k(c, avctx, high_bit_depth);
     if (ARCH_X86)
         ff_fdctdsp_init_x86(c, avctx, high_bit_depth);
 }
diff --git a/libavcodec/fdctdsp.h b/libavcodec/fdctdsp.h
index 3e1f683..75407d4 100644
--- a/libavcodec/fdctdsp.h
+++ b/libavcodec/fdctdsp.h
@@ -31,6 +31,8 @@ typedef struct FDCTDSPContext {
 void ff_fdctdsp_init(FDCTDSPContext *c, AVCodecContext *avctx);
 void ff_fdctdsp_init_ppc(FDCTDSPContext *c, AVCodecContext *avctx,
                          unsigned high_bit_depth);
+void ff_fdctdsp_init_e2k(FDCTDSPContext *c, AVCodecContext *avctx,
+                         unsigned high_bit_depth);
 void ff_fdctdsp_init_x86(FDCTDSPContext *c, AVCodecContext *avctx,
                          unsigned high_bit_depth);
 
diff --git a/libavcodec/fft.h b/libavcodec/fft.h
index c858570..6a30f7c 100644
--- a/libavcodec/fft.h
+++ b/libavcodec/fft.h
@@ -161,6 +161,7 @@ void ff_fft_init_x86(FFTContext *s);
 void ff_fft_init_arm(FFTContext *s);
 void ff_fft_init_mips(FFTContext *s);
 void ff_fft_init_ppc(FFTContext *s);
+void ff_fft_init_e2k(FFTContext *s);
 
 void ff_fft_fixed_init_arm(FFTContext *s);
 
diff --git a/libavcodec/fft_template.c b/libavcodec/fft_template.c
index 20a62e4..d565a71 100644
--- a/libavcodec/fft_template.c
+++ b/libavcodec/fft_template.c
@@ -245,6 +245,7 @@ av_cold int ff_fft_init(FFTContext *s, int nbits, int inverse)
     if (ARCH_AARCH64) ff_fft_init_aarch64(s);
     if (ARCH_ARM)     ff_fft_init_arm(s);
     if (ARCH_PPC)     ff_fft_init_ppc(s);
+    if (ARCH_E2K)     ff_fft_init_e2k(s);
     if (ARCH_X86)     ff_fft_init_x86(s);
     if (CONFIG_MDCT)  s->mdct_calcw = s->mdct_calc;
     if (HAVE_MIPSFPU) ff_fft_init_mips(s);
diff --git a/libavcodec/fmtconvert.c b/libavcodec/fmtconvert.c
index 3b33af6..141ffc0 100644
--- a/libavcodec/fmtconvert.c
+++ b/libavcodec/fmtconvert.c
@@ -61,6 +61,8 @@ av_cold void ff_fmt_convert_init(FmtConvertContext *c, AVCodecContext *avctx)
         ff_fmt_convert_init_arm(c, avctx);
     if (ARCH_PPC)
         ff_fmt_convert_init_ppc(c, avctx);
+    if (ARCH_E2K)
+        ff_fmt_convert_init_e2k(c, avctx);
     if (ARCH_X86)
         ff_fmt_convert_init_x86(c, avctx);
     if (HAVE_MIPSFPU)
diff --git a/libavcodec/fmtconvert.h b/libavcodec/fmtconvert.h
index a1b17e4..1053e0f 100644
--- a/libavcodec/fmtconvert.h
+++ b/libavcodec/fmtconvert.h
@@ -71,6 +71,7 @@ void ff_fmt_convert_init(FmtConvertContext *c, AVCodecContext *avctx);
 void ff_fmt_convert_init_aarch64(FmtConvertContext *c, AVCodecContext *avctx);
 void ff_fmt_convert_init_arm(FmtConvertContext *c, AVCodecContext *avctx);
 void ff_fmt_convert_init_ppc(FmtConvertContext *c, AVCodecContext *avctx);
+void ff_fmt_convert_init_e2k(FmtConvertContext *c, AVCodecContext *avctx);
 void ff_fmt_convert_init_x86(FmtConvertContext *c, AVCodecContext *avctx);
 void ff_fmt_convert_init_mips(FmtConvertContext *c);
 
diff --git a/libavcodec/h264chroma.c b/libavcodec/h264chroma.c
index c2f1f30..ea6196f 100644
--- a/libavcodec/h264chroma.c
+++ b/libavcodec/h264chroma.c
@@ -52,6 +52,8 @@ av_cold void ff_h264chroma_init(H264ChromaContext *c, int bit_depth)
         ff_h264chroma_init_arm(c, bit_depth);
     if (ARCH_PPC)
         ff_h264chroma_init_ppc(c, bit_depth);
+    if (ARCH_E2K)
+        ff_h264chroma_init_e2k(c, bit_depth);
     if (ARCH_X86)
         ff_h264chroma_init_x86(c, bit_depth);
     if (ARCH_MIPS)
diff --git a/libavcodec/h264chroma.h b/libavcodec/h264chroma.h
index 5c89fd1..0ec192c 100644
--- a/libavcodec/h264chroma.h
+++ b/libavcodec/h264chroma.h
@@ -34,6 +34,7 @@ void ff_h264chroma_init(H264ChromaContext *c, int bit_depth);
 void ff_h264chroma_init_aarch64(H264ChromaContext *c, int bit_depth);
 void ff_h264chroma_init_arm(H264ChromaContext *c, int bit_depth);
 void ff_h264chroma_init_ppc(H264ChromaContext *c, int bit_depth);
+void ff_h264chroma_init_e2k(H264ChromaContext *c, int bit_depth);
 void ff_h264chroma_init_x86(H264ChromaContext *c, int bit_depth);
 void ff_h264chroma_init_mips(H264ChromaContext *c, int bit_depth);
 
diff --git a/libavcodec/h264dsp.c b/libavcodec/h264dsp.c
index d26f552..64c19a9 100644
--- a/libavcodec/h264dsp.c
+++ b/libavcodec/h264dsp.c
@@ -156,6 +156,7 @@ av_cold void ff_h264dsp_init(H264DSPContext *c, const int bit_depth,
     if (ARCH_AARCH64) ff_h264dsp_init_aarch64(c, bit_depth, chroma_format_idc);
     if (ARCH_ARM) ff_h264dsp_init_arm(c, bit_depth, chroma_format_idc);
     if (ARCH_PPC) ff_h264dsp_init_ppc(c, bit_depth, chroma_format_idc);
+    if (ARCH_E2K) ff_h264dsp_init_e2k(c, bit_depth, chroma_format_idc);
     if (ARCH_X86) ff_h264dsp_init_x86(c, bit_depth, chroma_format_idc);
     if (ARCH_MIPS) ff_h264dsp_init_mips(c, bit_depth, chroma_format_idc);
 }
diff --git a/libavcodec/h264dsp.h b/libavcodec/h264dsp.h
index cbea317..ff781e1 100644
--- a/libavcodec/h264dsp.h
+++ b/libavcodec/h264dsp.h
@@ -125,6 +125,8 @@ void ff_h264dsp_init_arm(H264DSPContext *c, const int bit_depth,
                          const int chroma_format_idc);
 void ff_h264dsp_init_ppc(H264DSPContext *c, const int bit_depth,
                          const int chroma_format_idc);
+void ff_h264dsp_init_e2k(H264DSPContext *c, const int bit_depth,
+                         const int chroma_format_idc);
 void ff_h264dsp_init_x86(H264DSPContext *c, const int bit_depth,
                          const int chroma_format_idc);
 void ff_h264dsp_init_mips(H264DSPContext *c, const int bit_depth,
diff --git a/libavcodec/h264qpel.c b/libavcodec/h264qpel.c
index 50e82e2..5955069 100644
--- a/libavcodec/h264qpel.c
+++ b/libavcodec/h264qpel.c
@@ -102,6 +102,8 @@ av_cold void ff_h264qpel_init(H264QpelContext *c, int bit_depth)
         ff_h264qpel_init_arm(c, bit_depth);
     if (ARCH_PPC)
         ff_h264qpel_init_ppc(c, bit_depth);
+    if (ARCH_E2K)
+        ff_h264qpel_init_e2k(c, bit_depth);
     if (ARCH_X86)
         ff_h264qpel_init_x86(c, bit_depth);
     if (ARCH_MIPS)
diff --git a/libavcodec/h264qpel.h b/libavcodec/h264qpel.h
index 7c57ad0..7880b86 100644
--- a/libavcodec/h264qpel.h
+++ b/libavcodec/h264qpel.h
@@ -34,6 +34,7 @@ void ff_h264qpel_init(H264QpelContext *c, int bit_depth);
 void ff_h264qpel_init_aarch64(H264QpelContext *c, int bit_depth);
 void ff_h264qpel_init_arm(H264QpelContext *c, int bit_depth);
 void ff_h264qpel_init_ppc(H264QpelContext *c, int bit_depth);
+void ff_h264qpel_init_e2k(H264QpelContext *c, int bit_depth);
 void ff_h264qpel_init_x86(H264QpelContext *c, int bit_depth);
 void ff_h264qpel_init_mips(H264QpelContext *c, int bit_depth);
 
diff --git a/libavcodec/hevcdsp.c b/libavcodec/hevcdsp.c
index 957e40d..a7d0d5d 100644
--- a/libavcodec/hevcdsp.c
+++ b/libavcodec/hevcdsp.c
@@ -261,6 +261,8 @@ int i = 0;
         ff_hevc_dsp_init_arm(hevcdsp, bit_depth);
     if (ARCH_PPC)
         ff_hevc_dsp_init_ppc(hevcdsp, bit_depth);
+    if (ARCH_E2K)
+        ff_hevc_dsp_init_e2k(hevcdsp, bit_depth);
     if (ARCH_X86)
         ff_hevc_dsp_init_x86(hevcdsp, bit_depth);
     if (ARCH_MIPS)
diff --git a/libavcodec/hevcdsp.h b/libavcodec/hevcdsp.h
index 0ae67cb..3221a2a 100644
--- a/libavcodec/hevcdsp.h
+++ b/libavcodec/hevcdsp.h
@@ -129,6 +129,7 @@ extern const int8_t ff_hevc_qpel_filters[3][16];
 
 void ff_hevc_dsp_init_arm(HEVCDSPContext *c, const int bit_depth);
 void ff_hevc_dsp_init_ppc(HEVCDSPContext *c, const int bit_depth);
+void ff_hevc_dsp_init_e2k(HEVCDSPContext *c, const int bit_depth);
 void ff_hevc_dsp_init_x86(HEVCDSPContext *c, const int bit_depth);
 void ff_hevc_dsp_init_mips(HEVCDSPContext *c, const int bit_depth);
 
diff --git a/libavcodec/hpeldsp.c b/libavcodec/hpeldsp.c
index 8e2fd8f..dd4ef87 100644
--- a/libavcodec/hpeldsp.c
+++ b/libavcodec/hpeldsp.c
@@ -363,6 +363,8 @@ av_cold void ff_hpeldsp_init(HpelDSPContext *c, int flags)
         ff_hpeldsp_init_arm(c, flags);
     if (ARCH_PPC)
         ff_hpeldsp_init_ppc(c, flags);
+    if (ARCH_E2K)
+        ff_hpeldsp_init_e2k(c, flags);
     if (ARCH_X86)
         ff_hpeldsp_init_x86(c, flags);
     if (ARCH_MIPS)
diff --git a/libavcodec/hpeldsp.h b/libavcodec/hpeldsp.h
index 768139b..6d0c293 100644
--- a/libavcodec/hpeldsp.h
+++ b/libavcodec/hpeldsp.h
@@ -100,6 +100,7 @@ void ff_hpeldsp_init_aarch64(HpelDSPContext *c, int flags);
 void ff_hpeldsp_init_alpha(HpelDSPContext *c, int flags);
 void ff_hpeldsp_init_arm(HpelDSPContext *c, int flags);
 void ff_hpeldsp_init_ppc(HpelDSPContext *c, int flags);
+void ff_hpeldsp_init_e2k(HpelDSPContext *c, int flags);
 void ff_hpeldsp_init_x86(HpelDSPContext *c, int flags);
 void ff_hpeldsp_init_mips(HpelDSPContext *c, int flags);
 
diff --git a/libavcodec/idctdsp.c b/libavcodec/idctdsp.c
index 846ed0b..a2554aa 100644
--- a/libavcodec/idctdsp.c
+++ b/libavcodec/idctdsp.c
@@ -311,6 +311,8 @@ av_cold void ff_idctdsp_init(IDCTDSPContext *c, AVCodecContext *avctx)
         ff_idctdsp_init_arm(c, avctx, high_bit_depth);
     if (ARCH_PPC)
         ff_idctdsp_init_ppc(c, avctx, high_bit_depth);
+    if (ARCH_E2K)
+        ff_idctdsp_init_e2k(c, avctx, high_bit_depth);
     if (ARCH_X86)
         ff_idctdsp_init_x86(c, avctx, high_bit_depth);
     if (ARCH_MIPS)
diff --git a/libavcodec/idctdsp.h b/libavcodec/idctdsp.h
index ca21a31..1204bff 100644
--- a/libavcodec/idctdsp.h
+++ b/libavcodec/idctdsp.h
@@ -114,6 +114,8 @@ void ff_idctdsp_init_arm(IDCTDSPContext *c, AVCodecContext *avctx,
                          unsigned high_bit_depth);
 void ff_idctdsp_init_ppc(IDCTDSPContext *c, AVCodecContext *avctx,
                          unsigned high_bit_depth);
+void ff_idctdsp_init_e2k(IDCTDSPContext *c, AVCodecContext *avctx,
+                         unsigned high_bit_depth);
 void ff_idctdsp_init_x86(IDCTDSPContext *c, AVCodecContext *avctx,
                          unsigned high_bit_depth);
 void ff_idctdsp_init_mips(IDCTDSPContext *c, AVCodecContext *avctx,
diff --git a/libavcodec/lossless_audiodsp.c b/libavcodec/lossless_audiodsp.c
index 3781659..979e0c9 100644
--- a/libavcodec/lossless_audiodsp.c
+++ b/libavcodec/lossless_audiodsp.c
@@ -62,6 +62,8 @@ av_cold void ff_llauddsp_init(LLAudDSPContext *c)
         ff_llauddsp_init_arm(c);
     if (ARCH_PPC)
         ff_llauddsp_init_ppc(c);
+    if (ARCH_E2K)
+        ff_llauddsp_init_e2k(c);
     if (ARCH_X86)
         ff_llauddsp_init_x86(c);
 }
diff --git a/libavcodec/lossless_audiodsp.h b/libavcodec/lossless_audiodsp.h
index eea5d49..3de02d5 100644
--- a/libavcodec/lossless_audiodsp.h
+++ b/libavcodec/lossless_audiodsp.h
@@ -46,6 +46,7 @@ typedef struct LLAudDSPContext {
 void ff_llauddsp_init(LLAudDSPContext *c);
 void ff_llauddsp_init_arm(LLAudDSPContext *c);
 void ff_llauddsp_init_ppc(LLAudDSPContext *c);
+void ff_llauddsp_init_e2k(LLAudDSPContext *c);
 void ff_llauddsp_init_x86(LLAudDSPContext *c);
 
 #endif /* AVCODEC_LOSSLESS_AUDIODSP_H */
diff --git a/libavcodec/lossless_videodsp.c b/libavcodec/lossless_videodsp.c
index cff94c2..34b1db5 100644
--- a/libavcodec/lossless_videodsp.c
+++ b/libavcodec/lossless_videodsp.c
@@ -120,6 +120,8 @@ void ff_llviddsp_init(LLVidDSPContext *c)
 
     if (ARCH_PPC)
         ff_llviddsp_init_ppc(c);
+    if (ARCH_E2K)
+        ff_llviddsp_init_e2k(c);
     if (ARCH_X86)
         ff_llviddsp_init_x86(c);
 }
diff --git a/libavcodec/lossless_videodsp.h b/libavcodec/lossless_videodsp.h
index 8077898..917afb6 100644
--- a/libavcodec/lossless_videodsp.h
+++ b/libavcodec/lossless_videodsp.h
@@ -45,5 +45,6 @@ typedef struct LLVidDSPContext {
 void ff_llviddsp_init(LLVidDSPContext *llviddsp);
 void ff_llviddsp_init_x86(LLVidDSPContext *llviddsp);
 void ff_llviddsp_init_ppc(LLVidDSPContext *llviddsp);
+void ff_llviddsp_init_e2k(LLVidDSPContext *llviddsp);
 
 #endif //AVCODEC_LOSSLESS_VIDEODSP_H
diff --git a/libavcodec/me_cmp.c b/libavcodec/me_cmp.c
index ae248c5..cb967a4 100644
--- a/libavcodec/me_cmp.c
+++ b/libavcodec/me_cmp.c
@@ -1088,6 +1088,8 @@ av_cold void ff_me_cmp_init(MECmpContext *c, AVCodecContext *avctx)
         ff_me_cmp_init_arm(c, avctx);
     if (ARCH_PPC)
         ff_me_cmp_init_ppc(c, avctx);
+    if (ARCH_E2K)
+        ff_me_cmp_init_e2k(c, avctx);
     if (ARCH_X86)
         ff_me_cmp_init_x86(c, avctx);
     if (ARCH_MIPS)
diff --git a/libavcodec/me_cmp.h b/libavcodec/me_cmp.h
index 0a589e3..8989a1c 100644
--- a/libavcodec/me_cmp.h
+++ b/libavcodec/me_cmp.h
@@ -85,6 +85,7 @@ void ff_me_cmp_init(MECmpContext *c, AVCodecContext *avctx);
 void ff_me_cmp_init_alpha(MECmpContext *c, AVCodecContext *avctx);
 void ff_me_cmp_init_arm(MECmpContext *c, AVCodecContext *avctx);
 void ff_me_cmp_init_ppc(MECmpContext *c, AVCodecContext *avctx);
+void ff_me_cmp_init_e2k(MECmpContext *c, AVCodecContext *avctx);
 void ff_me_cmp_init_x86(MECmpContext *c, AVCodecContext *avctx);
 void ff_me_cmp_init_mips(MECmpContext *c, AVCodecContext *avctx);
 
diff --git a/libavcodec/mpegaudiodsp.c b/libavcodec/mpegaudiodsp.c
index 3cafca2..0514bd4 100644
--- a/libavcodec/mpegaudiodsp.c
+++ b/libavcodec/mpegaudiodsp.c
@@ -48,6 +48,7 @@ av_cold void ff_mpadsp_init(MPADSPContext *s)
     if (ARCH_AARCH64) ff_mpadsp_init_aarch64(s);
     if (ARCH_ARM)     ff_mpadsp_init_arm(s);
     if (ARCH_PPC)     ff_mpadsp_init_ppc(s);
+    if (ARCH_E2K)     ff_mpadsp_init_e2k(s);
     if (ARCH_X86)     ff_mpadsp_init_x86(s);
     if (HAVE_MIPSFPU)   ff_mpadsp_init_mipsfpu(s);
     if (HAVE_MIPSDSP) ff_mpadsp_init_mipsdsp(s);
diff --git a/libavcodec/mpegaudiodsp.h b/libavcodec/mpegaudiodsp.h
index 0e4352d..4514564 100644
--- a/libavcodec/mpegaudiodsp.h
+++ b/libavcodec/mpegaudiodsp.h
@@ -62,6 +62,7 @@ void ff_mpa_synth_filter_float(MPADSPContext *s,
 void ff_mpadsp_init_aarch64(MPADSPContext *s);
 void ff_mpadsp_init_arm(MPADSPContext *s);
 void ff_mpadsp_init_ppc(MPADSPContext *s);
+void ff_mpadsp_init_e2k(MPADSPContext *s);
 void ff_mpadsp_init_x86(MPADSPContext *s);
 void ff_mpadsp_init_mipsfpu(MPADSPContext *s);
 void ff_mpadsp_init_mipsdsp(MPADSPContext *s);
diff --git a/libavcodec/mpegvideo.c b/libavcodec/mpegvideo.c
index 49fd1c9..49c4476 100644
--- a/libavcodec/mpegvideo.c
+++ b/libavcodec/mpegvideo.c
@@ -318,6 +318,8 @@ static av_cold int dct_init(MpegEncContext *s)
         ff_mpv_common_init_arm(s);
     if (ARCH_PPC)
         ff_mpv_common_init_ppc(s);
+    if (ARCH_E2K)
+        ff_mpv_common_init_e2k(s);
     if (ARCH_X86)
         ff_mpv_common_init_x86(s);
     if (ARCH_MIPS)
diff --git a/libavcodec/mpegvideo.h b/libavcodec/mpegvideo.h
index 29e692f..85fddd2 100644
--- a/libavcodec/mpegvideo.h
+++ b/libavcodec/mpegvideo.h
@@ -685,6 +685,7 @@ void ff_mpv_common_init_arm(MpegEncContext *s);
 void ff_mpv_common_init_axp(MpegEncContext *s);
 void ff_mpv_common_init_neon(MpegEncContext *s);
 void ff_mpv_common_init_ppc(MpegEncContext *s);
+void ff_mpv_common_init_e2k(MpegEncContext *s);
 void ff_mpv_common_init_x86(MpegEncContext *s);
 void ff_mpv_common_init_mips(MpegEncContext *s);
 
diff --git a/libavcodec/mpegvideodsp.c b/libavcodec/mpegvideodsp.c
index a58e45a..2d7aa96 100644
--- a/libavcodec/mpegvideodsp.c
+++ b/libavcodec/mpegvideodsp.c
@@ -114,6 +114,8 @@ av_cold void ff_mpegvideodsp_init(MpegVideoDSPContext *c)
 
     if (ARCH_PPC)
         ff_mpegvideodsp_init_ppc(c);
+    if (ARCH_E2K)
+        ff_mpegvideodsp_init_e2k(c);
     if (ARCH_X86)
         ff_mpegvideodsp_init_x86(c);
 }
diff --git a/libavcodec/mpegvideodsp.h b/libavcodec/mpegvideodsp.h
index 293e254..56a0bf5 100644
--- a/libavcodec/mpegvideodsp.h
+++ b/libavcodec/mpegvideodsp.h
@@ -42,6 +42,7 @@ typedef struct MpegVideoDSPContext {
 
 void ff_mpegvideodsp_init(MpegVideoDSPContext *c);
 void ff_mpegvideodsp_init_ppc(MpegVideoDSPContext *c);
+void ff_mpegvideodsp_init_e2k(MpegVideoDSPContext *c);
 void ff_mpegvideodsp_init_x86(MpegVideoDSPContext *c);
 
 #endif /* AVCODEC_MPEGVIDEODSP_H */
diff --git a/libavcodec/mpegvideoencdsp.c b/libavcodec/mpegvideoencdsp.c
index a34ab35..f6ba6b4 100644
--- a/libavcodec/mpegvideoencdsp.c
+++ b/libavcodec/mpegvideoencdsp.c
@@ -249,6 +249,8 @@ av_cold void ff_mpegvideoencdsp_init(MpegvideoEncDSPContext *c,
         ff_mpegvideoencdsp_init_arm(c, avctx);
     if (ARCH_PPC)
         ff_mpegvideoencdsp_init_ppc(c, avctx);
+    if (ARCH_E2K)
+        ff_mpegvideoencdsp_init_e2k(c, avctx);
     if (ARCH_X86)
         ff_mpegvideoencdsp_init_x86(c, avctx);
     if (ARCH_MIPS)
diff --git a/libavcodec/mpegvideoencdsp.h b/libavcodec/mpegvideoencdsp.h
index 33f0282..2f2b191 100644
--- a/libavcodec/mpegvideoencdsp.h
+++ b/libavcodec/mpegvideoencdsp.h
@@ -50,6 +50,8 @@ void ff_mpegvideoencdsp_init_arm(MpegvideoEncDSPContext *c,
                                  AVCodecContext *avctx);
 void ff_mpegvideoencdsp_init_ppc(MpegvideoEncDSPContext *c,
                                  AVCodecContext *avctx);
+void ff_mpegvideoencdsp_init_e2k(MpegvideoEncDSPContext *c,
+                                 AVCodecContext *avctx);
 void ff_mpegvideoencdsp_init_x86(MpegvideoEncDSPContext *c,
                                  AVCodecContext *avctx);
 void ff_mpegvideoencdsp_init_mips(MpegvideoEncDSPContext *c,
diff --git a/libavcodec/pixblockdsp.c b/libavcodec/pixblockdsp.c
index 67393b9..8383663 100644
--- a/libavcodec/pixblockdsp.c
+++ b/libavcodec/pixblockdsp.c
@@ -109,6 +109,8 @@ av_cold void ff_pixblockdsp_init(PixblockDSPContext *c, AVCodecContext *avctx)
         ff_pixblockdsp_init_arm(c, avctx, high_bit_depth);
     if (ARCH_PPC)
         ff_pixblockdsp_init_ppc(c, avctx, high_bit_depth);
+    if (ARCH_E2K)
+        ff_pixblockdsp_init_e2k(c, avctx, high_bit_depth);
     if (ARCH_X86)
         ff_pixblockdsp_init_x86(c, avctx, high_bit_depth);
     if (ARCH_MIPS)
diff --git a/libavcodec/pixblockdsp.h b/libavcodec/pixblockdsp.h
index 07c2ec4..49e2b25 100644
--- a/libavcodec/pixblockdsp.h
+++ b/libavcodec/pixblockdsp.h
@@ -52,6 +52,8 @@ void ff_pixblockdsp_init_arm(PixblockDSPContext *c, AVCodecContext *avctx,
                              unsigned high_bit_depth);
 void ff_pixblockdsp_init_ppc(PixblockDSPContext *c, AVCodecContext *avctx,
                              unsigned high_bit_depth);
+void ff_pixblockdsp_init_e2k(PixblockDSPContext *c, AVCodecContext *avctx,
+                             unsigned high_bit_depth);
 void ff_pixblockdsp_init_x86(PixblockDSPContext *c, AVCodecContext *avctx,
                              unsigned high_bit_depth);
 void ff_pixblockdsp_init_mips(PixblockDSPContext *c, AVCodecContext *avctx,
diff --git a/libavcodec/svq1enc.c b/libavcodec/svq1enc.c
index cb215c2..d55fcd3 100644
--- a/libavcodec/svq1enc.c
+++ b/libavcodec/svq1enc.c
@@ -570,6 +570,8 @@ static av_cold int svq1_encode_init(AVCodecContext *avctx)
 
     if (ARCH_PPC)
         ff_svq1enc_init_ppc(s);
+    if (ARCH_E2K)
+        ff_svq1enc_init_e2k(s);
     if (ARCH_X86)
         ff_svq1enc_init_x86(s);
 
diff --git a/libavcodec/svq1enc.h b/libavcodec/svq1enc.h
index b4ef763..c070d80 100644
--- a/libavcodec/svq1enc.h
+++ b/libavcodec/svq1enc.h
@@ -80,6 +80,7 @@ typedef struct SVQ1EncContext {
 } SVQ1EncContext;
 
 void ff_svq1enc_init_ppc(SVQ1EncContext *c);
+void ff_svq1enc_init_e2k(SVQ1EncContext *c);
 void ff_svq1enc_init_x86(SVQ1EncContext *c);
 
 #endif /* AVCODEC_SVQ1ENC_H */
diff --git a/libavcodec/tests/dct.c b/libavcodec/tests/dct.c
index 2ca8039..755734c 100644
--- a/libavcodec/tests/dct.c
+++ b/libavcodec/tests/dct.c
@@ -100,6 +100,8 @@ static const struct algo idct_tab[] = {
 #include "arm/dct.c"
 #elif ARCH_PPC
 #include "ppc/dct.c"
+#elif ARCH_E2K
+#include "e2k/dct.c"
 #elif ARCH_X86
 #include "x86/dct.c"
 #else
diff --git a/libavcodec/tests/e2k/dct.c b/libavcodec/tests/e2k/dct.c
new file mode 100644
index 0000000..7c15b25
--- /dev/null
+++ b/libavcodec/tests/e2k/dct.c
@@ -0,0 +1,31 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+
+#include "libavcodec/e2k/dctdsp.h"
+
+static const struct algo fdct_tab_arch[] = {
+    { "FDCT-E2K", ff_fdct_e2k, FF_IDCT_PERM_NONE, AV_CPU_FLAG_E2K },
+    { 0 }
+};
+
+static const struct algo idct_tab_arch[] = {
+    { "IDCT-E2K", ff_idct_e2k, FF_IDCT_PERM_TRANSPOSE, AV_CPU_FLAG_E2K },
+    { 0 }
+};
diff --git a/libavcodec/vc1dsp.c b/libavcodec/vc1dsp.c
index c25a6f3..2953403 100644
--- a/libavcodec/vc1dsp.c
+++ b/libavcodec/vc1dsp.c
@@ -1035,6 +1035,8 @@ av_cold void ff_vc1dsp_init(VC1DSPContext *dsp)
         ff_vc1dsp_init_arm(dsp);
     if (ARCH_PPC)
         ff_vc1dsp_init_ppc(dsp);
+    if (ARCH_E2K)
+        ff_vc1dsp_init_e2k(dsp);
     if (ARCH_X86)
         ff_vc1dsp_init_x86(dsp);
     if (ARCH_MIPS)
diff --git a/libavcodec/vc1dsp.h b/libavcodec/vc1dsp.h
index 75db62b..eabf2c8 100644
--- a/libavcodec/vc1dsp.h
+++ b/libavcodec/vc1dsp.h
@@ -86,6 +86,7 @@ void ff_vc1dsp_init(VC1DSPContext* c);
 void ff_vc1dsp_init_aarch64(VC1DSPContext* dsp);
 void ff_vc1dsp_init_arm(VC1DSPContext* dsp);
 void ff_vc1dsp_init_ppc(VC1DSPContext *c);
+void ff_vc1dsp_init_e2k(VC1DSPContext *c);
 void ff_vc1dsp_init_x86(VC1DSPContext* dsp);
 void ff_vc1dsp_init_mips(VC1DSPContext* dsp);
 
diff --git a/libavcodec/videodsp.c b/libavcodec/videodsp.c
index ce9e9eb..087614a 100644
--- a/libavcodec/videodsp.c
+++ b/libavcodec/videodsp.c
@@ -50,6 +50,8 @@ av_cold void ff_videodsp_init(VideoDSPContext *ctx, int bpc)
         ff_videodsp_init_arm(ctx, bpc);
     if (ARCH_PPC)
         ff_videodsp_init_ppc(ctx, bpc);
+    if (ARCH_E2K)
+        ff_videodsp_init_e2k(ctx, bpc);
     if (ARCH_X86)
         ff_videodsp_init_x86(ctx, bpc);
     if (ARCH_MIPS)
diff --git a/libavcodec/videodsp.h b/libavcodec/videodsp.h
index c0545f2..566296f 100644
--- a/libavcodec/videodsp.h
+++ b/libavcodec/videodsp.h
@@ -82,6 +82,7 @@ void ff_videodsp_init(VideoDSPContext *ctx, int bpc);
 void ff_videodsp_init_aarch64(VideoDSPContext *ctx, int bpc);
 void ff_videodsp_init_arm(VideoDSPContext *ctx, int bpc);
 void ff_videodsp_init_ppc(VideoDSPContext *ctx, int bpc);
+void ff_videodsp_init_e2k(VideoDSPContext *ctx, int bpc);
 void ff_videodsp_init_x86(VideoDSPContext *ctx, int bpc);
 void ff_videodsp_init_mips(VideoDSPContext *ctx, int bpc);
 
diff --git a/libavcodec/vorbisdsp.c b/libavcodec/vorbisdsp.c
index 362a276..e0ecda3 100644
--- a/libavcodec/vorbisdsp.c
+++ b/libavcodec/vorbisdsp.c
@@ -31,6 +31,8 @@ av_cold void ff_vorbisdsp_init(VorbisDSPContext *dsp)
         ff_vorbisdsp_init_arm(dsp);
     if (ARCH_PPC)
         ff_vorbisdsp_init_ppc(dsp);
+    if (ARCH_E2K)
+        ff_vorbisdsp_init_e2k(dsp);
     if (ARCH_X86)
         ff_vorbisdsp_init_x86(dsp);
 }
diff --git a/libavcodec/vorbisdsp.h b/libavcodec/vorbisdsp.h
index 7abec4e..001151f 100644
--- a/libavcodec/vorbisdsp.h
+++ b/libavcodec/vorbisdsp.h
@@ -34,5 +34,6 @@ void ff_vorbisdsp_init_aarch64(VorbisDSPContext *dsp);
 void ff_vorbisdsp_init_x86(VorbisDSPContext *dsp);
 void ff_vorbisdsp_init_arm(VorbisDSPContext *dsp);
 void ff_vorbisdsp_init_ppc(VorbisDSPContext *dsp);
+void ff_vorbisdsp_init_e2k(VorbisDSPContext *dsp);
 
 #endif /* AVCODEC_VORBISDSP_H */
diff --git a/libavcodec/vp3dsp.c b/libavcodec/vp3dsp.c
index f485fba..c2ce815 100644
--- a/libavcodec/vp3dsp.c
+++ b/libavcodec/vp3dsp.c
@@ -456,6 +456,8 @@ av_cold void ff_vp3dsp_init(VP3DSPContext *c, int flags)
         ff_vp3dsp_init_arm(c, flags);
     if (ARCH_PPC)
         ff_vp3dsp_init_ppc(c, flags);
+    if (ARCH_E2K)
+        ff_vp3dsp_init_e2k(c, flags);
     if (ARCH_X86)
         ff_vp3dsp_init_x86(c, flags);
     if (ARCH_MIPS)
diff --git a/libavcodec/vp3dsp.h b/libavcodec/vp3dsp.h
index 3b849ec..a01bfd4 100644
--- a/libavcodec/vp3dsp.h
+++ b/libavcodec/vp3dsp.h
@@ -56,6 +56,7 @@ void ff_vp3dsp_idct10_add(uint8_t *dest, ptrdiff_t stride, int16_t *block);
 void ff_vp3dsp_init(VP3DSPContext *c, int flags);
 void ff_vp3dsp_init_arm(VP3DSPContext *c, int flags);
 void ff_vp3dsp_init_ppc(VP3DSPContext *c, int flags);
+void ff_vp3dsp_init_e2k(VP3DSPContext *c, int flags);
 void ff_vp3dsp_init_x86(VP3DSPContext *c, int flags);
 void ff_vp3dsp_init_mips(VP3DSPContext *c, int flags);
 
diff --git a/libavcodec/vp8dsp.c b/libavcodec/vp8dsp.c
index 4ff63d0..23c9db0 100644
--- a/libavcodec/vp8dsp.c
+++ b/libavcodec/vp8dsp.c
@@ -679,6 +679,8 @@ av_cold void ff_vp78dsp_init(VP8DSPContext *dsp)
         ff_vp78dsp_init_arm(dsp);
     if (ARCH_PPC)
         ff_vp78dsp_init_ppc(dsp);
+    if (ARCH_E2K)
+        ff_vp78dsp_init_e2k(dsp);
     if (ARCH_X86)
         ff_vp78dsp_init_x86(dsp);
 }
diff --git a/libavcodec/vp8dsp.h b/libavcodec/vp8dsp.h
index cfe1524..be4f412 100644
--- a/libavcodec/vp8dsp.h
+++ b/libavcodec/vp8dsp.h
@@ -94,6 +94,7 @@ void ff_vp78dsp_init(VP8DSPContext *c);
 void ff_vp78dsp_init_aarch64(VP8DSPContext *c);
 void ff_vp78dsp_init_arm(VP8DSPContext *c);
 void ff_vp78dsp_init_ppc(VP8DSPContext *c);
+void ff_vp78dsp_init_e2k(VP8DSPContext *c);
 void ff_vp78dsp_init_x86(VP8DSPContext *c);
 
 void ff_vp8dsp_init(VP8DSPContext *c);
diff --git a/libavutil/cpu.c b/libavutil/cpu.c
index 6548cc3..78e3f79 100644
--- a/libavutil/cpu.c
+++ b/libavutil/cpu.c
@@ -57,6 +57,8 @@ static int get_cpu_flags(void)
         return ff_get_cpu_flags_arm();
     if (ARCH_PPC)
         return ff_get_cpu_flags_ppc();
+    if (ARCH_E2K)
+        return ff_get_cpu_flags_e2k();
     if (ARCH_X86)
         return ff_get_cpu_flags_x86();
     return 0;
@@ -132,6 +134,8 @@ int av_parse_cpu_flags(const char *s)
         { "flags"   , NULL, 0, AV_OPT_TYPE_FLAGS, { .i64 = 0 }, INT64_MIN, INT64_MAX, .unit = "flags" },
 #if   ARCH_PPC
         { "altivec" , NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_ALTIVEC  },    .unit = "flags" },
+#elif ARCH_E2K
+        { "e2k"     , NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_E2K      },    .unit = "flags" },
 #elif ARCH_X86
         { "mmx"     , NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_MMX      },    .unit = "flags" },
         { "mmxext"  , NULL, 0, AV_OPT_TYPE_CONST, { .i64 = CPUFLAG_MMXEXT       },    .unit = "flags" },
@@ -194,6 +198,8 @@ int av_parse_cpu_caps(unsigned *flags, const char *s)
         { "flags"   , NULL, 0, AV_OPT_TYPE_FLAGS, { .i64 = 0 }, INT64_MIN, INT64_MAX, .unit = "flags" },
 #if   ARCH_PPC
         { "altivec" , NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_ALTIVEC  },    .unit = "flags" },
+#elif ARCH_E2K
+        { "e2k"     , NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_E2K      },    .unit = "flags" },
 #elif ARCH_X86
         { "mmx"     , NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_MMX      },    .unit = "flags" },
         { "mmx2"    , NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_MMX2     },    .unit = "flags" },
@@ -314,6 +320,8 @@ size_t av_cpu_max_align(void)
         return ff_get_cpu_max_align_arm();
     if (ARCH_PPC)
         return ff_get_cpu_max_align_ppc();
+    if (ARCH_E2K)
+        return ff_get_cpu_max_align_e2k();
     if (ARCH_X86)
         return ff_get_cpu_max_align_x86();
 
diff --git a/libavutil/cpu.h b/libavutil/cpu.h
index 8bb9eb6..537c6db 100644
--- a/libavutil/cpu.h
+++ b/libavutil/cpu.h
@@ -61,6 +61,8 @@
 #define AV_CPU_FLAG_VSX          0x0002 ///< ISA 2.06
 #define AV_CPU_FLAG_POWER8       0x0004 ///< ISA 2.07
 
+#define AV_CPU_FLAG_E2K          0x0001
+
 #define AV_CPU_FLAG_ARMV5TE      (1 << 0)
 #define AV_CPU_FLAG_ARMV6        (1 << 1)
 #define AV_CPU_FLAG_ARMV6T2      (1 << 2)
diff --git a/libavutil/cpu_internal.h b/libavutil/cpu_internal.h
index 37122d1..d40e28d 100644
--- a/libavutil/cpu_internal.h
+++ b/libavutil/cpu_internal.h
@@ -44,11 +44,13 @@
 int ff_get_cpu_flags_aarch64(void);
 int ff_get_cpu_flags_arm(void);
 int ff_get_cpu_flags_ppc(void);
+int ff_get_cpu_flags_e2k(void);
 int ff_get_cpu_flags_x86(void);
 
 size_t ff_get_cpu_max_align_aarch64(void);
 size_t ff_get_cpu_max_align_arm(void);
 size_t ff_get_cpu_max_align_ppc(void);
+size_t ff_get_cpu_max_align_e2k(void);
 size_t ff_get_cpu_max_align_x86(void);
 
 #endif /* AVUTIL_CPU_INTERNAL_H */
diff --git a/libavutil/e2k/Makefile b/libavutil/e2k/Makefile
new file mode 100644
index 0000000..67892b4
--- /dev/null
+++ b/libavutil/e2k/Makefile
@@ -0,0 +1,2 @@
+OBJS += e2k/cpu.o \
+        e2k/float_dsp.o
diff --git a/libavutil/e2k/cpu.c b/libavutil/e2k/cpu.c
new file mode 100644
index 0000000..6e52faa
--- /dev/null
+++ b/libavutil/e2k/cpu.c
@@ -0,0 +1,41 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+#include "libavutil/avassert.h"
+#include "libavutil/cpu.h"
+#include "libavutil/cpu_internal.h"
+
+int ff_get_cpu_flags_e2k(void)
+{
+#if HAVE_E2K
+    return AV_CPU_FLAG_E2K;
+#else
+    return 0;
+#endif
+}
+
+size_t ff_get_cpu_max_align_e2k(void)
+{
+    int flags = av_get_cpu_flags();
+
+    if (flags & AV_CPU_FLAG_E2K)
+        return 16;
+
+    return 8;
+}
diff --git a/libavutil/e2k/cpu.h b/libavutil/e2k/cpu.h
new file mode 100644
index 0000000..e9a3d66
--- /dev/null
+++ b/libavutil/e2k/cpu.h
@@ -0,0 +1,27 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVUTIL_E2K_CPU_H
+#define AVUTIL_E2K_CPU_H
+
+#include "libavutil/cpu.h"
+#include "libavutil/cpu_internal.h"
+
+#define E2K_BASE(flags) CPUEXT(flags, E2K)
+
+#endif /* AVUTIL_E2K_CPU_H */
diff --git a/libavutil/e2k/float_dsp.c b/libavutil/e2k/float_dsp.c
new file mode 100644
index 0000000..b9c1940
--- /dev/null
+++ b/libavutil/e2k/float_dsp.c
@@ -0,0 +1,120 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (c) 2006 Luca Barbato <lu_zero@gentoo.org>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "config.h"
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include "libavutil/float_dsp.h"
+#include "libavutil/e2k/cpu.h"
+#include "util_e2k.h"
+
+static void ff_vector_fmul_e2k(float *dst,
+                               const float *src0, const float *src1,
+                               int len)
+{
+    int i;
+    vec_f d0, d1;
+    for (i = 0; i < len - 7; i += 8) {
+        d0 = _mm_loadu_ps(src0 + i);
+        d1 = _mm_loadu_ps(src0 + i + 4);
+        d0 = _mm_mul_ps(d0, _mm_loadu_ps(src1 + i));
+        d1 = _mm_mul_ps(d1, _mm_loadu_ps(src1 + i + 4));
+        _mm_storeu_ps(dst + i, d0);
+        _mm_storeu_ps(dst + i + 4, d1);
+    }
+}
+
+static void ff_vector_fmul_window_e2k(float *dst, const float *src0,
+                                      const float *src1, const float *win,
+                                      int len)
+{
+    vec_f t0, t1, s0, s1, wi, wj;
+    int i, j;
+
+    dst  += len;
+    win  += len;
+    src0 += len;
+
+    for (i = -len, j = len - 4; i < 0; i += 4, j -= 4) {
+        s0 = _mm_loadu_ps(src0 + i);
+        s1 = _mm_loadu_ps(src1 + j);
+        wi = _mm_loadu_ps(win + i);
+        wj = _mm_loadu_ps(win + j);
+
+        s1 = _mm_shuffle_ps(s1, s1, 0x1b);
+        wj = _mm_shuffle_ps(wj, wj, 0x1b);
+
+        t0 = _mm_mul_ps(s0, wj);
+        t1 = _mm_mul_ps(s0, wi);
+        t0 = _mm_sub_ps(t0, _mm_mul_ps(s1, wi));
+        t1 = _mm_add_ps(t1, _mm_mul_ps(s1, wj));
+        t1 = _mm_shuffle_ps(t1, t1, 0x1b);
+
+        _mm_storeu_ps(dst + i, t0);
+        _mm_storeu_ps(dst + j, t1);
+    }
+}
+
+static void ff_vector_fmul_add_e2k(float *dst, const float *src0,
+                                   const float *src1, const float *src2,
+                                   int len)
+{
+    int i;
+    vec_f d, s0, s1, s2;
+
+    for (i = 0; i < len - 3; i += 4) {
+        s0 = _mm_loadu_ps(src0 + i);
+        s1 = _mm_loadu_ps(src1 + i);
+        s2 = _mm_loadu_ps(src2 + i);
+        d = _mm_add_ps(_mm_mul_ps(s0, s1), s2);
+        _mm_storeu_ps(dst + i, d);
+    }
+}
+
+static void ff_vector_fmul_reverse_e2k(float *dst, const float *src0,
+                                       const float *src1, int len)
+{
+    int i;
+    vec_f s0, s1, s2, s3;
+    src1 += len - 4;
+    for (i = 0; i < len - 7; i += 8) {
+        s1 = _mm_loadu_ps(src1 - i);
+        s0 = _mm_loadu_ps(src0 + i);
+        s3 = _mm_loadu_ps(src1 - i - 4);
+        s2 = _mm_loadu_ps(src0 + i + 4);
+        s1 = _mm_shuffle_ps(s1, s1, 0x1b);
+        s3 = _mm_shuffle_ps(s3, s3, 0x1b);
+        s0 = _mm_mul_ps(s0, s1);
+        s2 = _mm_mul_ps(s2, s3);
+        _mm_storeu_ps(dst + i, s0);
+        _mm_storeu_ps(dst + i + 4, s2);
+    }
+}
+
+av_cold void ff_float_dsp_init_e2k(AVFloatDSPContext *fdsp, int bit_exact)
+{
+    if (E2K_BASE(av_get_cpu_flags())) {
+        fdsp->vector_fmul = ff_vector_fmul_e2k;
+        fdsp->vector_fmul_add = ff_vector_fmul_add_e2k;
+        fdsp->vector_fmul_reverse = ff_vector_fmul_reverse_e2k;
+        fdsp->vector_fmul_window = ff_vector_fmul_window_e2k;
+    }
+}
diff --git a/libavutil/e2k/intreadwrite.h b/libavutil/e2k/intreadwrite.h
new file mode 100644
index 0000000..0387475
--- /dev/null
+++ b/libavutil/e2k/intreadwrite.h
@@ -0,0 +1,54 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVUTIL_E2K_INTREADWRITE_H
+#define AVUTIL_E2K_INTREADWRITE_H
+
+#include <stdint.h>
+#include "config.h"
+
+#include <x86intrin.h>
+
+#define AV_RB32 av_read_bswap32
+#define AV_WB32 av_write_bswap32
+#define AV_RB64 av_read_bswap64
+#define AV_WB64 av_write_bswap64
+
+static av_always_inline uint32_t av_read_bswap32(const void *p)
+{
+    return _bswap(*(const uint32_t*)p);
+}
+
+static av_always_inline void av_write_bswap32(void *p, uint32_t v)
+{
+    *(uint32_t*)p = _bswap(v);
+}
+
+static av_always_inline uint64_t av_read_bswap64(const void *p)
+{
+    return _bswap64(*(const uint64_t*)p);
+}
+
+static av_always_inline void av_write_bswap64(void *p, uint64_t v)
+{
+    *(uint64_t*)p = _bswap64(v);
+}
+
+#endif /* AVUTIL_E2K_INTREADWRITE_H */
diff --git a/libavutil/e2k/timer.h b/libavutil/e2k/timer.h
new file mode 100644
index 0000000..ea78175
--- /dev/null
+++ b/libavutil/e2k/timer.h
@@ -0,0 +1,35 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVUTIL_E2K_TIMER_H
+#define AVUTIL_E2K_TIMER_H
+
+#include <stdint.h>
+#include <x86intrin.h>
+
+#define AV_READ_TIME read_time
+
+static inline uint64_t read_time(void)
+{
+    unsigned aux;
+    return __rdtscp(&aux);
+}
+
+#endif /* AVUTIL_E2K_TIMER_H */
diff --git a/libavutil/e2k/util_e2k.h b/libavutil/e2k/util_e2k.h
new file mode 100644
index 0000000..d0bfa21
--- /dev/null
+++ b/libavutil/e2k/util_e2k.h
@@ -0,0 +1,201 @@
+/*
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * Contains misc utility macros and inline functions
+ */
+
+#ifndef AVUTIL_E2K_UTIL_E2K_H
+#define AVUTIL_E2K_UTIL_E2K_H
+
+#include <stdint.h>
+#include "config.h"
+#include <smmintrin.h> /* SSE4.1 */
+
+#define ALWAYS_INLINE __attribute__((__always_inline__)) inline
+#define ALIGNED(n) __attribute__((aligned(n))) 
+
+#ifdef __e2k__
+#define PRAGMA_E2K _Pragma
+#define _mm_shuffle2_pi8(a, b, c) \
+    ((__m64)__builtin_e2k_pshufb((uint64_t)(b), (uint64_t)(a), (uint64_t)(c)))
+#define _mm_shuffle2_epi8(a, b, c) \
+    ((__m128i)__builtin_e2k_qppermb((__v2di)(b), (__v2di)(a), (__v2di)(c)))
+#define _mm_blendv_pi8(a, b, c) \
+    ((__m64)__builtin_e2k_pmerge((uint64_t)(a), (uint64_t)(b), (uint64_t)(c)))
+#else
+#define PRAGMA_E2K(x)
+#define _mm_shuffle2_pi8(a, b, c) \
+    _mm_movepi64_pi64(_mm_shuffle_epi8(_mm_unpacklo_epi64( \
+        _mm_movpi64_epi64(a), _mm_movpi64_epi64(b)), _mm_movpi64_epi64(c)))
+#define _mm_shuffle2_epi8(a, b, c) \
+    _mm_blendv_epi8(_mm_shuffle_epi8(a, c), _mm_shuffle_epi8(b, c), \
+        _mm_slli_epi16(c, 3))
+#define _mm_blendv_pi8(a, b, c) \
+    _mm_movepi64_pi64(_mm_blendv_epi8(_mm_movpi64_epi64(a), \
+        _mm_movpi64_epi64(b), _mm_movpi64_epi64(c)))
+
+static ALWAYS_INLINE uint64_t __builtin_e2k_insfd(uint64_t a, uint64_t b, uint64_t c) {
+  int n = b & 63;
+  a = a >> n | a << (64 - n);
+  return c ^ ((a ^ c) & (~0ll << (b >> 6 & 63)));
+}
+#endif
+
+#define _mm_extract_pi32(a, b) _mm_extract_epi32(_mm_movpi64_epi64(a), b)
+#define VEC_ALIGNR8(a, b) _mm_castpd_si128(_mm_shuffle_pd(_mm_castsi128_pd(b), _mm_castsi128_pd(a), 1))
+
+#define _mm_unpacklo_ps2(a, b) _mm_castpd_ps(_mm_unpacklo_pd(_mm_castps_pd(a), _mm_castps_pd(b)));
+#define _mm_unpackhi_ps2(a, b) _mm_castpd_ps(_mm_unpackhi_pd(_mm_castps_pd(a), _mm_castps_pd(b)));
+#define _mm_alignr_ps(a, b, n) _mm_castsi128_ps(_mm_alignr_epi8(_mm_castps_si128(a), _mm_castps_si128(b), (n) * 4))
+#define _mm_bsrli_ps(a, n) _mm_castsi128_ps(_mm_bsrli_si128(_mm_castps_si128(a), (n) * 4))
+
+/***********************************************************************
+ * Vector types
+ **********************************************************************/
+#define vec_u8  __m128i
+#define vec_s8  __m128i
+#define vec_u16 __m128i
+#define vec_s16 __m128i
+#define vec_u32 __m128i
+#define vec_s32 __m128i
+#define vec_f   __m128
+
+/***********************************************************************
+ * Null vector
+ **********************************************************************/
+#define LOAD_ZERO const __m128i zerov = _mm_setzero_si128()
+
+#define zero_u8v  zerov
+#define zero_s8v  zerov
+#define zero_u16v zerov
+#define zero_s16v zerov
+#define zero_u32v zerov
+#define zero_s32v zerov
+
+/*
+// used to build registers permutation vectors (vcprm)
+// the 's' are for words in the _s_econd vector
+#define WORD_0 0x00,0x01,0x02,0x03
+#define WORD_1 0x04,0x05,0x06,0x07
+#define WORD_2 0x08,0x09,0x0a,0x0b
+#define WORD_3 0x0c,0x0d,0x0e,0x0f
+#define WORD_s0 0x10,0x11,0x12,0x13
+#define WORD_s1 0x14,0x15,0x16,0x17
+#define WORD_s2 0x18,0x19,0x1a,0x1b
+#define WORD_s3 0x1c,0x1d,0x1e,0x1f
+#define vcprm(a, b, c, d) _mm_setr_epi8(WORD_##a, WORD_##b, WORD_##c, WORD_##d)
+#define _mm_shuffle2_ps(a, b, c) _mm_castsi128_ps(_mm_shuffle2_epi8(_mm_castps_si128(a), _mm_castps_si128(b), c))
+
+#define SWP_W2S0 0x02,0x03,0x00,0x01
+#define SWP_W2S1 0x06,0x07,0x04,0x05
+#define SWP_W2S2 0x0a,0x0b,0x08,0x09
+#define SWP_W2S3 0x0e,0x0f,0x0c,0x0d
+#define SWP_W2Ss0 0x12,0x13,0x10,0x11
+#define SWP_W2Ss1 0x16,0x17,0x14,0x15
+#define SWP_W2Ss2 0x1a,0x1b,0x18,0x19
+#define SWP_W2Ss3 0x1e,0x1f,0x1c,0x1d
+#define vcswapi2s(a,b,c,d) (const vector unsigned char){SWP_W2S ## a, SWP_W2S ## b, SWP_W2S ## c, SWP_W2S ## d}
+
+#define vcswapc() \
+  (const vector unsigned char){0x0f,0x0e,0x0d,0x0c,0x0b,0x0a,0x09,0x08,0x07,0x06,0x05,0x04,0x03,0x02,0x01,0x00}
+*/
+
+// Transpose 8x8 matrix of 16-bit elements (in-place)
+#define TRANSPOSE8(a0, a1, a2, a3, a4, a5, a6, a7) \
+do { \
+    vec_s16 _b0, _b1, _b2, _b3, _b4, _b5, _b6, _b7; \
+    vec_s16 _c0, _c1, _c2, _c3, _c4, _c5, _c6, _c7; \
+    _b0 = _mm_unpacklo_epi16(a0, a2); \
+    _b1 = _mm_unpackhi_epi16(a0, a2); \
+    _b2 = _mm_unpacklo_epi16(a1, a3); \
+    _b3 = _mm_unpackhi_epi16(a1, a3); \
+    _b4 = _mm_unpacklo_epi16(a4, a6); \
+    _b5 = _mm_unpackhi_epi16(a4, a6); \
+    _b6 = _mm_unpacklo_epi16(a5, a7); \
+    _b7 = _mm_unpackhi_epi16(a5, a7); \
+    \
+    _c0 = _mm_unpacklo_epi16(_b0, _b2); \
+    _c1 = _mm_unpackhi_epi16(_b0, _b2); \
+    _c2 = _mm_unpacklo_epi16(_b1, _b3); \
+    _c3 = _mm_unpackhi_epi16(_b1, _b3); \
+    _c4 = _mm_unpacklo_epi16(_b4, _b6); \
+    _c5 = _mm_unpackhi_epi16(_b4, _b6); \
+    _c6 = _mm_unpacklo_epi16(_b5, _b7); \
+    _c7 = _mm_unpackhi_epi16(_b5, _b7); \
+    \
+    a0 = _mm_unpacklo_epi64(_c0, _c4); \
+    a1 = _mm_unpackhi_epi64(_c0, _c4); \
+    a2 = _mm_unpacklo_epi64(_c1, _c5); \
+    a3 = _mm_unpackhi_epi64(_c1, _c5); \
+    a4 = _mm_unpacklo_epi64(_c2, _c6); \
+    a5 = _mm_unpackhi_epi64(_c2, _c6); \
+    a6 = _mm_unpacklo_epi64(_c3, _c7); \
+    a7 = _mm_unpackhi_epi64(_c3, _c7); \
+} while (0)
+
+#define VEC_LD(a)     _mm_loadu_si128((const __m128i*)(a))
+#define VEC_ST(a, b)  _mm_storeu_si128((__m128i*)(a), b)
+#define VEC_LD8(a)    _mm_loadl_epi64((const __m128i*)(a))
+#define VEC_STL(a, b) _mm_storel_epi64((__m128i*)(a), b)
+#define VEC_STH(a, b) _mm_storeh_pd((double*)(a), _mm_castsi128_pd(b));
+
+#define VEC_SPLAT16(v, i) _mm_shuffle_epi8(v, _mm_set1_epi16((i) * 2 | ((i) * 2 + 1) << 8))
+
+/*
+#define VEC_LD(a) _mm_loadu_si128(a)
+#define VEC_ST(a,b) _mm_storeu_si128(a,b)
+
+#define unaligned_load(a,b) VEC_LD(a,b)
+#define load_with_perm_vec(a,b,c) VEC_LD(a,b)
+
+#define vec_unaligned_load(b)  VEC_LD(0, b)
+
+#define VEC_MERGEH(a, b) vec_mergeh(b, a)
+#define VEC_MERGEL(a, b) vec_mergel(b, a)
+
+#define VEC_SPLAT16(a,b) vec_splat((vec_s16)(vec_perm(a, a, vcswapi2s(0,1,2,3))), b)
+#define VEC_SLD16(a,b,c) vec_sld(b, a, c)
+
+#define vsx_ld_u8_s16(off, p)                               \
+    ((vec_s16)vec_mergeh((vec_u8)vec_vsx_ld((off), (p)),    \
+                         (vec_u8)vec_splat_u8(0)))
+*/
+
+#if !defined(__iset__) || __iset__ < 5
+#define NEED_ALIGN8
+#define ALIGN8_COMMON uint64_t src_shr; __m64 src_tmp0, src_tmp1;
+#define ALIGN8_VARS(src) __m64 *src##_ptr, src##_next, src##_index;
+#define ALIGN8_START(ptr, src) \
+  src_shr = (intptr_t)(ptr - 1) & 7; \
+  src##_ptr = (__m64*)((intptr_t)(ptr - 1) & -8); \
+  src##_next = src##_ptr[src_shr == 7]; \
+  src##_index = _mm_add_pi8(_mm_set1_pi8(src_shr), \
+                            _mm_setr_pi8(1, 2, 3, 4, 5, 6, 7, 8));
+#define ALIGN8_READ16(v0, src, i) \
+  src_tmp1 = src##_ptr[i * 2 + 1]; \
+  src_tmp0 = _mm_shuffle2_pi8(src##_next, src_tmp1, src##_index); \
+  src##_next = src##_ptr[i * 2 + 2]; \
+  src_tmp1 = _mm_shuffle2_pi8(src_tmp1, src##_next, src##_index); \
+  v0 = _mm_setr_epi64(src_tmp0, src_tmp1);
+#endif
+
+#endif /* AVUTIL_E2K_UTIL_E2K_H */
diff --git a/libavutil/float_dsp.c b/libavutil/float_dsp.c
index 6e28d71..5241c3e 100644
--- a/libavutil/float_dsp.c
+++ b/libavutil/float_dsp.c
@@ -156,6 +156,8 @@ av_cold AVFloatDSPContext *avpriv_float_dsp_alloc(int bit_exact)
         ff_float_dsp_init_arm(fdsp);
     if (ARCH_PPC)
         ff_float_dsp_init_ppc(fdsp, bit_exact);
+    if (ARCH_E2K)
+        ff_float_dsp_init_e2k(fdsp, bit_exact);
     if (ARCH_X86)
         ff_float_dsp_init_x86(fdsp);
     if (ARCH_MIPS)
diff --git a/libavutil/float_dsp.h b/libavutil/float_dsp.h
index 9c66459..97d9d79 100644
--- a/libavutil/float_dsp.h
+++ b/libavutil/float_dsp.h
@@ -205,6 +205,7 @@ float avpriv_scalarproduct_float_c(const float *v1, const float *v2, int len);
 void ff_float_dsp_init_aarch64(AVFloatDSPContext *fdsp);
 void ff_float_dsp_init_arm(AVFloatDSPContext *fdsp);
 void ff_float_dsp_init_ppc(AVFloatDSPContext *fdsp, int strict);
+void ff_float_dsp_init_e2k(AVFloatDSPContext *fdsp, int strict);
 void ff_float_dsp_init_x86(AVFloatDSPContext *fdsp);
 void ff_float_dsp_init_mips(AVFloatDSPContext *fdsp);
 
diff --git a/libavutil/intreadwrite.h b/libavutil/intreadwrite.h
index 4c8413a..b8a698e 100644
--- a/libavutil/intreadwrite.h
+++ b/libavutil/intreadwrite.h
@@ -72,6 +72,8 @@ typedef union {
 #   include "mips/intreadwrite.h"
 #elif ARCH_PPC
 #   include "ppc/intreadwrite.h"
+#elif ARCH_E2K
+#   include "e2k/intreadwrite.h"
 #elif ARCH_TOMI
 #   include "tomi/intreadwrite.h"
 #elif ARCH_X86
diff --git a/libavutil/tests/cpu.c b/libavutil/tests/cpu.c
index ce45b71..21c30cf 100644
--- a/libavutil/tests/cpu.c
+++ b/libavutil/tests/cpu.c
@@ -49,6 +49,8 @@ static const struct {
     { AV_CPU_FLAG_SETEND,    "setend"     },
 #elif ARCH_PPC
     { AV_CPU_FLAG_ALTIVEC,   "altivec"    },
+#elif ARCH_E2K
+    { AV_CPU_FLAG_E2K,       "e2k"        },
 #elif ARCH_X86
     { AV_CPU_FLAG_MMX,       "mmx"        },
     { AV_CPU_FLAG_MMXEXT,    "mmxext"     },
diff --git a/libavutil/timer.h b/libavutil/timer.h
index 0bb353c..cc0c282 100644
--- a/libavutil/timer.h
+++ b/libavutil/timer.h
@@ -54,6 +54,8 @@
 #   include "arm/timer.h"
 #elif ARCH_PPC
 #   include "ppc/timer.h"
+#elif ARCH_E2K
+#   include "e2k/timer.h"
 #elif ARCH_X86
 #   include "x86/timer.h"
 #endif
diff --git a/libswscale/e2k/Makefile b/libswscale/e2k/Makefile
new file mode 100644
index 0000000..f35371d
--- /dev/null
+++ b/libswscale/e2k/Makefile
@@ -0,0 +1,3 @@
+OBJS += e2k/swscale.o \
+        e2k/yuv2rgb.o \
+        e2k/yuv2yuv.o
diff --git a/libswscale/e2k/swscale.c b/libswscale/e2k/swscale.c
new file mode 100644
index 0000000..24a857f
--- /dev/null
+++ b/libswscale/e2k/swscale.c
@@ -0,0 +1,2046 @@
+/*
+ * Elbrus-enhanced yuv2yuvX
+ *
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (C) 2004 Romain Dolbeau <romain@dolbeau.org>
+ * based on the equivalent C code in swscale.c
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include <inttypes.h>
+
+#include "config.h"
+#include "libswscale/swscale.h"
+#include "libswscale/swscale_internal.h"
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+
+#include "yuv2rgb.h"
+
+static void yuv2planeX_8_e2k(const int16_t *filter, int filterSize,
+                           const int16_t **src, uint8_t *dest, int dstW,
+                           const uint8_t *dither, int offset)
+{
+    int i = 0, j;
+    __m64 h0;
+    __m128i d0, d1, zerov = _mm_setzero_si128();
+    h0 = (__m64)__builtin_e2k_insfd(*(uint64_t*)dither, ((offset + i) & 7) * 8, 0);
+    d1 = _mm_unpacklo_epi8(_mm_movpi64_epi64(h0), zerov);
+    d0 = _mm_unpacklo_epi16(d1, zerov);
+    d1 = _mm_unpackhi_epi16(d1, zerov);
+    d0 = _mm_slli_epi32(d0, 12);
+    d1 = _mm_slli_epi32(d1, 12);
+
+    for (; i < dstW - 15; i += 16) {
+        __m128i r0, r1, r2, r3, v0, v1, v2, v3;
+
+        r2 = r0 = d0;
+        r3 = r1 = d1;
+        for (j = 0; j < filterSize; j++) {
+            v1 = _mm_set1_epi16(filter[j]);
+
+            v0 = VEC_LD(src[j] + i);
+            v2 = _mm_mullo_epi16(v0, v1);
+            v3 = _mm_mulhi_epi16(v0, v1);
+            v0 = _mm_unpacklo_epi16(v2, v3);
+            v3 = _mm_unpackhi_epi16(v2, v3);
+            r0 = _mm_add_epi32(r0, v0);
+            r1 = _mm_add_epi32(r1, v3);
+
+            v0 = VEC_LD(src[j] + i + 8);
+            v2 = _mm_mullo_epi16(v0, v1);
+            v3 = _mm_mulhi_epi16(v0, v1);
+            v0 = _mm_unpacklo_epi16(v2, v3);
+            v3 = _mm_unpackhi_epi16(v2, v3);
+            r2 = _mm_add_epi32(r2, v0);
+            r3 = _mm_add_epi32(r3, v3);
+        }
+        r0 = _mm_srai_epi32(r0, 19);
+        r1 = _mm_srai_epi32(r1, 19);
+        r2 = _mm_srai_epi32(r2, 19);
+        r3 = _mm_srai_epi32(r3, 19);
+        r0 = _mm_packs_epi32(r0, r1);
+        r2 = _mm_packs_epi32(r2, r3);
+        r0 = _mm_packus_epi16(r0, r2);
+        VEC_ST(dest + i, r0);
+    }
+
+    for (; i < dstW; i++) {
+        int val = dither[(i + offset) & 7] << 12;
+        for (j = 0; j < filterSize; j++)
+            val += src[j][i] * filter[j];
+        dest[i] = av_clip_uint8(val >> 19);
+    }
+}
+
+static void hScale_real_e2k(SwsContext *c, int16_t *dst, int dstW,
+                            const uint8_t *src, const int16_t *filter,
+                            const int32_t *filterPos, int filterSize)
+{
+    int i;
+    LOAD_ZERO;
+    switch (filterSize) {
+
+    case 1:
+        PRAGMA_E2K("ivdep")
+        for (i = 0; i < dstW; i++) {
+            int val, srcPos = filterPos[i];
+            val = (int)src[srcPos] * filter[filterSize * i];
+            dst[i] = FFMIN(val >> 7, (1 << 15) - 1);
+        }
+    break;
+
+    case 2:
+        PRAGMA_E2K("ivdep")
+        for (i = 0; i < dstW; i++) {
+            int val, srcPos = filterPos[i];
+            val = (int)src[srcPos] * filter[filterSize * i];
+            val += (int)src[srcPos + 1] * filter[filterSize * i + 1];
+            dst[i] = FFMIN(val >> 7, (1 << 15) - 1);
+        }
+    break;
+
+    case 4: {
+        __m64 zerov = _mm_setzero_si64();
+        PRAGMA_E2K("ivdep")
+        for (i = 0; i < dstW; i++, filter += filterSize) {
+            int val;
+            __m64 v0, v2, accv;
+            const uint8_t *srci = src + filterPos[i];
+
+            v0 = _mm_cvtsi32_si64(*(uint32_t*)srci);
+            v0 = _mm_unpacklo_pi8(v0, zerov);
+            v2 = *(__m64*)filter;
+            accv = _mm_madd_pi16(v0, v2);
+            val = _mm_extract_pi32(accv, 0) + _mm_extract_pi32(accv, 1);
+            dst[i] = FFMIN(val >> 7, (1 << 15) - 1);
+        }
+    }
+    break;
+
+    case 8:
+        PRAGMA_E2K("ivdep")
+        for (i = 0; i < dstW; i++, filter += filterSize) {
+            int val, j = 0;
+            __m128i v0, v2, accv;
+            const uint8_t *srci = src + filterPos[i];
+
+            v0 = VEC_LD8(srci + j);
+            v0 = _mm_unpacklo_epi8(v0, zerov);
+            v2 = VEC_LD(filter + j);
+            accv = _mm_madd_epi16(v0, v2);
+            accv = _mm_hadd_epi32(accv, accv);
+            val = _mm_extract_epi32(accv, 0) + _mm_extract_epi32(accv, 1);
+            dst[i] = FFMIN(val >> 7, (1 << 15) - 1);
+        }
+    break;
+
+    case 16:
+        PRAGMA_E2K("ivdep")
+        for (i = 0; i < dstW; i++, filter += filterSize) {
+            int val, j = 0;
+            __m128i v0, v1, v2, v3, accv;
+            const uint8_t *srci = src + filterPos[i];
+
+            v1 = VEC_LD(srci + j);
+            v0 = _mm_unpacklo_epi8(v1, zerov);
+            v1 = _mm_unpackhi_epi8(v1, zerov);
+            v2 = VEC_LD(filter + j);
+            v3 = VEC_LD(filter + j + 8);
+            accv = _mm_madd_epi16(v0, v2);
+            accv = _mm_add_epi32(accv, _mm_madd_epi16(v1, v3));
+            accv = _mm_hadd_epi32(accv, accv);
+            val = _mm_extract_epi32(accv, 0) + _mm_extract_epi32(accv, 1);
+            dst[i] = FFMIN(val >> 7, (1 << 15) - 1);
+        }
+    break;
+
+    default:
+        av_assert0((filterSize & 7) == 0);
+
+        for (i = 0; i < dstW; i++, filter += filterSize) {
+            int val, j = 0;
+            __m128i v0, v1, v2, v3, accv = zerov;
+            const uint8_t *srci = src + filterPos[i];
+
+            for (; j < filterSize - 15; j += 16) {
+                v1 = VEC_LD(srci + j);
+                v0 = _mm_unpacklo_epi8(v1, zerov);
+                v1 = _mm_unpackhi_epi8(v1, zerov);
+                v2 = VEC_LD(filter + j);
+                v3 = VEC_LD(filter + j + 8);
+                accv = _mm_add_epi32(accv, _mm_madd_epi16(v0, v2));
+                accv = _mm_add_epi32(accv, _mm_madd_epi16(v1, v3));
+            }
+            if (filterSize & 8) {
+                v1 = VEC_LD8(srci + j);
+                v0 = _mm_unpacklo_epi8(v1, zerov);
+                v2 = VEC_LD(filter + j);
+                accv = _mm_add_epi32(accv, _mm_madd_epi16(v0, v2));
+            }
+            accv = _mm_hadd_epi32(accv, accv);
+            val = _mm_extract_epi32(accv, 0) + _mm_extract_epi32(accv, 1);
+            dst[i] = FFMIN(val >> 7, (1 << 15) - 1);
+        }
+    }
+}
+
+static void yuv2plane1_floatLE_e2k(const int16_t *_src, uint8_t *_dest,
+                                   int dstW, const uint8_t *dither, int offset)
+{
+    const int32_t *src = (const int32_t*)_src;
+    float *dest = (float*)_dest;
+    int shift = 3;
+    int add = (1 << shift) >> 1;
+    int clip = (1 << 16) - 1;
+    float fmult = 1.0f / 65535.0f;
+    LOAD_ZERO;
+    vec_u32 vadd = _mm_set1_epi32(add);
+    vec_u32 vlargest = _mm_set1_epi32(clip);
+    vec_f vmul = _mm_set1_ps(fmult);
+    vec_u32 v0;
+    vec_f v1;
+    int i = 0;
+
+    PRAGMA_E2K("ivdep")
+    for (; i < dstW - 3; i += 4) {
+        v0 = VEC_LD(src + i);
+        v0 = _mm_add_epi32(v0, vadd);
+        v0 = _mm_srai_epi32(v0, shift);
+        v0 = _mm_max_epi32(v0, zerov);
+        v0 = _mm_min_epi32(v0, vlargest);
+        v1 = _mm_mul_ps(_mm_cvtepi32_ps(v0), vmul);
+        _mm_storeu_ps(dest + i, v1);
+    }
+
+    PRAGMA_E2K("ivdep")
+    for (; i < dstW; ++i){
+        int val = src[i] + add;
+        val = av_clip_uint16(val >> shift);
+        dest[i] = fmult * (float)val;
+    }
+}
+
+static void yuv2plane1_floatBE_e2k(const int16_t *_src, uint8_t *_dest,
+                                   int dstW, const uint8_t *dither, int offset)
+{
+    const int32_t *src = (const int32_t*)_src;
+    uint32_t *dest = (uint32_t*)_dest;
+    int shift = 3;
+    int add = (1 << shift) >> 1;
+    int clip = (1 << 16) - 1;
+    float fmult = 1.0f / 65535.0f;
+    LOAD_ZERO;
+    vec_u32 vadd = _mm_set1_epi32(add);
+    vec_u32 vlargest = _mm_set1_epi32(clip);
+    vec_f vmul = _mm_set1_ps(fmult);
+    vec_u8 vswap = _mm_setr_epi8(3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12);
+    vec_u32 v0;
+    vec_f v1;
+    int i = 0;
+
+    PRAGMA_E2K("ivdep")
+    for (; i < dstW - 3; i += 4) {
+        v0 = VEC_LD(src + i);
+        v0 = _mm_add_epi32(v0, vadd);
+        v0 = _mm_srai_epi32(v0, shift);
+        v0 = _mm_max_epi32(v0, zerov);
+        v0 = _mm_min_epi32(v0, vlargest);
+        v1 = _mm_mul_ps(_mm_cvtepi32_ps(v0), vmul);
+        v0 = _mm_shuffle_epi8(_mm_castps_si128(v1), vswap);
+        VEC_ST(dest + i, v0);
+    }
+
+    PRAGMA_E2K("ivdep")
+    for (; i < dstW; i++) {
+        int val = src[i] + add;
+        val = av_clip_uint16(val >> shift);
+        dest[i] = av_bswap32(av_float2int(fmult * (float)val));
+    }
+}
+
+static void yuv2plane1_8_e2k(const int16_t *src, uint8_t *dest, int dstW,
+                             const uint8_t *dither, int offset)
+{
+    int i = 0;
+    __m128i v0, v1, ditherv;
+    LOAD_ZERO;
+    __m64 h0;
+    h0 = (__m64)__builtin_e2k_insfd(*(uint64_t*)dither, ((offset + i) & 7) * 8, 0);
+    ditherv = _mm_unpacklo_epi8(_mm_movpi64_epi64(h0), zerov);
+
+    PRAGMA_E2K("ivdep")
+    for (; i < dstW - 15; i += 16) {
+        v0 = VEC_LD(src + i);
+        v1 = VEC_LD(src + i + 8);
+        v0 = _mm_adds_epi16(v0, ditherv);
+        v1 = _mm_adds_epi16(v1, ditherv);
+        v0 = _mm_srai_epi16(v0, 7);
+        v1 = _mm_srai_epi16(v1, 7);
+        v0 = _mm_packus_epi16(v0, v1);
+        VEC_ST(dest + i, v0);
+    }
+
+    PRAGMA_E2K("ivdep")
+    for (; i < dstW; i++) {
+        int val = (src[i] + dither[(i + offset) & 7]) >> 7;
+        dest[i] = av_clip_uint8(val);
+    }
+}
+
+#define output_pixel(pos, val) \
+    if (big_endian) { \
+        AV_WB16(pos, av_clip_uintp2(val >> shift, output_bits)); \
+    } else { \
+        AV_WL16(pos, av_clip_uintp2(val >> shift, output_bits)); \
+    }
+
+static av_always_inline
+void yuv2plane1_10_e2k(const int16_t *src, uint16_t *dest, int dstW,
+                       const int big_endian, const int output_bits)
+{
+    int shift = 15 - output_bits;
+    int add = 1 << (shift - 1);
+    int clip = (1 << output_bits) - 1;
+    vec_u16 vadd = _mm_set1_epi16(add);
+    vec_u16 vlargest = _mm_set1_epi16(clip);
+    vec_u8 vswap = _mm_setr_epi8(1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14);
+    LOAD_ZERO;
+    __m128i v0;
+    int i = 0;
+
+    PRAGMA_E2K("ivdep")
+    for (; i < dstW - 7; i += 8) {
+        v0 = VEC_LD(src + i);
+        v0 = _mm_adds_epi16(v0, vadd);
+        v0 = _mm_srai_epi16(v0, shift);
+        v0 = _mm_max_epi16(v0, zerov);
+        v0 = _mm_min_epu16(v0, vlargest);
+        if (big_endian) {
+            v0 = _mm_shuffle_epi8(v0, vswap);
+        }
+        VEC_ST(dest + i, v0);
+    }
+
+    PRAGMA_E2K("ivdep")
+    for (; i < dstW; i++) {
+        int val = src[i] + add;
+        output_pixel(&dest[i], val);
+    }
+}
+
+static av_always_inline
+void yuv2planeX_10_e2k(const int16_t *filter, int filterSize,
+                       const int16_t **src, uint16_t *dest, int dstW,
+                       int big_endian, int output_bits)
+{
+    int shift = 11 + 16 - output_bits;
+    int add = 1 << (shift - 1);
+    int clip = (1 << output_bits) - 1;
+    vec_u16 vadd = _mm_set1_epi32(add);
+    vec_u16 vlargest = _mm_set1_epi16(clip);
+    vec_u8 vswap = _mm_setr_epi8(1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14);
+    __m128i v0, v1, v2, v3, v4, v5;
+    int i = 0, j;
+
+    for (; i < dstW - 7; i += 8) {
+        v4 = v5 = vadd;
+        for (j = 0; j < filterSize; j++) {
+            v0 = VEC_LD(src[j] + i);
+            v1 = _mm_set1_epi16(filter[j]);
+            v2 = _mm_mullo_epi16(v0, v1);
+            v3 = _mm_mulhi_epi16(v0, v1);
+            v0 = _mm_unpacklo_epi16(v2, v3);
+            v1 = _mm_unpackhi_epi16(v2, v3);
+            v4 = _mm_add_epi32(v4, v0);
+            v5 = _mm_add_epi32(v5, v1);
+        }
+        v4 = _mm_srai_epi32(v4, shift);
+        v5 = _mm_srai_epi32(v5, shift);
+        v0 = _mm_packus_epi32(v4, v5);
+        v0 = _mm_min_epu16(v0, vlargest);
+        if (big_endian) {
+            v0 = _mm_shuffle_epi8(v0, vswap);
+        }
+        VEC_ST(dest + i, v0);
+    }
+
+    for (; i < dstW; i++) {
+        int val = 1 << (shift - 1);
+        for (j = 0; j < filterSize; j++)
+            val += src[j][i] * filter[j];
+        output_pixel(&dest[i], val);
+    }
+}
+
+#undef output_pixel
+
+#define output_pixel(pos, val, bias, signedness) \
+    if (big_endian) { \
+        AV_WB16(pos, bias + av_clip_##signedness##16(val >> shift)); \
+    } else { \
+        AV_WL16(pos, bias + av_clip_##signedness##16(val >> shift)); \
+    }
+
+static av_always_inline
+void yuv2plane1_16_e2k(const int32_t *src, uint16_t *dest, int dstW,
+                       const int big_endian, int output_bits)
+{
+    int shift = 3;
+    int add = 1 << (shift - 1);
+    vec_u32 vadd = _mm_set1_epi32(add);
+    vec_u8 vswap = _mm_setr_epi8(1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14);
+    __m128i v0, v1;
+    int i = 0;
+
+    PRAGMA_E2K("ivdep")
+    for (; i < dstW - 7; i += 8) {
+        v0 = VEC_LD(src + i);
+        v1 = VEC_LD(src + i + 4);
+        v0 = _mm_add_epi32(v0, vadd);
+        v1 = _mm_add_epi32(v1, vadd);
+        v0 = _mm_srai_epi32(v0, shift);
+        v1 = _mm_srai_epi32(v1, shift);
+        v0 = _mm_packus_epi32(v0, v1);
+        if (big_endian) {
+            v0 = _mm_shuffle_epi8(v0, vswap);
+        }
+        VEC_ST(dest + i, v0);
+    }
+
+    PRAGMA_E2K("ivdep")
+    for (; i < dstW; i++) {
+        int val = src[i] + add;
+        output_pixel(&dest[i], val, 0, uint);
+    }
+}
+
+/* range of val is [0,0x7FFFFFFF], so 31 bits, but with lanczos/spline
+ * filters (or anything with negative coeffs, the range can be slightly
+ * wider in both directions. To account for this overflow, we subtract
+ * a constant so it always fits in the signed range (assuming a
+ * reasonable filterSize), and re-add that at the end. */
+
+static av_always_inline
+void yuv2planeX_16_e2k(const int16_t *filter, int filterSize,
+                       const int32_t **src, uint16_t *dest, int dstW,
+                       int big_endian, int output_bits)
+{
+    int shift = 15, bias = 0x8000;
+    int add = (1 << (shift - 1)) - 0x40000000;
+    vec_u32 vadd = _mm_set1_epi32(add);
+    vec_u16 vbias = _mm_set1_epi16(bias);
+    vec_u8 vswap = _mm_setr_epi8(1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14);
+    __m128i v0, v1, v2, v4, v5;
+    int i = 0, j;
+
+    for (; i < dstW - 7; i += 8) {
+        v4 = v5 = vadd;
+        for (j = 0; j < filterSize; j++) {
+            v0 = VEC_LD(src[j] + i);
+            v1 = VEC_LD(src[j] + i + 4);
+            v2 = _mm_set1_epi32(filter[j]);
+            v4 = _mm_add_epi32(v4, _mm_mullo_epi32(v0, v2));
+            v5 = _mm_add_epi32(v5, _mm_mullo_epi32(v1, v2));
+        }
+        v4 = _mm_srai_epi32(v4, shift);
+        v5 = _mm_srai_epi32(v5, shift);
+        v0 = _mm_packs_epi32(v4, v5);
+        v0 = _mm_add_epi16(v0, vbias);
+        if (big_endian) {
+            v0 = _mm_shuffle_epi8(v0, vswap);
+        }
+        VEC_ST(dest + i, v0);
+    }
+
+    for (; i < dstW; i++) {
+        int val = add;
+        for (j = 0; j < filterSize; j++)
+            val += src[j][i] * (unsigned)filter[j];
+        output_pixel(&dest[i], val, bias, int);
+    }
+}
+
+#undef output_pixel
+
+#define yuv2NBPS(bits, BE_LE, is_be, template_size, typeX_t) \
+static void yuv2plane1_##bits##BE_LE##_e2k(const int16_t *src, \
+                             uint8_t *dest, int dstW, \
+                             const uint8_t *dither, int offset) \
+{ \
+    yuv2plane1_##template_size##_e2k((const typeX_t *) src, \
+                         (uint16_t *) dest, dstW, is_be, bits); \
+} \
+static void yuv2planeX_##bits##BE_LE##_e2k(const int16_t *filter, int filterSize, \
+                              const int16_t **src, uint8_t *dest, int dstW, \
+                              const uint8_t *dither, int offset)\
+{ \
+    yuv2planeX_##template_size##_e2k(filter, \
+                         filterSize, (const typeX_t **) src, \
+                         (uint16_t *) dest, dstW, is_be, bits); \
+}
+
+yuv2NBPS( 9, BE, 1, 10, int16_t)
+yuv2NBPS( 9, LE, 0, 10, int16_t)
+yuv2NBPS(10, BE, 1, 10, int16_t)
+yuv2NBPS(10, LE, 0, 10, int16_t)
+yuv2NBPS(12, BE, 1, 10, int16_t)
+yuv2NBPS(12, LE, 0, 10, int16_t)
+yuv2NBPS(14, BE, 1, 10, int16_t)
+yuv2NBPS(14, LE, 0, 10, int16_t)
+yuv2NBPS(16, BE, 1, 16, int32_t)
+yuv2NBPS(16, LE, 0, 16, int32_t)
+
+#define INIT_RGB(R, B) \
+    __m64 rgb_index0 = _mm_setr_pi8(0, 1, 3, 4, 5, 7, 8, 9); \
+    __m64 rgb_index1 = _mm_setr_pi8(3, 4, 5, 7, 8, 9, 11, 12); \
+    __m64 rgb_index2 = _mm_setr_pi8(5, 7, 8, 9, 11, 12, 13, 15);
+
+#define INIT_RGBX(R, B) \
+    __m128i A_h = _mm_set1_epi16(-256);
+
+#define INIT_XRGB(R, B) \
+    __m128i A_l = _mm_set1_epi16(255);
+
+#define WRITE_RGB(R, B) \
+    v0 = _mm_srai_epi32(R##_l, 22); \
+    v1 = _mm_srai_epi32(R##_h, 22); \
+    v4 = _mm_srai_epi32(G_l, 22 - 16); \
+    v5 = _mm_srai_epi32(G_h, 22 - 16); \
+    v2 = _mm_srai_epi32(B##_l, 22 - 8); \
+    v3 = _mm_srai_epi32(B##_h, 22 - 8); \
+    v0 = _mm_blend_epi16(v0, v4, 0xaa); \
+    v1 = _mm_blend_epi16(v1, v5, 0xaa); \
+    v2 = _mm_packus_epi32(v2, v3); \
+    v0 = _mm_packus_epi16(v0, v1); \
+    v1 = _mm_unpacklo_epi16(v0, v2); \
+    v2 = _mm_unpackhi_epi16(v0, v2); \
+    { \
+        union { __m128i v; __m64 d[2]; } a = { v1 }, b = { v2 }; \
+        __m64 *p = (__m64*)dest; \
+        p[0] = _mm_shuffle2_pi8(a.d[0], a.d[1], rgb_index0); \
+        p[1] = _mm_shuffle2_pi8(a.d[1], b.d[0], rgb_index1); \
+        p[2] = _mm_shuffle2_pi8(b.d[0], b.d[1], rgb_index2); \
+        dest += 24; \
+    }
+
+#define WRITE_RGBX(R, B) \
+    v0 = _mm_srai_epi32(R##_l, 22); \
+    v1 = _mm_srai_epi32(R##_h, 22); \
+    v4 = _mm_srai_epi32(G_l, 22 - 16); \
+    v5 = _mm_srai_epi32(G_h, 22 - 16); \
+    v2 = _mm_srai_epi32(B##_l, 22); \
+    v3 = _mm_srai_epi32(B##_h, 22); \
+    v0 = _mm_blend_epi16(v0, v4, 0xaa); \
+    v1 = _mm_blend_epi16(v1, v5, 0xaa); \
+    v0 = _mm_packus_epi16(v0, v1); \
+    v2 = _mm_packus_epi16(v2, v3); \
+    v2 = _mm_or_si128(v2, A_h); \
+    v1 = _mm_unpacklo_epi16(v0, v2); \
+    v3 = _mm_unpackhi_epi16(v0, v2); \
+    VEC_ST(dest, v1); \
+    VEC_ST(dest + 16, v3); \
+    dest += 32;
+
+#define WRITE_XRGB(R, B) \
+    v0 = _mm_srai_epi32(R##_l, 22 - 16); \
+    v1 = _mm_srai_epi32(R##_h, 22 - 16); \
+    v4 = _mm_srai_epi32(G_l, 22); \
+    v5 = _mm_srai_epi32(G_h, 22); \
+    v2 = _mm_srai_epi32(B##_l, 22 - 16); \
+    v3 = _mm_srai_epi32(B##_h, 22 - 16); \
+    v2 = _mm_blend_epi16(v4, v2, 0xaa); \
+    v3 = _mm_blend_epi16(v5, v3, 0xaa); \
+    v0 = _mm_packus_epi16(v0, v1); \
+    v2 = _mm_packus_epi16(v2, v3); \
+    v0 = _mm_or_si128(v0, A_l); \
+    v1 = _mm_unpacklo_epi16(v0, v2); \
+    v3 = _mm_unpackhi_epi16(v0, v2); \
+    VEC_ST(dest, v1); \
+    VEC_ST(dest + 16, v3); \
+    dest += 32;
+
+#define CALC_RGB \
+    vy_l = _mm_add_epi32(_mm_mullo_epi32(vy_l, y_coeff), y_add); \
+    vy_h = _mm_add_epi32(_mm_mullo_epi32(vy_h, y_coeff), y_add); \
+    \
+    v0 = _mm_mullo_epi32(vv_l, v2g_coeff); \
+    v1 = _mm_mullo_epi32(vu_l, u2g_coeff); \
+    v2 = _mm_mullo_epi32(vv_h, v2g_coeff); \
+    v3 = _mm_mullo_epi32(vu_h, u2g_coeff); \
+    G_l = _mm_add_epi32(_mm_add_epi32(v0, vy_l), v1); \
+    G_h = _mm_add_epi32(_mm_add_epi32(v2, vy_h), v3); \
+    \
+    R_l = _mm_add_epi32(vy_l, _mm_mullo_epi32(vv_l, v2r_coeff)); \
+    R_h = _mm_add_epi32(vy_h, _mm_mullo_epi32(vv_h, v2r_coeff)); \
+    B_l = _mm_add_epi32(vy_l, _mm_mullo_epi32(vu_l, u2b_coeff)); \
+    B_h = _mm_add_epi32(vy_h, _mm_mullo_epi32(vu_h, u2b_coeff));
+
+#define WITH_ALPHA(...) __VA_ARGS__
+#define NO_ALPHA(...)
+
+#define YUV2RGBWRAPPERXF(ext, fmt, R, B, hasAlpha) \
+static void yuv2##ext##_X_e2k(SwsContext *c, const int16_t *lumFilter, \
+                              const int16_t **lumSrc, int lumFilterSize, \
+                              const int16_t *chrFilter, const int16_t **chrUSrc, \
+                              const int16_t **chrVSrc, int chrFilterSize, \
+                              const int16_t **alpSrc, uint8_t *dest, int dstW, \
+                              int y) \
+{ \
+    vec_s32 vy_l, vy_h, vu_l, vu_h, vv_l, vv_h; \
+    vec_s32 hasAlpha(A_l, A_h,) R_l, R_h, G_l, G_h, B_l, B_h; \
+    vec_s32 ystart = _mm_set1_epi32(1 << 9); \
+    vec_s32 uvstart = _mm_set1_epi32((1 << 9) - (128 << 19)); \
+    vec_s32 y_coeff = _mm_set1_epi32(c->yuv2rgb_y_coeff); \
+    vec_s32 y_add = _mm_set1_epi32((1 << 21) - c->yuv2rgb_y_offset * c->yuv2rgb_y_coeff); \
+    vec_s32 v2r_coeff = _mm_set1_epi32(c->yuv2rgb_v2r_coeff); \
+    vec_s32 v2g_coeff = _mm_set1_epi32(c->yuv2rgb_v2g_coeff); \
+    vec_s32 u2g_coeff = _mm_set1_epi32(c->yuv2rgb_u2g_coeff); \
+    vec_s32 u2b_coeff = _mm_set1_epi32(c->yuv2rgb_u2b_coeff); \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    int i, j; \
+    INIT_##fmt(R, B) \
+    \
+    for (i = 0; i < dstW; i += 8) { \
+        vy_l = vy_h = ystart; \
+        for (j = 0; j < lumFilterSize; j++) { \
+            v0 = VEC_LD(lumSrc[j] + i); \
+            v1 = _mm_set1_epi16(lumFilter[j]); \
+            v2 = _mm_mullo_epi16(v0, v1); \
+            v3 = _mm_mulhi_epi16(v0, v1); \
+            v0 = _mm_unpacklo_epi16(v2, v3); \
+            v1 = _mm_unpackhi_epi16(v2, v3); \
+            vy_l = _mm_add_epi32(vy_l, v0); \
+            vy_h = _mm_add_epi32(vy_h, v1); \
+        } \
+        vy_l = _mm_srai_epi32(vy_l, 10); \
+        vy_h = _mm_srai_epi32(vy_h, 10); \
+        \
+        vu_l = vu_h = vv_l = vv_h = uvstart; \
+        for (j = 0; j < chrFilterSize; j++) { \
+            v0 = VEC_LD(chrUSrc[j] + i); \
+            v1 = VEC_LD(chrVSrc[j] + i); \
+            v5 = _mm_set1_epi16(chrFilter[j]); \
+            v2 = _mm_mullo_epi16(v0, v5); \
+            v3 = _mm_mulhi_epi16(v0, v5); \
+            v4 = _mm_mullo_epi16(v1, v5); \
+            v5 = _mm_mulhi_epi16(v1, v5); \
+            v0 = _mm_unpacklo_epi16(v2, v3); \
+            v1 = _mm_unpackhi_epi16(v2, v3); \
+            v2 = _mm_unpacklo_epi16(v4, v5); \
+            v3 = _mm_unpackhi_epi16(v4, v5); \
+            vu_l = _mm_add_epi32(vu_l, v0); \
+            vu_h = _mm_add_epi32(vu_h, v1); \
+            vv_l = _mm_add_epi32(vv_l, v2); \
+            vv_h = _mm_add_epi32(vv_h, v3); \
+        } \
+        vu_l = _mm_srai_epi32(vu_l, 10); \
+        vu_h = _mm_srai_epi32(vu_h, 10); \
+        vv_l = _mm_srai_epi32(vv_l, 10); \
+        vv_h = _mm_srai_epi32(vv_h, 10); \
+        \
+        CALC_RGB \
+        WRITE_##fmt(R, B) \
+    } \
+}
+
+#define SETUP(buf, i, alpha, r0, r1) { \
+    v0 = VEC_LD(buf##0 + i); \
+    v1 = VEC_LD(buf##1 + i); \
+    v2 = _mm_unpacklo_epi16(v0, v1); \
+    v3 = _mm_unpackhi_epi16(v0, v1); \
+    r0 = _mm_madd_epi16(v2, alpha); \
+    r1 = _mm_madd_epi16(v3, alpha); \
+}
+
+#define YUV2RGBWRAPPER2F(ext, fmt, R, B, hasAlpha) \
+static void yuv2##ext##_2_e2k(SwsContext *c, const int16_t *buf[2], \
+                              const int16_t *ubuf[2], const int16_t *vbuf[2], \
+                              const int16_t *abuf[2], uint8_t *dest, int dstW, \
+                              int yalpha, int uvalpha, int y) \
+{ \
+    const int16_t hasAlpha(*abuf0 = abuf[0], *abuf1 = abuf[1],) \
+                  *buf0 = buf[0], *buf1 = buf[1], \
+                  *ubuf0 = ubuf[0], *ubuf1 = ubuf[1], \
+                  *vbuf0 = vbuf[0], *vbuf1 = vbuf[1]; \
+    vec_s32 vy_l, vy_h, vu_l, vu_h, vv_l, vv_h; \
+    vec_s32 hasAlpha(A_l, A_h,) R_l, R_h, G_l, G_h, B_l, B_h; \
+    vec_s16 vyalpha = _mm_set1_epi32(4096 + yalpha * 0xffff); \
+    vec_s16 vuvalpha = _mm_set1_epi32(4096 + uvalpha * 0xffff); \
+    vec_s32 y_coeff = _mm_set1_epi32(c->yuv2rgb_y_coeff); \
+    vec_s32 y_add = _mm_set1_epi32((1 << 21) - c->yuv2rgb_y_offset * c->yuv2rgb_y_coeff); \
+    vec_s32 v2r_coeff = _mm_set1_epi32(c->yuv2rgb_v2r_coeff); \
+    vec_s32 v2g_coeff = _mm_set1_epi32(c->yuv2rgb_v2g_coeff); \
+    vec_s32 u2g_coeff = _mm_set1_epi32(c->yuv2rgb_u2g_coeff); \
+    vec_s32 u2b_coeff = _mm_set1_epi32(c->yuv2rgb_u2b_coeff); \
+    vec_s32 dec128 = _mm_set1_epi32(128 << 19); \
+    hasAlpha(vec_s32 add18 = _mm_set1_epi32(1 << 18);) \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    int i; \
+    INIT_##fmt(R, B) \
+    \
+    av_assert2(yalpha <= 4096U); \
+    av_assert2(uvalpha <= 4096U); \
+    \
+    for (i = 0; i < dstW; i += 8) { \
+        SETUP(buf, i, vyalpha, v0, v1); \
+        vy_l = _mm_srai_epi32(v0, 10); \
+        vy_h = _mm_srai_epi32(v1, 10); \
+        \
+        SETUP(ubuf, i, vuvalpha, v0, v1); \
+        vu_l = _mm_srai_epi32(_mm_sub_epi32(v0, dec128), 10); \
+        vu_h = _mm_srai_epi32(_mm_sub_epi32(v1, dec128), 10); \
+        \
+        SETUP(vbuf, i, vuvalpha, v0, v1); \
+        vv_l = _mm_srai_epi32(_mm_sub_epi32(v0, dec128), 10); \
+        vv_h = _mm_srai_epi32(_mm_sub_epi32(v1, dec128), 10); \
+        \
+        hasAlpha( \
+            SETUP(abuf, i, vyalpha, v0, v1); \
+            A_l = _mm_add_epi32(v0, add18); \
+            A_h = _mm_add_epi32(v1, add18); \
+        ) \
+        \
+        CALC_RGB \
+        WRITE_##fmt(R, B) \
+    } \
+}
+
+#define YUV2RGBWRAPPER1F(ext, fmt, R, B, hasAlpha) \
+static void yuv2##ext##_1_e2k(SwsContext *c, const int16_t *buf0, \
+                              const int16_t *ubuf[2], const int16_t *vbuf[2], \
+                              const int16_t *abuf0, uint8_t *dest, int dstW, \
+                              int uvalpha, int y) \
+{ \
+    const int16_t *ubuf0 = ubuf[0], *vbuf0 = vbuf[0]; \
+    const int16_t *ubuf1 = ubuf[1], *vbuf1 = vbuf[1]; \
+    int uvshl = uvalpha < 2048 ? 2 : 1; \
+    vec_s32 vy_l, vy_h, vu_l, vu_h, vv_l, vv_h; \
+    vec_s32 R_l, R_h, G_l, G_h, B_l, B_h; \
+    vec_s32 y_coeff = _mm_set1_epi32(c->yuv2rgb_y_coeff * 4); \
+    vec_s32 y_add = _mm_set1_epi32((1 << 21) - c->yuv2rgb_y_offset * c->yuv2rgb_y_coeff); \
+    vec_s32 v2r_coeff = _mm_set1_epi32(c->yuv2rgb_v2r_coeff << uvshl); \
+    vec_s32 v2g_coeff = _mm_set1_epi32(c->yuv2rgb_v2g_coeff << uvshl); \
+    vec_s32 u2g_coeff = _mm_set1_epi32(c->yuv2rgb_u2g_coeff << uvshl); \
+    vec_s32 u2b_coeff = _mm_set1_epi32(c->yuv2rgb_u2b_coeff << uvshl); \
+    vec_u16 uvsub = _mm_set1_epi16(uvalpha < 2048 ? 128 << 7 : 128 << 8); \
+    hasAlpha(vec_s16 A, add64 = _mm_set1_epi16(64);) \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    int i; \
+    INIT_##fmt(R, B) \
+    \
+    for (i = 0; i < dstW; i += 8) { \
+        v0 = VEC_LD(buf0 + i); \
+        v2 = _mm_unpacklo_epi16(v0, v0); \
+        v3 = _mm_unpackhi_epi16(v0, v0); \
+        vy_l = _mm_srai_epi32(v2, 16); \
+        vy_h = _mm_srai_epi32(v3, 16); \
+        \
+        v0 = VEC_LD(ubuf0 + i); \
+        v1 = VEC_LD(vbuf0 + i); \
+        if (uvalpha >= 2048) { \
+            v2 = VEC_LD(ubuf1 + i); \
+            v3 = VEC_LD(vbuf1 + i); \
+            v0 = _mm_add_epi16(v0, v2); \
+            v1 = _mm_add_epi16(v1, v3); \
+        } \
+        v0 = _mm_sub_epi16(v0, uvsub); \
+        v1 = _mm_sub_epi16(v1, uvsub); \
+        v2 = _mm_unpacklo_epi16(v0, v0); \
+        v3 = _mm_unpackhi_epi16(v0, v0); \
+        vu_l = _mm_srai_epi32(v2, 16); \
+        vu_h = _mm_srai_epi32(v3, 16); \
+        v2 = _mm_unpacklo_epi16(v1, v1); \
+        v3 = _mm_unpackhi_epi16(v1, v1); \
+        vv_l = _mm_srai_epi32(v2, 16); \
+        vv_h = _mm_srai_epi32(v3, 16); \
+        \
+        hasAlpha( \
+            A = VEC_LD(abuf0 + i); \
+            A = _mm_add_epi16(A, add64); \
+            A = _mm_srai_epi16(A, 7); \
+        ) \
+        \
+        CALC_RGB \
+        WRITE_##fmt(R, B) \
+    } \
+}
+
+YUV2RGBWRAPPERXF(rgbx32_full, RGBX, R, B, NO_ALPHA)
+YUV2RGBWRAPPERXF(bgrx32_full, RGBX, B, R, NO_ALPHA)
+YUV2RGBWRAPPERXF(xrgb32_full, XRGB, R, B, NO_ALPHA)
+YUV2RGBWRAPPERXF(xbgr32_full, XRGB, B, R, NO_ALPHA)
+YUV2RGBWRAPPERXF(rgb24_full, RGB, R, B, NO_ALPHA)
+YUV2RGBWRAPPERXF(bgr24_full, RGB, B, R, NO_ALPHA)
+
+YUV2RGBWRAPPER2F(rgbx32_full, RGBX, R, B, NO_ALPHA)
+YUV2RGBWRAPPER2F(bgrx32_full, RGBX, B, R, NO_ALPHA)
+YUV2RGBWRAPPER2F(xrgb32_full, XRGB, R, B, NO_ALPHA)
+YUV2RGBWRAPPER2F(xbgr32_full, XRGB, B, R, NO_ALPHA)
+YUV2RGBWRAPPER2F(rgb24_full, RGB, R, B, NO_ALPHA)
+YUV2RGBWRAPPER2F(bgr24_full, RGB, B, R, NO_ALPHA)
+
+YUV2RGBWRAPPER1F(rgbx32_full, RGBX, R, B, NO_ALPHA)
+YUV2RGBWRAPPER1F(bgrx32_full, RGBX, B, R, NO_ALPHA)
+YUV2RGBWRAPPER1F(xrgb32_full, XRGB, R, B, NO_ALPHA)
+YUV2RGBWRAPPER1F(xbgr32_full, XRGB, B, R, NO_ALPHA)
+YUV2RGBWRAPPER1F(rgb24_full, RGB, R, B, NO_ALPHA)
+YUV2RGBWRAPPER1F(bgr24_full, RGB, B, R, NO_ALPHA)
+
+#if 1 // performance
+
+#define INIT2_RGB(R, B) \
+    __m128i perm_unp8 = _mm_setr_epi8( \
+        0, 8, 1, 9, 2, 10, 3, 11, 4, 12, 5, 13, 6, 14, 7, 15); \
+    __m64 rgb_index0 = _mm_setr_pi8(0, 1, 3, 4, 5, 7, 8, 9); \
+    __m64 rgb_index1 = _mm_setr_pi8(3, 4, 5, 7, 8, 9, 11, 12); \
+    __m64 rgb_index2 = _mm_setr_pi8(5, 7, 8, 9, 11, 12, 13, 15);
+
+#define INIT2_RGBX(R, B) INIT2_XRGB(R, B)
+#define INIT2_XRGB(R, B) \
+    __m128i A_l = _mm_set1_epi16(255); \
+    __m128i perm_unp8 = _mm_setr_epi8( \
+        0, 8, 1, 9, 2, 10, 3, 11, 4, 12, 5, 13, 6, 14, 7, 15);
+
+#define WRITE2_RGB(R, B) \
+    v4 = _mm_packus_epi16(R##_l, G_l); \
+    v5 = _mm_packus_epi16(B##_l, B##_l); \
+    v0 = _mm_shuffle_epi8(v4, perm_unp8); \
+    v1 = _mm_unpacklo_epi8(v5, v5); \
+    v2 = _mm_unpacklo_epi16(v0, v1); \
+    v3 = _mm_unpackhi_epi16(v0, v1); \
+    { \
+        union { __m128i v; __m64 d[2]; } a = { v2 }, b = { v3 }; \
+        __m64 *p = (__m64*)dest; \
+        p[0] = _mm_shuffle2_pi8(a.d[0], a.d[1], rgb_index0); \
+        p[1] = _mm_shuffle2_pi8(a.d[1], b.d[0], rgb_index1); \
+        p[2] = _mm_shuffle2_pi8(b.d[0], b.d[1], rgb_index2); \
+        dest += 24; \
+    }
+
+#define WRITE2_RGBX(R, B) \
+    v4 = _mm_packus_epi16(R##_l, G_l); \
+    v5 = _mm_packus_epi16(B##_l, A_l); \
+    v0 = _mm_shuffle_epi8(v4, perm_unp8); \
+    v1 = _mm_shuffle_epi8(v5, perm_unp8); \
+    v2 = _mm_unpacklo_epi16(v0, v1); \
+    v3 = _mm_unpackhi_epi16(v0, v1); \
+    VEC_ST(dest, v2); \
+    VEC_ST(dest + 16, v3); \
+    dest += 32;
+
+#define WRITE2_XRGB(R, B) \
+    v4 = _mm_packus_epi16(A_l, R##_l); \
+    v5 = _mm_packus_epi16(G_l, B##_l); \
+    v0 = _mm_shuffle_epi8(v4, perm_unp8); \
+    v1 = _mm_shuffle_epi8(v5, perm_unp8); \
+    v2 = _mm_unpacklo_epi16(v0, v1); \
+    v3 = _mm_unpackhi_epi16(v0, v1); \
+    VEC_ST(dest, v2); \
+    VEC_ST(dest + 16, v3); \
+    dest += 32;
+
+#define CALC2_RGB \
+    vy_l = _mm_mulhrs_epi16(_mm_sub_epi16(vy_l, y_sub), y_coeff); \
+    \
+    v0 = _mm_mulhrs_epi16(vv_l, v2g_coeff); \
+    v1 = _mm_mulhrs_epi16(vu_l, u2g_coeff); \
+    G_l = _mm_add_epi16(_mm_add_epi16(v0, vy_l), v1); \
+    \
+    R_l = _mm_add_epi16(vy_l, _mm_mulhrs_epi16(vv_l, v2r_coeff)); \
+    B_l = _mm_add_epi16(vy_l, _mm_mulhrs_epi16(vu_l, u2b_coeff));
+
+#define YUV2RGBWRAPPERX(ext, fmt, R, B, hasAlpha) \
+static void yuv2##ext##_X_e2k(SwsContext *c, const int16_t *lumFilter, \
+                              const int16_t **lumSrc, int lumFilterSize, \
+                              const int16_t *chrFilter, const int16_t **chrUSrc, \
+                              const int16_t **chrVSrc, int chrFilterSize, \
+                              const int16_t **alpSrc, uint8_t *dest, int dstW, \
+                              int y) \
+{ \
+    vec_s32 vy_l, vy_h, vu_l, vv_l, vu2_l, vu2_h, vv2_l, vv2_h; \
+    vec_s32 hasAlpha(A_l,) R_l, G_l, B_l; \
+    vec_s32 ystart = _mm_set1_epi32(0); \
+    vec_s32 uvstart = _mm_set1_epi32(-(128 << 19)); \
+    vec_s32 y_coeff = _mm_set1_epi16(c->yuv2rgb_y_coeff); \
+    vec_s32 y_sub = _mm_set1_epi16((c->yuv2rgb_y_offset + 64) >> 7); \
+    vec_s32 v2r_coeff = _mm_set1_epi16(c->yuv2rgb_v2r_coeff); \
+    vec_s32 v2g_coeff = _mm_set1_epi16(c->yuv2rgb_v2g_coeff); \
+    vec_s32 u2g_coeff = _mm_set1_epi16(c->yuv2rgb_u2g_coeff); \
+    vec_s32 u2b_coeff = _mm_set1_epi16(c->yuv2rgb_u2b_coeff); \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    int i, j; \
+    INIT2_##fmt(R, B) \
+    \
+    for (i = 0; i < (dstW + 1) >> 1; i += 8) { \
+        vy_l = vy_h = ystart; \
+        for (j = 0; j < lumFilterSize; j++) { \
+            v0 = VEC_LD(lumSrc[j] + i * 2); \
+            v1 = _mm_set1_epi16(lumFilter[j]); \
+            v2 = _mm_mullo_epi16(v0, v1); \
+            v3 = _mm_mulhi_epi16(v0, v1); \
+            v0 = _mm_unpacklo_epi16(v2, v3); \
+            v1 = _mm_unpackhi_epi16(v2, v3); \
+            vy_l = _mm_add_epi32(vy_l, v0); \
+            vy_h = _mm_add_epi32(vy_h, v1); \
+        } \
+        vy_l = _mm_srai_epi32(vy_l, 17); \
+        vy_h = _mm_srai_epi32(vy_h, 17); \
+        \
+        vu2_l = vu2_h = vv2_l = vv2_h = uvstart; \
+        for (j = 0; j < chrFilterSize; j++) { \
+            v0 = VEC_LD(chrUSrc[j] + i); \
+            v1 = VEC_LD(chrVSrc[j] + i); \
+            v5 = _mm_set1_epi16(chrFilter[j]); \
+            v2 = _mm_mullo_epi16(v0, v5); \
+            v3 = _mm_mulhi_epi16(v0, v5); \
+            v4 = _mm_mullo_epi16(v1, v5); \
+            v5 = _mm_mulhi_epi16(v1, v5); \
+            v0 = _mm_unpacklo_epi16(v2, v3); \
+            v1 = _mm_unpackhi_epi16(v2, v3); \
+            v2 = _mm_unpacklo_epi16(v4, v5); \
+            v3 = _mm_unpackhi_epi16(v4, v5); \
+            vu2_l = _mm_add_epi32(vu2_l, v0); \
+            vu2_h = _mm_add_epi32(vu2_h, v1); \
+            vv2_l = _mm_add_epi32(vv2_l, v2); \
+            vv2_h = _mm_add_epi32(vv2_h, v3); \
+        } \
+        vu2_l = _mm_srai_epi32(vu2_l, 17); \
+        vu2_h = _mm_srai_epi32(vu2_h, 17); \
+        vv2_l = _mm_srai_epi32(vv2_l, 17); \
+        vv2_h = _mm_srai_epi32(vv2_h, 17); \
+        vu2_l = _mm_packs_epi32(vu2_l, vu2_h); \
+        vv2_l = _mm_packs_epi32(vv2_l, vv2_h); \
+        \
+        vu_l = _mm_unpacklo_epi16(vu2_l, vu2_l); \
+        vv_l = _mm_unpacklo_epi16(vv2_l, vv2_l); \
+        vy_l = _mm_packs_epi32(vy_l, vy_h); \
+        \
+        CALC2_RGB \
+        WRITE2_##fmt(R, B) \
+        \
+        vy_l = vy_h = ystart; \
+        for (j = 0; j < lumFilterSize; j++) { \
+            v0 = VEC_LD(lumSrc[j] + i * 2 + 8); \
+            v1 = _mm_set1_epi16(lumFilter[j]); \
+            v2 = _mm_mullo_epi16(v0, v1); \
+            v3 = _mm_mulhi_epi16(v0, v1); \
+            v0 = _mm_unpacklo_epi16(v2, v3); \
+            v1 = _mm_unpackhi_epi16(v2, v3); \
+            vy_l = _mm_add_epi32(vy_l, v0); \
+            vy_h = _mm_add_epi32(vy_h, v1); \
+        } \
+        vy_l = _mm_srai_epi32(vy_l, 17); \
+        vy_h = _mm_srai_epi32(vy_h, 17); \
+        \
+        vu_l = _mm_unpackhi_epi16(vu2_l, vu2_l); \
+        vv_l = _mm_unpackhi_epi16(vv2_l, vv2_l); \
+        vy_l = _mm_packs_epi32(vy_l, vy_h); \
+        \
+        CALC2_RGB \
+        WRITE2_##fmt(R, B) \
+    } \
+}
+
+#define SETUP2(buf, i, alpha, r0) { \
+    v0 = VEC_LD(buf##0 + i); \
+    v1 = VEC_LD(buf##1 + i); \
+    v1 = _mm_subs_epi16(v0, v1); \
+    v1 = _mm_mulhrs_epi16(v1, alpha); \
+    r0 = _mm_add_epi16(v0, v1); \
+}
+
+#define YUV2RGBWRAPPER2(ext, fmt, R, B, hasAlpha) \
+static void yuv2##ext##_2_e2k(SwsContext *c, const int16_t *buf[2], \
+                              const int16_t *ubuf[2], const int16_t *vbuf[2], \
+                              const int16_t *abuf[2], uint8_t *dest, int dstW, \
+                              int yalpha, int uvalpha, int y) \
+{ \
+    const int16_t hasAlpha(*abuf0 = abuf[0], *abuf1 = abuf[1],) \
+                  *buf0 = buf[0], *buf1 = buf[1], \
+                  *ubuf0 = ubuf[0], *ubuf1 = ubuf[1], \
+                  *vbuf0 = vbuf[0], *vbuf1 = vbuf[1]; \
+    vec_s32 vy_l, vu_l, vv_l, vu2_l, vv2_l; \
+    vec_s32 hasAlpha(A_l,) R_l, G_l, B_l; \
+    vec_s16 vyalpha = _mm_set1_epi16(-yalpha << 3); \
+    vec_s16 vuvalpha = _mm_set1_epi16(-uvalpha << 3); \
+    vec_s32 y_coeff = _mm_set1_epi16(c->yuv2rgb_y_coeff); \
+    vec_s32 y_sub = _mm_set1_epi16((c->yuv2rgb_y_offset + 64) >> 7); \
+    vec_s32 v2r_coeff = _mm_set1_epi16(c->yuv2rgb_v2r_coeff); \
+    vec_s32 v2g_coeff = _mm_set1_epi16(c->yuv2rgb_v2g_coeff); \
+    vec_s32 u2g_coeff = _mm_set1_epi16(c->yuv2rgb_u2g_coeff); \
+    vec_s32 u2b_coeff = _mm_set1_epi16(c->yuv2rgb_u2b_coeff); \
+    vec_s32 dec128 = _mm_set1_epi16(128 << 2); \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    int i; \
+    INIT2_##fmt(R, B) \
+    \
+    av_assert2(yalpha <= 4096U); \
+    av_assert2(uvalpha <= 4096U); \
+    \
+    for (i = 0; i < (dstW + 1) >> 1; i += 8) { \
+        SETUP2(buf, i * 2, vyalpha, vy_l); \
+        vy_l = _mm_srai_epi16(vy_l, 5); \
+        \
+        SETUP2(ubuf, i, vuvalpha, vu2_l); \
+        vu2_l = _mm_srai_epi16(vu2_l, 5); \
+        \
+        SETUP2(vbuf, i, vuvalpha, vv2_l); \
+        vv2_l = _mm_srai_epi16(vv2_l, 5); \
+        \
+        vu2_l = _mm_sub_epi16(vu2_l, dec128); \
+        vv2_l = _mm_sub_epi16(vv2_l, dec128); \
+        \
+        hasAlpha( \
+            SETUP2(abuf, i * 2, vyalpha, A_l); \
+        ) \
+        \
+        vu_l = _mm_unpacklo_epi16(vu2_l, vu2_l); \
+        vv_l = _mm_unpacklo_epi16(vv2_l, vv2_l); \
+        \
+        CALC2_RGB \
+        WRITE2_##fmt(R, B) \
+        \
+        SETUP2(buf, i * 2 + 8, vyalpha, vy_l); \
+        vy_l = _mm_srai_epi16(vy_l, 5); \
+        \
+        hasAlpha( \
+            SETUP2(abuf, i * 2 + 8, vyalpha, A_l); \
+        ) \
+        \
+        vu_l = _mm_unpackhi_epi16(vu2_l, vu2_l); \
+        vv_l = _mm_unpackhi_epi16(vv2_l, vv2_l); \
+        \
+        CALC2_RGB \
+        WRITE2_##fmt(R, B) \
+    } \
+}
+
+#define YUV2RGBWRAPPER1(ext, fmt, R, B, hasAlpha) \
+static void yuv2##ext##_1_e2k(SwsContext *c, const int16_t *buf0, \
+                              const int16_t *ubuf[2], const int16_t *vbuf[2], \
+                              const int16_t *abuf0, uint8_t *dest, int dstW, \
+                              int uvalpha, int y) \
+{ \
+    const int16_t *ubuf0 = ubuf[0], *vbuf0 = vbuf[0]; \
+    const int16_t *ubuf1 = ubuf[1], *vbuf1 = vbuf[1]; \
+    int uvshr = uvalpha < 2048 ? 5 : 6; \
+    vec_s32 vy_l, vu_l, vv_l, vu2_l, vv2_l; \
+    vec_s32 hasAlpha(A_l,) R_l, G_l, B_l; \
+    vec_s32 y_coeff = _mm_set1_epi16(c->yuv2rgb_y_coeff); \
+    vec_s32 y_sub = _mm_set1_epi16((c->yuv2rgb_y_offset + 64) >> 7); \
+    vec_s32 v2r_coeff = _mm_set1_epi16(c->yuv2rgb_v2r_coeff); \
+    vec_s32 v2g_coeff = _mm_set1_epi16(c->yuv2rgb_v2g_coeff); \
+    vec_s32 u2g_coeff = _mm_set1_epi16(c->yuv2rgb_u2g_coeff); \
+    vec_s32 u2b_coeff = _mm_set1_epi16(c->yuv2rgb_u2b_coeff); \
+    vec_u16 uvsub = _mm_set1_epi16(uvalpha < 2048 ? 128 << 7 : 128 << 8); \
+    hasAlpha(vec_s16 A, add64 = _mm_set1_epi16(64);) \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    int i; \
+    INIT2_##fmt(R, B) \
+    \
+    for (i = 0; i < (dstW + 1) >> 1; i += 8) { \
+        v0 = VEC_LD(buf0 + i * 2); \
+        vy_l = _mm_srai_epi16(v0, 5); \
+        \
+        v0 = VEC_LD(ubuf0 + i); \
+        v1 = VEC_LD(vbuf0 + i); \
+        if (uvalpha >= 2048) { \
+            v2 = VEC_LD(ubuf1 + i); \
+            v3 = VEC_LD(vbuf1 + i); \
+            v0 = _mm_add_epi16(v0, v2); \
+            v1 = _mm_add_epi16(v1, v3); \
+        } \
+        v0 = _mm_sub_epi16(v0, uvsub); \
+        v1 = _mm_sub_epi16(v1, uvsub); \
+        vu2_l = _mm_srai_epi16(v0, uvshr); \
+        vv2_l = _mm_srai_epi16(v1, uvshr); \
+        \
+        hasAlpha( \
+            A_l = VEC_LD(abuf0 + i * 2); \
+            A_l = _mm_add_epi16(A_l, add64); \
+            A_l = _mm_srai_epi16(A_l, 7); \
+        ) \
+        \
+        vu_l = _mm_unpacklo_epi16(vu2_l, vu2_l); \
+        vv_l = _mm_unpacklo_epi16(vv2_l, vv2_l); \
+        \
+        CALC2_RGB \
+        WRITE2_##fmt(R, B) \
+        \
+        v0 = VEC_LD(buf0 + i * 2 + 8); \
+        vy_l = _mm_srai_epi16(v0, 5); \
+        \
+        hasAlpha( \
+            A_l = VEC_LD(abuf0 + i * 2 + 8); \
+            A_l = _mm_add_epi16(A_l, add64); \
+            A_l = _mm_srai_epi16(A_l, 7); \
+        ) \
+        \
+        vu_l = _mm_unpackhi_epi16(vu2_l, vu2_l); \
+        vv_l = _mm_unpackhi_epi16(vv2_l, vv2_l); \
+        \
+        CALC2_RGB \
+        WRITE2_##fmt(R, B) \
+    } \
+}
+
+#else // quality
+
+#define YUV2RGBWRAPPERX(ext, fmt, R, B, hasAlpha) \
+static void yuv2##ext##_X_e2k(SwsContext *c, const int16_t *lumFilter, \
+                              const int16_t **lumSrc, int lumFilterSize, \
+                              const int16_t *chrFilter, const int16_t **chrUSrc, \
+                              const int16_t **chrVSrc, int chrFilterSize, \
+                              const int16_t **alpSrc, uint8_t *dest, int dstW, \
+                              int y) \
+{ \
+    vec_s32 vy_l, vy_h, vu_l, vu_h, vv_l, vv_h, vu2_l, vu2_h, vv2_l, vv2_h; \
+    vec_s32 hasAlpha(A_l, A_h,) R_l, R_h, G_l, G_h, B_l, B_h; \
+    vec_s32 ystart = _mm_set1_epi32(1 << 9); \
+    vec_s32 uvstart = _mm_set1_epi32((1 << 9) - (128 << 19)); \
+    vec_s32 y_coeff = _mm_set1_epi32(c->yuv2rgb_y_coeff); \
+    vec_s32 y_add = _mm_set1_epi32((1 << 21) - c->yuv2rgb_y_offset * c->yuv2rgb_y_coeff); \
+    vec_s32 v2r_coeff = _mm_set1_epi32(c->yuv2rgb_v2r_coeff); \
+    vec_s32 v2g_coeff = _mm_set1_epi32(c->yuv2rgb_v2g_coeff); \
+    vec_s32 u2g_coeff = _mm_set1_epi32(c->yuv2rgb_u2g_coeff); \
+    vec_s32 u2b_coeff = _mm_set1_epi32(c->yuv2rgb_u2b_coeff); \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    int i, j; \
+    INIT_##fmt(R, B) \
+    \
+    for (i = 0; i < (dstW + 1) >> 1; i += 8) { \
+        vy_l = vy_h = ystart; \
+        for (j = 0; j < lumFilterSize; j++) { \
+            v0 = VEC_LD(lumSrc[j] + i * 2); \
+            v1 = _mm_set1_epi16(lumFilter[j]); \
+            v2 = _mm_mullo_epi16(v0, v1); \
+            v3 = _mm_mulhi_epi16(v0, v1); \
+            v0 = _mm_unpacklo_epi16(v2, v3); \
+            v1 = _mm_unpackhi_epi16(v2, v3); \
+            vy_l = _mm_add_epi32(vy_l, v0); \
+            vy_h = _mm_add_epi32(vy_h, v1); \
+        } \
+        vy_l = _mm_srai_epi32(vy_l, 10); \
+        vy_h = _mm_srai_epi32(vy_h, 10); \
+        \
+        vu2_l = vu2_h = vv2_l = vv2_h = uvstart; \
+        for (j = 0; j < chrFilterSize; j++) { \
+            v0 = VEC_LD(chrUSrc[j] + i); \
+            v1 = VEC_LD(chrVSrc[j] + i); \
+            v5 = _mm_set1_epi16(chrFilter[j]); \
+            v2 = _mm_mullo_epi16(v0, v5); \
+            v3 = _mm_mulhi_epi16(v0, v5); \
+            v4 = _mm_mullo_epi16(v1, v5); \
+            v5 = _mm_mulhi_epi16(v1, v5); \
+            v0 = _mm_unpacklo_epi16(v2, v3); \
+            v1 = _mm_unpackhi_epi16(v2, v3); \
+            v2 = _mm_unpacklo_epi16(v4, v5); \
+            v3 = _mm_unpackhi_epi16(v4, v5); \
+            vu2_l = _mm_add_epi32(vu2_l, v0); \
+            vu2_h = _mm_add_epi32(vu2_h, v1); \
+            vv2_l = _mm_add_epi32(vv2_l, v2); \
+            vv2_h = _mm_add_epi32(vv2_h, v3); \
+        } \
+        vu2_l = _mm_srai_epi32(vu2_l, 10); \
+        vu2_h = _mm_srai_epi32(vu2_h, 10); \
+        vv2_l = _mm_srai_epi32(vv2_l, 10); \
+        vv2_h = _mm_srai_epi32(vv2_h, 10); \
+        \
+        vu_l = _mm_unpacklo_epi32(vu2_l, vu2_l); \
+        vu_h = _mm_unpackhi_epi32(vu2_l, vu2_l); \
+        vv_l = _mm_unpacklo_epi32(vv2_l, vv2_l); \
+        vv_h = _mm_unpackhi_epi32(vv2_l, vv2_l); \
+        \
+        CALC_RGB \
+        WRITE_##fmt(R, B) \
+        \
+        vy_l = vy_h = ystart; \
+        for (j = 0; j < lumFilterSize; j++) { \
+            v0 = VEC_LD(lumSrc[j] + i * 2 + 8); \
+            v1 = _mm_set1_epi16(lumFilter[j]); \
+            v2 = _mm_mullo_epi16(v0, v1); \
+            v3 = _mm_mulhi_epi16(v0, v1); \
+            v0 = _mm_unpacklo_epi16(v2, v3); \
+            v1 = _mm_unpackhi_epi16(v2, v3); \
+            vy_l = _mm_add_epi32(vy_l, v0); \
+            vy_h = _mm_add_epi32(vy_h, v1); \
+        } \
+        vy_l = _mm_srai_epi32(vy_l, 10); \
+        vy_h = _mm_srai_epi32(vy_h, 10); \
+        \
+        vu_l = _mm_unpacklo_epi32(vu2_h, vu2_h); \
+        vu_h = _mm_unpackhi_epi32(vu2_h, vu2_h); \
+        vv_l = _mm_unpacklo_epi32(vv2_h, vv2_h); \
+        vv_h = _mm_unpackhi_epi32(vv2_h, vv2_h); \
+        \
+        CALC_RGB \
+        WRITE_##fmt(R, B) \
+    } \
+}
+
+#define YUV2RGBWRAPPER2(ext, fmt, R, B, hasAlpha) \
+static void yuv2##ext##_2_e2k(SwsContext *c, const int16_t *buf[2], \
+                              const int16_t *ubuf[2], const int16_t *vbuf[2], \
+                              const int16_t *abuf[2], uint8_t *dest, int dstW, \
+                              int yalpha, int uvalpha, int y) \
+{ \
+    const int16_t hasAlpha(*abuf0 = abuf[0], *abuf1 = abuf[1],) \
+                  *buf0 = buf[0], *buf1 = buf[1], \
+                  *ubuf0 = ubuf[0], *ubuf1 = ubuf[1], \
+                  *vbuf0 = vbuf[0], *vbuf1 = vbuf[1]; \
+    vec_s32 vy_l, vy_h, vu_l, vu_h, vv_l, vv_h, vu2_l, vu2_h, vv2_l, vv2_h; \
+    vec_s32 hasAlpha(A_l, A_h,) R_l, R_h, G_l, G_h, B_l, B_h; \
+    vec_s16 vyalpha = _mm_set1_epi32(4096 + yalpha * 0xffff); \
+    vec_s16 vuvalpha = _mm_set1_epi32(4096 + uvalpha * 0xffff); \
+    vec_s32 y_coeff = _mm_set1_epi32(c->yuv2rgb_y_coeff); \
+    vec_s32 y_add = _mm_set1_epi32((1 << 21) - c->yuv2rgb_y_offset * c->yuv2rgb_y_coeff); \
+    vec_s32 v2r_coeff = _mm_set1_epi32(c->yuv2rgb_v2r_coeff); \
+    vec_s32 v2g_coeff = _mm_set1_epi32(c->yuv2rgb_v2g_coeff); \
+    vec_s32 u2g_coeff = _mm_set1_epi32(c->yuv2rgb_u2g_coeff); \
+    vec_s32 u2b_coeff = _mm_set1_epi32(c->yuv2rgb_u2b_coeff); \
+    vec_s32 dec128 = _mm_set1_epi32(128 << 19); \
+    hasAlpha(vec_s32 add18 = _mm_set1_epi32(1 << 18);) \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    int i; \
+    INIT_##fmt(R, B) \
+    \
+    av_assert2(yalpha <= 4096U); \
+    av_assert2(uvalpha <= 4096U); \
+    \
+    for (i = 0; i < (dstW + 1) >> 1; i += 8) { \
+        SETUP(buf, i * 2, vyalpha, v0, v1); \
+        vy_l = _mm_srai_epi32(v0, 10); \
+        vy_h = _mm_srai_epi32(v1, 10); \
+        \
+        SETUP(ubuf, i, vuvalpha, v0, v1); \
+        vu2_l = _mm_srai_epi32(_mm_sub_epi32(v0, dec128), 10); \
+        vu2_h = _mm_srai_epi32(_mm_sub_epi32(v1, dec128), 10); \
+        \
+        SETUP(vbuf, i, vuvalpha, v0, v1); \
+        vv2_l = _mm_srai_epi32(_mm_sub_epi32(v0, dec128), 10); \
+        vv2_h = _mm_srai_epi32(_mm_sub_epi32(v1, dec128), 10); \
+        \
+        hasAlpha( \
+            SETUP(abuf, i * 2, vyalpha, v0, v1); \
+            A_l = _mm_add_epi32(v0, add18); \
+            A_h = _mm_add_epi32(v1, add18); \
+        ) \
+        \
+        vu_l = _mm_unpacklo_epi32(vu2_l, vu2_l); \
+        vu_h = _mm_unpackhi_epi32(vu2_l, vu2_l); \
+        vv_l = _mm_unpacklo_epi32(vv2_l, vv2_l); \
+        vv_h = _mm_unpackhi_epi32(vv2_l, vv2_l); \
+        \
+        CALC_RGB \
+        WRITE_##fmt(R, B) \
+        \
+        SETUP(buf, i * 2 + 8, vyalpha, v0, v1); \
+        vy_l = _mm_srai_epi32(v0, 10); \
+        vy_h = _mm_srai_epi32(v1, 10); \
+        \
+        hasAlpha( \
+            SETUP(abuf, i * 2 + 8, vyalpha, v0, v1); \
+            A_l = _mm_add_epi32(v0, add18); \
+            A_h = _mm_add_epi32(v1, add18); \
+        ) \
+        \
+        vu_l = _mm_unpacklo_epi32(vu2_h, vu2_h); \
+        vu_h = _mm_unpackhi_epi32(vu2_h, vu2_h); \
+        vv_l = _mm_unpacklo_epi32(vv2_h, vv2_h); \
+        vv_h = _mm_unpackhi_epi32(vv2_h, vv2_h); \
+        \
+        CALC_RGB \
+        WRITE_##fmt(R, B) \
+    } \
+}
+
+#define YUV2RGBWRAPPER1(ext, fmt, R, B, hasAlpha) \
+static void yuv2##ext##_1_e2k(SwsContext *c, const int16_t *buf0, \
+                              const int16_t *ubuf[2], const int16_t *vbuf[2], \
+                              const int16_t *abuf0, uint8_t *dest, int dstW, \
+                              int uvalpha, int y) \
+{ \
+    const int16_t *ubuf0 = ubuf[0], *vbuf0 = vbuf[0]; \
+    const int16_t *ubuf1 = ubuf[1], *vbuf1 = vbuf[1]; \
+    int uvshl = uvalpha < 2048 ? 2 : 1; \
+    vec_s32 vy_l, vy_h, vu_l, vu_h, vv_l, vv_h, vu2_l, vu2_h, vv2_l, vv2_h; \
+    vec_s32 R_l, R_h, G_l, G_h, B_l, B_h; \
+    vec_s32 y_coeff = _mm_set1_epi32(c->yuv2rgb_y_coeff * 4); \
+    vec_s32 y_add = _mm_set1_epi32((1 << 21) - c->yuv2rgb_y_offset * c->yuv2rgb_y_coeff); \
+    vec_s32 v2r_coeff = _mm_set1_epi32(c->yuv2rgb_v2r_coeff << uvshl); \
+    vec_s32 v2g_coeff = _mm_set1_epi32(c->yuv2rgb_v2g_coeff << uvshl); \
+    vec_s32 u2g_coeff = _mm_set1_epi32(c->yuv2rgb_u2g_coeff << uvshl); \
+    vec_s32 u2b_coeff = _mm_set1_epi32(c->yuv2rgb_u2b_coeff << uvshl); \
+    vec_u16 uvsub = _mm_set1_epi16(uvalpha < 2048 ? 128 << 7 : 128 << 8); \
+    hasAlpha(vec_s16 A, add64 = _mm_set1_epi16(64);) \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    int i; \
+    INIT_##fmt(R, B) \
+    \
+    for (i = 0; i < (dstW + 1) >> 1; i += 8) { \
+        v0 = VEC_LD(buf0 + i * 2); \
+        v2 = _mm_unpacklo_epi16(v0, v0); \
+        v3 = _mm_unpackhi_epi16(v0, v0); \
+        vy_l = _mm_srai_epi32(v2, 16); \
+        vy_h = _mm_srai_epi32(v3, 16); \
+        \
+        v0 = VEC_LD(ubuf0 + i); \
+        v1 = VEC_LD(vbuf0 + i); \
+        if (uvalpha >= 2048) { \
+            v2 = VEC_LD(ubuf1 + i); \
+            v3 = VEC_LD(vbuf1 + i); \
+            v0 = _mm_add_epi16(v0, v2); \
+            v1 = _mm_add_epi16(v1, v3); \
+        } \
+        v0 = _mm_sub_epi16(v0, uvsub); \
+        v1 = _mm_sub_epi16(v1, uvsub); \
+        v2 = _mm_unpacklo_epi16(v0, v0); \
+        v3 = _mm_unpackhi_epi16(v0, v0); \
+        vu2_l = _mm_srai_epi32(v2, 16); \
+        vu2_h = _mm_srai_epi32(v3, 16); \
+        v2 = _mm_unpacklo_epi16(v1, v1); \
+        v3 = _mm_unpackhi_epi16(v1, v1); \
+        vv2_l = _mm_srai_epi32(v2, 16); \
+        vv2_h = _mm_srai_epi32(v3, 16); \
+        \
+        hasAlpha( \
+            A_l = VEC_LD(abuf0 + i * 2); \
+            A_l = _mm_add_epi16(A_l, add64); \
+            A_l = _mm_srai_epi16(A_l, 7); \
+        ) \
+        \
+        vu_l = _mm_unpacklo_epi32(vu2_l, vu2_l); \
+        vu_h = _mm_unpackhi_epi32(vu2_l, vu2_l); \
+        vv_l = _mm_unpacklo_epi32(vv2_l, vv2_l); \
+        vv_h = _mm_unpackhi_epi32(vv2_l, vv2_l); \
+        \
+        CALC_RGB \
+        WRITE_##fmt(R, B) \
+        \
+        v0 = VEC_LD(buf0 + i * 2 + 8); \
+        v2 = _mm_unpacklo_epi16(v0, v0); \
+        v3 = _mm_unpackhi_epi16(v0, v0); \
+        vy_l = _mm_srai_epi32(v2, 16); \
+        vy_h = _mm_srai_epi32(v3, 16); \
+        \
+        hasAlpha( \
+            A_l = VEC_LD(abuf0 + i * 2 + 8); \
+            A_l = _mm_add_epi16(A_l, add64); \
+            A_l = _mm_srai_epi16(A_l, 7); \
+        ) \
+        \
+        vu_l = _mm_unpacklo_epi32(vu2_h, vu2_h); \
+        vu_h = _mm_unpackhi_epi32(vu2_h, vu2_h); \
+        vv_l = _mm_unpacklo_epi32(vv2_h, vv2_h); \
+        vv_h = _mm_unpackhi_epi32(vv2_h, vv2_h); \
+        \
+        CALC_RGB \
+        WRITE_##fmt(R, B) \
+    } \
+}
+
+#endif
+
+YUV2RGBWRAPPERX(rgbx32, RGBX, R, B, NO_ALPHA)
+YUV2RGBWRAPPERX(bgrx32, RGBX, B, R, NO_ALPHA)
+YUV2RGBWRAPPERX(xrgb32, XRGB, R, B, NO_ALPHA)
+YUV2RGBWRAPPERX(xbgr32, XRGB, B, R, NO_ALPHA)
+YUV2RGBWRAPPERX(rgb24, RGB, R, B, NO_ALPHA)
+YUV2RGBWRAPPERX(bgr24, RGB, B, R, NO_ALPHA)
+
+YUV2RGBWRAPPER2(rgbx32, RGBX, R, B, NO_ALPHA)
+YUV2RGBWRAPPER2(bgrx32, RGBX, B, R, NO_ALPHA)
+YUV2RGBWRAPPER2(xrgb32, XRGB, R, B, NO_ALPHA)
+YUV2RGBWRAPPER2(xbgr32, XRGB, B, R, NO_ALPHA)
+YUV2RGBWRAPPER2(rgb24, RGB, R, B, NO_ALPHA)
+YUV2RGBWRAPPER2(bgr24, RGB, B, R, NO_ALPHA)
+
+YUV2RGBWRAPPER1(rgbx32, RGBX, R, B, NO_ALPHA)
+YUV2RGBWRAPPER1(bgrx32, RGBX, B, R, NO_ALPHA)
+YUV2RGBWRAPPER1(xrgb32, XRGB, R, B, NO_ALPHA)
+YUV2RGBWRAPPER1(xbgr32, XRGB, B, R, NO_ALPHA)
+YUV2RGBWRAPPER1(rgb24, RGB, R, B, NO_ALPHA)
+YUV2RGBWRAPPER1(bgr24, RGB, B, R, NO_ALPHA)
+
+#define WRITE_422(vu, vv, x0, x1) \
+    vy0 = _mm_srai_epi32(vy0, 19); \
+    vy1 = _mm_srai_epi32(vy1, 19); \
+    vy2 = _mm_srai_epi32(vy2, 19); \
+    vy3 = _mm_srai_epi32(vy3, 19); \
+    vu##0 = _mm_srai_epi32(vu##0, 19); \
+    vu##1 = _mm_srai_epi32(vu##1, 19); \
+    vv##0 = _mm_srai_epi32(vv##0, 19 - 16); \
+    vv##1 = _mm_srai_epi32(vv##1, 19 - 16); \
+    v0 = _mm_packs_epi32(vy0, vy1); \
+    v1 = _mm_packs_epi32(vy2, vy3); \
+    v2 = _mm_blend_epi16(vu##0, vv##0, 0xaa); \
+    v3 = _mm_blend_epi16(vu##1, vv##1, 0xaa); \
+    v4 = _mm_packus_epi16(v0, v1); \
+    v5 = _mm_packus_epi16(v2, v3); \
+    v0 = _mm_unpacklo_epi8(x0, x1); \
+    v1 = _mm_unpackhi_epi8(x0, x1); \
+    VEC_ST(dest, v0); \
+    VEC_ST(dest + 16, v1); \
+    dest += 32;
+
+#define WRITE_YUYV422 WRITE_422(vu, vv, v4, v5)
+#define WRITE_YVYU422 WRITE_422(vv, vu, v4, v5)
+#define WRITE_UYVY422 WRITE_422(vu, vv, v5, v4)
+
+#define YUV2PACKEDWRAPPERX(ext, fmt) \
+static void yuv2##ext##_X_e2k(SwsContext *c, const int16_t *lumFilter, \
+                              const int16_t **lumSrc, int lumFilterSize, \
+                              const int16_t *chrFilter, const int16_t **chrUSrc, \
+                              const int16_t **chrVSrc, int chrFilterSize, \
+                              const int16_t **alpSrc, uint8_t *dest, int dstW, \
+                              int y) \
+{ \
+    int i, j; \
+    __m128i vy0, vy1, vy2, vy3, vu0, vu1, vv0, vv1; \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    vec_s32 start = _mm_set1_epi32(1 << 18); \
+    \
+    for (i = 0; i < (dstW + 1) >> 1; i += 8) { \
+        vy0 = vy1 = vy2 = vy3 = start; \
+        for (j = 0; j < lumFilterSize; j++) { \
+            v0 = VEC_LD(lumSrc[j] + i * 2); \
+            v1 = VEC_LD(lumSrc[j] + i * 2 + 8); \
+            v5 = _mm_set1_epi16(lumFilter[j]); \
+            v2 = _mm_mullo_epi16(v0, v5); \
+            v3 = _mm_mulhi_epi16(v0, v5); \
+            v4 = _mm_mullo_epi16(v1, v5); \
+            v5 = _mm_mulhi_epi16(v1, v5); \
+            v0 = _mm_unpacklo_epi16(v2, v3); \
+            v1 = _mm_unpackhi_epi16(v2, v3); \
+            v2 = _mm_unpacklo_epi16(v4, v5); \
+            v3 = _mm_unpackhi_epi16(v4, v5); \
+            vy0 = _mm_add_epi32(vy0, v0); \
+            vy1 = _mm_add_epi32(vy1, v1); \
+            vy2 = _mm_add_epi32(vy2, v2); \
+            vy3 = _mm_add_epi32(vy3, v3); \
+        } \
+        \
+        vu0 = vu1 = vv0 = vv1 = start; \
+        for (j = 0; j < chrFilterSize; j++) { \
+            v0 = VEC_LD(chrUSrc[j] + i); \
+            v1 = VEC_LD(chrVSrc[j] + i); \
+            v5 = _mm_set1_epi16(chrFilter[j]); \
+            v2 = _mm_mullo_epi16(v0, v5); \
+            v3 = _mm_mulhi_epi16(v0, v5); \
+            v4 = _mm_mullo_epi16(v1, v5); \
+            v5 = _mm_mulhi_epi16(v1, v5); \
+            v0 = _mm_unpacklo_epi16(v2, v3); \
+            v1 = _mm_unpackhi_epi16(v2, v3); \
+            v2 = _mm_unpacklo_epi16(v4, v5); \
+            v3 = _mm_unpackhi_epi16(v4, v5); \
+            vu0 = _mm_add_epi32(vu0, v0); \
+            vu1 = _mm_add_epi32(vu1, v1); \
+            vv0 = _mm_add_epi32(vv0, v2); \
+            vv1 = _mm_add_epi32(vv1, v3); \
+        } \
+        \
+        WRITE_##fmt##422 \
+    } \
+}
+
+#define YUV2PACKEDWRAPPER2(ext, fmt) \
+static void yuv2##ext##_2_e2k(SwsContext *c, const int16_t *buf[2], \
+                              const int16_t *ubuf[2], const int16_t *vbuf[2], \
+                              const int16_t *abuf[2], uint8_t *dest, int dstW, \
+                              int yalpha, int uvalpha, int y) \
+{ \
+    const int16_t *buf0 = buf[0], *buf1 = buf[1], \
+                  *ubuf0 = ubuf[0], *ubuf1 = ubuf[1], \
+                  *vbuf0 = vbuf[0], *vbuf1 = vbuf[1]; \
+    vec_s16 vyalpha = _mm_set1_epi32(4096 + yalpha * 0xffff); \
+    vec_s16 vuvalpha = _mm_set1_epi32(4096 + uvalpha * 0xffff); \
+    __m128i vy0, vy1, vy2, vy3, vu0, vu1, vv0, vv1; \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    int i; \
+    av_assert2(yalpha <= 4096U); \
+    av_assert2(uvalpha <= 4096U); \
+    \
+    for (i = 0; i < (dstW + 1) >> 1; i += 8) { \
+        SETUP(buf, i * 2, vyalpha, vy0, vy1); \
+        SETUP(buf, i * 2 + 8, vyalpha, vy2, vy3); \
+        SETUP(ubuf, i, vuvalpha, vu0, vu1); \
+        SETUP(vbuf, i, vuvalpha, vv0, vv1); \
+        \
+        WRITE_##fmt##422 \
+    } \
+}
+
+#define INIT1_422 __m128i blenduv = _mm_set1_epi16(255);
+
+#define WRITE1_422(vu, vv, x0, x1) \
+    v5 = _mm_srli_epi16(vu, 8); \
+    v4 = _mm_packus_epi16(vy0, vy1); \
+    v5 = _mm_blendv_epi8(vv, v5, blenduv); \
+    v0 = _mm_unpacklo_epi8(x0, x1); \
+    v1 = _mm_unpackhi_epi8(x0, x1); \
+    VEC_ST(dest, v0); \
+    VEC_ST(dest + 16, v1); \
+    dest += 32;
+
+#define WRITE1_YUYV422 WRITE1_422(vu, vv, v4, v5)
+#define WRITE1_YVYU422 WRITE1_422(vv, vu, v4, v5)
+#define WRITE1_UYVY422 WRITE1_422(vu, vv, v5, v4)
+
+#define YUV2PACKEDWRAPPER1(ext, fmt) \
+static void yuv2##ext##_1_e2k(SwsContext *c, const int16_t *buf0, \
+                              const int16_t *ubuf[2], const int16_t *vbuf[2], \
+                              const int16_t *abuf0, uint8_t *dest, int dstW, \
+                              int uvalpha, int y) \
+{ \
+    const int16_t *ubuf0 = ubuf[0], *vbuf0 = vbuf[0]; \
+    vec_s16 vy0, vy1, vu, vv; \
+    vec_s16 add64 = _mm_set1_epi16(64); \
+    int i; \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    LOAD_ZERO; \
+    INIT1_422 \
+    \
+    if (uvalpha < 2048) { \
+        for (i = 0; i < (dstW + 1) >> 1; i += 8) { \
+            vy0 = VEC_LD(buf0 + i * 2); \
+            vy1 = VEC_LD(buf0 + i * 2 + 8); \
+            vu = VEC_LD(ubuf0 + i); \
+            vv = VEC_LD(vbuf0 + i); \
+            vy0 = _mm_adds_epi16(vy0, add64); \
+            vy1 = _mm_adds_epi16(vy1, add64); \
+            vu = _mm_max_epi16(vu, zerov); \
+            vv = _mm_max_epi16(vv, zerov); \
+            vy0 = _mm_srai_epi16(vy0, 7); \
+            vy1 = _mm_srai_epi16(vy1, 7); \
+            vu = _mm_add_epi16(vu, add64); \
+            vv = _mm_add_epi16(vv, add64); \
+            vu = _mm_adds_epu16(vu, vu); \
+            vv = _mm_adds_epu16(vv, vv); \
+            \
+            WRITE1_##fmt##422 \
+        } \
+    } else { \
+        const int16_t *ubuf1 = ubuf[1], *vbuf1 = vbuf[1]; \
+        vec_s16 add128 = _mm_add_epi16(add64, add64); \
+        for (i = 0; i < (dstW + 1) >> 1; i += 8) { \
+            vy0 = VEC_LD(buf0 + i * 2); \
+            vy1 = VEC_LD(buf0 + i * 2 + 8); \
+            v0 = VEC_LD(ubuf0 + i); \
+            v1 = VEC_LD(vbuf0 + i); \
+            v2 = VEC_LD(ubuf1 + i); \
+            v3 = VEC_LD(vbuf1 + i); \
+            vy0 = _mm_adds_epi16(vy0, add64); \
+            vy1 = _mm_adds_epi16(vy1, add64); \
+            v0 = _mm_max_epi16(v0, zerov); \
+            v1 = _mm_max_epi16(v1, zerov); \
+            v2 = _mm_max_epi16(v2, zerov); \
+            v3 = _mm_max_epi16(v3, zerov); \
+            v0 = _mm_add_epi16(v0, add128); \
+            v1 = _mm_add_epi16(v1, add128); \
+            vy0 = _mm_srai_epi16(vy0, 7); \
+            vy1 = _mm_srai_epi16(vy1, 7); \
+            vu = _mm_adds_epu16(v0, v2); \
+            vv = _mm_adds_epu16(v1, v3); \
+            \
+            WRITE1_##fmt##422 \
+        } \
+    } \
+}
+
+YUV2PACKEDWRAPPERX(yuyv422, YUYV)
+YUV2PACKEDWRAPPERX(yvyu422, YVYU)
+YUV2PACKEDWRAPPERX(uyvy422, UYVY)
+
+YUV2PACKEDWRAPPER2(yuyv422, YUYV)
+YUV2PACKEDWRAPPER2(yvyu422, YVYU)
+YUV2PACKEDWRAPPER2(uyvy422, UYVY)
+
+YUV2PACKEDWRAPPER1(yuyv422, YUYV)
+YUV2PACKEDWRAPPER1(yvyu422, YVYU)
+YUV2PACKEDWRAPPER1(uyvy422, UYVY)
+
+#define HSCALE_INIT() \
+    __m128i v0, v1, v2, v3, v4, v5, v6; \
+    vec_u32 vadd = _mm_setr_epi32(0, xInc, xInc * 2, xInc * 3); \
+    vec_u16 vadd16 = _mm_setr_epi16(0, xInc, xInc * 2, xInc * 3, \
+        xInc * 4, xInc * 5, xInc * 6, xInc * 7)
+
+#define HSCALE1() \
+    v4 = _mm_set1_epi16(xpos); \
+    v5 = _mm_set1_epi16(xpos + xInc * 8); \
+    v4 = _mm_add_epi16(v4, vadd16); \
+    v5 = _mm_add_epi16(v5, vadd16); \
+    v4 = _mm_srli_epi16(v4, 9); \
+    v5 = _mm_srli_epi16(v5, 9); \
+    \
+    v0 = _mm_set1_epi32(xpos & 0xffff); \
+    v1 = _mm_set1_epi32((xpos & 0xffff) + xInc * 4); \
+    v2 = _mm_set1_epi32((xpos & 0xffff) + xInc * 8); \
+    v3 = _mm_set1_epi32((xpos & 0xffff) + xInc * 12); \
+    v0 = _mm_add_epi32(v0, vadd); \
+    v1 = _mm_add_epi32(v1, vadd); \
+    v2 = _mm_add_epi32(v2, vadd); \
+    v3 = _mm_add_epi32(v3, vadd); \
+    v0 = _mm_srli_epi32(v0, 16); \
+    v1 = _mm_srli_epi32(v1, 16); \
+    v2 = _mm_srli_epi32(v2, 16); \
+    v3 = _mm_srli_epi32(v3, 16); \
+    v0 = _mm_packs_epi32(v0, v1); \
+    v2 = _mm_packs_epi32(v2, v3); \
+    v6 = _mm_packus_epi16(v0, v2); \
+    \
+    xx = xpos >> 16
+
+static void hyscale_fast_e2k(SwsContext *c, int16_t *dst, int dstWidth,
+                           const uint8_t *src, int srcW, int xInc)
+{
+    int i, xpos = 0, xx, a1;
+    LOAD_ZERO;
+    HSCALE_INIT();
+
+    for (i = 0; i < dstWidth; i += 16) {
+        HSCALE1();
+
+        v1 = VEC_LD(src + xx);
+        v3 = VEC_LD(src + xx + 1);
+
+        v1 = _mm_shuffle_epi8(v1, v6);
+        v3 = _mm_shuffle_epi8(v3, v6);
+        v0 = _mm_unpacklo_epi8(v1, zerov);
+        v1 = _mm_unpackhi_epi8(v1, zerov);
+        v2 = _mm_unpacklo_epi8(v3, zerov);
+        v3 = _mm_unpackhi_epi8(v3, zerov);
+        v2 = _mm_sub_epi16(v2, v0);
+        v3 = _mm_sub_epi16(v3, v1);
+        v0 = _mm_slli_epi16(v0, 7);
+        v1 = _mm_slli_epi16(v1, 7);
+        v2 = _mm_mullo_epi16(v2, v4);
+        v3 = _mm_mullo_epi16(v3, v5);
+        v0 = _mm_add_epi16(v0, v2);
+        v1 = _mm_add_epi16(v1, v3);
+
+        VEC_ST(dst + i, v0);
+        VEC_ST(dst + i + 8, v1);
+        xpos += xInc * 16;
+    }
+
+    a1 = src[srcW - 1] * 128;
+    for (i = dstWidth - 1; (i * xInc) >> 16 >= srcW - 1; i--)
+        dst[i] = a1;
+}
+
+#define HSCALE2() \
+    v0 = _mm_shuffle_epi8(v0, v6); \
+    v1 = _mm_shuffle_epi8(v1, v6); \
+    v2 = _mm_unpacklo_epi8(v0, v1); \
+    v3 = _mm_unpackhi_epi8(v0, v1); \
+    v2 = _mm_maddubs_epi16(v2, v4); \
+    v3 = _mm_maddubs_epi16(v3, v5)
+
+static void hcscale_fast_e2k(SwsContext *c, int16_t *dst1, int16_t *dst2,
+                           int dstWidth, const uint8_t *src1,
+                           const uint8_t *src2, int srcW, int xInc)
+{
+    int i, xpos = 0, xx, a1, a2;
+    HSCALE_INIT();
+    __m128i xorv = _mm_set1_epi8(127);
+
+    for (i = 0; i < dstWidth; i += 16) {
+        HSCALE1();
+
+        v0 = _mm_packus_epi16(v4, v5);
+        v1 = _mm_xor_si128(v0, xorv);
+        v4 = _mm_unpacklo_epi8(v1, v0);
+        v5 = _mm_unpackhi_epi8(v1, v0);
+
+        v0 = VEC_LD(src1 + xx);
+        v1 = VEC_LD(src1 + xx + 1);
+        HSCALE2();
+        v0 = VEC_LD(src2 + xx);
+        v1 = VEC_LD(src2 + xx + 1);
+        VEC_ST(dst1 + i, v2);
+        VEC_ST(dst1 + i + 8, v3);
+        HSCALE2();
+        VEC_ST(dst2 + i, v2);
+        VEC_ST(dst2 + i + 8, v3);
+        xpos += xInc * 16;
+    }
+
+    a1 = src1[srcW - 1] * 128;
+    a2 = src2[srcW - 1] * 128;
+    for (i = dstWidth - 1; (i * xInc) >> 16 >= srcW - 1; i--) {
+        dst1[i] = a1;
+        dst2[i] = a2;
+    }
+}
+
+static void hScale8To19_e2k(SwsContext *c, int16_t *_dst, int dstW,
+                            const uint8_t *src, const int16_t *filter,
+                            const int32_t *filterPos, int filterSize)
+{
+    int i, j;
+    int32_t *dst = (int32_t*)_dst;
+    LOAD_ZERO;
+    __m128i v0, v1, accv;
+
+    if (filterSize == 1) {
+        for (i = 0; i < dstW; i++, filter += filterSize) {
+            int val = 0, srcPos = filterPos[i];
+            for (j = 0; j < filterSize; j++) {
+                val += (int)src[srcPos + j] * filter[j];
+            }
+            dst[i] = FFMIN(val >> 3, (1 << 19) - 1); // the cubic equation does overflow ...
+        }
+    } else {
+        __m64 h0, maskv;
+        uint64_t mask = ~0ll;
+        mask >>= (-filterSize & 7) * 8;
+        maskv = (__m64)mask; // 8, 1, 2, 3, 4, 5, 6, 7
+
+        for (i = 0; i < dstW; i++, filter += filterSize) {
+            int val;
+            const uint8_t *srci = src + filterPos[i];
+            accv = zerov;
+            for (j = 0; j + 7 < filterSize; j += 8) {
+                v0 = VEC_LD8(srci + j);
+                v1 = VEC_LD(filter + j);
+                v0 = _mm_unpacklo_epi8(v0, zerov);
+                accv = _mm_add_epi32(accv, _mm_madd_epi16(v0, v1));
+            }
+            if (filterSize & 7) {
+                h0 = *(__m64*)(srci + j);
+                // Remove the unused elements on the last round
+                h0 = _mm_and_si64(h0, maskv);
+                v0 = _mm_movpi64_epi64(h0);
+                v1 = VEC_LD(filter + j);
+                v0 = _mm_unpacklo_epi8(v0, zerov);
+                accv = _mm_add_epi32(accv, _mm_madd_epi16(v0, v1));
+            }
+            accv = _mm_hadd_epi32(accv, accv);
+            val = _mm_extract_epi32(accv, 0) + _mm_extract_epi32(accv, 1);
+            dst[i] = FFMIN(val >> 3, (1 << 19) - 1);
+        }
+    }
+}
+
+static void hScale16To19_e2k(SwsContext *c, int16_t *_dst, int dstW,
+                             const uint8_t *_src, const int16_t *filter,
+                             const int32_t *filterPos, int filterSize)
+{
+    const AVPixFmtDescriptor *desc = av_pix_fmt_desc_get(c->srcFormat);
+    int i, j;
+    int32_t *dst = (int32_t*)_dst;
+    const uint16_t *src = (const uint16_t*)_src;
+    int bits = desc->comp[0].depth - 1;
+    int sh = bits - 4;
+    LOAD_ZERO;
+    __m128i v0, v1, accv;
+
+    if ((isAnyRGB(c->srcFormat) || c->srcFormat == AV_PIX_FMT_PAL8) && desc->comp[0].depth < 16) {
+        sh = 9;
+    } else if (desc->flags & AV_PIX_FMT_FLAG_FLOAT) { /* float input are process like uint 16bpc */
+        sh = 16 - 1 - 4;
+    }
+
+    if (filterSize == 1) {
+        for (i = 0; i < dstW; i++) {
+            int val = 0, srcPos = filterPos[i];
+            for (j = 0; j < filterSize; j++) {
+                val += (int)src[srcPos + j] * filter[filterSize * i + j];
+            }
+            // filter=14 bit, input=16 bit, output=30 bit, >> 11 makes 19 bit
+            dst[i] = FFMIN(val >> sh, (1 << 19) - 1);
+        }
+    } else {
+        __m128i maskv, signv = _mm_set1_epi16(-0x8000), initv = zerov;
+        uint64_t mask = ~0ll;
+        mask >>= (-filterSize & 7) * 8;
+        maskv = _mm_movpi64_epi64((__m64)mask);
+        maskv = _mm_unpacklo_epi8(maskv, maskv);
+
+        for (j = 0; j + 7 < filterSize; j += 8) {
+            v1 = VEC_LD(filter + j);
+            initv = _mm_sub_epi32(initv, _mm_madd_epi16(signv, v1));
+        }
+        if (filterSize & 7) {
+            v1 = VEC_LD(filter + j);
+            v1 = _mm_and_si128(v1, maskv);
+            initv = _mm_sub_epi32(initv, _mm_madd_epi16(signv, v1));
+        }
+
+        for (i = 0; i < dstW; i++, filter += filterSize) {
+            int val;
+            const int16_t *srci = src + filterPos[i];
+            accv = initv;
+            for (j = 0; j + 7 < filterSize; j += 8) {
+                v0 = VEC_LD(srci + j);
+                v0 = _mm_xor_si128(v0, signv);
+                v1 = VEC_LD(filter + j);
+                accv = _mm_add_epi32(accv, _mm_madd_epi16(v0, v1));
+            }
+            if (filterSize & 7) {
+                v0 = VEC_LD(srci + j);
+                v0 = _mm_xor_si128(v0, signv);
+                v1 = VEC_LD(filter + j);
+                // Remove the unused elements on the last round
+                v1 = _mm_and_si128(v1, maskv);
+                accv = _mm_add_epi32(accv, _mm_madd_epi16(v0, v1));
+            }
+            accv = _mm_hadd_epi32(accv, accv);
+            val = _mm_extract_epi32(accv, 0) + _mm_extract_epi32(accv, 1);
+            dst[i] = FFMIN(val >> sh, (1 << 19) - 1);
+        }
+    }
+}
+
+static void hScale16To15_e2k(SwsContext *c, int16_t *dst, int dstW,
+                             const uint8_t *_src, const int16_t *filter,
+                             const int32_t *filterPos, int filterSize)
+{
+    const AVPixFmtDescriptor *desc = av_pix_fmt_desc_get(c->srcFormat);
+    int i, j;
+    const uint16_t *src = (const uint16_t*)_src;
+    int sh = desc->comp[0].depth - 1;
+    LOAD_ZERO;
+    __m128i v0, v1, accv;
+
+    if (sh < 15) {
+        sh = isAnyRGB(c->srcFormat) || c->srcFormat == AV_PIX_FMT_PAL8 ? 13 : (desc->comp[0].depth - 1);
+    } else if (desc->flags & AV_PIX_FMT_FLAG_FLOAT) { /* float input are process like uint 16bpc */
+        sh = 16 - 1;
+    }
+
+    if (filterSize == 1) {
+        for (i = 0; i < dstW; i++) {
+            int val = 0, srcPos = filterPos[i];
+            for (j = 0; j < filterSize; j++) {
+                val += (int)src[srcPos + j] * filter[filterSize * i + j];
+            }
+            // filter=14 bit, input=16 bit, output=30 bit, >> 15 makes 15 bit
+            dst[i] = FFMIN(val >> sh, (1 << 15) - 1);
+        }
+    } else {
+        __m128i maskv, signv = _mm_set1_epi16(-0x8000), initv = zerov;
+        uint64_t mask = ~0ll;
+        mask >>= (-filterSize & 7) * 8;
+        maskv = _mm_movpi64_epi64((__m64)mask);
+        maskv = _mm_unpacklo_epi8(maskv, maskv);
+
+        for (j = 0; j + 7 < filterSize; j += 8) {
+            v1 = VEC_LD(filter + j);
+            initv = _mm_sub_epi32(initv, _mm_madd_epi16(signv, v1));
+        }
+        if (filterSize & 7) {
+            v1 = VEC_LD(filter + j);
+            v1 = _mm_and_si128(v1, maskv);
+            initv = _mm_sub_epi32(initv, _mm_madd_epi16(signv, v1));
+        }
+
+        for (i = 0; i < dstW; i++, filter += filterSize) {
+            int val;
+            const int16_t *srci = src + filterPos[i];
+            accv = initv;
+            for (j = 0; j + 7 < filterSize; j += 8) {
+                v0 = VEC_LD(srci + j);
+                v0 = _mm_xor_si128(v0, signv);
+                v1 = VEC_LD(filter + j);
+                accv = _mm_add_epi32(accv, _mm_madd_epi16(v0, v1));
+            }
+            if (filterSize & 7) {
+                v0 = VEC_LD(srci + j);
+                v0 = _mm_xor_si128(v0, signv);
+                // Remove the unused elements on the last round
+                v1 = VEC_LD(filter + j);
+                v1 = _mm_and_si128(v1, maskv);
+                accv = _mm_add_epi32(accv, _mm_madd_epi16(v0, v1));
+            }
+            accv = _mm_hadd_epi32(accv, accv);
+            val = _mm_extract_epi32(accv, 0) + _mm_extract_epi32(accv, 1);
+            dst[i] = FFMIN(val >> sh, (1 << 15) - 1);
+        }
+    }
+}
+
+av_cold void ff_sws_init_swscale_e2k(SwsContext *c)
+{
+    enum AVPixelFormat dstFormat = c->dstFormat;
+    const AVPixFmtDescriptor *desc = av_pix_fmt_desc_get(dstFormat);
+
+    if (!(av_get_cpu_flags() & AV_CPU_FLAG_E2K))
+        return;
+
+    if (dstFormat == AV_PIX_FMT_P010LE || dstFormat == AV_PIX_FMT_P010BE) {
+        // c->yuv2plane1 = isBE(dstFormat) ? yuv2p010l1_BE_e2k : yuv2p010l1_LE_e2k;
+        // c->yuv2planeX = isBE(dstFormat) ? yuv2p010lX_BE_e2k : yuv2p010lX_LE_e2k;
+        // c->yuv2nv12cX = yuv2p010cX_e2k;
+    } else if (is16BPS(dstFormat)) {
+        c->yuv2planeX = isBE(dstFormat) ? yuv2planeX_16BE_e2k  : yuv2planeX_16LE_e2k;
+        c->yuv2plane1 = isBE(dstFormat) ? yuv2plane1_16BE_e2k  : yuv2plane1_16LE_e2k;
+        if (dstFormat == AV_PIX_FMT_P016LE || dstFormat == AV_PIX_FMT_P016BE) {
+          // c->yuv2nv12cX = yuv2p016cX_e2k;
+        }
+    } else if (isNBPS(dstFormat)) {
+        if (desc->comp[0].depth == 9) {
+            c->yuv2planeX = isBE(dstFormat) ? yuv2planeX_9BE_e2k  : yuv2planeX_9LE_e2k;
+            c->yuv2plane1 = isBE(dstFormat) ? yuv2plane1_9BE_e2k  : yuv2plane1_9LE_e2k;
+        } else if (desc->comp[0].depth == 10) {
+            c->yuv2planeX = isBE(dstFormat) ? yuv2planeX_10BE_e2k  : yuv2planeX_10LE_e2k;
+            c->yuv2plane1 = isBE(dstFormat) ? yuv2plane1_10BE_e2k  : yuv2plane1_10LE_e2k;
+        } else if (desc->comp[0].depth == 12) {
+            c->yuv2planeX = isBE(dstFormat) ? yuv2planeX_12BE_e2k  : yuv2planeX_12LE_e2k;
+            c->yuv2plane1 = isBE(dstFormat) ? yuv2plane1_12BE_e2k  : yuv2plane1_12LE_e2k;
+        } else if (desc->comp[0].depth == 14) {
+            c->yuv2planeX = isBE(dstFormat) ? yuv2planeX_14BE_e2k  : yuv2planeX_14LE_e2k;
+            c->yuv2plane1 = isBE(dstFormat) ? yuv2plane1_14BE_e2k  : yuv2plane1_14LE_e2k;
+        } else
+            av_assert0(0);
+    } else if (dstFormat == AV_PIX_FMT_GRAYF32BE) {
+        // c->yuv2planeX = yuv2planeX_floatBE_e2k;
+        c->yuv2plane1 = yuv2plane1_floatBE_e2k;
+    } else if (dstFormat == AV_PIX_FMT_GRAYF32LE) {
+        // c->yuv2planeX = yuv2planeX_floatLE_e2k;
+        c->yuv2plane1 = yuv2plane1_floatLE_e2k;
+    } else {
+        c->yuv2plane1 = yuv2plane1_8_e2k;
+        c->yuv2planeX = yuv2planeX_8_e2k;
+#if 0
+        if (dstFormat == AV_PIX_FMT_NV12 || dstFormat == AV_PIX_FMT_NV21 ||
+            dstFormat == AV_PIX_FMT_NV24 || dstFormat == AV_PIX_FMT_NV42)
+            c->yuv2nv12cX = yuv2nv12cX_e2k;
+#endif
+    }
+
+    if (c->srcBpc == 8) {
+        if (c->dstBpc <= 14) {
+            c->hyScale = c->hcScale = hScale_real_e2k;
+            if (c->flags & SWS_FAST_BILINEAR && c->dstW >= c->srcW && c->chrDstW >= c->chrSrcW) {
+                c->hyscale_fast = hyscale_fast_e2k;
+                c->hcscale_fast = hcscale_fast_e2k;
+            }
+        } else {
+            c->hyScale = c->hcScale = hScale8To19_e2k;
+        }
+    } else {
+            c->hyScale = c->hcScale = c->dstBpc > 14 ? hScale16To19_e2k
+                                                     : hScale16To15_e2k;
+    }
+
+    if (c->flags & SWS_FULL_CHR_H_INT) {
+        switch (dstFormat) {
+            case AV_PIX_FMT_RGB24:
+                c->yuv2packed1 = yuv2rgb24_full_1_e2k;
+                c->yuv2packed2 = yuv2rgb24_full_2_e2k;
+                c->yuv2packedX = yuv2rgb24_full_X_e2k;
+            break;
+            case AV_PIX_FMT_BGR24:
+                c->yuv2packed1 = yuv2bgr24_full_1_e2k;
+                c->yuv2packed2 = yuv2bgr24_full_2_e2k;
+                c->yuv2packedX = yuv2bgr24_full_X_e2k;
+            break;
+            case AV_PIX_FMT_BGRA:
+                if (!c->needAlpha) {
+                    c->yuv2packed1 = yuv2bgrx32_full_1_e2k;
+                    c->yuv2packed2 = yuv2bgrx32_full_2_e2k;
+                    c->yuv2packedX = yuv2bgrx32_full_X_e2k;
+                }
+            break;
+            case AV_PIX_FMT_RGBA:
+                if (!c->needAlpha) {
+                    c->yuv2packed1 = yuv2rgbx32_full_1_e2k;
+                    c->yuv2packed2 = yuv2rgbx32_full_2_e2k;
+                    c->yuv2packedX = yuv2rgbx32_full_X_e2k;
+                }
+            break;
+            case AV_PIX_FMT_ARGB:
+                if (!c->needAlpha) {
+                    c->yuv2packed1 = yuv2xrgb32_full_1_e2k;
+                    c->yuv2packed2 = yuv2xrgb32_full_2_e2k;
+                    c->yuv2packedX = yuv2xrgb32_full_X_e2k;
+                }
+            break;
+            case AV_PIX_FMT_ABGR:
+                if (!c->needAlpha) {
+                    c->yuv2packed1 = yuv2xbgr32_full_1_e2k;
+                    c->yuv2packed2 = yuv2xbgr32_full_2_e2k;
+                    c->yuv2packedX = yuv2xbgr32_full_X_e2k;
+                 }
+            break;
+        }
+    } else if (!(c->flags & SWS_BITEXACT)) { /* !SWS_FULL_CHR_H_INT */
+        switch (dstFormat) {
+            case AV_PIX_FMT_RGB24:
+                c->yuv2packed1 = yuv2rgb24_1_e2k;
+                c->yuv2packed2 = yuv2rgb24_2_e2k;
+                c->yuv2packedX = yuv2rgb24_X_e2k;
+            break;
+            case AV_PIX_FMT_BGR24:
+                c->yuv2packed1 = yuv2bgr24_1_e2k;
+                c->yuv2packed2 = yuv2bgr24_2_e2k;
+                c->yuv2packedX = yuv2bgr24_X_e2k;
+            break;
+            case AV_PIX_FMT_BGRA:
+                if (!c->needAlpha) {
+                    c->yuv2packed1 = yuv2bgrx32_1_e2k;
+                    c->yuv2packed2 = yuv2bgrx32_2_e2k;
+                    c->yuv2packedX = yuv2bgrx32_X_e2k;
+                }
+            break;
+            case AV_PIX_FMT_RGBA:
+                if (!c->needAlpha) {
+                    c->yuv2packed1 = yuv2rgbx32_1_e2k;
+                    c->yuv2packed2 = yuv2rgbx32_2_e2k;
+                    c->yuv2packedX = yuv2rgbx32_X_e2k;
+                }
+            break;
+            case AV_PIX_FMT_ARGB:
+                if (!c->needAlpha) {
+                    c->yuv2packed1 = yuv2xrgb32_1_e2k;
+                    c->yuv2packed2 = yuv2xrgb32_2_e2k;
+                    c->yuv2packedX = yuv2xrgb32_X_e2k;
+                }
+            break;
+            case AV_PIX_FMT_ABGR:
+                if (!c->needAlpha) {
+                    c->yuv2packed1 = yuv2xbgr32_1_e2k;
+                    c->yuv2packed2 = yuv2xbgr32_2_e2k;
+                    c->yuv2packedX = yuv2xbgr32_X_e2k;
+                }
+            break;
+        }
+    }
+
+    switch (dstFormat) {
+        case AV_PIX_FMT_YUYV422:
+            c->yuv2packed1 = yuv2yuyv422_1_e2k;
+            c->yuv2packed2 = yuv2yuyv422_2_e2k;
+            c->yuv2packedX = yuv2yuyv422_X_e2k;
+        break;
+        case AV_PIX_FMT_YVYU422:
+            c->yuv2packed1 = yuv2yvyu422_1_e2k;
+            c->yuv2packed2 = yuv2yvyu422_2_e2k;
+            c->yuv2packedX = yuv2yvyu422_X_e2k;
+        break;
+        case AV_PIX_FMT_UYVY422:
+            c->yuv2packed1 = yuv2uyvy422_1_e2k;
+            c->yuv2packed2 = yuv2uyvy422_2_e2k;
+            c->yuv2packedX = yuv2uyvy422_X_e2k;
+        break;
+    }
+}
diff --git a/libswscale/e2k/yuv2rgb.c b/libswscale/e2k/yuv2rgb.c
new file mode 100644
index 0000000..92f153f
--- /dev/null
+++ b/libswscale/e2k/yuv2rgb.c
@@ -0,0 +1,248 @@
+/*
+ * Elbrus acceleration for colorspace conversion
+ *
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * copyright (C) 2004 Marc Hoffman <marc.hoffman@analog.com>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <inttypes.h>
+
+#include "config.h"
+#include "libswscale/rgb2rgb.h"
+#include "libswscale/swscale.h"
+#include "libswscale/swscale_internal.h"
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include "libavutil/e2k/util_e2k.h"
+#include "libavutil/pixdesc.h"
+#include "yuv2rgb.h"
+
+/*
+ * ------------------------------------------------------------------------------
+ * CS converters
+ * ------------------------------------------------------------------------------
+ */
+
+#define INIT2_RGB(R, B) \
+    __m128i perm_unp8 = _mm_setr_epi8( \
+        0, 8, 1, 9, 2, 10, 3, 11, 4, 12, 5, 13, 6, 14, 7, 15); \
+    __m64 rgb_index0 = _mm_setr_pi8(0, 1, 3, 4, 5, 7, 8, 9); \
+    __m64 rgb_index1 = _mm_setr_pi8(3, 4, 5, 7, 8, 9, 11, 12); \
+    __m64 rgb_index2 = _mm_setr_pi8(5, 7, 8, 9, 11, 12, 13, 15);
+
+#define INIT2_RGBX(R, B) INIT2_XRGB(R, B)
+#define INIT2_XRGB(R, B) \
+    __m128i A_l = _mm_set1_epi16(255); \
+    __m128i perm_unp8 = _mm_setr_epi8( \
+        0, 8, 1, 9, 2, 10, 3, 11, 4, 12, 5, 13, 6, 14, 7, 15);
+
+#define WRITE2_RGB(dest, R, B) \
+    v4 = _mm_packus_epi16(R##_l, G_l); \
+    v5 = _mm_packus_epi16(B##_l, B##_l); \
+    v0 = _mm_shuffle_epi8(v4, perm_unp8); \
+    v1 = _mm_unpacklo_epi8(v5, v5); \
+    v2 = _mm_unpacklo_epi16(v0, v1); \
+    v3 = _mm_unpackhi_epi16(v0, v1); \
+    { \
+        union { __m128i v; __m64 d[2]; } a = { v2 }, b = { v3 }; \
+        __m64 *p = (__m64*)dest; \
+        p[0] = _mm_shuffle2_pi8(a.d[0], a.d[1], rgb_index0); \
+        p[1] = _mm_shuffle2_pi8(a.d[1], b.d[0], rgb_index1); \
+        p[2] = _mm_shuffle2_pi8(b.d[0], b.d[1], rgb_index2); \
+        dest += 24; \
+    }
+
+#define WRITE2_RGBX(dest, R, B) \
+    v4 = _mm_packus_epi16(R##_l, G_l); \
+    v5 = _mm_packus_epi16(B##_l, A_l); \
+    v0 = _mm_shuffle_epi8(v4, perm_unp8); \
+    v1 = _mm_shuffle_epi8(v5, perm_unp8); \
+    v2 = _mm_unpacklo_epi16(v0, v1); \
+    v3 = _mm_unpackhi_epi16(v0, v1); \
+    VEC_ST(dest, v2); \
+    VEC_ST(dest + 16, v3); \
+    dest += 32;
+
+#define WRITE2_XRGB(dest, R, B) \
+    v4 = _mm_packus_epi16(A_l, R##_l); \
+    v5 = _mm_packus_epi16(G_l, B##_l); \
+    v0 = _mm_shuffle_epi8(v4, perm_unp8); \
+    v1 = _mm_shuffle_epi8(v5, perm_unp8); \
+    v2 = _mm_unpacklo_epi16(v0, v1); \
+    v3 = _mm_unpackhi_epi16(v0, v1); \
+    VEC_ST(dest, v2); \
+    VEC_ST(dest + 16, v3); \
+    dest += 32;
+
+#define DEFCSP420_CVT(name, fmt, R, B) \
+static int yuv2##name##_e2k(SwsContext *c, const unsigned char **in, \
+                      int *instrides, int srcSliceY, int srcSliceH, \
+                      unsigned char **oplanes, int *outstrides) \
+{ \
+    vec_s32 R_l, G_l, B_l; \
+    vec_s32 y_coeff = _mm_set1_epi16(c->yuv2rgb_y_coeff); \
+    vec_s32 y_sub = _mm_set1_epi16((c->yuv2rgb_y_offset + 64) >> 7); \
+    vec_s32 v2r_coeff = _mm_set1_epi16(c->yuv2rgb_v2r_coeff); \
+    vec_s32 v2g_coeff = _mm_set1_epi16(c->yuv2rgb_v2g_coeff); \
+    vec_s32 u2g_coeff = _mm_set1_epi16(c->yuv2rgb_u2g_coeff); \
+    vec_s32 u2b_coeff = _mm_set1_epi16(c->yuv2rgb_u2b_coeff); \
+    vec_s32 dec128 = _mm_set1_epi16(128); \
+    __m128i v0, v1, v2, v3, v4, v5; \
+    LOAD_ZERO; \
+    INIT2_##fmt(R, B) \
+    int i, j, w = c->dstW & -16, h = srcSliceH & -2; \
+    vec_s16 Y0, Y1, Y2, Y3, U, V; \
+    vec_s16 vx, ux, uvx, vx0, ux0, uvx0, vx1, ux1, uvx1; \
+    const uint8_t *y1i = in[0]; \
+    const uint8_t *y2i = in[0] + instrides[0]; \
+    const uint8_t *ui = in[1], *vi = in[2]; \
+    uint8_t *out0, *out1; \
+    int vshift = c->srcFormat == AV_PIX_FMT_YUV422P; \
+    int instrides0 = instrides[0] * 2 - w; \
+    int instrides1 = (instrides[1] << vshift) - w / 2; \
+    int instrides2 = (instrides[2] << vshift) - w / 2; \
+    \
+    for (i = 0; i < h; i += 2) { \
+        out0 = oplanes[0] + (i + srcSliceY) * outstrides[0]; \
+        out1 = out0 + outstrides[0]; \
+        for (j = 0; j < w >> 4; j++) { \
+            Y1 = VEC_LD(y1i); \
+            Y3 = VEC_LD(y2i); \
+            U = VEC_LD8(ui); \
+            V = VEC_LD8(vi); \
+            U = _mm_unpacklo_epi8(U, zerov); \
+            V = _mm_unpacklo_epi8(V, zerov); \
+            Y0 = _mm_unpacklo_epi8(Y1, zerov); \
+            Y1 = _mm_unpackhi_epi8(Y1, zerov); \
+            Y2 = _mm_unpacklo_epi8(Y3, zerov); \
+            Y3 = _mm_unpackhi_epi8(Y3, zerov); \
+            U = _mm_sub_epi16(U, dec128); \
+            V = _mm_sub_epi16(V, dec128); \
+            U = _mm_slli_epi16(U, 2); \
+            V = _mm_slli_epi16(V, 2); \
+            Y0 = _mm_slli_epi16(Y0, 2); \
+            Y1 = _mm_slli_epi16(Y1, 2); \
+            Y2 = _mm_slli_epi16(Y2, 2); \
+            Y3 = _mm_slli_epi16(Y3, 2); \
+            \
+            Y0 = _mm_mulhrs_epi16(_mm_sub_epi16(Y0, y_sub), y_coeff); \
+            Y1 = _mm_mulhrs_epi16(_mm_sub_epi16(Y1, y_sub), y_coeff); \
+            Y2 = _mm_mulhrs_epi16(_mm_sub_epi16(Y2, y_sub), y_coeff); \
+            Y3 = _mm_mulhrs_epi16(_mm_sub_epi16(Y3, y_sub), y_coeff); \
+            \
+            ux = _mm_mulhrs_epi16(U, u2b_coeff); \
+            vx = _mm_mulhrs_epi16(V, v2r_coeff); \
+            ux0 = _mm_unpacklo_epi16(ux, ux); \
+            ux1 = _mm_unpackhi_epi16(ux, ux); \
+            vx0 = _mm_unpacklo_epi16(vx, vx); \
+            vx1 = _mm_unpackhi_epi16(vx, vx); \
+            \
+            uvx = _mm_mulhrs_epi16(U, u2g_coeff); \
+            uvx = _mm_add_epi16(_mm_mulhrs_epi16(V, v2g_coeff), uvx); \
+            uvx0 = _mm_unpacklo_epi16(uvx, uvx); \
+            uvx1 = _mm_unpackhi_epi16(uvx, uvx); \
+            \
+            R_l = _mm_add_epi16(Y0, vx0); \
+            G_l = _mm_add_epi16(Y0, uvx0); \
+            B_l = _mm_add_epi16(Y0, ux0); \
+            \
+            WRITE2_##fmt(out0, R, B) \
+            \
+            R_l = _mm_add_epi16(Y1, vx1); \
+            G_l = _mm_add_epi16(Y1, uvx1); \
+            B_l = _mm_add_epi16(Y1, ux1); \
+            \
+            WRITE2_##fmt(out0, R, B) \
+            \
+            R_l = _mm_add_epi16(Y2, vx0); \
+            G_l = _mm_add_epi16(Y2, uvx0); \
+            B_l = _mm_add_epi16(Y2, ux0); \
+            \
+            WRITE2_##fmt(out1, R, B) \
+            \
+            R_l = _mm_add_epi16(Y3, vx1); \
+            G_l = _mm_add_epi16(Y3, uvx1); \
+            B_l = _mm_add_epi16(Y3, ux1); \
+            \
+            WRITE2_##fmt(out1, R, B) \
+            \
+            y1i += 16; ui += 8; \
+            y2i += 16; vi += 8; \
+        } \
+        y1i += instrides0; ui += instrides1; \
+        y2i += instrides0; vi += instrides2; \
+    } \
+    return srcSliceH; \
+}
+
+DEFCSP420_CVT(rgbx32, RGBX, R, B)
+DEFCSP420_CVT(bgrx32, RGBX, B, R)
+DEFCSP420_CVT(xrgb32, XRGB, R, B)
+DEFCSP420_CVT(xbgr32, XRGB, B, R)
+DEFCSP420_CVT(rgb24, RGB, R, B)
+DEFCSP420_CVT(bgr24, RGB, B, R)
+
+/* Ok currently the acceleration routine only supports
+ * inputs of widths a multiple of 16
+ * and heights a multiple 2
+ *
+ * So we just fall back to the C codes for this.
+ */
+av_cold SwsFunc ff_yuv2rgb_init_e2k(SwsContext *c)
+{
+    SwsFunc ret;
+    if (!(av_get_cpu_flags() & AV_CPU_FLAG_E2K))
+        return NULL;
+
+    if (c->flags & SWS_BITEXACT || c->needAlpha)
+        return NULL;
+
+    ret = NULL;
+    switch (c->srcFormat) {
+    case AV_PIX_FMT_YUV422P:
+    case AV_PIX_FMT_YUV420P:
+        if (c->dstW & 15 || c->dstH & 1) break;
+        switch (c->dstFormat) {
+        case AV_PIX_FMT_RGB24:
+            ret = yuv2rgb24_e2k; break;
+        case AV_PIX_FMT_BGR24:
+            ret = yuv2bgr24_e2k; break;
+        case AV_PIX_FMT_ARGB:
+            ret = yuv2xrgb32_e2k; break;
+        case AV_PIX_FMT_ABGR:
+            ret = yuv2xbgr32_e2k; break;
+        case AV_PIX_FMT_RGBA:
+            ret = yuv2rgbx32_e2k; break;
+        case AV_PIX_FMT_BGRA:
+            ret = yuv2bgrx32_e2k; break;
+        default: break;
+        }
+        break;
+    }
+    if (ret) {
+        av_log(c, AV_LOG_WARNING, "E2K: yuv2rgb(%s, %s)\n",
+                av_get_pix_fmt_name(c->srcFormat),
+                av_get_pix_fmt_name(c->dstFormat));
+    }
+    return ret;
+}
+
diff --git a/libswscale/e2k/yuv2rgb.h b/libswscale/e2k/yuv2rgb.h
new file mode 100644
index 0000000..59637bc
--- /dev/null
+++ b/libswscale/e2k/yuv2rgb.h
@@ -0,0 +1,52 @@
+/*
+ * Elbrus-enhanced yuv2yuvX
+ *
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (C) 2004 Romain Dolbeau <romain@dolbeau.org>
+ * based on the equivalent C code in swscale.c
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef SWSCALE_E2K_YUV2RGB_H
+#define SWSCALE_E2K_YUV2RGB_H
+
+#include <stdint.h>
+
+#include "libswscale/swscale_internal.h"
+
+#define YUV2PACKEDX_HEADER(suffix)                              \
+    void ff_yuv2##suffix##_X_e2k(SwsContext *c,             \
+                                     const int16_t *lumFilter,  \
+                                     const int16_t **lumSrc,    \
+                                     int lumFilterSize,         \
+                                     const int16_t *chrFilter,  \
+                                     const int16_t **chrUSrc,   \
+                                     const int16_t **chrVSrc,   \
+                                     int chrFilterSize,         \
+                                     const int16_t **alpSrc,    \
+                                     uint8_t *dest,             \
+                                     int dstW, int dstY);
+
+YUV2PACKEDX_HEADER(abgr);
+YUV2PACKEDX_HEADER(bgra);
+YUV2PACKEDX_HEADER(argb);
+YUV2PACKEDX_HEADER(rgba);
+YUV2PACKEDX_HEADER(rgb24);
+YUV2PACKEDX_HEADER(bgr24);
+
+#endif /* SWSCALE_E2K_YUV2RGB_H */
diff --git a/libswscale/e2k/yuv2yuv.c b/libswscale/e2k/yuv2yuv.c
new file mode 100644
index 0000000..7423fa8
--- /dev/null
+++ b/libswscale/e2k/yuv2yuv.c
@@ -0,0 +1,146 @@
+/*
+ * Elbrus-enhanced yuv-to-yuv conversion routines.
+ *
+ * Copyright (C) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ * Copyright (C) 2004 Romain Dolbeau <romain@dolbeau.org>
+ * based on the equivalent C code in swscale.c
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include <inttypes.h>
+
+#include "config.h"
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include "libswscale/swscale.h"
+#include "libswscale/swscale_internal.h"
+#include "libavutil/e2k/util_e2k.h"
+
+/* This code assumes:
+ *
+ * 1) dst is 16 bytes-aligned
+ * 2) dstStride is a multiple of 16
+ * 3) width is a multiple of 16
+ * 4) lum & chrom stride are multiples of 8
+ */
+
+static int yv12toyuy2_unscaled_e2k(SwsContext *c, const uint8_t *src[],
+                                   int srcStride[], int srcSliceY,
+                                   int srcSliceH, uint8_t *dstParam[],
+                                   int dstStride_a[])
+{
+    const uint8_t *ysrc = src[0], *usrc = src[1], *vsrc = src[2];
+    int dstStride = dstStride_a[0];
+    uint8_t *dst = dstParam[0] + dstStride * srcSliceY;
+    int width = (c->dstW + 1) >> 1, height = srcSliceH;
+    int lumStride = srcStride[0];
+    int chromStride = srcStride[1];
+    int y, i;
+
+    for (y = 0; y < height; y++) {
+        PRAGMA_E2K("ivdep")
+        for (i = 0; i < width - 7; i += 8) {
+            __m128i v0, v1, v2, v3;
+            v0 = VEC_LD(ysrc + i * 2);
+            v2 = VEC_LD8(usrc + i);
+            v3 = VEC_LD8(vsrc + i);
+            v1 = _mm_unpacklo_epi8(v2, v3);
+            VEC_ST(dst + i * 4, _mm_unpacklo_epi8(v0, v1));
+            VEC_ST(dst + i * 4 + 16, _mm_unpackhi_epi8(v0, v1));
+        }
+
+        PRAGMA_E2K("ivdep")
+        for (; i < width; i++) {
+            *(uint32_t*)(dst + i * 4) =
+                ysrc[i * 2] | usrc[i] << 8 |
+                ysrc[i * 2 + 1] << 16 | vsrc[i] << 24;
+        }
+
+        if (y & 1) {
+            usrc += chromStride;
+            vsrc += chromStride;
+        }
+        ysrc += lumStride;
+        dst += dstStride;
+    }
+
+    return srcSliceH;
+}
+
+static int yv12touyvy_unscaled_e2k(SwsContext *c, const uint8_t *src[],
+                                   int srcStride[], int srcSliceY,
+                                   int srcSliceH, uint8_t *dstParam[],
+                                   int dstStride_a[])
+{
+    const uint8_t *ysrc = src[0], *usrc = src[1], *vsrc = src[2];
+    int dstStride = dstStride_a[0];
+    uint8_t *dst = dstParam[0] + dstStride * srcSliceY;
+    int width = (c->dstW + 1) >> 1, height = srcSliceH;
+    int lumStride = srcStride[0];
+    int chromStride = srcStride[1];
+    int y, i;
+
+    for (y = 0; y < height; y++) {
+        PRAGMA_E2K("ivdep")
+        for (i = 0; i < width - 7; i += 8) {
+            __m128i v0, v1, v2, v3;
+            v0 = VEC_LD(ysrc + i * 2);
+            v2 = VEC_LD8(usrc + i);
+            v3 = VEC_LD8(vsrc + i);
+            v1 = _mm_unpacklo_epi8(v2, v3);
+            VEC_ST(dst + i * 4, _mm_unpacklo_epi8(v1, v0));
+            VEC_ST(dst + i * 4 + 16, _mm_unpackhi_epi8(v1, v0));
+        }
+
+        PRAGMA_E2K("ivdep")
+        for (; i < width; i++) {
+            *(uint32_t*)(dst + i * 4) =
+                usrc[i] | ysrc[i * 2] << 8 |
+                vsrc[i] << 16 | ysrc[i * 2 + 1] << 24;
+        }
+
+        if (y & 1) {
+            usrc += chromStride;
+            vsrc += chromStride;
+        }
+        ysrc += lumStride;
+        dst += dstStride;
+    }
+    return srcSliceH;
+}
+
+av_cold void ff_get_unscaled_swscale_e2k(SwsContext *c)
+{
+    if (!(av_get_cpu_flags() & AV_CPU_FLAG_E2K))
+        return;
+
+    if (c->flags & SWS_BITEXACT)
+        return;
+
+    if (c->srcFormat == AV_PIX_FMT_YUV420P) {
+        enum AVPixelFormat dstFormat = c->dstFormat;
+        switch (dstFormat) {
+        case AV_PIX_FMT_YUYV422:
+            c->swscale = yv12toyuy2_unscaled_e2k;
+            break;
+        case AV_PIX_FMT_UYVY422:
+            c->swscale = yv12touyvy_unscaled_e2k;
+            break;
+        }
+    }
+}
diff --git a/libswscale/swscale.c b/libswscale/swscale.c
index 9cb7e8f..7760885 100644
--- a/libswscale/swscale.c
+++ b/libswscale/swscale.c
@@ -586,6 +586,8 @@ SwsFunc ff_getSwsFunc(SwsContext *c)
 
     if (ARCH_PPC)
         ff_sws_init_swscale_ppc(c);
+    if (ARCH_E2K)
+        ff_sws_init_swscale_e2k(c);
     if (ARCH_X86)
         ff_sws_init_swscale_x86(c);
     if (ARCH_AARCH64)
diff --git a/libswscale/swscale_internal.h b/libswscale/swscale_internal.h
index ee46092..8dcffb2 100644
--- a/libswscale/swscale_internal.h
+++ b/libswscale/swscale_internal.h
@@ -31,7 +31,9 @@
 #include "libavutil/log.h"
 #include "libavutil/pixfmt.h"
 #include "libavutil/pixdesc.h"
+#if HAVE_ALTIVEC
 #include "libavutil/ppc/util_altivec.h"
+#endif
 
 #define STR(s) AV_TOSTRING(s) // AV_STRINGIFY is too long
 
@@ -639,6 +641,7 @@ av_cold void ff_sws_init_range_convert(SwsContext *c);
 
 SwsFunc ff_yuv2rgb_init_x86(SwsContext *c);
 SwsFunc ff_yuv2rgb_init_ppc(SwsContext *c);
+SwsFunc ff_yuv2rgb_init_e2k(SwsContext *c);
 
 static av_always_inline int is16BPS(enum AVPixelFormat pix_fmt)
 {
@@ -853,6 +856,7 @@ extern const AVClass ff_sws_context_class;
  */
 void ff_get_unscaled_swscale(SwsContext *c);
 void ff_get_unscaled_swscale_ppc(SwsContext *c);
+void ff_get_unscaled_swscale_e2k(SwsContext *c);
 void ff_get_unscaled_swscale_arm(SwsContext *c);
 void ff_get_unscaled_swscale_aarch64(SwsContext *c);
 
@@ -873,6 +877,7 @@ void ff_sws_init_output_funcs(SwsContext *c,
                               yuv2anyX_fn *yuv2anyX);
 void ff_sws_init_swscale_ppc(SwsContext *c);
 void ff_sws_init_swscale_vsx(SwsContext *c);
+void ff_sws_init_swscale_e2k(SwsContext *c);
 void ff_sws_init_swscale_x86(SwsContext *c);
 void ff_sws_init_swscale_aarch64(SwsContext *c);
 void ff_sws_init_swscale_arm(SwsContext *c);
diff --git a/libswscale/swscale_unscaled.c b/libswscale/swscale_unscaled.c
index 5fb572b..cacb11c 100644
--- a/libswscale/swscale_unscaled.c
+++ b/libswscale/swscale_unscaled.c
@@ -2172,6 +2172,8 @@ void ff_get_unscaled_swscale(SwsContext *c)
 
     if (ARCH_PPC)
         ff_get_unscaled_swscale_ppc(c);
+    if (ARCH_E2K)
+        ff_get_unscaled_swscale_e2k(c);
      if (ARCH_ARM)
          ff_get_unscaled_swscale_arm(c);
     if (ARCH_AARCH64)
diff --git a/libswscale/utils.c b/libswscale/utils.c
index 111062e..db58be1 100644
--- a/libswscale/utils.c
+++ b/libswscale/utils.c
@@ -51,6 +51,7 @@
 #include "libavutil/pixdesc.h"
 #include "libavutil/aarch64/cpu.h"
 #include "libavutil/ppc/cpu.h"
+#include "libavutil/e2k/cpu.h"
 #include "libavutil/x86/asm.h"
 #include "libavutil/x86/cpu.h"
 
@@ -600,6 +601,14 @@ static av_cold int initFilter(int16_t **outFilter, int32_t **filterPos,
             filterAlign = 1;
     }
 
+    if (E2K_BASE(cpu_flags)) {
+        if (minFilterSize < 5)
+            filterAlign = 4;
+
+        if (minFilterSize < 3)
+            filterAlign = 1;
+    }
+
     if (HAVE_MMX && cpu_flags & AV_CPU_FLAG_MMX) {
         // special case for unscaled vertical filtering
         if (minFilterSize == 1 && filterAlign == 2)
@@ -1679,6 +1688,7 @@ av_cold int sws_init_context(SwsContext *c, SwsFilter *srcFilter,
         {
             const int filterAlign = X86_MMX(cpu_flags)     ? 4 :
                                     PPC_ALTIVEC(cpu_flags) ? 8 :
+                                    E2K_BASE(cpu_flags)    ? 8 :
                                     have_neon(cpu_flags)   ? 8 : 1;
 
             if ((ret = initFilter(&c->hLumFilter, &c->hLumFilterPos,
@@ -1706,6 +1716,7 @@ av_cold int sws_init_context(SwsContext *c, SwsFilter *srcFilter,
     {
         const int filterAlign = X86_MMX(cpu_flags)     ? 2 :
                                 PPC_ALTIVEC(cpu_flags) ? 8 :
+                                E2K_BASE(cpu_flags)    ? 8 :
                                 have_neon(cpu_flags)   ? 2 : 1;
 
         if ((ret = initFilter(&c->vLumFilter, &c->vLumFilterPos, &c->vLumFilterSize,
@@ -1790,6 +1801,8 @@ av_cold int sws_init_context(SwsContext *c, SwsFilter *srcFilter,
             cpucaps = "MMX";
         else if (PPC_ALTIVEC(cpu_flags))
             cpucaps = "AltiVec";
+        else if (E2K_BASE(cpu_flags))
+            cpucaps = "Elbrus";
         else
             cpucaps = "C";
 
diff --git a/libswscale/yuv2rgb.c b/libswscale/yuv2rgb.c
index 5884625..26d41fe 100644
--- a/libswscale/yuv2rgb.c
+++ b/libswscale/yuv2rgb.c
@@ -682,6 +682,8 @@ SwsFunc ff_yuv2rgb_get_func_ptr(SwsContext *c)
 
     if (ARCH_PPC)
         t = ff_yuv2rgb_init_ppc(c);
+    if (ARCH_E2K)
+        t = ff_yuv2rgb_init_e2k(c);
     if (ARCH_X86)
         t = ff_yuv2rgb_init_x86(c);
 
diff --git a/tests/checkasm/checkasm.c b/tests/checkasm/checkasm.c
index 899f68b..3768647 100644
--- a/tests/checkasm/checkasm.c
+++ b/tests/checkasm/checkasm.c
@@ -213,6 +213,8 @@ static const struct {
     { "ALTIVEC",  "altivec",  AV_CPU_FLAG_ALTIVEC },
     { "VSX",      "vsx",      AV_CPU_FLAG_VSX },
     { "POWER8",   "power8",   AV_CPU_FLAG_POWER8 },
+#elif ARCH_E2K
+    { "E2K",      "e2k",      AV_CPU_FLAG_E2K },
 #elif ARCH_X86
     { "MMX",      "mmx",      AV_CPU_FLAG_MMX|AV_CPU_FLAG_CMOV },
     { "MMXEXT",   "mmxext",   AV_CPU_FLAG_MMXEXT },
diff --git a/tests/checkasm/huffyuvdsp.c b/tests/checkasm/huffyuvdsp.c
index 8392022..d5eba83 100644
--- a/tests/checkasm/huffyuvdsp.c
+++ b/tests/checkasm/huffyuvdsp.c
@@ -24,10 +24,14 @@
 #include "libavutil/intreadwrite.h"
 #include "libavutil/mem.h"
 
-#include "libavcodec/huffyuvdsp.h"
-
 #include "checkasm.h"
 
+/* Short defines (B,G,R,A) in "huffyuvdsp.h" cause problems for Elbrus (e2k)
+ * system includes, so this header file must be included after "checkasm.h".
+ * Ilya Kurdyukov <jpegqs@gmail.com>
+ */
+#include "libavcodec/huffyuvdsp.h"
+
 #define randomize_buffers(buf, size)     \
     do {                                 \
         int j;                           \
-- 
2.17.1

