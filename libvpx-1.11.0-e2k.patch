From 0468fb9a5e3683f5214d61023be82bdcd198c169 Mon Sep 17 00:00:00 2001
From: Ilya Kurdyukov <jpegqs@gmail.com>
Date: Thu, 18 Nov 2021 16:28:04 +0700
Subject: [PATCH] libvpx-1.11.0 e2k support

---
 build/make/Makefile                       |    4 +
 build/make/configure.sh                   |   10 +
 build/make/rtcd.pl                        |   33 +
 configure                                 |    2 +
 test/convolve_test.cc                     |    6 +-
 vp8/common/e2k/loopfilter_e2k.c           |  730 +++++++++
 vp8/common/e2k/subpixel_e2k.c             |  354 +++++
 vp8/common/x86/idct_blk_mmx.c             |    2 +
 vp8/common/x86/idct_blk_sse2.c            |    2 +
 vp8/common/x86/loopfilter_x86.c           |    2 +-
 vp8/common/x86/vp8_asm_stubs.c            |    4 +-
 vp8/encoder/e2k/block_error_e2k.c         |   72 +
 vp8/encoder/e2k/dct_e2k.c                 |  292 ++++
 vp8/encoder/x86/quantize_sse4.c           |   95 +-
 vp8/encoder/x86/vp8_quantize_ssse3.c      |   33 +-
 vp8/vp8_common.mk                         |    6 +-
 vp8/vp8cx.mk                              |    2 +
 vp9/common/vp9_rtcd_defs.pl               |    2 +-
 vp9/encoder/e2k/vp9_error_e2k.c           |   75 +
 vp9/vp9cx.mk                              |    1 +
 vpx_dsp/e2k/highbd_intrapred_e2k.c        |  133 ++
 vpx_dsp/e2k/highbd_sad4d_e2k.c            |  306 ++++
 vpx_dsp/e2k/highbd_sad_e2k.c              |  182 +++
 vpx_dsp/e2k/highbd_subpel_variance_e2k.c  |  254 +++
 vpx_dsp/e2k/highbd_variance_impl_e2k.c    |  104 ++
 vpx_dsp/e2k/intrapred_e2k.c               |  408 +++++
 vpx_dsp/e2k/inv_wht_e2k.c                 |  100 ++
 vpx_dsp/e2k/sad4d_e2k.c                   |  237 +++
 vpx_dsp/e2k/sad_e2k.c                     |  216 +++
 vpx_dsp/e2k/subpel_variance_e2k.c         | 1760 +++++++++++++++++++++
 vpx_dsp/e2k/subtract_e2k.c                |  111 ++
 vpx_dsp/e2k/vpx_convolve_copy_e2k.c       |  125 ++
 vpx_dsp/e2k/vpx_high_subpixel_8t_e2k.c    |  390 +++++
 vpx_dsp/e2k/vpx_subpixel_8t_e2k.c         |  404 +++++
 vpx_dsp/vpx_dsp.mk                        |   15 +
 vpx_dsp/vpx_dsp_rtcd_defs.pl              |    2 +-
 vpx_dsp/x86/variance_sse2.c               |    8 +
 vpx_dsp/x86/vpx_subpixel_4t_intrin_sse2.c |   12 +-
 vpx_ports/bitops.h                        |   23 +-
 vpx_ports/e2k.h                           |  133 ++
 vpx_ports/vpx_ports.mk                    |    2 +
 vpx_ports/x86.h                           |    4 +
 42 files changed, 6575 insertions(+), 81 deletions(-)
 create mode 100644 vp8/common/e2k/loopfilter_e2k.c
 create mode 100644 vp8/common/e2k/subpixel_e2k.c
 create mode 100644 vp8/encoder/e2k/block_error_e2k.c
 create mode 100644 vp8/encoder/e2k/dct_e2k.c
 create mode 100644 vp9/encoder/e2k/vp9_error_e2k.c
 create mode 100644 vpx_dsp/e2k/highbd_intrapred_e2k.c
 create mode 100644 vpx_dsp/e2k/highbd_sad4d_e2k.c
 create mode 100644 vpx_dsp/e2k/highbd_sad_e2k.c
 create mode 100644 vpx_dsp/e2k/highbd_subpel_variance_e2k.c
 create mode 100644 vpx_dsp/e2k/highbd_variance_impl_e2k.c
 create mode 100644 vpx_dsp/e2k/intrapred_e2k.c
 create mode 100644 vpx_dsp/e2k/inv_wht_e2k.c
 create mode 100644 vpx_dsp/e2k/sad4d_e2k.c
 create mode 100644 vpx_dsp/e2k/sad_e2k.c
 create mode 100644 vpx_dsp/e2k/subpel_variance_e2k.c
 create mode 100644 vpx_dsp/e2k/subtract_e2k.c
 create mode 100644 vpx_dsp/e2k/vpx_convolve_copy_e2k.c
 create mode 100644 vpx_dsp/e2k/vpx_high_subpixel_8t_e2k.c
 create mode 100644 vpx_dsp/e2k/vpx_subpixel_8t_e2k.c
 create mode 100644 vpx_ports/e2k.h

diff --git a/build/make/Makefile b/build/make/Makefile
index 9ca97c8..1984cd0 100644
--- a/build/make/Makefile
+++ b/build/make/Makefile
@@ -143,6 +143,10 @@ $(BUILD_PFX)%_avx2.c.o: CFLAGS += -mavx2
 $(BUILD_PFX)%_avx512.c.d: CFLAGS += -mavx512f -mavx512cd -mavx512bw -mavx512dq -mavx512vl
 $(BUILD_PFX)%_avx512.c.o: CFLAGS += -mavx512f -mavx512cd -mavx512bw -mavx512dq -mavx512vl
 
+# Elbrus 2000
+$(BUILD_PFX)%_e2k.c.d: CFLAGS += -msse4.1 -Wno-missing-declarations -Wno-declaration-after-statement -Wno-missing-prototypes
+$(BUILD_PFX)%_e2k.c.o: CFLAGS += -msse4.1 -Wno-missing-declarations -Wno-declaration-after-statement -Wno-missing-prototypes
+
 # POWER
 $(BUILD_PFX)%_vsx.c.d: CFLAGS += -maltivec -mvsx
 $(BUILD_PFX)%_vsx.c.o: CFLAGS += -maltivec -mvsx
diff --git a/build/make/configure.sh b/build/make/configure.sh
index 81d30a1..f0fcb99 100644
--- a/build/make/configure.sh
+++ b/build/make/configure.sh
@@ -754,6 +754,9 @@ process_common_toolchain() {
       *i[3456]86*)
         tgt_isa=x86
         ;;
+      e2k*)
+        tgt_isa=e2k
+        ;;
       *sparc*)
         tgt_isa=sparc
         ;;
@@ -1245,6 +1248,11 @@ EOF
         esac
       fi
       ;;
+    e2k*)
+      link_with_cc=gcc
+      setup_gnu_toolchain
+      soft_enable mmx sse sse2 sse3 ssse3 sse4_1
+      ;;
     x86*)
       case  ${tgt_os} in
         android)
@@ -1483,6 +1491,8 @@ EOF
 static inline function() {}
 EOF
 
+  enabled e2k && INLINE="__attribute__((__always_inline__)) inline"
+
   # Almost every platform uses pthreads.
   if enabled multithread; then
     case ${toolchain} in
diff --git a/build/make/rtcd.pl b/build/make/rtcd.pl
index acb9f6e..bc7fd3d 100755
--- a/build/make/rtcd.pl
+++ b/build/make/rtcd.pl
@@ -278,6 +278,36 @@ EOF
   common_bottom;
 }
 
+sub e2k() {
+  determine_indirection("c", @ALL_ARCHS);
+
+  # Assign the helper variable for each enabled extension
+  foreach my $opt (@ALL_ARCHS) {
+    my $opt_uc = uc $opt;
+    eval "\$have_${opt}=\"flags & HAS_${opt_uc}\"";
+  }
+
+  common_top;
+  print <<EOF;
+#include "vpx_ports/e2k.h"
+#ifdef RTCD_C
+static void setup_rtcd_internal(void)
+{
+    int flags = e2k_simd_caps();
+
+    (void)flags;
+
+EOF
+
+  set_function_pointers("c", @ALL_ARCHS);
+
+  print <<EOF;
+}
+#endif
+EOF
+  common_bottom;
+}
+
 sub arm() {
   determine_indirection("c", @ALL_ARCHS);
 
@@ -421,6 +451,9 @@ if ($opts{arch} eq 'x86') {
   @REQUIRES = filter(qw/mmx sse sse2/);
   &require(@REQUIRES);
   x86;
+} elsif ($opts{arch} eq 'e2k') {
+  @ALL_ARCHS = filter(qw/mmx sse sse2 sse3 ssse3 sse4_1/);
+  e2k;
 } elsif ($opts{arch} eq 'mips32' || $opts{arch} eq 'mips64') {
   my $have_dspr2 = 0;
   my $have_msa = 0;
diff --git a/configure b/configure
index da631a4..f9ca9b0 100755
--- a/configure
+++ b/configure
@@ -113,6 +113,7 @@ all_platforms="${all_platforms} armv7-win32-vs14"
 all_platforms="${all_platforms} armv7-win32-vs15"
 all_platforms="${all_platforms} armv7s-darwin-gcc"
 all_platforms="${all_platforms} armv8-linux-gcc"
+all_platforms="${all_platforms} e2k-linux-gcc"
 all_platforms="${all_platforms} mips32-linux-gcc"
 all_platforms="${all_platforms} mips64-linux-gcc"
 all_platforms="${all_platforms} ppc64le-linux-gcc"
@@ -235,6 +236,7 @@ ARCH_LIST="
     x86
     x86_64
     ppc
+    e2k
 "
 ARCH_EXT_LIST_X86="
     mmx
diff --git a/test/convolve_test.cc b/test/convolve_test.cc
index 4b2dade..9ca4bab 100644
--- a/test/convolve_test.cc
+++ b/test/convolve_test.cc
@@ -1135,7 +1135,7 @@ using std::make_tuple;
                       x0_q4, x_step_q4, y0_q4, y_step_q4, w, h, bd);         \
   }
 
-#if HAVE_SSE2 && VPX_ARCH_X86_64
+#if HAVE_SSE2 && (VPX_ARCH_X86_64 || VPX_ARCH_E2K)
 WRAP(convolve_copy_sse2, 8)
 WRAP(convolve_avg_sse2, 8)
 WRAP(convolve_copy_sse2, 10)
@@ -1160,7 +1160,7 @@ WRAP(convolve8_vert_sse2, 12)
 WRAP(convolve8_avg_vert_sse2, 12)
 WRAP(convolve8_sse2, 12)
 WRAP(convolve8_avg_sse2, 12)
-#endif  // HAVE_SSE2 && VPX_ARCH_X86_64
+#endif  // HAVE_SSE2 && (VPX_ARCH_X86_64 || VPX_ARCH_E2K)
 
 #if HAVE_AVX2
 WRAP(convolve_copy_avx2, 8)
@@ -1281,7 +1281,7 @@ const ConvolveParam kArrayConvolve_c[] = { ALL_SIZES(convolve8_c) };
 INSTANTIATE_TEST_SUITE_P(C, ConvolveTest,
                          ::testing::ValuesIn(kArrayConvolve_c));
 
-#if HAVE_SSE2 && VPX_ARCH_X86_64
+#if HAVE_SSE2 && (VPX_ARCH_X86_64 || VPX_ARCH_E2K)
 #if CONFIG_VP9_HIGHBITDEPTH
 const ConvolveFunctions convolve8_sse2(
     wrap_convolve_copy_sse2_8, wrap_convolve_avg_sse2_8,
diff --git a/vp8/common/e2k/loopfilter_e2k.c b/vp8/common/e2k/loopfilter_e2k.c
new file mode 100644
index 0000000..cca1762
--- /dev/null
+++ b/vp8/common/e2k/loopfilter_e2k.c
@@ -0,0 +1,730 @@
+/*
+ *  Copyright (c) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *  Copyright (c) 2010 The WebM project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "vpx_config.h"
+#include "vp8/common/loopfilter.h"
+#include "vp8_rtcd.h"
+#include "vpx_ports/mem.h"
+
+#define LOAD1(v0, off) \
+  v0 = _mm_load_si128((__m128i*)(ptr + off));
+
+#define LOAD0(v0, off) \
+  v0 = _mm_unpacklo_epi64(  \
+    _mm_loadl_epi64((__m128i*)(u + off)), \
+    _mm_loadl_epi64((__m128i*)(v + off)));
+
+#define LFH_FILTER_AND_HEV_MASK(type) \
+  LOAD##type(v2, step * 3);    /* q3 */ \
+  LOAD##type(q2, step * 2);    /* q2 */ \
+  LOAD##type(q1, step);        /* q1 */ \
+  LOAD##type(v0, 0);           /* q0 */ \
+  v7 = _mm_load_si128((const __m128i*)limit); \
+  v1 = _mm_subs_epu8(q2, v2);  /* q2-=q3 */ \
+  v2 = _mm_subs_epu8(v2, q2);  /* q3-=q2 */ \
+  v4 = _mm_subs_epu8(q1, q2);  /* q1-=q2 */ \
+  v6 = _mm_subs_epu8(q2, q1);  /* q2-=q1 */ \
+  v4 = _mm_or_si128(v4, v6);   /* abs(q2-q1) */ \
+  v1 = _mm_or_si128(v1, v2);   /* abs(q3-q2) */ \
+  v1 = _mm_max_epu8(v1, v4); \
+  v5 = _mm_subs_epu8(v0, q1);  /* q0-=q1 */ \
+  v3 = _mm_subs_epu8(q1, v0);  /* q1-=q0 */ \
+  t0 = _mm_or_si128(v5, v3);   /* abs(q0-q1) */ \
+  v1 = _mm_max_epu8(v1, t0); \
+  LOAD##type(v2, -step * 4);   /* p3 */ \
+  LOAD##type(p2, -step * 3);   /* p2 */ \
+  LOAD##type(p1, -step * 2);   /* p1 */ \
+  v4 = _mm_subs_epu8(p2, v2);  /* p2-=p3 */ \
+  v2 = _mm_subs_epu8(v2, p2);  /* p3-=p2 */ \
+  v3 = _mm_subs_epu8(p1, p2);  /* p1-=p2 */ \
+  v1 = _mm_max_epu8(v1, v4);   /* abs(p3 - p2) */ \
+  v5 = _mm_subs_epu8(p2, p1);  /* p2-=p1 */ \
+  v1 = _mm_max_epu8(v1, v2);   /* abs(p3 - p2) */ \
+  v1 = _mm_max_epu8(v1, v5);   /* abs(p2 - p1) */ \
+  v1 = _mm_max_epu8(v1, v3);   /* abs(p2 - p1) */ \
+  LOAD##type(v6, -step);       /* p0 */ \
+  v4 = _mm_subs_epu8(v6, p1);  /* p0-=p1 */ \
+  v5 = _mm_subs_epu8(p1, v6);  /* p1-=p0 */ \
+  t1 = _mm_or_si128(v5, v4);   /* abs(p1 - p0) */ \
+  v1 = _mm_max_epu8(v1, t1); \
+  v3 = _mm_subs_epu8(q1, p1);  /* q1-=p1 */ \
+  v2 = _mm_subs_epu8(p1, q1);  /* p1-=q1 */ \
+  v1 = _mm_subs_epu8(v1, v7); \
+  v2 = _mm_or_si128(v2, v3);   /* abs(p1-q1) */ \
+  v7 = _mm_load_si128((const __m128i*)blimit); \
+  v2 = _mm_and_si128(v2, _mm_set1_epi8(-2));  /* set lsb of each byte to zero */ \
+  v2 = _mm_srli_epi16(v2, 1);  /* abs(p1-q1)/2 */ \
+  v5 = _mm_subs_epu8(v6, v0);  /* p0-=q0 */ \
+  v3 = _mm_subs_epu8(v0, v6);  /* q0-=p0 */ \
+  v5 = _mm_or_si128(v5, v3);   /* abs(p0 - q0) */ \
+  v5 = _mm_adds_epu8(v5, v5);  /* abs(p0-q0)*2 */ \
+  v5 = _mm_adds_epu8(v5, v2);  /* abs (p0 - q0) *2 + abs(p1-q1)/2 */ \
+  v2 = _mm_load_si128((const __m128i*)thresh);  /* hev */ \
+  v5 = _mm_subs_epu8(v5, v7);  /* abs (p0 - q0) *2 + abs(p1-q1)/2  > blimit */ \
+  v4 = _mm_subs_epu8(t0, v2);  /* hev */ \
+  v3 = _mm_subs_epu8(t1, v2);  /* hev */ \
+  v1 = _mm_or_si128(v1, v5); \
+  v7 = _mm_setzero_si128(); \
+  v4 = _mm_add_epi8(v4, v3);   /* hev abs(q1 - q0) > thresh || abs(p1 - p0) > thresh */ \
+  v4 = _mm_cmpeq_epi8(v4, v5); /* hev */ \
+  v1 = _mm_cmpeq_epi8(v1, v7); /* mask v1 */
+
+#define B_FILTER(type) \
+  v7 = _mm_set1_epi8(0x80); \
+  p1 = _mm_xor_si128(p1, v7);              /* p1 offset to convert to signed values */ \
+  q1 = _mm_xor_si128(q1, v7);              /* q1 offset to convert to signed values */ \
+  v2 = _mm_subs_epi8(p1, q1);              /* p1 - q1 */ \
+  v6 = _mm_xor_si128(v6, v7);              /* offset to convert to signed values */ \
+  v2 = _mm_andnot_si128(v4, v2);           /* high var mask (hvm)(p1 - q1) */ \
+  v3 = _mm_xor_si128(v0, v7);              /* offset to convert to signed values */ \
+  v0 = _mm_subs_epi8(v3, v6);              /* q0 - p0 */ \
+  v2 = _mm_adds_epi8(v2, v0);              /* 1 * (q0 - p0) + hvm(p1 - q1) */ \
+  v2 = _mm_adds_epi8(v2, v0);              /* 2 * (q0 - p0) + hvm(p1 - q1) */ \
+  v2 = _mm_adds_epi8(v2, v0);              /* 3 * (q0 - p0) + hvm(p1 - q1) */ \
+  v2 = _mm_and_si128(v1, v2);              /* mask filter values we don't care about */ \
+  v2 = _mm_xor_si128(v2, v7); \
+  v0 = _mm_set1_epi8(31); \
+  t0 = _mm_set1_epi8(16); \
+  v1 = _mm_adds_epu8(v2, _mm_set1_epi8(4));  /* 3* (q0 - p0) + hvm(p1 - q1) + 4 */ \
+  v2 = _mm_adds_epu8(v2, _mm_set1_epi8(3));  /* 3* (q0 - p0) + hvm(p1 - q1) + 3 */ \
+  v1 = _mm_srli_epi16(v1, 3); \
+  v2 = _mm_srli_epi16(v2, 3); \
+  v1 = _mm_and_si128(v1, v0); \
+  v2 = _mm_and_si128(v2, v0); \
+  v0 = _mm_sub_epi8(v1, t0);               /* (3* (q0 - p0) + hvm(p1 - q1) + 4) >> 3 */ \
+  v2 = _mm_sub_epi8(v2, t0);               /* (3* (q0 - p0) + hvm(p1 - q1) + 3) >> 3 */ \
+  v5 = _mm_avg_epu8(v1, t0);               /* 0 */ \
+  v3 = _mm_subs_epi8(v3, v0);              /* q0-= q0 add */ \
+  v5 = _mm_sub_epi8(v5, t0);               /* 8, (3* (q0 - p0) + hvm(p1 - q1) + 4) >> 4 */ \
+  v6 = _mm_adds_epi8(v6, v2);              /* p0+= p0 add */ \
+  v4 = _mm_and_si128(v4, v5);              /* high edge variance additive */ \
+  v6 = _mm_xor_si128(v6, v7);              /* unoffset */ \
+  v3 = _mm_xor_si128(v3, v7);              /* unoffset */ \
+  q1 = _mm_subs_epi8(q1, v4);              /* q1-= q1 add */ \
+  p1 = _mm_adds_epi8(p1, v4);              /* p1+= p1 add */ \
+  q1 = _mm_xor_si128(q1, v7);              /* unoffset */ \
+  p1 = _mm_xor_si128(p1, v7);              /* unoffset */ \
+  STORE##type(q1, step);       /* q1 */ \
+  STORE##type(v3, 0);          /* q0 */ \
+  STORE##type(v6, -step);      /* p0 */ \
+  STORE##type(p1, -step * 2);  /* p1 */
+
+#define STORE2(v0, off)
+#define STORE1(v0, off) \
+  _mm_store_si128((__m128i*)(ptr + off), v0);
+
+#define STORE0(v0, off) \
+  _mm_storel_epi64((__m128i*)(u + off), v0); \
+  _mm_storel_epi64((__m128i*)(v + off), _mm_bsrli_si128(v0, 8));
+
+void vp8_loop_filter_bh_sse2(unsigned char *y_ptr, unsigned char *u_ptr,
+                             unsigned char *v_ptr, int y_stride, int uv_stride,
+                             loop_filter_info *lfi) {
+  __m128i v0, v1, v2, v3, v4, v5, v6, v7, p1, p2, q1, q2, t0, t1;
+  const unsigned char *blimit = lfi->blim, *limit = lfi->lim, *thresh = lfi->hev_thr;
+  unsigned char *ptr = y_ptr, *u = u_ptr, *v = v_ptr; int i, step = y_stride;
+
+  for (i = 0; i < 3; i++) {
+    ptr += 4 * y_stride;
+    // calculate breakout conditions and high edge variance
+    LFH_FILTER_AND_HEV_MASK(1)
+    // filter and write back the result
+    B_FILTER(1)
+  }
+
+  if (u_ptr) {
+    u += 4 * uv_stride; v += 4 * uv_stride; step = uv_stride;
+
+    // calculate breakout conditions and high edge variance
+    LFH_FILTER_AND_HEV_MASK(0)
+    // filter and write back the results
+    B_FILTER(0)
+  }
+}
+
+#define MB_FILTER_AND_WRITEBACK(type) \
+  v3 = _mm_set1_epi8(0x80); \
+  v2 = _mm_xor_si128(p1, v3);            /* p1 offset to convert to signed values */ \
+  v7 = _mm_xor_si128(q1, v3);            /* q1 offset to convert to signed values */ \
+  v6 = _mm_xor_si128(v6, v3);            /* offset to convert to signed values */ \
+  v3 = _mm_xor_si128(v0, v3);            /* offset to convert to signed values */ \
+  v2 = _mm_subs_epi8(v2, v7);            /* p1 - q1 */ \
+  v0 = _mm_subs_epi8(v3, v6);            /* q0 - p0 */ \
+  v2 = _mm_adds_epi8(v2, v0);            /* 1 * (q0 - p0) + (p1 - q1) */ \
+  v2 = _mm_adds_epi8(v2, v0);            /* 2 * (q0 - p0) */ \
+  v2 = _mm_adds_epi8(v2, v0);            /* 3 * (q0 - p0) + (p1 - q1) */ \
+  v1 = _mm_and_si128(v1, v2);            /* mask filter values we don't care about */ \
+  /* vp8_filter */ \
+  v2 = _mm_andnot_si128(v4, v1);         /* Filter2 = vp8_filter & hev */ \
+  v4 = _mm_and_si128(v4, v1);            /* vp8_filter&=~hev */ \
+  v1 = _mm_setzero_si128(); \
+  v0 = _mm_unpacklo_epi8(v1, v4);        /* Filter 2 (hi) */ \
+  v1 = _mm_unpackhi_epi8(v1, v4);        /* Filter 2 (lo) */ \
+  v4 = _mm_set1_epi16(0x900); \
+  v5 = _mm_adds_epi8(v2, _mm_set1_epi8(3));    /* vp8_signed_char_clamp(Filter2 + 3) */ \
+  v2 = _mm_adds_epi8(v2, _mm_set1_epi8(4));    /* vp8_signed_char_clamp(Filter2 + 4) */ \
+  t1 = _mm_mulhi_epi16(v1, v4);          /* Filter 2 (lo) * 9 */ \
+  t0 = _mm_mulhi_epi16(v0, v4);          /* Filter 2 (hi) * 9 */ \
+  v7 = _mm_unpackhi_epi8(v7, v5);        /* axbxcxdx */ \
+  v5 = _mm_unpacklo_epi8(v5, v5);        /* exfxgxhx */ \
+  v7 = _mm_srai_epi16(v7, 11);           /* sign extended shift right by 3 */ \
+  v5 = _mm_srai_epi16(v5, 11);           /* sign extended shift right by 3 */ \
+  v4 = _mm_unpackhi_epi8(v4, v2);        /* axbxcxdx */ \
+  v2 = _mm_unpacklo_epi8(v2, v2);        /* exfxgxhx */ \
+  v4 = _mm_srai_epi16(v4, 11);           /* sign extended shift right by 3 */ \
+  v5 = _mm_packs_epi16(v5, v7);          /* Filter2 >>=3 */ \
+  v2 = _mm_srai_epi16(v2, 11);           /* sign extended shift right by 3 */ \
+  v2 = _mm_packs_epi16(v2, v4);          /* Filter1 >>=3 */ \
+  v6 = _mm_adds_epi8(v6, v5);            /* ps0 += Fitler2 */ \
+  v3 = _mm_subs_epi8(v3, v2);            /* qs0 -= Filter1 */ \
+  v4 = _mm_set1_epi16(0x3f); \
+  v0 = _mm_add_epi16(t0, v4);            /* Filter 2 (hi) * 9 + 63 */ \
+  v1 = _mm_add_epi16(t1, v4);            /* Filter 2 (lo) * 9 + 63 */ \
+  v5 = _mm_add_epi16(t0, t0);            /* Filter 2 (hi) * 18 */ \
+  v7 = _mm_add_epi16(t1, t1);            /* Filter 2 (lo) * 18 */ \
+  v5 = _mm_add_epi16(v5, v0);            /* Filter 2 (hi) * 27 + 63 */ \
+  v7 = _mm_add_epi16(v7, v1);            /* Filter 2 (lo) * 27 + 63 */ \
+  v2 = _mm_add_epi16(t0, v0);            /* Filter 2 (hi) * 18 + 63 */ \
+  v0 = _mm_srai_epi16(v0, 7);            /* (Filter 2 (hi) * 9 + 63) >> 7 */ \
+  v4 = _mm_add_epi16(t1, v1);            /* Filter 2 (lo) * 18 + 63 */ \
+  v1 = _mm_srai_epi16(v1, 7);            /* (Filter 2 (lo) * 9 + 63) >> 7 */ \
+  v2 = _mm_srai_epi16(v2, 7);            /* (Filter 2 (hi) * 18 + 63) >> 7 */ \
+  v0 = _mm_packs_epi16(v0, v1);          /* u1 = vp8_signed_char_clamp((63 + Filter2 * 9)>>7) */ \
+  v4 = _mm_srai_epi16(v4, 7);            /* (Filter 2 (lo) * 18 + 63) >> 7 */ \
+  v5 = _mm_srai_epi16(v5, 7);            /* (Filter 2 (hi) * 27 + 63) >> 7 */ \
+  v7 = _mm_srai_epi16(v7, 7);            /* (Filter 2 (lo) * 27 + 63) >> 7 */ \
+  v5 = _mm_packs_epi16(v5, v7);          /* u3 = vp8_signed_char_clamp((63 + Filter2 * 27)>>7) */ \
+  v2 = _mm_packs_epi16(v2, v4);          /* u2 = vp8_signed_char_clamp((63 + Filter2 * 18)>>7) */ \
+  v7 = _mm_set1_epi8(0x80); \
+  v1 = _mm_xor_si128(q1, v7); \
+  v4 = _mm_xor_si128(p1, v7); \
+  v3 = _mm_subs_epi8(v3, v5);            /* sq = vp8_signed_char_clamp(qs0 - u3) */ \
+  v6 = _mm_adds_epi8(v6, v5);            /* sp = vp8_signed_char_clamp(ps0 - u3) */ \
+  v1 = _mm_subs_epi8(v1, v2);            /* sq = vp8_signed_char_clamp(qs1 - u2) */ \
+  v4 = _mm_adds_epi8(v4, v2);            /* sp = vp8_signed_char_clamp(ps1 - u2) */ \
+  v1 = _mm_xor_si128(v1, v7);            /* *oq1 = sq^0x80 */ \
+  v4 = _mm_xor_si128(v4, v7);            /* *op1 = sp^0x80 */ \
+  v2 = _mm_xor_si128(p2, v7); \
+  v5 = _mm_xor_si128(q2, v7); \
+  v2 = _mm_adds_epi8(v2, v0);            /* sp = vp8_signed_char_clamp(ps2 - u) */ \
+  v5 = _mm_subs_epi8(v5, v0);            /* sq = vp8_signed_char_clamp(qs2 - u) */ \
+  v2 = _mm_xor_si128(v2, v7);            /* *op2 = sp^0x80 */ \
+  v5 = _mm_xor_si128(v5, v7);            /* *oq2 = sq^0x80 */ \
+  v3 = _mm_xor_si128(v3, v7);            /* *oq0 = sq^0x80 */ \
+  v6 = _mm_xor_si128(v6, v7);            /* *oq0 = sp^0x80 */ \
+  STORE##type(v5, step * 2);   /* q2 */ \
+  STORE##type(v1, step);       /* q1 */ \
+  STORE##type(v3, 0);          /* q0 */ \
+  STORE##type(v6, -step);      /* p0 */ \
+  STORE##type(v4, -step * 2);  /* p1 */ \
+  STORE##type(v2, -step * 3);  /* p2 */
+
+void vp8_loop_filter_mbh_sse2(unsigned char *y_ptr, unsigned char *u_ptr,
+                              unsigned char *v_ptr, int y_stride, int uv_stride,
+                              loop_filter_info *lfi) {
+  __m128i v0, v1, v2, v3, v4, v5, v6, v7, p1, p2, q1, q2, t0, t1;
+  const unsigned char *blimit = lfi->mblim, *limit = lfi->lim, *thresh = lfi->hev_thr;
+  unsigned char *ptr = y_ptr, *u = u_ptr, *v = v_ptr; int step = y_stride;
+
+  // calculate breakout conditions and high edge variance
+  LFH_FILTER_AND_HEV_MASK(1)
+  // filter and write back the results
+  MB_FILTER_AND_WRITEBACK(1)
+
+  if (u_ptr) {
+    step = uv_stride;
+
+    // calculate breakout conditions and high edge variance
+    LFH_FILTER_AND_HEV_MASK(0)
+    // filter and write back the results
+    MB_FILTER_AND_WRITEBACK(0)
+  }
+}
+
+#define TRANSPOSE_16X8 \
+  v4 = _mm_loadl_epi64((__m128i*)u);               /* xx xx xx xx xx xx xx xx 07 06 05 04 03 02 01 00 */ \
+  v1 = _mm_loadl_epi64((__m128i*)(u + step));      /* xx xx xx xx xx xx xx xx 17 16 15 14 13 12 11 10 */ \
+  v0 = _mm_loadl_epi64((__m128i*)(u + step * 2));  /* xx xx xx xx xx xx xx xx 27 26 25 24 23 22 21 20 */ \
+  v7 = _mm_loadl_epi64((__m128i*)(u + step * 3));  /* xx xx xx xx xx xx xx xx 37 36 35 34 33 32 31 30 */ \
+  v5 = _mm_loadl_epi64((__m128i*)(u + step * 4));  /* xx xx xx xx xx xx xx xx 47 46 45 44 43 42 41 40 */ \
+  v2 = _mm_loadl_epi64((__m128i*)(u + step * 5));  /* xx xx xx xx xx xx xx xx 57 56 55 54 53 52 51 50 */ \
+  v4 = _mm_unpacklo_epi8(v4, v1);                  /* 17 07 16 06 15 05 14 04 13 03 12 02 11 01 10 00 */ \
+  v1 = _mm_loadl_epi64((__m128i*)(u + step * 7));  /* xx xx xx xx xx xx xx xx 77 76 75 74 73 72 71 70 */ \
+  v0 = _mm_unpacklo_epi8(v0, v7);                  /* 37 27 36 36 35 25 34 24 33 23 32 22 31 21 30 20 */ \
+  v7 = _mm_loadl_epi64((__m128i*)(u + step * 6));  /* xx xx xx xx xx xx xx xx 67 66 65 64 63 62 61 60 */ \
+  v6 = _mm_unpacklo_epi8(v5, v2);                  /* 57 47 56 46 55 45 54 44 53 43 52 42 51 41 50 40 */ \
+  v7 = _mm_unpacklo_epi8(v7, v1);    /* 77 67 76 66 75 65 74 64 73 63 72 62 71 61 70 60 */ \
+  v5 = _mm_unpacklo_epi16(v6, v7);   /* 73 63 53 43 72 62 52 42 71 61 51 41 70 60 50 40 */ \
+  v6 = _mm_unpackhi_epi16(v6, v7);   /* 77 67 57 47 76 66 56 46 75 65 55 45 74 64 54 44 */ \
+  v2 = _mm_unpacklo_epi16(v4, v0);   /* 33 23 13 03 32 22 12 02 31 21 11 01 30 20 10 00 */ \
+  v4 = _mm_unpackhi_epi16(v4, v0);   /* 37 27 17 07 36 26 16 06 35 25 15 05 34 24 14 04 */ \
+  v3 = _mm_unpackhi_epi32(v2, v5);   /* 73 63 53 43 33 23 13 03 72 62 52 42 32 22 12 02 */ \
+  v7 = _mm_unpackhi_epi32(v4, v6);   /* 77 67 57 47 37 27 17 07 76 66 56 46 36 26 16 06 */ \
+  v4 = _mm_unpacklo_epi32(v4, v6);   /* 75 65 55 45 35 25 15 05 74 64 54 44 34 24 14 04 */ \
+  t0 = _mm_unpacklo_epi32(v2, v5);   /* 71 61 51 41 31 21 11 01 70 60 50 40 30 20 10 00 */ \
+  \
+  v2 = _mm_loadl_epi64((__m128i*)v);               /* xx xx xx xx xx xx xx xx 87 86 85 84 83 82 81 80 */ \
+  v6 = _mm_loadl_epi64((__m128i*)(v + step));      /* xx xx xx xx xx xx xx xx 97 96 95 94 93 92 91 90 */ \
+  v0 = _mm_loadl_epi64((__m128i*)(v + step * 2));  /* xx xx xx xx xx xx xx xx a7 a6 a5 a4 a3 a2 a1 a0 */ \
+  v5 = _mm_loadl_epi64((__m128i*)(v + step * 3));  /* xx xx xx xx xx xx xx xx b7 b6 b5 b4 b3 b2 b1 b0 */ \
+  v1 = _mm_loadl_epi64((__m128i*)(v + step * 4));  /* xx xx xx xx xx xx xx xx c7 c6 c5 c4 c3 c2 c1 c0 */ \
+  v2 = _mm_unpacklo_epi8(v2, v6);                  /* 97 87 96 86 95 85 94 84 93 83 92 82 91 81 90 80 */ \
+  v6 = _mm_loadl_epi64((__m128i*)(v + step * 5));  /* xx xx xx xx xx xx xx xx d7 d6 d5 d4 d3 d2 d1 d0 */ \
+  v0 = _mm_unpacklo_epi8(v0, v5);                  /* b7 a7 b6 a6 b5 a5 b4 a4 b3 a3 b2 a2 b1 a1 b0 a0 */ \
+  v5 = _mm_loadl_epi64((__m128i*)(v + step * 6));  /* xx xx xx xx xx xx xx xx e7 e6 e5 e4 e3 e2 e1 e0 */ \
+  v1 = _mm_unpacklo_epi8(v1, v6);                  /* d7 c7 d6 c6 d5 c5 d4 c4 d3 c3 d2 c2 d1 e1 d0 c0 */ \
+  v6 = _mm_loadl_epi64((__m128i*)(v + step * 7));  /* xx xx xx xx xx xx xx xx f7 f6 f5 f4 f3 f2 f1 f0 */ \
+  v5 = _mm_unpacklo_epi8(v5, v6);                  /* f7 e7 f6 e6 f5 e5 f4 e4 f3 e3 f2 e2 f1 e1 f0 e0 */ \
+  v6 = _mm_unpackhi_epi16(v1, v5);   /* f7 e7 d7 c7 f6 e6 d6 c6 f5 e5 d5 c5 f4 e4 d4 c4 */ \
+  v1 = _mm_unpacklo_epi16(v1, v5);   /* f3 e3 d3 c3 f2 e2 d2 c2 f1 e1 d1 c1 f0 e0 d0 c0 */ \
+  v5 = _mm_unpacklo_epi16(v2, v0);   /* b3 a3 93 83 b2 a2 92 82 b1 a1 91 81 b0 a0 90 80 */ \
+  v2 = _mm_unpackhi_epi16(v2, v0);   /* b7 a7 97 87 b6 a6 96 86 b5 a5 95 85 b4 a4 94 84 */ \
+  v0 = _mm_unpacklo_epi32(v5, v1);   /* f1 e1 d1 c1 b1 a1 91 81 f0 e0 d0 c0 b0 a0 90 80 */ \
+  v5 = _mm_unpackhi_epi32(v5, v1);   /* f3 e3 d3 c3 b3 a3 93 83 f2 e2 d2 c2 b2 a2 92 82 */ \
+  v1 = _mm_unpacklo_epi32(v2, v6);   /* f5 e5 d5 c5 b5 a5 95 85 f4 e4 d4 c4 b4 a4 94 84 */ \
+  v2 = _mm_unpackhi_epi32(v2, v6);   /* f7 e7 d7 c7 b7 a7 97 87 f6 e6 d6 c6 b6 a6 96 86 */ \
+  q2 = _mm_unpacklo_epi64(v7, v2);   /* f6 e6 d6 c6 b6 a6 96 86 76 66 56 46 36 26 16 06 */ \
+  q3 = _mm_unpackhi_epi64(v7, v2);   /* f7 e7 d7 c7 b7 a7 97 87 77 67 57 47 37 27 17 07 */ \
+  p0 = _mm_unpackhi_epi64(v3, v5);   /* f3 e3 d3 c3 b3 a3 93 83 73 63 53 43 33 23 13 03 */ \
+  p1 = _mm_unpacklo_epi64(v3, v5);   /* f2 e2 d2 c2 b2 a2 92 82 72 62 52 42 32 22 12 02 */ \
+  q0 = _mm_unpacklo_epi64(v4, v1);   /* f4 e4 d4 c4 b4 a4 94 84 74 64 54 44 34 24 14 04 */ \
+  q1 = _mm_unpackhi_epi64(v4, v1);   /* f5 e5 d5 c5 b5 a5 95 85 75 65 55 45 35 25 15 05 */ \
+  p2 = _mm_unpackhi_epi64(t0, v0);   /* f1 e1 d1 c1 b1 a1 91 81 71 61 51 41 31 21 11 01 */ \
+  p3 = _mm_unpacklo_epi64(t0, v0);   /* f0 e0 d0 c0 b0 a0 90 80 70 60 50 40 30 20 10 00 */ \
+
+#define LFV_FILTER_MASK_HEV_MASK \
+  v0 = _mm_subs_epu8(q2, q3);           /* q2-q3 */ \
+  v7 = _mm_subs_epu8(q3, q2);           /* q3-q2 */ \
+  v7 = _mm_or_si128(v7, v0);            /* abs (q3-q2) */ \
+  v4 = _mm_subs_epu8(q1, q2);           /* q1-q2 */ \
+  v6 = _mm_subs_epu8(q2, q1);           /* q2-q1 */ \
+  v6 = _mm_or_si128(v6, v4);            /* abs (q2-q1) */ \
+  v0 = _mm_subs_epu8(p2, p3);           /* p2 - p3 */ \
+  v2 = _mm_subs_epu8(p3, p2);           /* p3 - p2 */ \
+  v0 = _mm_or_si128(v0, v2);            /* abs(p2-p3) */ \
+  v0 = _mm_max_epu8(v0, v7); \
+  v5 = _mm_subs_epu8(p1, p2);           /* p1-p2 */ \
+  v1 = _mm_subs_epu8(p2, p1);           /* p2-p1 */ \
+  v7 = _mm_subs_epu8(p0, p1);           /* p0-p1 */ \
+  v1 = _mm_or_si128(v1, v5);            /* abs(p2-p1) */ \
+  v0 = _mm_max_epu8(v0, v6); \
+  v0 = _mm_max_epu8(v0, v1); \
+  v2 = _mm_subs_epu8(p1, p0);           /* p1-p0 */ \
+  v2 = _mm_or_si128(v2, v7);            /* abs(p1-p0) */ \
+  v0 = _mm_max_epu8(v0, v2); \
+  v5 = _mm_subs_epu8(q0, q1);           /* q0-q1 */ \
+  v7 = _mm_subs_epu8(q1, q0);           /* q1-q0 */ \
+  v7 = _mm_or_si128(v7, v5);            /* abs(q1-q0) */ \
+  v0 = _mm_max_epu8(v0, v7); \
+  v0 = _mm_subs_epu8(v0, _mm_load_si128((const __m128i*)limit)); \
+  v5 = _mm_subs_epu8(q1, p1);           /* q1-=p1 */ \
+  v1 = _mm_subs_epu8(p1, q1);           /* p1-=q1 */ \
+  v5 = _mm_or_si128(v5, v1);            /* abs(p1-q1) */ \
+  v5 = _mm_and_si128(v5, _mm_set1_epi8(-2));  /* set lsb of each byte to zero */ \
+  v1 = _mm_subs_epu8(p0, q0);           /* p0-q0 */ \
+  v4 = _mm_load_si128((const __m128i*)blimit); \
+  v5 = _mm_srli_epi16(v5, 1);           /* abs(p1-q1)/2 */ \
+  v6 = _mm_subs_epu8(q0, p0);           /* q0-p0 */ \
+  v1 = _mm_or_si128(v1, v6);            /* abs(q0-p0) */ \
+  v1 = _mm_adds_epu8(v1, v1);           /* abs(q0-p0)*2 */ \
+  v3 = _mm_load_si128((const __m128i*)thresh); \
+  v1 = _mm_adds_epu8(v1, v5);           /* abs (p0 - q0) *2 + abs(p1-q1)/2 */ \
+  v2 = _mm_subs_epu8(v2, v3);           /* abs(q1 - q0) > thresh */ \
+  v7 = _mm_subs_epu8(v7, v3);           /* abs(p1 - p0)> thresh */ \
+  v1 = _mm_subs_epu8(v1, v4);           /* abs (p0 - q0) *2 + abs(p1-q1)/2  > blimit */ \
+  v2 = _mm_or_si128(v2, v7);            /* abs(q1 - q0) > thresh || abs(p1 - p0) > thresh */ \
+  v1 = _mm_or_si128(v1, v0);            /* mask */ \
+  v4 = _mm_cmpeq_epi8(v2, v0); \
+  v0 = _mm_setzero_si128(); \
+  v1 = _mm_cmpeq_epi8(v1, v0);
+
+#define MBV_TRANSPOSE \
+  v3 = _mm_unpacklo_epi8(p3, v2);           /* 71 70 61 60 51 50 41 40 31 30 21 20 11 10 01 00 */ \
+  v4 = _mm_unpackhi_epi8(p3, v2);           /* f1 f0 e1 e0 d1 d0 c1 c0 b1 b0 a1 a0 91 90 81 80 */ \
+  v7 = _mm_unpacklo_epi8(p1, p0);           /* 73 72 63 62 53 52 43 42 33 32 23 22 13 12 03 02 */ \
+  v0 = _mm_unpackhi_epi8(p1, p0);           /* f3 f2 e3 e2 d3 d2 c3 c2 b3 b2 a3 a2 93 92 83 82 */ \
+  v6 = _mm_unpacklo_epi16(v3, v7);          /* 33 32 31 30 23 22 21 20 13 12 11 10 03 02 01 00 */ \
+  v3 = _mm_unpackhi_epi16(v3, v7);          /* 73 72 71 70 63 62 61 60 53 52 51 50 43 42 41 40 */ \
+  v1 = _mm_unpacklo_epi16(v4, v0);          /* b3 b2 b1 b0 a3 a2 a1 a0 93 92 91 90 83 82 81 80 */ \
+  v4 = _mm_unpackhi_epi16(v4, v0);          /* f3 f2 f1 f0 e3 e2 e1 e0 d3 d2 d1 d0 c3 c2 c1 c0 */ \
+  v2 = _mm_unpacklo_epi8(q0, q1);           /* 75 74 65 64 55 54 45 44 35 34 25 24 15 14 05 04 */ \
+  v0 = _mm_unpacklo_epi8(v5, q3);           /* 77 76 67 66 57 56 47 46 37 36 27 26 17 16 07 06 */ \
+  v7 = _mm_unpacklo_epi16(v2, v0);          /* 37 36 35 34 27 26 25 24 17 16 15 14 07 06 05 04 */ \
+  v2 = _mm_unpackhi_epi16(v2, v0);          /* 77 76 75 74 67 66 65 64 57 56 55 54 47 46 45 44 */ \
+  v0 = _mm_unpacklo_epi32(v6, v7);          /* 17 16 15 14 13 12 11 10 07 06 05 04 03 02 01 00 */ \
+  v6 = _mm_unpackhi_epi32(v6, v7);          /* 37 36 35 34 33 32 31 30 27 26 25 24 23 22 21 20 */ \
+
+#define BV_TRANSPOSE \
+  /* v1 = f2 e2 d2 c2 b2 a2 92 82 72 62 52 42 32 22 12 02 */ \
+  /* v6 = f3 e3 d3 c3 b3 a3 93 83 73 63 53 43 33 23 13 03 */ \
+  /* v3 = f4 e4 d4 c4 b4 a4 94 84 74 64 54 44 34 24 14 04 */ \
+  /* v7 = f5 e5 d5 c5 b5 a5 95 85 75 65 55 45 35 25 15 05 */ \
+  v2 = _mm_unpacklo_epi8(p1, v6);   /* 73 72 63 62 53 52 43 42 33 32 23 22 13 12 03 02 */ \
+  v1 = _mm_unpackhi_epi8(p1, v6);   /* f3 f2 e3 e2 d3 d2 c3 c2 b3 b2 a3 a2 93 92 83 82 */ \
+  v4 = _mm_unpacklo_epi8(v3, q1);   /* 75 74 65 64 55 54 45 44 35 34 25 24 15 14 05 04 */ \
+  v3 = _mm_unpackhi_epi8(v3, q1);   /* f5 f4 e5 e4 d5 d4 c5 c4 b5 b4 a5 a4 95 94 85 84 */ \
+  v6 = _mm_unpacklo_epi16(v2, v4);  /* 35 34 33 32 25 24 23 22 15 14 13 12 05 04 03 02 */ \
+  v2 = _mm_unpackhi_epi16(v2, v4);  /* 75 74 73 72 65 64 63 62 55 54 53 52 45 44 43 42 */ \
+  v5 = _mm_unpacklo_epi16(v1, v3);  /* b5 b4 b3 b2 a5 a4 a3 a2 95 94 93 92 85 84 83 82 */ \
+  v1 = _mm_unpackhi_epi16(v1, v3);  /* f5 f4 f3 f2 e5 e4 e3 e2 d5 d4 d3 d2 c5 c4 c3 c2 */
+
+#define BV_WRITEBACK(ptr, v0, v1) \
+  *(int32_t*)(ptr)            = _mm_cvtsi128_si32(v0); \
+  *(int32_t*)(ptr + step * 4) = _mm_cvtsi128_si32(v1); \
+  *(int32_t*)(ptr + step)     = _mm_extract_epi32(v0, 1); \
+  *(int32_t*)(ptr + step * 5) = _mm_extract_epi32(v1, 1); \
+  *(int32_t*)(ptr + step * 2) = _mm_extract_epi32(v0, 2); \
+  *(int32_t*)(ptr + step * 6) = _mm_extract_epi32(v1, 2); \
+  *(int32_t*)(ptr + step * 3) = _mm_extract_epi32(v0, 3); \
+  *(int32_t*)(ptr + step * 7) = _mm_extract_epi32(v1, 3);
+
+void vp8_loop_filter_bv_sse2(unsigned char *y_ptr, unsigned char *u_ptr,
+                             unsigned char *v_ptr, int y_stride, int uv_stride,
+                             loop_filter_info *lfi) {
+  __m128i v0, v1, v2, v3, v4, v5, v6, v7, p0, p1, p2, p3, q0, q1, q2, q3, t0;
+  const unsigned char *blimit = lfi->blim, *limit = lfi->lim, *thresh = lfi->hev_thr;
+  unsigned char *u = y_ptr, *v; int i, step = y_stride;
+
+  for (i = 0; i < 3; i++, u += 4) {
+    v = u + step * 8;
+
+    TRANSPOSE_16X8
+    // calculate filter mask and high edge variance
+    LFV_FILTER_MASK_HEV_MASK
+    // start work on filters
+    v6 = p0; v0 = q0;
+    B_FILTER(2)
+    // transpose and write back - only work on q1, q0, p0, p1
+    BV_TRANSPOSE
+    BV_WRITEBACK(v + 2, v5, v1)
+    BV_WRITEBACK(u + 2, v6, v2)
+  }
+
+  if (u_ptr) {
+    u = u_ptr; v = v_ptr; step = uv_stride;
+
+    TRANSPOSE_16X8
+    // calculate filter mask and high edge variance
+    LFV_FILTER_MASK_HEV_MASK
+    // start work on filters
+    v6 = p0; v0 = q0;
+    B_FILTER(2)
+    // transpose and write back - only work on q1, q0, p0, p1
+    BV_TRANSPOSE
+    BV_WRITEBACK(v + 2, v5, v1)
+    BV_WRITEBACK(u + 2, v6, v2)
+  }
+}
+
+#define MBV_WRITEBACK \
+  _mm_storel_epi64((__m128i*)u, v0); \
+  _mm_storel_epi64((__m128i*)(u + step), _mm_bsrli_si128(v0, 8)); \
+  _mm_storel_epi64((__m128i*)(u + step * 2), v6); \
+  _mm_storel_epi64((__m128i*)(u + step * 3), _mm_bsrli_si128(v6, 8)); \
+  v0 = _mm_unpacklo_epi32(v3, v2);                /* 57 56 55 54 53 52 51 50 47 46 45 44 43 42 41 40 */ \
+  v3 = _mm_unpackhi_epi32(v3, v2);                /* 77 76 75 74 73 72 71 70 67 66 65 64 63 62 61 60 */ \
+  _mm_storel_epi64((__m128i*)(u + step * 4), v0); \
+  _mm_storel_epi64((__m128i*)(u + step * 5), _mm_bsrli_si128(v0, 8)); \
+  _mm_storel_epi64((__m128i*)(u + step * 6), v3); \
+  _mm_storel_epi64((__m128i*)(u + step * 7), _mm_bsrli_si128(v3, 8)); \
+  v7 = _mm_unpackhi_epi8(q0, q1);                 /* f5 f4 e5 e4 d5 d4 c5 c4 b5 b4 a5 a4 95 94 85 84 */ \
+  v5 = _mm_unpackhi_epi8(v5, q3);                 /* f7 f6 e7 e6 d7 d6 c7 c6 b7 b6 a7 a6 97 96 87 86 */ \
+  v0 = _mm_unpacklo_epi16(v7, v5);                /* b7 b6 b4 b4 a7 a6 a5 a4 97 96 95 94 87 86 85 84 */ \
+  v7 = _mm_unpackhi_epi16(v7, v5);                /* f7 f6 f5 f4 e7 e6 e5 e4 d7 d6 d5 d4 c7 c6 c5 c4 */ \
+  v5 = _mm_unpacklo_epi32(v1, v0);                /* 97 96 95 94 93 92 91 90 87 86 85 83 84 82 81 80 */ \
+  v1 = _mm_unpackhi_epi32(v1, v0);                /* b7 b6 b5 b4 b3 b2 b1 b0 a7 a6 a5 a4 a3 a2 a1 a0 */ \
+  _mm_storel_epi64((__m128i*)v, v5); \
+  _mm_storel_epi64((__m128i*)(v + step), _mm_bsrli_si128(v5, 8)); \
+  _mm_storel_epi64((__m128i*)(v + step * 2), v1); \
+  _mm_storel_epi64((__m128i*)(v + step * 3), _mm_bsrli_si128(v1, 8)); \
+  v1 = _mm_unpacklo_epi32(v4, v7);                /* d7 d6 d5 d4 d3 d2 d1 d0 c7 c6 c5 c4 c3 c2 c1 c0 */ \
+  v4 = _mm_unpackhi_epi32(v4, v7);                /* f7 f6 f4 f4 f3 f2 f1 f0 e7 e6 e5 e4 e3 e2 e1 e0 */ \
+  _mm_storel_epi64((__m128i*)(v + step * 4), v1); \
+  _mm_storel_epi64((__m128i*)(v + step * 5), _mm_bsrli_si128(v1, 8)); \
+  _mm_storel_epi64((__m128i*)(v + step * 6), v4); \
+  _mm_storel_epi64((__m128i*)(v + step * 7), _mm_bsrli_si128(v4, 8));
+
+void vp8_loop_filter_mbv_sse2(unsigned char *y_ptr, unsigned char *u_ptr,
+                              unsigned char *v_ptr, int y_stride, int uv_stride,
+                              loop_filter_info *lfi) {
+  __m128i v0, v1, v2, v3, v4, v5, v6, v7, p0, p1, p2, p3, q0, q1, q2, q3, t0, t1;
+  const unsigned char *blimit = lfi->mblim, *limit = lfi->lim, *thresh = lfi->hev_thr;
+  unsigned char *u = y_ptr - 4, *v = u + y_stride * 8; int step = y_stride;
+
+  TRANSPOSE_16X8
+  // calculate filter mask and high edge variance
+  LFV_FILTER_MASK_HEV_MASK
+  // start work on filters
+  v6 = p0; v0 = q0;
+  MB_FILTER_AND_WRITEBACK(2)
+  p1 = v4; p0 = v6; q0 = v3; q1 = v1;
+  MBV_TRANSPOSE
+  MBV_WRITEBACK
+
+  if (u_ptr) {
+    u = u_ptr - 4; v = v_ptr - 4; step = uv_stride;
+
+    TRANSPOSE_16X8
+    // calculate filter mask and high edge variance
+    LFV_FILTER_MASK_HEV_MASK
+    // start work on filters
+    v6 = p0; v0 = q0;
+    MB_FILTER_AND_WRITEBACK(2)
+    p1 = v4; p0 = v6; q0 = v3; q1 = v1;
+    MBV_TRANSPOSE
+    MBV_WRITEBACK
+  }
+}
+
+void vp8_loop_filter_simple_horizontal_edge_sse2(
+    unsigned char *ptr, int step, const unsigned char *blimit) {
+  __m128i v0, v1, v2, v3, v4, v5, v6, v7;
+
+  // calculate mask
+  v0 = _mm_load_si128((__m128i*)(ptr + step));      // q1
+  v1 = _mm_load_si128((__m128i*)(ptr - step * 2));  // p1
+  v2 = v1;
+  v3 = v0;
+  v0 = _mm_subs_epu8(v0, v1);  // q1-=p1
+  v1 = _mm_subs_epu8(v1, v3);  // p1-=q1
+  v1 = _mm_or_si128(v1, v0);   // abs(p1-q1)
+  v1 = _mm_and_si128(v1, _mm_set1_epi8(-2)); // set lsb of each byte to zero
+  v1 = _mm_srli_epi16(v1, 1);  // abs(p1-q1)/2
+
+  v7 = _mm_load_si128((const __m128i*)blimit);
+
+  v5 = _mm_load_si128((__m128i*)(ptr - step)); // p0
+  v4 = _mm_load_si128((__m128i*)ptr);          // q0
+  v0 = v4;                     // q0
+  v6 = v5;                     // p0
+  v5 = _mm_subs_epu8(v5, v4);  // p0-=q0
+  v4 = _mm_subs_epu8(v4, v6);  // q0-=p0
+  v5 = _mm_or_si128(v5, v4);   // abs(p0 - q0)
+
+  v4 = _mm_set1_epi8(0x80);
+
+  v5 = _mm_adds_epu8(v5, v5);              // abs(p0-q0)*2
+  v5 = _mm_adds_epu8(v5, v1);              // abs (p0 - q0) *2 + abs(p1-q1)/2
+  v5 = _mm_subs_epu8(v5, v7);              // abs(p0 - q0) *2 + abs(p1-q1)/2  > blimit
+  v7 = _mm_setzero_si128();
+  v5 = _mm_cmpeq_epi8(v5, v7);
+
+  // start work on filters
+  v2 = _mm_xor_si128(v2, v4);              // p1 offset to convert to signed values
+  v3 = _mm_xor_si128(v3, v4);              // q1 offset to convert to signed values
+  v2 = _mm_subs_epi8(v2, v3);              // p1 - q1
+
+  v6 = _mm_xor_si128(v6, v4);              // offset to convert to signed values
+  v0 = _mm_xor_si128(v0, v4);              // offset to convert to signed values
+  v3 = v0;                                 // q0
+  v0 = _mm_subs_epi8(v0, v6);              // q0 - p0
+  v2 = _mm_adds_epi8(v2, v0);              // p1 - q1 + 1 * (q0 - p0)
+  v2 = _mm_adds_epi8(v2, v0);              // p1 - q1 + 2 * (q0 - p0)
+  v2 = _mm_adds_epi8(v2, v0);              // p1 - q1 + 3 * (q0 - p0)
+  v5 = _mm_and_si128(v5, v2);              // mask filter values we don't care about
+
+  v0 = v5;
+  v5 = _mm_adds_epi8(v5, _mm_set1_epi8(3));  //  3* (q0 - p0) + (p1 - q1) + 4
+  v0 = _mm_adds_epi8(v0, _mm_set1_epi8(4));  // +3 instead of +4
+
+  v1 = _mm_set1_epi8(0xe0);
+  v2 = _mm_set1_epi8(0x1f);
+
+  v7 = _mm_cmpgt_epi8(v7, v0);             // save sign
+  v7 = _mm_and_si128(v7, v1);              // preserve the upper 3 bits
+  v0 = _mm_srli_epi16(v0, 3);
+  v0 = _mm_and_si128(v0, v2);              // clear out upper 3 bits
+  v0 = _mm_or_si128(v0, v7);               // add sign
+  v3 = _mm_subs_epi8(v3, v0);              // q0-= q0sz add
+
+  v7 = _mm_setzero_si128();
+  v7 = _mm_cmpgt_epi8(v7, v5);             // save sign
+  v7 = _mm_and_si128(v7, v1);              // preserve the upper 3 bits
+  v5 = _mm_srli_epi16(v5, 3);
+  v5 = _mm_and_si128(v5, v2);              // clear out upper 3 bits
+  v5 = _mm_or_si128(v5, v7);               // add sign
+  v6 = _mm_adds_epi8(v6, v5);              // p0+= p0 add
+
+  // unoffset
+  v3 = _mm_xor_si128(v3, v4);
+  v6 = _mm_xor_si128(v6, v4); 
+  // write back
+  _mm_store_si128((__m128i*)ptr, v3);
+  _mm_store_si128((__m128i*)(ptr - step), v6);
+}
+
+void vp8_loop_filter_simple_vertical_edge_sse2(
+    unsigned char *ptr, int step, const unsigned char *blimit) {
+  __m128i v0, v1, v2, v3, v4, v5, v6, v7, t0, t1;
+
+  ptr -= 2;
+  v0 = _mm_cvtsi32_si128(*(int32_t*)ptr);               // (high 96 bits unused) 03 02 01 00
+  v1 = _mm_cvtsi32_si128(*(int32_t*)(ptr + step * 4));  // (high 96 bits unused) 43 42 41 40
+  v2 = _mm_cvtsi32_si128(*(int32_t*)(ptr + step));      // 13 12 11 10
+  v3 = _mm_cvtsi32_si128(*(int32_t*)(ptr + step * 5));  // 53 52 51 50
+  v0 = _mm_unpacklo_epi32(v0, v1);                      // (high 64 bits unused) 43 42 41 40 03 02 01 00
+  v2 = _mm_unpacklo_epi32(v2, v3);                      // 53 52 51 50 13 12 11 10
+  v4 = _mm_cvtsi32_si128(*(int32_t*)(ptr + step * 2));  // 23 22 21 20
+  v5 = _mm_cvtsi32_si128(*(int32_t*)(ptr + step * 6));  // 63 62 61 60
+  v6 = _mm_cvtsi32_si128(*(int32_t*)(ptr + step * 3));  // 33 32 31 30
+  v7 = _mm_cvtsi32_si128(*(int32_t*)(ptr + step * 7));  // 73 72 71 70
+  v4 = _mm_unpacklo_epi32(v4, v5);                      // 63 62 61 60 23 22 21 20
+  v6 = _mm_unpacklo_epi32(v6, v7);                      // 73 72 71 70 33 32 31 30
+  v0 = _mm_unpacklo_epi8(v0, v2);                       // 53 43 52 42 51 41 50 40 13 03 12 02 11 01 10 00
+  v4 = _mm_unpacklo_epi8(v4, v6);                       // 73 63 72 62 71 61 70 60 33 23 32 22 31 21 30 20
+
+  v1 = v0;
+  v0 = _mm_unpacklo_epi16(v0, v4);                      // 33 23 13 03 32 22 12 02 31 21 11 01 30 20 10 00
+  v1 = _mm_unpackhi_epi16(v1, v4);                      // 73 63 53 43 72 62 52 42 71 61 51 41 70 60 50 40
+  v2 = v0;
+  v0 = _mm_unpacklo_epi32(v0, v1);                      // 71 61 51 41 31 21 11 01 70 60 50 40 30 20 10 00
+  v2 = _mm_unpackhi_epi32(v2, v1);                      // 73 63 53 43 33 23 13 03 72 62 52 42 32 22 12 02
+
+  ptr += step * 8;
+  v4 = _mm_cvtsi32_si128(*(int32_t*)ptr);               // 83 82 81 80
+  v1 = _mm_cvtsi32_si128(*(int32_t*)(ptr + step * 4));  // c3 c2 c1 c0
+  v6 = _mm_cvtsi32_si128(*(int32_t*)(ptr + step));      // 93 92 91 90
+  v3 = _mm_cvtsi32_si128(*(int32_t*)(ptr + step * 5));  // d3 d2 d1 d0
+  v4 = _mm_unpacklo_epi32(v4, v1);                      // c3 c2 c1 c0 83 82 81 80
+  v6 = _mm_unpacklo_epi32(v6, v3);                      // d3 d2 d1 d0 93 92 91 90
+  v1 = _mm_cvtsi32_si128(*(int32_t*)(ptr + step * 2));  // a3 a2 a1 a0
+  v5 = _mm_cvtsi32_si128(*(int32_t*)(ptr + step * 6));  // e3 e2 e1 e0
+  v3 = _mm_cvtsi32_si128(*(int32_t*)(ptr + step * 3));  // b3 b2 b1 b0
+  v7 = _mm_cvtsi32_si128(*(int32_t*)(ptr + step * 7));  // f3 f2 f1 f0
+  v1 = _mm_unpacklo_epi32(v1, v5);                      // e3 e2 e1 e0 a3 a2 a1 a0
+  v3 = _mm_unpacklo_epi32(v3, v7);                      // f3 f2 f1 f0 b3 b2 b1 b0
+  v4 = _mm_unpacklo_epi8(v4, v6);                       // d3 c3 d2 c2 d1 c1 d0 c0 93 83 92 82 91 81 90 80
+  v1 = _mm_unpacklo_epi8(v1, v3);                       // f3 e3 f2 e2 f1 e1 f0 e0 b3 a3 b2 a2 b1 a1 b0 a0
+
+  v7 = v4;
+  v4 = _mm_unpacklo_epi16(v4, v1);                      // b3 a3 93 83 b2 a2 92 82 b1 a1 91 81 b0 a0 90 80
+  v7 = _mm_unpackhi_epi16(v7, v1);                      // f3 e3 d3 c3 f2 e2 d2 c2 f1 e1 d1 c1 f0 e0 d0 c0
+  v6 = v4;
+  v4 = _mm_unpacklo_epi32(v4, v7);                      // f1 e1 d1 c1 b1 a1 91 81 f0 e0 d0 c0 b0 a0 90 80
+  v6 = _mm_unpackhi_epi32(v6, v7);                      // f3 e3 d3 c3 b3 a3 93 83 f2 e2 d2 c2 b2 a2 92 82
+
+  v1 = v0;
+  v3 = v2;
+  v0 = _mm_unpacklo_epi64(v0, v4);                      // p1  f0 e0 d0 c0 b0 a0 90 80 70 60 50 40 30 20 10 00
+  v1 = _mm_unpackhi_epi64(v1, v4);                      // p0  f1 e1 d1 c1 b1 a1 91 81 71 61 51 41 31 21 11 01
+  v2 = _mm_unpacklo_epi64(v2, v6);                      // q0  f2 e2 d2 c2 b2 a2 92 82 72 62 52 42 32 22 12 02
+  v3 = _mm_unpackhi_epi64(v3, v6);                      // q1  f3 e3 d3 c3 b3 a3 93 83 73 63 53 43 33 23 13 03
+
+  // calculate mask
+  v6 = v0;                     // p1
+  v7 = v3;                     // q1
+  v7 = _mm_subs_epu8(v7, v0);  // q1-=p1
+  v6 = _mm_subs_epu8(v6, v3);  // p1-=q1
+  v6 = _mm_or_si128(v6, v7);   // abs(p1-q1)
+  v6 = _mm_and_si128(v6, _mm_set1_epi8(-2));  // set lsb of each byte to zero
+  v6 = _mm_srli_epi16(v6, 1);  // abs(p1-q1)/2
+
+  v7 = _mm_load_si128((const __m128i*)blimit);
+
+  v5 = v1;                     // p0
+  v4 = v2;                     // q0
+  v5 = _mm_subs_epu8(v5, v2);  // p0-=q0
+  v4 = _mm_subs_epu8(v4, v1);  // q0-=p0
+  v5 = _mm_or_si128(v5, v4);   // abs(p0 - q0)
+  v5 = _mm_adds_epu8(v5, v5);  // abs(p0-q0)*2
+  v5 = _mm_adds_epu8(v5, v6);  // abs (p0 - q0) *2 + abs(p1-q1)/2
+
+  v4 = _mm_set1_epi8(0x80);
+  v5 = _mm_subs_epu8(v5, v7);  // abs(p0 - q0) *2 + abs(p1-q1)/2  > blimit
+  v7 = _mm_setzero_si128();
+  v5 = _mm_cmpeq_epi8(v5, v7); // mm5 = mask
+
+  // start work on filters
+  t0 = v0;
+  t1 = v3;
+  v0 = _mm_xor_si128(v0, v4);                  // p1 offset to convert to signed values
+  v3 = _mm_xor_si128(v3, v4);                  // q1 offset to convert to signed values
+  v0 = _mm_subs_epi8(v0, v3);                  // p1 - q1
+  v1 = _mm_xor_si128(v1, v4);                  // offset to convert to signed values
+  v2 = _mm_xor_si128(v2, v4);                  // offset to convert to signed values
+
+  v3 = v2;                                     // offseted ; q0
+  v2 = _mm_subs_epi8(v2, v1);                  // q0 - p0
+  v0 = _mm_adds_epi8(v0, v2);                  // p1 - q1 + 1 * (q0 - p0)
+  v0 = _mm_adds_epi8(v0, v2);                  // p1 - q1 + 2 * (q0 - p0)
+  v0 = _mm_adds_epi8(v0, v2);                  // p1 - q1 + 3 * (q0 - p0)
+  v5 = _mm_and_si128(v5, v0);                  // mask filter values we don't care about
+
+  v0 = v5;
+  v5 = _mm_adds_epi8(v5, _mm_set1_epi8(3));    //  3* (q0 - p0) + (p1 - q1) + 4
+  v0 = _mm_adds_epi8(v0, _mm_set1_epi8(4));    // +3 instead of +4
+
+  v6 = _mm_set1_epi8(0xe0);
+  v2 = _mm_set1_epi8(0x1f);
+
+  v7 = _mm_cmpgt_epi8(v7, v0);                 // save sign
+  v7 = _mm_and_si128(v7, v6);                  // preserve the upper 3 bits
+  v0 = _mm_srli_epi16(v0, 3);
+  v0 = _mm_and_si128(v0, v2);                  // clear out upper 3 bits
+  v0 = _mm_or_si128(v0, v7);                   // add sign
+  v3 = _mm_subs_epi8(v3, v0);                  // q0-= q0sz add
+
+  v7 = _mm_setzero_si128();
+  v7 = _mm_cmpgt_epi8(v7, v5);                 // save sign
+  v7 = _mm_and_si128(v7, v6);                  // preserve the upper 3 bits
+  v5 = _mm_srli_epi16(v5, 3);
+  v5 = _mm_and_si128(v5, v2);                  // clear out upper 3 bits
+  v5 = _mm_or_si128(v5, v7);                   // add sign
+  v1 = _mm_adds_epi8(v1, v5);                  // p0+= p0 add
+
+  v3 = _mm_xor_si128(v3, v4);                  // unoffset   q0
+  v1 = _mm_xor_si128(v1, v4);                  // unoffset   p0
+
+  v0 = t0;                             // p1
+  v4 = t1;                             // q1
+
+  // write out order: v0 v2 v1 v3
+  // transpose back to write out
+  // p1  f0 e0 d0 c0 b0 a0 90 80 70 60 50 40 30 20 10 00
+  // p0  f1 e1 d1 c1 b1 a1 91 81 71 61 51 41 31 21 11 01
+  // q0  f2 e2 d2 c2 b2 a2 92 82 72 62 52 42 32 22 12 02
+  // q1  f3 e3 d3 c3 b3 a3 93 83 73 63 53 43 33 23 13 03
+  v6 = v0;
+  v0 = _mm_unpacklo_epi8(v0, v1);                      // 71 70 61 60 51 50 41 40 31 30 21 20 11 10 01 00
+  v6 = _mm_unpackhi_epi8(v6, v1);                      // f1 f0 e1 e0 d1 d0 c1 c0 b1 b0 a1 a0 91 90 81 80
+  v5 = v3;
+  v3 = _mm_unpacklo_epi8(v3, v4);                      // 73 72 63 62 53 52 43 42 33 32 23 22 13 12 03 02
+  v5 = _mm_unpackhi_epi8(v5, v4);                      // f3 f2 e3 e2 d3 d2 c3 c2 b3 b2 a3 a2 93 92 83 82
+  v2 = v0;
+  v0 = _mm_unpacklo_epi16(v0, v3);                     // 33 32 31 30 23 22 21 20 13 12 11 10 03 02 01 00
+  v2 = _mm_unpackhi_epi16(v2, v3);                     // 73 72 71 70 63 62 61 60 53 52 51 50 43 42 41 40
+  v3 = v6;
+  v6 = _mm_unpacklo_epi16(v6, v5);                     // b3 b2 b1 b0 a3 a2 a1 a0 93 92 91 90 83 82 81 80
+  v3 = _mm_unpackhi_epi16(v3, v5);                     // f3 f2 f1 f0 e3 e2 e1 e0 d3 d2 d1 d0 c3 c2 c1 c0
+
+  // write the second 8-line result
+  BV_WRITEBACK(ptr, v6, v3)
+  ptr -= step * 8;
+  // write the first 8-line result
+  BV_WRITEBACK(ptr, v0, v2)
+}
+
+void vp8_loop_filter_bhs_sse2(unsigned char *y_ptr, int y_stride,
+                              const unsigned char *blimit) {
+  vp8_loop_filter_simple_horizontal_edge_sse2(y_ptr + 4 * y_stride, y_stride, blimit);
+  vp8_loop_filter_simple_horizontal_edge_sse2(y_ptr + 8 * y_stride, y_stride, blimit);
+  vp8_loop_filter_simple_horizontal_edge_sse2(y_ptr + 12 * y_stride, y_stride, blimit);
+}
+
+void vp8_loop_filter_bvs_sse2(unsigned char *y_ptr, int y_stride,
+                              const unsigned char *blimit) {
+  vp8_loop_filter_simple_vertical_edge_sse2(y_ptr + 4, y_stride, blimit);
+  vp8_loop_filter_simple_vertical_edge_sse2(y_ptr + 8, y_stride, blimit);
+  vp8_loop_filter_simple_vertical_edge_sse2(y_ptr + 12, y_stride, blimit);
+}
+
diff --git a/vp8/common/e2k/subpixel_e2k.c b/vp8/common/e2k/subpixel_e2k.c
new file mode 100644
index 0000000..4d59486
--- /dev/null
+++ b/vp8/common/e2k/subpixel_e2k.c
@@ -0,0 +1,354 @@
+/*
+ *  Copyright (c) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *  Copyright (c) 2010 The WebM project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "vpx_config.h"
+#include "vp8_rtcd.h"
+#include "vpx_ports/mem.h"
+
+#define REP8(a, b) { a, b, a, b, a, b, a, b,  a, b, a, b, a, b, a, b }
+
+DECLARE_ALIGNED(16, static const uint8_t, k0_k5[][16]) = {
+  REP8(0, 0), REP8(0, 0), REP8(2, 1), REP8(0, 0),
+  REP8(3, 3), REP8(0, 0), REP8(1, 2), REP8(0, 0),
+  /* k1_k3 */
+  REP8( 0,   0), REP8(-6, 12), REP8(-11,  36), REP8(-9,  50),
+  REP8(-16, 77), REP8(-6, 93), REP8( -8, 108), REP8(-1, 123),
+  /* k2_k4 */
+  REP8(128,   0), REP8(123, -1), REP8(108,  -8), REP8(93, -6),
+  REP8( 77, -16), REP8( 50, -9), REP8( 36, -11), REP8(12, -6)
+};
+
+#define FILTER(name) void vp8_filter_##name##_ssse3( \
+    unsigned char *src, unsigned int stride, \
+    unsigned char *out, unsigned int ostride, \
+    unsigned int height, unsigned int filter_index)
+
+/*
+ * Notes: filter_block1d_h6 applies a 6 tap filter horizontally to the input pixels. The
+ * input pixel array has output_height rows. This routine assumes that output_height is an
+ * even number. This function handles 8 pixels in horizontal direction, calculating ONE
+ * rows each iteration to take advantage of the 128 bits operations.
+ *
+ * This is an implementation of some of the SSE optimizations first seen in ffvp8
+ */
+
+FILTER(block1d8_h6) {
+  __m128i v0, v1, v2, v4, v5, v6; 
+  __m128i shuf2bfrom1 = _mm_setr_epi8(4, 8, 6, 1, 8, 3, 1, 5, 3, 7, 5, 9, 7,11, 9,13);
+  __m128i shuf3bfrom1 = _mm_setr_epi8(2, 6, 4, 8, 6, 1, 8, 3, 1, 5, 3, 7, 5, 9, 7,11);
+  __m128i rd = _mm_set1_epi16(0x40);
+  const uint8_t *kptr = k0_k5[filter_index];
+  v4 = _mm_load_si128((const __m128i*)kptr);          // k0_k5
+  v5 = _mm_load_si128((const __m128i*)(kptr + 256));  // k2_k4
+  v6 = _mm_load_si128((const __m128i*)(kptr + 128));  // k1_k3
+  if (_mm_cvtsi128_si64(v4)) {
+    do {
+      v0 = _mm_loadl_epi64((const __m128i*)(src - 2));  // -2 -1  0  1  2  3  4  5
+      v2 = _mm_loadl_epi64((const __m128i*)(src + 3));  //  3  4  5  6  7  8  9 10
+      v2 = _mm_unpacklo_epi8(v0, v2);                   // -2  3 -1  4  0  5  1  6  2  7  3  8  4  9  5 10
+      v0 = _mm_maddubs_epi16(v2, v4);
+      v1 = _mm_shuffle_epi8(v2, shuf2bfrom1);
+      v2 = _mm_shuffle_epi8(v2, shuf3bfrom1);
+      v1 = _mm_maddubs_epi16(v1, v5);
+      v2 = _mm_maddubs_epi16(v2, v6);
+      v0 = _mm_adds_epi16(v0, v1);
+      v2 = _mm_adds_epi16(v2, rd);
+      v0 = _mm_adds_epi16(v0, v2);
+      v0 = _mm_srai_epi16(v0, 7);
+      v0 = _mm_packus_epi16(v0, v0);
+      _mm_storel_epi64((__m128i*)out, v0);
+      src += stride; out += ostride;
+    } while (--height);
+  } else { /* block1d8_h4 */
+    do {
+      v0 = _mm_loadl_epi64((const __m128i*)(src - 2));  // -2 -1  0  1  2  3  4  5
+      v1 = _mm_loadl_epi64((const __m128i*)(src + 3));  //  3  4  5  6  7  8  9 10
+      v2 = _mm_unpacklo_epi8(v0, v1);                   // -2  3 -1  4  0  5  1  6  2  7  3  8  4  9  5 10
+      v0 = _mm_shuffle_epi8(v2, shuf2bfrom1);
+      v2 = _mm_shuffle_epi8(v2, shuf3bfrom1);
+      v0 = _mm_maddubs_epi16(v0, v5);
+      v2 = _mm_maddubs_epi16(v2, v6);
+      v0 = _mm_adds_epi16(v0, rd);
+      v0 = _mm_adds_epi16(v0, v2);
+      v0 = _mm_srai_epi16(v0, 7);
+      v0 = _mm_packus_epi16(v0, v0);
+      _mm_storel_epi64((__m128i*)out, v0);
+      src += stride; out += ostride;
+    } while (--height);
+  }
+}
+
+FILTER(block1d16_h6) {
+  __m128i v0, v1, v2, v3, v4, v5, v6, v7; 
+  __m128i shuf2bfrom1 = _mm_setr_epi8(4, 8, 6, 1, 8, 3, 1, 5, 3, 7, 5, 9, 7,11, 9,13);
+  __m128i shuf3bfrom1 = _mm_setr_epi8(2, 6, 4, 8, 6, 1, 8, 3, 1, 5, 3, 7, 5, 9, 7,11);
+  __m128i rd = _mm_set1_epi16(0x40);
+  const uint8_t *kptr = k0_k5[filter_index];
+  v4 = _mm_load_si128((const __m128i*)kptr);          // k0_k5
+  v5 = _mm_load_si128((const __m128i*)(kptr + 256));  // k2_k4
+  v6 = _mm_load_si128((const __m128i*)(kptr + 128));  // k1_k3
+  do {
+    v0 = _mm_loadl_epi64((const __m128i*)(src - 2));  // -2 -1  0  1  2  3  4  5
+    v3 = _mm_loadl_epi64((const __m128i*)(src + 3));  //  3  4  5  6  7  8  9 10
+    v2 = _mm_unpacklo_epi8(v0, v3);                   // -2  3 -1  4  0  5  1  6  2  7  3  8  4  9  5 10
+    v0 = _mm_maddubs_epi16(v2, v4);
+    v1 = _mm_shuffle_epi8(v2, shuf2bfrom1);
+    v2 = _mm_shuffle_epi8(v2, shuf3bfrom1);
+    v3 = _mm_loadl_epi64((const __m128i*)(src + 6));
+    v1 = _mm_maddubs_epi16(v1, v5);
+    v7 = _mm_loadl_epi64((const __m128i*)(src + 11));
+    v2 = _mm_maddubs_epi16(v2, v6);
+    v3 = _mm_unpacklo_epi8(v3, v7);
+    v0 = _mm_adds_epi16(v0, v1);
+    v1 = v3;
+    v3 = _mm_maddubs_epi16(v3, v4);
+    v0 = _mm_adds_epi16(v0, v2);
+    v2 = v1;
+    v0 = _mm_adds_epi16(v0, rd);
+    v1 = _mm_shuffle_epi8(v1, shuf2bfrom1);
+    v2 = _mm_shuffle_epi8(v2, shuf3bfrom1);
+    v0 = _mm_srai_epi16(v0, 7);
+    v1 = _mm_maddubs_epi16(v1, v5);
+    v2 = _mm_maddubs_epi16(v2, v6);
+    v3 = _mm_adds_epi16(v3, v1);
+    v3 = _mm_adds_epi16(v3, v2);
+    v3 = _mm_adds_epi16(v3, rd);
+    v3 = _mm_srai_epi16(v3, 7);
+    v0 = _mm_packus_epi16(v0, v3);
+    _mm_store_si128((__m128i*)out, v0);
+    src += stride; out += ostride;
+  } while (--height);
+}
+
+FILTER(block1d4_h6) {
+  __m128i v0, v1, v2, v4, v5, v6;
+  __m128i shuf2b = _mm_setr_epi8(2, 4, 3, 5, 4, 6, 5, 7, 6, 8, 7, 9, 8, 10, 9, 11);
+  __m128i shuf3b = _mm_setr_epi8(1, 3, 2, 4, 3, 5, 4, 6, 5, 7, 6, 8, 7, 9, 8, 10);
+  __m128i rd = _mm_set1_epi16(0x40);
+  const uint8_t *kptr = k0_k5[filter_index];
+  v4 = _mm_load_si128((const __m128i*)kptr);          // k0_k5
+  v5 = _mm_load_si128((const __m128i*)(kptr + 256));  // k2_k4
+  v6 = _mm_load_si128((const __m128i*)(kptr + 128));  // k1_k3
+  if (_mm_cvtsi128_si64(v4)) {
+    __m128i shuf1b = _mm_setr_epi8(0, 5, 1, 6, 2, 7, 3, 8, 4, 9, 5, 10, 6, 11, 7, 12);
+    do {
+      v2 = _mm_loadu_si128((const __m128i*)(src - 2));
+      v0 = _mm_shuffle_epi8(v2, shuf1b);
+      v1 = _mm_shuffle_epi8(v2, shuf2b);
+      v0 = _mm_maddubs_epi16(v0, v4);
+      v2 = _mm_shuffle_epi8(v2, shuf3b);
+      v1 = _mm_maddubs_epi16(v1, v5);
+      v2 = _mm_maddubs_epi16(v2, v6);
+      v0 = _mm_adds_epi16(v0, v1);
+      v0 = _mm_adds_epi16(v0, rd);
+      v0 = _mm_adds_epi16(v0, v2);
+      v0 = _mm_srai_epi16(v0, 7);
+      v0 = _mm_packus_epi16(v0, v0);
+      *(int32_t*)out = _mm_cvtsi128_si32(v0);
+      src += stride; out += ostride;
+    } while (--height);
+  } else { /* block1d4_h4 */
+    do {
+      v2 = _mm_loadu_si128((const __m128i*)(src - 2));
+      v1 = _mm_shuffle_epi8(v2, shuf2b);
+      v2 = _mm_shuffle_epi8(v2, shuf3b);
+      v1 = _mm_maddubs_epi16(v1, v5);
+      v2 = _mm_maddubs_epi16(v2, v6);
+      v1 = _mm_adds_epi16(v1, rd);
+      v1 = _mm_adds_epi16(v1, v2);
+      v1 = _mm_srai_epi16(v1, 7);
+      v1 = _mm_packus_epi16(v1, v1);
+      *(int32_t*)out = _mm_cvtsi128_si32(v1);
+      src += stride; out += ostride;
+    } while (--height);
+  }
+}
+
+FILTER(block1d16_v6) {
+  __m128i v0, v1, v2, v3, v4, v5, v6, v7;
+  __m128i rd = _mm_set1_epi16(0x40);
+  const uint8_t *kptr = k0_k5[filter_index];
+  v5 = _mm_load_si128((const __m128i*)kptr);           // k0_k5
+  v6 = _mm_load_si128((const __m128i*)(kptr + 256));   // k2_k4
+  v7 = _mm_load_si128((const __m128i*)(kptr + 128));   // k1_k3
+  if (_mm_cvtsi128_si64(v5)) {
+    do {
+      v1 = _mm_loadl_epi64((const __m128i*)src);                 // A
+      v2 = _mm_loadl_epi64((const __m128i*)(src + stride));      // B
+      v3 = _mm_loadl_epi64((const __m128i*)(src + stride * 2));  // C
+      v4 = _mm_loadl_epi64((const __m128i*)(src + stride * 3));  // D
+      v0 = _mm_loadl_epi64((const __m128i*)(src + stride * 4));  // E
+      v2 = _mm_unpacklo_epi8(v2, v4);                  // B D
+      v3 = _mm_unpacklo_epi8(v3, v0);                  // C E
+      v0 = _mm_loadl_epi64((const __m128i*)(src + stride * 5));  // F
+      v3 = _mm_maddubs_epi16(v3, v6);
+      v1 = _mm_unpacklo_epi8(v1, v0);                  // A F
+      v2 = _mm_maddubs_epi16(v2, v7);
+      v1 = _mm_maddubs_epi16(v1, v5);
+      v2 = _mm_adds_epi16(v2, v3);
+      v2 = _mm_adds_epi16(v2, v1);
+      v2 = _mm_adds_epi16(v2, rd);
+      v2 = _mm_srai_epi16(v2, 7);
+      v2 = _mm_packus_epi16(v2, v2);
+      _mm_storel_epi64((__m128i*)out, v2);
+      v1 = _mm_loadl_epi64((const __m128i*)(src + 8));               // A
+      v2 = _mm_loadl_epi64((const __m128i*)(src + stride + 8));      // B
+      v3 = _mm_loadl_epi64((const __m128i*)(src + stride * 2 + 8));  // C
+      v4 = _mm_loadl_epi64((const __m128i*)(src + stride * 3 + 8));  // D
+      v0 = _mm_loadl_epi64((const __m128i*)(src + stride * 4 + 8));  // E
+      v2 = _mm_unpacklo_epi8(v2, v4);                  // B D
+      v3 = _mm_unpacklo_epi8(v3, v0);                  // C E
+      v0 = _mm_loadl_epi64((const __m128i*)(src + stride * 5 + 8));  // F
+      v3 = _mm_maddubs_epi16(v3, v6);
+      v1 = _mm_unpacklo_epi8(v1, v0);                  // A F
+      v2 = _mm_maddubs_epi16(v2, v7);
+      v1 = _mm_maddubs_epi16(v1, v5);
+      v2 = _mm_adds_epi16(v2, v3);
+      v2 = _mm_adds_epi16(v2, v1);
+      v2 = _mm_adds_epi16(v2, rd);
+      v2 = _mm_srai_epi16(v2, 7);
+      v2 = _mm_packus_epi16(v2, v2);
+      _mm_storel_epi64((__m128i*)(out + 8), v2);
+      src += stride; out += ostride;
+    } while (--height);
+  } else { /* block1d16_v4 */
+    do {
+      v2 = _mm_loadl_epi64((const __m128i*)(src + stride));      // B
+      v3 = _mm_loadl_epi64((const __m128i*)(src + stride * 2));  // C
+      v4 = _mm_loadl_epi64((const __m128i*)(src + stride * 3));  // D
+      v0 = _mm_loadl_epi64((const __m128i*)(src + stride * 4));  // E
+      v2 = _mm_unpacklo_epi8(v2, v4);                  // B D
+      v3 = _mm_unpacklo_epi8(v3, v0);                  // C E
+      v3 = _mm_maddubs_epi16(v3, v6);
+      v2 = _mm_maddubs_epi16(v2, v7);
+      v5 = _mm_loadl_epi64((const __m128i*)(src + stride + 8));      // B
+      v1 = _mm_loadl_epi64((const __m128i*)(src + stride * 2 + 8));  // C
+      v4 = _mm_loadl_epi64((const __m128i*)(src + stride * 3 + 8));  // D
+      v0 = _mm_loadl_epi64((const __m128i*)(src + stride * 4 + 8));  // E
+      v2 = _mm_adds_epi16(v2, rd);
+      v2 = _mm_adds_epi16(v2, v3);
+      v2 = _mm_srai_epi16(v2, 7);
+      v2 = _mm_packus_epi16(v2, v2);
+      v5 = _mm_unpacklo_epi8(v5, v4);                  // B D
+      v1 = _mm_unpacklo_epi8(v1, v0);                  // C E
+      v1 = _mm_maddubs_epi16(v1, v6);
+      v5 = _mm_maddubs_epi16(v5, v7);
+      v5 = _mm_adds_epi16(v5, v1);
+      v5 = _mm_adds_epi16(v5, rd);
+      v5 = _mm_srai_epi16(v5, 7);
+      v5 = _mm_packus_epi16(v5, v5);
+      v2 = _mm_unpacklo_epi64(v2, v5);
+      _mm_store_si128((__m128i*)out, v2);
+      src += stride; out += ostride;
+    } while (--height);
+  }
+}
+
+FILTER(block1d8_v6) {
+  __m128i v0, v1, v2, v3, v4, v5, v6, v7;
+  __m128i rd = _mm_set1_epi16(0x40);
+  const uint8_t *kptr = k0_k5[filter_index];
+  v5 = _mm_load_si128((const __m128i*)kptr);           // k0_k5
+  v6 = _mm_load_si128((const __m128i*)(kptr + 256));   // k2_k4
+  v7 = _mm_load_si128((const __m128i*)(kptr + 128));   // k1_k3
+  if (_mm_cvtsi128_si64(v5)) {
+    do {
+      v1 = _mm_loadl_epi64((const __m128i*)src);                 // A
+      v2 = _mm_loadl_epi64((const __m128i*)(src + stride));      // B
+      v3 = _mm_loadl_epi64((const __m128i*)(src + stride * 2));  // C
+      v4 = _mm_loadl_epi64((const __m128i*)(src + stride * 3));  // D
+      v0 = _mm_loadl_epi64((const __m128i*)(src + stride * 4));  // E
+      v2 = _mm_unpacklo_epi8(v2, v4);                  // B D
+      v3 = _mm_unpacklo_epi8(v3, v0);                  // C E
+      v0 = _mm_loadl_epi64((const __m128i*)(src + stride * 5));  // F
+      v3 = _mm_maddubs_epi16(v3, v6);
+      v1 = _mm_unpacklo_epi8(v1, v0);                  // A F
+      v2 = _mm_maddubs_epi16(v2, v7);
+      v1 = _mm_maddubs_epi16(v1, v5);
+      v2 = _mm_adds_epi16(v2, v3);
+      v2 = _mm_adds_epi16(v2, v1);
+      v2 = _mm_adds_epi16(v2, rd);
+      v2 = _mm_srai_epi16(v2, 7);
+      v2 = _mm_packus_epi16(v2, v2);
+      _mm_storel_epi64((__m128i*)out, v2);
+      src += stride; out += ostride;
+    } while (--height);
+  } else {
+    do {
+      v2 = _mm_loadl_epi64((const __m128i*)(src + stride));      // B
+      v3 = _mm_loadl_epi64((const __m128i*)(src + stride * 2));  // C
+      v4 = _mm_loadl_epi64((const __m128i*)(src + stride * 3));  // D
+      v0 = _mm_loadl_epi64((const __m128i*)(src + stride * 4));  // E
+      v2 = _mm_unpacklo_epi8(v2, v4);                  // B D
+      v3 = _mm_unpacklo_epi8(v3, v0);                  // C E
+      v3 = _mm_maddubs_epi16(v3, v6);
+      v2 = _mm_maddubs_epi16(v2, v7);
+      v2 = _mm_adds_epi16(v2, v3);
+      v2 = _mm_adds_epi16(v2, rd);
+      v2 = _mm_srai_epi16(v2, 7);
+      v2 = _mm_packus_epi16(v2, v2);
+      _mm_storel_epi64((__m128i*)out, v2);
+      src += stride; out += ostride;
+    } while (--height);
+  }
+}
+
+FILTER(block1d4_v6) {
+  __m64 v0, v1, v2, v3, v4, v5, v6, v7;
+  __m64 rd = _mm_set1_pi16(0x40);
+  const uint8_t *kptr = k0_k5[filter_index];
+  v5 = *(const __m64*)kptr;           // k0_k5
+  v6 = *(const __m64*)(kptr + 256);   // k2_k4
+  v7 = *(const __m64*)(kptr + 128);   // k1_k3
+  if (_mm_cvtm64_si64(v5)) {
+    do {
+      v1 = _mm_cvtsi32_si64(*(const int32_t*)src);                 // A
+      v2 = _mm_cvtsi32_si64(*(const int32_t*)(src + stride));      // B
+      v3 = _mm_cvtsi32_si64(*(const int32_t*)(src + stride * 2));  // C
+      v4 = _mm_cvtsi32_si64(*(const int32_t*)(src + stride * 3));  // D
+      v0 = _mm_cvtsi32_si64(*(const int32_t*)(src + stride * 4));  // E
+      v2 = _mm_unpacklo_pi8(v2, v4);                  // B D
+      v3 = _mm_unpacklo_pi8(v3, v0);                  // C E
+      v0 = _mm_cvtsi32_si64(*(const int32_t*)(src + stride * 5));  // F
+      v3 = _mm_maddubs_pi16(v3, v6);
+      v1 = _mm_unpacklo_pi8(v1, v0);                  // A F
+      v2 = _mm_maddubs_pi16(v2, v7);
+      v1 = _mm_maddubs_pi16(v1, v5);
+      v2 = _mm_adds_pi16(v2, v3);
+      v2 = _mm_adds_pi16(v2, v1);
+      v2 = _mm_adds_pi16(v2, rd);
+      v2 = _mm_srai_pi16(v2, 7);
+      v2 = _mm_packs_pu16(v2, v2);
+      *(int32_t*)out = _mm_cvtsi64_si32(v2);
+      src += stride; out += ostride;
+    } while (--height);
+  } else { /* block1d4_v4 */
+    do {
+      v2 = _mm_cvtsi32_si64(*(const int32_t*)(src + stride));      // B
+      v3 = _mm_cvtsi32_si64(*(const int32_t*)(src + stride * 2));  // C
+      v4 = _mm_cvtsi32_si64(*(const int32_t*)(src + stride * 3));  // D
+      v0 = _mm_cvtsi32_si64(*(const int32_t*)(src + stride * 4));  // E
+      v2 = _mm_unpacklo_pi8(v2, v4);                  // B D
+      v3 = _mm_unpacklo_pi8(v3, v0);                  // C E
+      v3 = _mm_maddubs_pi16(v3, v6);
+      v2 = _mm_maddubs_pi16(v2, v7);
+      v2 = _mm_adds_pi16(v2, v3);
+      v2 = _mm_adds_pi16(v2, rd);
+      v2 = _mm_srai_pi16(v2, 7);
+      v2 = _mm_packs_pu16(v2, v2);
+      *(int32_t*)out = _mm_cvtsi64_si32(v2);
+      src += stride; out += ostride;
+    } while (--height);
+  }
+}
+
diff --git a/vp8/common/x86/idct_blk_mmx.c b/vp8/common/x86/idct_blk_mmx.c
index fd804b1..993cc1b 100644
--- a/vp8/common/x86/idct_blk_mmx.c
+++ b/vp8/common/x86/idct_blk_mmx.c
@@ -15,9 +15,11 @@
 
 extern void vp8_dequantize_b_impl_mmx(short *sq, short *dq, short *q);
 
+#if !VPX_ARCH_E2K
 void vp8_dequantize_b_mmx(BLOCKD *d, short *DQC) {
   short *sq = (short *)d->qcoeff;
   short *dq = (short *)d->dqcoeff;
 
   vp8_dequantize_b_impl_mmx(sq, dq, DQC);
 }
+#endif
diff --git a/vp8/common/x86/idct_blk_sse2.c b/vp8/common/x86/idct_blk_sse2.c
index 897ed5b..664cb3d 100644
--- a/vp8/common/x86/idct_blk_sse2.c
+++ b/vp8/common/x86/idct_blk_sse2.c
@@ -16,6 +16,7 @@ void vp8_idct_dequant_0_2x_sse2(short *q, short *dq, unsigned char *dst,
 void vp8_idct_dequant_full_2x_sse2(short *q, short *dq, unsigned char *dst,
                                    int dst_stride);
 
+#if !VPX_ARCH_E2K
 void vp8_dequant_idct_add_y_block_sse2(short *q, short *dq, unsigned char *dst,
                                        int stride, char *eobs) {
   int i;
@@ -82,3 +83,4 @@ void vp8_dequant_idct_add_uv_block_sse2(short *q, short *dq,
     }
   }
 }
+#endif
diff --git a/vp8/common/x86/loopfilter_x86.c b/vp8/common/x86/loopfilter_x86.c
index cfa13a2..26a6ff2 100644
--- a/vp8/common/x86/loopfilter_x86.c
+++ b/vp8/common/x86/loopfilter_x86.c
@@ -38,7 +38,7 @@ extern loop_filter_uvfunction vp8_mbloop_filter_horizontal_edge_uv_sse2;
 extern loop_filter_uvfunction vp8_mbloop_filter_vertical_edge_uv_sse2;
 
 /* Horizontal MB filtering */
-#if HAVE_SSE2
+#if HAVE_SSE2 && !VPX_ARCH_E2K
 void vp8_loop_filter_mbh_sse2(unsigned char *y_ptr, unsigned char *u_ptr,
                               unsigned char *v_ptr, int y_stride, int uv_stride,
                               loop_filter_info *lfi) {
diff --git a/vp8/common/x86/vp8_asm_stubs.c b/vp8/common/x86/vp8_asm_stubs.c
index 7fb83c2..50af226 100644
--- a/vp8/common/x86/vp8_asm_stubs.c
+++ b/vp8/common/x86/vp8_asm_stubs.c
@@ -74,7 +74,7 @@ extern void vp8_filter_block1d8_v6_only_sse2(unsigned char *src_ptr,
                                              unsigned int output_height,
                                              const short *vp8_filter);
 
-#if HAVE_MMX
+#if HAVE_MMX && !VPX_ARCH_E2K
 void vp8_sixtap_predict4x4_mmx(unsigned char *src_ptr, int src_pixels_per_line,
                                int xoffset, int yoffset, unsigned char *dst_ptr,
                                int dst_pitch) {
@@ -90,7 +90,7 @@ void vp8_sixtap_predict4x4_mmx(unsigned char *src_ptr, int src_pixels_per_line,
 }
 #endif
 
-#if HAVE_SSE2
+#if HAVE_SSE2 && !VPX_ARCH_E2K
 void vp8_sixtap_predict16x16_sse2(unsigned char *src_ptr,
                                   int src_pixels_per_line, int xoffset,
                                   int yoffset, unsigned char *dst_ptr,
diff --git a/vp8/encoder/e2k/block_error_e2k.c b/vp8/encoder/e2k/block_error_e2k.c
new file mode 100644
index 0000000..44f6ffa
--- /dev/null
+++ b/vp8/encoder/e2k/block_error_e2k.c
@@ -0,0 +1,72 @@
+/*
+ *  Copyright (c) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *  Copyright (c) 2010 The WebM project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "vpx_config.h"
+#include "vp8_rtcd.h"
+
+#define RETURN_SUM(v2) \
+  v2 = _mm_add_epi32(v2, _mm_unpackhi_epi64(v2, v2)); \
+  v2 = _mm_add_epi32(v2, _mm_srli_epi64(v2, 32)); \
+  return _mm_cvtsi128_si32(v2);
+
+int vp8_block_error_sse2(short *s_ptr, short *d_ptr) {
+  __m128i v0, v1, v2, v3;
+
+  v0 = _mm_load_si128((__m128i*)s_ptr);
+  v1 = _mm_load_si128((__m128i*)d_ptr);
+  v2 = _mm_load_si128((__m128i*)s_ptr + 1);
+  v3 = _mm_load_si128((__m128i*)d_ptr + 1);
+  v0 = _mm_sub_epi16(v0, v1);
+  v2 = _mm_sub_epi16(v2, v3);
+  v0 = _mm_madd_epi16(v0, v0);
+  v2 = _mm_madd_epi16(v2, v2);
+  v0 = _mm_add_epi32(v0, v2);
+
+  RETURN_SUM(v0)
+}
+
+int vp8_mbblock_error_sse2_impl(short *s_ptr, short *d_ptr, int dc) {
+  __m128i v0, v1, v2, v3, v4 = _mm_setzero_si128(), v5;
+  int i;
+  v5 = _mm_cmpeq_epi16(_mm_cvtsi32_si128(dc), v4);
+
+  for (i = 0; i < 16; i++) {
+    v0 = _mm_load_si128((__m128i*)s_ptr);
+    v1 = _mm_load_si128((__m128i*)d_ptr);
+    v2 = _mm_load_si128((__m128i*)s_ptr + 1); s_ptr += 16;
+    v3 = _mm_load_si128((__m128i*)d_ptr + 1); d_ptr += 16;
+    v0 = _mm_sub_epi16(v0, v1);
+    v2 = _mm_sub_epi16(v2, v3);
+    v0 = _mm_and_si128(v0, v5);
+    v2 = _mm_madd_epi16(v2, v2);
+    v0 = _mm_madd_epi16(v0, v0);
+    v4 = _mm_add_epi32(v4, v2);
+    v4 = _mm_add_epi32(v4, v0);
+  }
+  RETURN_SUM(v4)
+}
+
+int vp8_mbuverror_sse2_impl(short *s_ptr, short *d_ptr) {
+  __m128i v0, v1, v2 = _mm_setzero_si128();
+  int i;
+
+  for (i = 0; i < 16; i++) {
+    v0 = _mm_load_si128((__m128i*)s_ptr); s_ptr += 8;
+    v1 = _mm_load_si128((__m128i*)d_ptr); d_ptr += 8;
+    v0 = _mm_sub_epi16(v0, v1);
+    v0 = _mm_madd_epi16(v0, v0);
+    v2 = _mm_add_epi32(v2, v0);
+  }
+  RETURN_SUM(v2)
+}
+
diff --git a/vp8/encoder/e2k/dct_e2k.c b/vp8/encoder/e2k/dct_e2k.c
new file mode 100644
index 0000000..4342f9e
--- /dev/null
+++ b/vp8/encoder/e2k/dct_e2k.c
@@ -0,0 +1,292 @@
+/*
+ *  Copyright (c) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *  Copyright (c) 2010 The WebM project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "vpx_config.h"
+#include "vp8_rtcd.h"
+
+void vp8_short_fdct4x4_sse2(short *input, short *output, int pitch) {
+  __m128i v0, v1, v2, v3, v4, v5;
+  __m128i c5352_2217 = _mm_set1_epi32(2217 << 16 | 5352);
+  __m128i c2217_n5352 = _mm_set1_epi32(-5352 << 16 | 2217);
+  __m128i c7 = _mm_set1_epi32(7);
+  __m128i c14500 = _mm_set1_epi32(14500);
+  __m128i c7500 = _mm_set1_epi32(7500);
+  __m128i c12000 = _mm_set1_epi32(12000);
+  __m128i c51000 = _mm_set1_epi32(51000);
+
+  __m128i mult_add = _mm_set1_epi16(1);
+  __m128i mult_sub = _mm_set1_epi32(-1 << 16 | 1);
+  __m128i cmp_mask = _mm_setr_epi16(1, 1, 1, 1, 0, 0, 0, 0);
+
+  v0 = _mm_loadl_epi64((const __m128i*)input);                      // 03 02 01 00
+  v2 = _mm_loadl_epi64((const __m128i*)((char*)input + pitch));     // 13 12 11 10
+  v1 = _mm_loadl_epi64((const __m128i*)((char*)input + pitch * 2)); // 23 22 21 20
+  v3 = _mm_loadl_epi64((const __m128i*)((char*)input + pitch * 3)); // 33 32 31 30
+
+  v0 = _mm_unpacklo_epi64(v0, v2);      // 13 12 11 10 03 02 01 00
+  v1 = _mm_unpacklo_epi64(v1, v3);      // 33 32 31 30 23 22 21 20
+
+  v2 = v0;
+  v0 = _mm_unpacklo_epi32(v0, v1);      // 23 22 03 02 21 20 01 00
+  v2 = _mm_unpackhi_epi32(v2, v1);      // 33 32 13 12 31 30 11 10
+  v1 = v0;
+  v0 = _mm_unpacklo_epi32(v0, v2);      // 31 21 30 20 11 10 01 00
+  v1 = _mm_shufflehi_epi16(v1, 0xb1);   // 22 23 02 03 xx xx xx xx
+  v2 = _mm_shufflehi_epi16(v2, 0xb1);   // 32 33 12 13 xx xx xx xx
+
+  v1 = _mm_unpackhi_epi32(v1, v2);      // 32 33 22 23 12 13 02 03
+  v3 = v0;
+  v0 = _mm_add_epi16(v0, v1);           // b1 a1 b1 a1 b1 a1 b1 a1
+  v3 = _mm_sub_epi16(v3, v1);           // c1 d1 c1 d1 c1 d1 c1 d1
+  v0 = _mm_slli_epi16(v0, 3);           // b1 <<= 3 a1 <<= 3
+  v3 = _mm_slli_epi16(v3, 3);           // c1 <<= 3 d1 <<= 3
+
+  v1 = v0;
+  v0 = _mm_madd_epi16(v0, mult_add);    // a1 + b1
+  v1 = _mm_madd_epi16(v1, mult_sub);    // a1 - b1
+  v4 = v3;
+  v3 = _mm_madd_epi16(v3, c5352_2217);  // c1*2217 + d1*5352
+  v4 = _mm_madd_epi16(v4, c2217_n5352); // d1*2217 - c1*5352
+
+  v3 = _mm_add_epi32(v3, c14500);
+  v4 = _mm_add_epi32(v4, c7500);
+  v3 = _mm_srai_epi32(v3, 12);          // (c1 * 2217 + d1 * 5352 +  14500)>>12
+  v4 = _mm_srai_epi32(v4, 12);          // (d1 * 2217 - c1 * 5352 +   7500)>>12
+
+  v0 = _mm_packs_epi32(v0, v1);         // op[2] op[0]
+  v3 = _mm_packs_epi32(v3, v4);         // op[3] op[1]
+
+  // 23 22 21 20 03 02 01 00
+  // 33 32 31 30 13 12 11 10
+
+  v2 = v0;
+  v0 = _mm_unpacklo_epi64(v0, v3);      // 13 12 11 10 03 02 01 00
+  v2 = _mm_unpackhi_epi64(v2, v3);      // 23 22 21 20 33 32 31 30
+
+  v3 = v0;
+  v0 = _mm_unpacklo_epi16(v0, v2);      // 32 30 22 20 12 10 02 00
+  v3 = _mm_unpackhi_epi16(v3, v2);      // 33 31 23 21 13 11 03 01
+  v2 = v0;
+  v0 = _mm_unpacklo_epi16(v0, v3);      // 13 12 11 10 03 02 01 00
+  v2 = _mm_unpackhi_epi16(v2, v3);      // 33 32 31 30 23 22 21 20
+
+  v5 = c7;
+  v2 = _mm_shuffle_epi32(v2, 0x4e);
+  v3 = v0;
+  v0 = _mm_add_epi16(v0, v2);           // b1 b1 b1 b1 a1 a1 a1 a1
+  v3 = _mm_sub_epi16(v3, v2);           // c1 c1 c1 c1 d1 d1 d1 d1
+
+  v0 = _mm_shuffle_epi32(v0, 0xd8);     // b1 b1 a1 a1 b1 b1 a1 a1
+  v2 = v3;                              // save d1 for compare
+  v3 = _mm_shuffle_epi32(v3, 0xd8);     // c1 c1 d1 d1 c1 c1 d1 d1
+  v0 = _mm_shufflelo_epi16(v0, 0xd8);   // b1 b1 a1 a1 b1 a1 b1 a1
+  v3 = _mm_shufflelo_epi16(v3, 0xd8);   // c1 c1 d1 d1 c1 d1 c1 d1
+  v0 = _mm_shufflehi_epi16(v0, 0xd8);   // b1 a1 b1 a1 b1 a1 b1 a1
+  v3 = _mm_shufflehi_epi16(v3, 0xd8);   // c1 d1 c1 d1 c1 d1 c1 d1
+  v1 = v0;
+  v0 = _mm_madd_epi16(v0, mult_add);    // a1 + b1
+  v1 = _mm_madd_epi16(v1, mult_sub);    // a1 - b1
+
+  v4 = _mm_xor_si128(v4, v4);           // zero out for compare
+  v0 = _mm_add_epi32(v0, v5);
+  v1 = _mm_add_epi32(v1, v5);
+  v2 = _mm_cmpeq_epi16(v2, v4);
+  v0 = _mm_srai_epi32(v0, 4);           // (a1 + b1 + 7)>>4
+  v1 = _mm_srai_epi32(v1, 4);           // (a1 - b1 + 7)>>4
+  v2 = _mm_andnot_si128(v2, cmp_mask);  // clear upper, and keep bit 0 of lower
+
+  v4 = v3;
+  v3 = _mm_madd_epi16(v3, c5352_2217);  // c1*2217 + d1*5352
+  v4 = _mm_madd_epi16(v4, c2217_n5352); // d1*2217 - c1*5352
+  v3 = _mm_add_epi32(v3, c12000);
+  v4 = _mm_add_epi32(v4, c51000);
+  v0 = _mm_packs_epi32(v0, v1);         // op[8] op[0]
+  v3 = _mm_srai_epi32(v3, 16);          // (c1 * 2217 + d1 * 5352 +  12000)>>16
+  v4 = _mm_srai_epi32(v4, 16);          // (d1 * 2217 - c1 * 5352 +  51000)>>16
+
+  v3 = _mm_packs_epi32(v3, v4);         // op[12] op[4]
+  v1 = v0;
+  v3 = _mm_add_epi16(v3, v2);           // op[4] += (d1!=0)
+  v0 = _mm_unpacklo_epi64(v0, v3);      // op[4] op[0]
+  v1 = _mm_unpackhi_epi64(v1, v3);      // op[12] op[8]
+
+  _mm_store_si128((__m128i*)output, v0);
+  _mm_store_si128((__m128i*)output + 1, v1);
+}
+
+void vp8_short_fdct8x4_sse2(short *input, short *output, int pitch) {
+  __m128i v0, v1, v2, v3, v4, v5, v6;
+  __m128i c5352_2217 = _mm_set1_epi32(2217 << 16 | 5352);
+  __m128i c2217_n5352 = _mm_set1_epi32(-5352 << 16 | 2217);
+  __m128i c14500 = _mm_set1_epi32(14500);
+  __m128i c7500 = _mm_set1_epi32(7500);
+  __m128i c12000 = _mm_set1_epi32(12000);
+  __m128i c51000 = _mm_set1_epi32(51000);
+  __m128i cmp_mask8x4 = _mm_set1_epi16(1);
+  __m128i c7w = _mm_set1_epi16(7);
+
+  // read the input data
+  v0 = _mm_load_si128((const __m128i*)input);
+  v2 = _mm_load_si128((const __m128i*)((char*)input + pitch));
+  v4 = _mm_load_si128((const __m128i*)((char*)input + pitch * 2));
+  v3 = _mm_load_si128((const __m128i*)((char*)input + pitch * 3));
+
+  // transpose for the first stage
+  v1 = v0;                              // 00 01 02 03 04 05 06 07
+  v5 = v4;                              // 20 21 22 23 24 25 26 27
+  v0 = _mm_unpacklo_epi16(v0, v2);      // 00 10 01 11 02 12 03 13
+  v1 = _mm_unpackhi_epi16(v1, v2);      // 04 14 05 15 06 16 07 17
+  v4 = _mm_unpacklo_epi16(v4, v3);      // 20 30 21 31 22 32 23 33
+  v5 = _mm_unpackhi_epi16(v5, v3);      // 24 34 25 35 26 36 27 37
+  v2 = v0;                              // 00 10 01 11 02 12 03 13
+  v0 = _mm_unpacklo_epi32(v0, v4);      // 00 10 20 30 01 11 21 31
+  v2 = _mm_unpackhi_epi32(v2, v4);      // 02 12 22 32 03 13 23 33
+  v4 = v1;                              // 04 14 05 15 06 16 07 17
+  v4 = _mm_unpacklo_epi32(v4, v5);      // 04 14 24 34 05 15 25 35
+  v1 = _mm_unpackhi_epi32(v1, v5);      // 06 16 26 36 07 17 27 37
+  v3 = v2;                              // 02 12 22 32 03 13 23 33
+  v3 = _mm_unpackhi_epi64(v3, v1);      // 03 13 23 33 07 17 27 37
+  v2 = _mm_unpacklo_epi64(v2, v1);      // 02 12 22 32 06 16 26 36
+  v1 = v0;                              // 00 10 20 30 01 11 21 31
+  v0 = _mm_unpacklo_epi64(v0, v4);      // 00 10 20 30 04 14 24 34
+  v1 = _mm_unpackhi_epi64(v1, v4);      // 01 11 21 32 05 15 25 35
+
+  // v0 0
+  // v1 1
+  // v2 2
+  // v3 3
+
+  // first stage
+  v5 = v0;
+  v4 = v1;
+  v0 = _mm_add_epi16(v0, v3);           // a1 = 0 + 3
+  v1 = _mm_add_epi16(v1, v2);           // b1 = 1 + 2
+  v4 = _mm_sub_epi16(v4, v2);           // c1 = 1 - 2
+  v5 = _mm_sub_epi16(v5, v3);           // d1 = 0 - 3
+  v5 = _mm_slli_epi16(v5, 3);
+  v4 = _mm_slli_epi16(v4, 3);
+  v0 = _mm_slli_epi16(v0, 3);
+  v1 = _mm_slli_epi16(v1, 3);
+
+  // output 0 and 2
+  v2 = v0;        // a1
+  v0 = _mm_add_epi16(v0, v1);           // op[0] = a1 + b1
+  v2 = _mm_sub_epi16(v2, v1);           // op[2] = a1 - b1
+
+  // output 1 and 3
+  // interleave c1, d1
+  v1 = v5;        // d1
+  v1 = _mm_unpacklo_epi16(v1, v4);      // c1 d1
+  v5 = _mm_unpackhi_epi16(v5, v4);      // c1 d1
+
+  v3 = v1;
+  v4 = v5;
+  v1 = _mm_madd_epi16(v1, c5352_2217);  // c1*2217 + d1*5352
+  v4 = _mm_madd_epi16(v4, c5352_2217);  // c1*2217 + d1*5352
+  v3 = _mm_madd_epi16(v3, c2217_n5352); // d1*2217 - c1*5352
+  v5 = _mm_madd_epi16(v5, c2217_n5352); // d1*2217 - c1*5352
+  v1 = _mm_add_epi32(v1, c14500);
+  v4 = _mm_add_epi32(v4, c14500);
+  v3 = _mm_add_epi32(v3, c7500);
+  v5 = _mm_add_epi32(v5, c7500);
+  v1 = _mm_srai_epi32(v1, 12);          // (c1 * 2217 + d1 * 5352 +  14500)>>12
+  v4 = _mm_srai_epi32(v4, 12);          // (c1 * 2217 + d1 * 5352 +  14500)>>12
+  v3 = _mm_srai_epi32(v3, 12);          // (d1 * 2217 - c1 * 5352 +   7500)>>12
+  v5 = _mm_srai_epi32(v5, 12);          // (d1 * 2217 - c1 * 5352 +   7500)>>12
+  v1 = _mm_packs_epi32(v1, v4);         // op[1]
+  v3 = _mm_packs_epi32(v3, v5);         // op[3]
+
+  // done with vertical
+  // transpose for the second stage
+  v4 = v0;                              // 00 10 20 30 04 14 24 34
+  v5 = v2;                              // 02 12 22 32 06 16 26 36
+  v0 = _mm_unpacklo_epi16(v0, v1);      // 00 01 10 11 20 21 30 31
+  v4 = _mm_unpackhi_epi16(v4, v1);      // 04 05 14 15 24 25 34 35
+  v2 = _mm_unpacklo_epi16(v2, v3);      // 02 03 12 13 22 23 32 33
+  v5 = _mm_unpackhi_epi16(v5, v3);      // 06 07 16 17 26 27 36 37
+  v1 = v0;                              // 00 01 10 11 20 21 30 31
+  v0 = _mm_unpacklo_epi32(v0, v2);      // 00 01 02 03 10 11 12 13
+  v1 = _mm_unpackhi_epi32(v1, v2);      // 20 21 22 23 30 31 32 33
+  v2 = v4;                              // 04 05 14 15 24 25 34 35
+  v2 = _mm_unpacklo_epi32(v2, v5);      // 04 05 06 07 14 15 16 17
+  v4 = _mm_unpackhi_epi32(v4, v5);      // 24 25 26 27 34 35 36 37
+  v3 = v1;                              // 20 21 22 23 30 31 32 33
+  v3 = _mm_unpackhi_epi64(v3, v4);      // 30 31 32 33 34 35 36 37
+  v1 = _mm_unpacklo_epi64(v1, v4);      // 20 21 22 23 24 25 26 27
+  v4 = v0;                              // 00 01 02 03 10 11 12 13
+  v0 = _mm_unpacklo_epi64(v0, v2);      // 00 01 02 03 04 05 06 07
+  v4 = _mm_unpackhi_epi64(v4, v2);      // 10 11 12 13 14 15 16 17
+
+  // v0 0
+  // v1 4
+  // v2 1
+  // v3 3
+
+  v5 = v0;
+  v2 = v1;
+  v0 = _mm_add_epi16(v0, v3);           // a1 = 0 + 3
+  v1 = _mm_add_epi16(v1, v4);           // b1 = 1 + 2
+  v4 = _mm_sub_epi16(v4, v2);           // c1 = 1 - 2
+  v5 = _mm_sub_epi16(v5, v3);           // d1 = 0 - 3
+  v6 = _mm_setzero_si128();             // zero out for compare
+  v6 = _mm_cmpeq_epi16(v6, v5);         // d1 != 0
+  v6 = _mm_andnot_si128(v6, cmp_mask8x4); // clear upper, and keep bit 0 of lower
+  // output 0 and 2
+  v2 = v0;        // a1
+  v0 = _mm_add_epi16(v0, v1);           // a1 + b1
+  v2 = _mm_sub_epi16(v2, v1);           // a1 - b1
+  v0 = _mm_add_epi16(v0, c7w);
+  v2 = _mm_add_epi16(v2, c7w);
+  v0 = _mm_srai_epi16(v0, 4);           // op[0] = (a1 + b1 + 7)>>4
+  v2 = _mm_srai_epi16(v2, 4);           // op[8] = (a1 - b1 + 7)>>4
+
+  // output 1 and 3
+  // interleave c1, d1
+  v1 = v5;        // d1
+  v1 = _mm_unpacklo_epi16(v1, v4);      // c1 d1
+  v5 = _mm_unpackhi_epi16(v5, v4);      // c1 d1
+  v3 = v1;
+  v4 = v5;
+
+  v1 = _mm_madd_epi16(v1, c5352_2217);  // c1*2217 + d1*5352
+  v4 = _mm_madd_epi16(v4, c5352_2217);  // c1*2217 + d1*5352
+  v3 = _mm_madd_epi16(v3, c2217_n5352); // d1*2217 - c1*5352
+  v5 = _mm_madd_epi16(v5, c2217_n5352); // d1*2217 - c1*5352
+
+  v1 = _mm_add_epi32(v1, c12000);
+  v4 = _mm_add_epi32(v4, c12000);
+  v3 = _mm_add_epi32(v3, c51000);
+  v5 = _mm_add_epi32(v5, c51000);
+
+  v1 = _mm_srai_epi32(v1, 16);          // (c1 * 2217 + d1 * 5352 +  14500)>>16
+  v4 = _mm_srai_epi32(v4, 16);          // (c1 * 2217 + d1 * 5352 +  14500)>>16
+  v3 = _mm_srai_epi32(v3, 16);          // (d1 * 2217 - c1 * 5352 +   7500)>>16
+  v5 = _mm_srai_epi32(v5, 16);          // (d1 * 2217 - c1 * 5352 +   7500)>>16
+
+  v1 = _mm_packs_epi32(v1, v4);         // op[4]
+  v3 = _mm_packs_epi32(v3, v5);         // op[12]
+  v1 = _mm_add_epi16(v1, v6);           // op[4] += (d1!=0)
+
+  v4 = v0;
+  v5 = v2;
+  v0 = _mm_unpacklo_epi64(v0, v1);
+  v4 = _mm_unpackhi_epi64(v4, v1);
+  v2 = _mm_unpacklo_epi64(v2, v3);
+  v5 = _mm_unpackhi_epi64(v5, v3);
+
+  _mm_store_si128((__m128i*)output, v0);
+  _mm_store_si128((__m128i*)output + 1, v2);
+  _mm_store_si128((__m128i*)output + 2, v4);
+  _mm_store_si128((__m128i*)output + 3, v5);
+}
+
diff --git a/vp8/encoder/x86/quantize_sse4.c b/vp8/encoder/x86/quantize_sse4.c
index 389c167..6d03365 100644
--- a/vp8/encoder/x86/quantize_sse4.c
+++ b/vp8/encoder/x86/quantize_sse4.c
@@ -11,28 +11,14 @@
 #include <smmintrin.h> /* SSE4.1 */
 
 #include "./vp8_rtcd.h"
-#include "vp8/common/entropy.h" /* vp8_default_inv_zig_zag */
 #include "vp8/encoder/block.h"
-
-#define SELECT_EOB(i, z, x, y, q)                         \
-  do {                                                    \
-    short boost = *zbin_boost_ptr;                        \
-    /* Technically _mm_extract_epi16() returns an int: */ \
-    /* https://bugs.llvm.org/show_bug.cgi?id=41657 */     \
-    short x_z = (short)_mm_extract_epi16(x, z);           \
-    short y_z = (short)_mm_extract_epi16(y, z);           \
-    int cmp = (x_z < boost) | (y_z == 0);                 \
-    zbin_boost_ptr++;                                     \
-    if (cmp) break;                                       \
-    q = _mm_insert_epi16(q, y_z, z);                      \
-    eob = i;                                              \
-    zbin_boost_ptr = b->zrun_zbin_boost;                  \
-  } while (0)
+#include "vpx_ports/bitops.h" /* get_lsb */
 
 void vp8_regular_quantize_b_sse4_1(BLOCK *b, BLOCKD *d) {
-  char eob = 0;
+  int eob = -1;
   short *zbin_boost_ptr = b->zrun_zbin_boost;
-
+  __m128i zbin_boost0 = _mm_load_si128((__m128i *)(zbin_boost_ptr));
+  __m128i zbin_boost1 = _mm_load_si128((__m128i *)(zbin_boost_ptr + 8));
   __m128i x0, x1, y0, y1, x_minus_zbin0, x_minus_zbin1, dqcoeff0, dqcoeff1;
   __m128i quant_shift0 = _mm_load_si128((__m128i *)(b->quant_shift));
   __m128i quant_shift1 = _mm_load_si128((__m128i *)(b->quant_shift + 8));
@@ -47,8 +33,12 @@ void vp8_regular_quantize_b_sse4_1(BLOCK *b, BLOCKD *d) {
   __m128i quant1 = _mm_load_si128((__m128i *)(b->quant + 8));
   __m128i dequant0 = _mm_load_si128((__m128i *)(d->dequant));
   __m128i dequant1 = _mm_load_si128((__m128i *)(d->dequant + 8));
-  __m128i qcoeff0 = _mm_setzero_si128();
-  __m128i qcoeff1 = _mm_setzero_si128();
+  __m128i qcoeff0, qcoeff1, t0, t1, x_shuf0, x_shuf1;
+  uint32_t mask, ymask;
+  DECLARE_ALIGNED(16, static const uint8_t,
+                  zig_zag_mask[16]) = { 0, 1,  4,  8,  5, 2,  3,  6,
+                                        9, 12, 13, 10, 7, 11, 14, 15 };
+  DECLARE_ALIGNED(16, uint16_t, qcoeff[16]) = { 0 };
 
   /* Duplicate to all lanes. */
   zbin_extra = _mm_shufflelo_epi16(zbin_extra, 0);
@@ -88,23 +78,52 @@ void vp8_regular_quantize_b_sse4_1(BLOCK *b, BLOCKD *d) {
   y0 = _mm_sign_epi16(y0, z0);
   y1 = _mm_sign_epi16(y1, z1);
 
-  /* The loop gets unrolled anyway. Avoid the vp8_default_zig_zag1d lookup. */
-  SELECT_EOB(1, 0, x_minus_zbin0, y0, qcoeff0);
-  SELECT_EOB(2, 1, x_minus_zbin0, y0, qcoeff0);
-  SELECT_EOB(3, 4, x_minus_zbin0, y0, qcoeff0);
-  SELECT_EOB(4, 0, x_minus_zbin1, y1, qcoeff1);
-  SELECT_EOB(5, 5, x_minus_zbin0, y0, qcoeff0);
-  SELECT_EOB(6, 2, x_minus_zbin0, y0, qcoeff0);
-  SELECT_EOB(7, 3, x_minus_zbin0, y0, qcoeff0);
-  SELECT_EOB(8, 6, x_minus_zbin0, y0, qcoeff0);
-  SELECT_EOB(9, 1, x_minus_zbin1, y1, qcoeff1);
-  SELECT_EOB(10, 4, x_minus_zbin1, y1, qcoeff1);
-  SELECT_EOB(11, 5, x_minus_zbin1, y1, qcoeff1);
-  SELECT_EOB(12, 2, x_minus_zbin1, y1, qcoeff1);
-  SELECT_EOB(13, 7, x_minus_zbin0, y0, qcoeff0);
-  SELECT_EOB(14, 3, x_minus_zbin1, y1, qcoeff1);
-  SELECT_EOB(15, 6, x_minus_zbin1, y1, qcoeff1);
-  SELECT_EOB(16, 7, x_minus_zbin1, y1, qcoeff1);
+  {
+    const __m128i zig_zag_i16_0 =
+        _mm_setr_epi8(0, 1, 2, 3, 8, 9, 14, 15, 10, 11, 4, 5, 6, 7, 12, 13);
+    const __m128i zig_zag_i16_1 =
+        _mm_setr_epi8(0, 1, 6, 7, 8, 9, 2, 3, 14, 15, 4, 5, 10, 11, 12, 13);
+
+    /* The first part of the zig zag needs a value
+     * from x_minus_zbin1 and vice versa. */
+    t1 = _mm_alignr_epi8(x_minus_zbin1, x_minus_zbin1, 2);
+    t0 = _mm_blend_epi16(x_minus_zbin0, t1, 0x80);
+    t1 = _mm_blend_epi16(t1, x_minus_zbin0, 0x80);
+    x_shuf0 = _mm_shuffle_epi8(t0, zig_zag_i16_0);
+    x_shuf1 = _mm_shuffle_epi8(t1, zig_zag_i16_1);
+  }
+
+  /* Check if y is nonzero and put it in zig zag order. */
+  t0 = _mm_packs_epi16(y0, y1);
+  t0 = _mm_cmpeq_epi8(t0, _mm_setzero_si128());
+  t0 = _mm_shuffle_epi8(t0, _mm_load_si128((const __m128i *)zig_zag_mask));
+  ymask = _mm_movemask_epi8(t0) ^ 0xffff;
+
+  for (;;) {
+    t0 = _mm_cmpgt_epi16(zbin_boost0, x_shuf0);
+    t1 = _mm_cmpgt_epi16(zbin_boost1, x_shuf1);
+    t0 = _mm_packs_epi16(t0, t1);
+    mask = _mm_movemask_epi8(t0);
+    mask = ~mask & ymask;
+    if (!mask) break;
+    /* |eob| will contain the index of the next found element where:
+     * boost[i - old_eob - 1] <= x[zigzag[i]] && y[zigzag[i]] != 0 */
+    eob = get_lsb(mask);
+    /* Need to clear the mask from processed elements so that
+     * they are no longer counted in the next iteration. */
+    ymask &= ~1U << eob;
+    /* It's safe to read ahead of this buffer if struct VP8_COMP has at
+     * least 32 bytes before the zrun_zbin_boost_* fields (it has 384).
+     * Any data read outside of the buffer is masked by the updated |ymask|. */
+    zbin_boost0 = _mm_loadu_si128((__m128i *)(zbin_boost_ptr - eob - 1));
+    zbin_boost1 = _mm_loadu_si128((__m128i *)(zbin_boost_ptr - eob + 7));
+    qcoeff[zig_zag_mask[eob]] = 0xffff;
+  }
+
+  qcoeff0 = _mm_load_si128((__m128i *)(qcoeff));
+  qcoeff1 = _mm_load_si128((__m128i *)(qcoeff + 8));
+  qcoeff0 = _mm_and_si128(qcoeff0, y0);
+  qcoeff1 = _mm_and_si128(qcoeff1, y1);
 
   _mm_store_si128((__m128i *)(d->qcoeff), qcoeff0);
   _mm_store_si128((__m128i *)(d->qcoeff + 8), qcoeff1);
@@ -115,5 +134,5 @@ void vp8_regular_quantize_b_sse4_1(BLOCK *b, BLOCKD *d) {
   _mm_store_si128((__m128i *)(d->dqcoeff), dqcoeff0);
   _mm_store_si128((__m128i *)(d->dqcoeff + 8), dqcoeff1);
 
-  *d->eob = eob;
+  *d->eob = eob + 1;
 }
diff --git a/vp8/encoder/x86/vp8_quantize_ssse3.c b/vp8/encoder/x86/vp8_quantize_ssse3.c
index 147c30c..f6df146 100644
--- a/vp8/encoder/x86/vp8_quantize_ssse3.c
+++ b/vp8/encoder/x86/vp8_quantize_ssse3.c
@@ -12,31 +12,7 @@
 
 #include "./vp8_rtcd.h"
 #include "vp8/encoder/block.h"
-
-/* bitscan reverse (bsr) */
-#if defined(_MSC_VER)
-#include <intrin.h>
-#pragma intrinsic(_BitScanReverse)
-static int bsr(int mask) {
-  unsigned long eob;
-  _BitScanReverse(&eob, mask);
-  eob++;
-  if (mask == 0) eob = 0;
-  return eob;
-}
-#else
-static int bsr(int mask) {
-  int eob;
-#if defined(__GNUC__) && __GNUC__
-  __asm__ __volatile__("bsr %1, %0" : "=r"(eob) : "r"(mask) : "flags");
-#elif defined(__SUNPRO_C) || defined(__SUNPRO_CC)
-  asm volatile("bsr %1, %0" : "=r"(eob) : "r"(mask) : "flags");
-#endif
-  eob++;
-  if (mask == 0) eob = 0;
-  return eob;
-}
-#endif
+#include "vpx_ports/bitops.h" /* get_msb */
 
 void vp8_fast_quantize_b_ssse3(BLOCK *b, BLOCKD *d) {
   int eob, mask;
@@ -108,7 +84,10 @@ void vp8_fast_quantize_b_ssse3(BLOCK *b, BLOCKD *d) {
 
   mask = _mm_movemask_epi8(x);
 
-  eob = bsr(mask);
+  /* x2 is needed to increase the result from non-zero masks by 1,
+   * +1 is needed to mask undefined behavior for a null argument,
+   * the result of get_msb(1) is 0 */
+  eob = get_msb(mask * 2 + 1);
 
-  *d->eob = 0xFF & eob;
+  *d->eob = eob;
 }
diff --git a/vp8/vp8_common.mk b/vp8/vp8_common.mk
index 286a93a..d00c28f 100644
--- a/vp8/vp8_common.mk
+++ b/vp8/vp8_common.mk
@@ -69,8 +69,8 @@ VP8_COMMON_SRCS-yes += common/vp8_entropymodedata.h
 
 VP8_COMMON_SRCS-yes += common/treecoder.c
 
-VP8_COMMON_SRCS-$(VPX_ARCH_X86)$(VPX_ARCH_X86_64) += common/x86/vp8_asm_stubs.c
-VP8_COMMON_SRCS-$(VPX_ARCH_X86)$(VPX_ARCH_X86_64) += common/x86/loopfilter_x86.c
+VP8_COMMON_SRCS-$(VPX_ARCH_X86)$(VPX_ARCH_X86_64)$(VPX_ARCH_E2K) += common/x86/vp8_asm_stubs.c
+VP8_COMMON_SRCS-$(VPX_ARCH_X86)$(VPX_ARCH_X86_64)$(VPX_ARCH_E2K) += common/x86/loopfilter_x86.c
 VP8_COMMON_SRCS-$(CONFIG_POSTPROC) += common/mfqe.c
 VP8_COMMON_SRCS-$(CONFIG_POSTPROC) += common/postproc.h
 VP8_COMMON_SRCS-$(CONFIG_POSTPROC) += common/postproc.c
@@ -87,6 +87,8 @@ VP8_COMMON_SRCS-$(HAVE_SSE2) += common/x86/subpixel_sse2.asm
 VP8_COMMON_SRCS-$(HAVE_SSE2) += common/x86/loopfilter_sse2.asm
 VP8_COMMON_SRCS-$(HAVE_SSE2) += common/x86/iwalsh_sse2.asm
 VP8_COMMON_SRCS-$(HAVE_SSSE3) += common/x86/subpixel_ssse3.asm
+VP8_COMMON_SRCS-$(VPX_ARCH_E2K) += common/e2k/loopfilter_e2k.c
+VP8_COMMON_SRCS-$(VPX_ARCH_E2K) += common/e2k/subpixel_e2k.c
 
 ifeq ($(CONFIG_POSTPROC),yes)
 VP8_COMMON_SRCS-$(HAVE_SSE2) += common/x86/mfqe_sse2.asm
diff --git a/vp8/vp8cx.mk b/vp8/vp8cx.mk
index 3a8f8ea..43d0463 100644
--- a/vp8/vp8cx.mk
+++ b/vp8/vp8cx.mk
@@ -90,6 +90,7 @@ VP8_CX_SRCS-$(HAVE_SSE2) += encoder/x86/fwalsh_sse2.asm
 VP8_CX_SRCS-$(HAVE_SSE2) += encoder/x86/vp8_quantize_sse2.c
 VP8_CX_SRCS-$(HAVE_SSSE3) += encoder/x86/vp8_quantize_ssse3.c
 VP8_CX_SRCS-$(HAVE_SSE4_1) += encoder/x86/quantize_sse4.c
+VP8_CX_SRCS-$(VPX_ARCH_E2K) += encoder/e2k/dct_e2k.c
 
 ifeq ($(CONFIG_TEMPORAL_DENOISING),yes)
 VP8_CX_SRCS-$(HAVE_SSE2) += encoder/x86/denoising_sse2.c
@@ -98,6 +99,7 @@ endif
 VP8_CX_SRCS-$(HAVE_SSE2) += encoder/x86/block_error_sse2.asm
 VP8_CX_SRCS-$(HAVE_SSE2) += encoder/x86/temporal_filter_apply_sse2.asm
 VP8_CX_SRCS-$(HAVE_SSE2) += encoder/x86/vp8_enc_stubs_sse2.c
+VP8_CX_SRCS-$(VPX_ARCH_E2K) += encoder/e2k/block_error_e2k.c
 
 ifeq ($(CONFIG_REALTIME_ONLY),yes)
 VP8_CX_SRCS_REMOVE-$(HAVE_SSE2) += encoder/x86/temporal_filter_apply_sse2.asm
diff --git a/vp9/common/vp9_rtcd_defs.pl b/vp9/common/vp9_rtcd_defs.pl
index 6980b9b..d5016b5 100644
--- a/vp9/common/vp9_rtcd_defs.pl
+++ b/vp9/common/vp9_rtcd_defs.pl
@@ -34,7 +34,7 @@ forward_decls qw/vp9_common_forward_decls/;
 
 # functions that are 64 bit only.
 $mmx_x86_64 = $sse2_x86_64 = $ssse3_x86_64 = $avx_x86_64 = $avx2_x86_64 = '';
-if ($opts{arch} eq "x86_64") {
+if ($opts{arch} eq "x86_64" or $opts{arch} eq "e2k") {
   $mmx_x86_64 = 'mmx';
   $sse2_x86_64 = 'sse2';
   $ssse3_x86_64 = 'ssse3';
diff --git a/vp9/encoder/e2k/vp9_error_e2k.c b/vp9/encoder/e2k/vp9_error_e2k.c
new file mode 100644
index 0000000..8da71d4
--- /dev/null
+++ b/vp9/encoder/e2k/vp9_error_e2k.c
@@ -0,0 +1,75 @@
+/*
+ *  Copyright (c) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *  Copyright (c) 2010 The WebM project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "./vp9_rtcd.h"
+#include "./vpx_dsp_rtcd.h"
+
+#if CONFIG_VP9_HIGHBITDEPTH
+#define LOAD_TRAN_LOW(v0, p) \
+  v0 = _mm_load_si128((const __m128i*)(p)); \
+  v0 = _mm_packs_epi32(v0, _mm_load_si128((const __m128i*)(p) + 1));
+#else
+#define LOAD_TRAN_LOW(v0, p) \
+  v0 = _mm_load_si128((const __m128i*)(p));
+#endif
+
+#define BLOCK_DIFF \
+  LOAD_TRAN_LOW(v2, coeff) \
+  LOAD_TRAN_LOW(v0, dqcoeff) \
+  LOAD_TRAN_LOW(v3, coeff + 8) \
+  LOAD_TRAN_LOW(v1, dqcoeff + 8) \
+  coeff += 16; dqcoeff += 16; \
+  block_size -= 16; \
+  v0 = _mm_sub_epi16(v0, v2); \
+  v1 = _mm_sub_epi16(v1, v3);
+
+int64_t vp9_block_error_sse2(const tran_low_t *coeff, const tran_low_t *dqcoeff,
+                             intptr_t block_size, int64_t *ssz) {
+  __m128i v0, v1, v2, v3, c0 = _mm_setzero_si128();
+  __m128i sse = c0, sqr = c0;
+  do {
+    BLOCK_DIFF
+    v0 = _mm_madd_epi16(v0, v0);
+    v1 = _mm_madd_epi16(v1, v1);
+    v2 = _mm_madd_epi16(v2, v2);
+    v3 = _mm_madd_epi16(v3, v3);
+    v0 = _mm_add_epi32(v0, v1);
+    v2 = _mm_add_epi32(v2, v3);
+    sse = _mm_add_epi64(sse, _mm_unpacklo_epi32(v0, c0));
+    sse = _mm_add_epi64(sse, _mm_unpackhi_epi32(v0, c0));
+    sqr = _mm_add_epi64(sqr, _mm_unpacklo_epi32(v2, c0));
+    sqr = _mm_add_epi64(sqr, _mm_unpackhi_epi32(v2, c0));
+  } while (block_size > 0);
+
+  sse = _mm_add_epi64(sse, _mm_unpackhi_epi64(sse, sse));
+  sqr = _mm_add_epi64(sqr, _mm_unpackhi_epi64(sqr, sqr));
+  *ssz = _mm_cvtsi128_si64(sqr);
+  return _mm_cvtsi128_si64(sse);
+}
+
+int64_t vp9_block_error_fp_sse2(const tran_low_t *coeff, const tran_low_t *dqcoeff,
+                                int block_size) {
+  __m128i v0, v1, v2, v3, c0 = _mm_setzero_si128(), sse = c0;
+  do {
+    BLOCK_DIFF
+    v0 = _mm_madd_epi16(v0, v0);
+    v1 = _mm_madd_epi16(v1, v1);
+    v0 = _mm_add_epi32(v0, v1);
+    sse = _mm_add_epi64(sse, _mm_unpacklo_epi32(v0, c0));
+    sse = _mm_add_epi64(sse, _mm_unpackhi_epi32(v0, c0));
+  } while (block_size > 0);
+
+  sse = _mm_add_epi64(sse, _mm_unpackhi_epi64(sse, sse));
+  return _mm_cvtsi128_si64(sse);
+}
+
diff --git a/vp9/vp9cx.mk b/vp9/vp9cx.mk
index 38e9916..28bbf00 100644
--- a/vp9/vp9cx.mk
+++ b/vp9/vp9cx.mk
@@ -120,6 +120,7 @@ endif
 
 VP9_CX_SRCS-$(HAVE_SSE2) += encoder/x86/vp9_dct_sse2.asm
 VP9_CX_SRCS-$(HAVE_SSE2) += encoder/x86/vp9_error_sse2.asm
+VP9_CX_SRCS-$(VPX_ARCH_E2K) += encoder/e2k/vp9_error_e2k.c
 
 ifeq ($(VPX_ARCH_X86_64),yes)
 VP9_CX_SRCS-$(HAVE_SSSE3) += encoder/x86/vp9_quantize_ssse3_x86_64.asm
diff --git a/vpx_dsp/e2k/highbd_intrapred_e2k.c b/vpx_dsp/e2k/highbd_intrapred_e2k.c
new file mode 100644
index 0000000..5266292
--- /dev/null
+++ b/vpx_dsp/e2k/highbd_intrapred_e2k.c
@@ -0,0 +1,133 @@
+/*
+ *  Copyright (c) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *  Copyright (c) 2016 The WebM project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "./vpx_config.h"
+#include "./vpx_dsp_rtcd.h"
+
+#define CU64P const uint64_t*
+
+#define FUNC(name) void vpx_##name##_sse2( \
+  uint16_t *dst, ptrdiff_t stride, \
+  const uint16_t *above, const uint16_t *left, int bd)
+
+FUNC(highbd_dc_predictor_4x4) {
+  __m64 v0;
+  uint64_t sum; int i; (void)bd;
+  sum = *(CU64P)above + *(CU64P)left;
+  sum += sum >> 32;
+  sum += sum >> 16;
+  sum = ((uint16_t)sum + 4) >> 3;
+  v0 = _mm_set1_pi16(sum);
+  for (i = 0; i < 4; i++, dst += stride) {
+    *(__m64*)dst = v0;
+  }
+}
+
+FUNC(highbd_dc_predictor_8x8) {
+  __m128i v0;
+  uint64_t sum; int i; (void)bd;
+  sum = *(CU64P)above + *(CU64P)(above + 4);
+  sum += *(CU64P)left + *(CU64P)(left + 4);
+  sum += sum >> 32;
+  sum += sum >> 16;
+  sum = ((int64_t)(uint16_t)sum + 8) >> 4;
+  v0 = _mm_set1_epi16(sum);
+  for (i = 0; i < 8; i++, dst += stride) {
+    _mm_store_si128((__m128i *)dst, v0);
+  }
+}
+
+FUNC(highbd_dc_predictor_16x16) {
+  __m128i v0; __m64 h0, vzero = _mm_setzero_si64();
+  uint64_t sum; int i; (void)bd;
+  sum = *(CU64P)above + *(CU64P)(above + 4);
+  sum += *(CU64P)(above + 8) + *(CU64P)(above + 12);
+  sum += *(CU64P)left + *(CU64P)(left + 4);
+  sum += *(CU64P)(left + 8) + *(CU64P)(left + 12);
+  h0 = _mm_cvtsi64_m64(sum);
+  sum = _mm_cvtm64_si64(_mm_unpacklo_pi16(h0, vzero));
+  sum += _mm_cvtm64_si64(_mm_unpackhi_pi16(h0, vzero));
+  sum += sum >> 32;
+  sum = ((uint64_t)(uint32_t)sum + 16) >> 5;
+  v0 = _mm_set1_epi16(sum);
+  for (i = 0; i < 16; i++, dst += stride) {
+    _mm_store_si128((__m128i *)dst, v0);
+    _mm_store_si128((__m128i *)(dst + 8), v0);
+  }
+}
+
+FUNC(highbd_dc_predictor_32x32) {
+  __m128i v0; __m64 h0, vzero = _mm_setzero_si64();
+  uint64_t sum; int i; (void)bd;
+  sum = *(CU64P)above + *(CU64P)(above + 4);
+  sum += *(CU64P)(above + 8) + *(CU64P)(above + 12);
+  sum += *(CU64P)(above + 16) + *(CU64P)(above + 20);
+  sum += *(CU64P)(above + 24) + *(CU64P)(above + 28);
+  sum += *(CU64P)left + *(CU64P)(left + 4);
+  sum += *(CU64P)(left + 8) + *(CU64P)(left + 12);
+  sum += *(CU64P)(left + 16) + *(CU64P)(left + 20);
+  sum += *(CU64P)(left + 24) + *(CU64P)(left + 28);
+  h0 = _mm_cvtsi64_m64(sum);
+  sum = _mm_cvtm64_si64(_mm_unpacklo_pi16(h0, vzero));
+  sum += _mm_cvtm64_si64(_mm_unpackhi_pi16(h0, vzero));
+  sum += sum >> 32;
+  sum = ((uint64_t)(uint32_t)sum + 32) >> 6;
+  v0 = _mm_set1_epi16(sum);
+  for (i = 0; i < 32; i++, dst += stride) {
+    _mm_store_si128((__m128i *)dst, v0);
+    _mm_store_si128((__m128i *)(dst + 8), v0);
+    _mm_store_si128((__m128i *)(dst + 16), v0);
+    _mm_store_si128((__m128i *)(dst + 24), v0);
+  }
+}
+
+FUNC(highbd_v_predictor_4x4) {
+  __m64 v0 = *(__m64 const *)above; int i;
+  (void)left; (void)bd;
+  for (i = 0; i < 4; i++, dst += stride) {
+    *(__m64*)dst = v0;
+  }
+}
+
+FUNC(highbd_v_predictor_8x8) {
+  __m128i v0 = _mm_load_si128((__m128i const *)above);
+  int i; (void)left; (void)bd;
+  for (i = 0; i < 8; i++, dst += stride) {
+    _mm_store_si128((__m128i *)dst, v0);
+  }
+}
+
+FUNC(highbd_v_predictor_16x16) {
+  __m128i v0 = _mm_load_si128((__m128i const *)above);
+  __m128i v1 = _mm_load_si128((__m128i const *)(above + 8));
+  int i; (void)left; (void)bd;
+  for (i = 0; i < 16; i++, dst += stride) {
+    _mm_store_si128((__m128i *)dst, v0);
+    _mm_store_si128((__m128i *)(dst + 8), v1);
+  }
+}
+
+FUNC(highbd_v_predictor_32x32) {
+  __m128i v0 = _mm_load_si128((__m128i const *)above);
+  __m128i v1 = _mm_load_si128((__m128i const *)(above + 8));
+  __m128i v2 = _mm_load_si128((__m128i const *)(above + 16));
+  __m128i v3 = _mm_load_si128((__m128i const *)(above + 24));
+  int i; (void)left; (void)bd;
+  for (i = 0; i < 32; i++, dst += stride) {
+    _mm_store_si128((__m128i *)dst, v0);
+    _mm_store_si128((__m128i *)(dst + 8), v1);
+    _mm_store_si128((__m128i *)(dst + 16), v2);
+    _mm_store_si128((__m128i *)(dst + 24), v3);
+  }
+}
+
diff --git a/vpx_dsp/e2k/highbd_sad4d_e2k.c b/vpx_dsp/e2k/highbd_sad4d_e2k.c
new file mode 100644
index 0000000..3eada9b
--- /dev/null
+++ b/vpx_dsp/e2k/highbd_sad4d_e2k.c
@@ -0,0 +1,306 @@
+/*
+ *  Copyright (c) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *  Copyright (c) 2016 The WebM project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "./vpx_config.h"
+#include "./vpx_dsp_rtcd.h"
+#include "vpx_ports/mem.h"
+
+#define NO_AVG(...)
+#define WITH_AVG(...) __VA_ARGS__
+
+#define FN(w, h) \
+  SAD##w##XN(h, NO_AVG, vpx_highbd_sad, x4d_sse2) \
+  /* SAD##w##XN(h, WITH_AVG, vpx_highbd_sad, x4d_avg_sse2) */
+#define SAD_LOOP(h, n) \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < h; i += n)
+#define RESULT_SHL(x, pi) x
+
+#define SAD_START(IS_AVG, vec, si) (const uint8_t *_src, int src_stride, \
+                                    const uint8_t * const ref_ptr[], int ref_stride \
+                                    IS_AVG(, const uint8_t *_second_pred), uint32_t *sad_array) { \
+  vec v0, v1, v2, v3, v4, t0, t1, t2, t3, vzero = _mm_setzero_##si(); \
+  vec vsum0 = vzero, vsum1 = vzero, vsum2 = vzero, vsum3 = vzero; \
+  const uint16_t *src = CONVERT_TO_SHORTPTR(_src); \
+  const uint16_t *ref0 = CONVERT_TO_SHORTPTR(ref_ptr[0]); \
+  const uint16_t *ref1 = CONVERT_TO_SHORTPTR(ref_ptr[1]); \
+  const uint16_t *ref2 = CONVERT_TO_SHORTPTR(ref_ptr[2]); \
+  const uint16_t *ref3 = CONVERT_TO_SHORTPTR(ref_ptr[3]); \
+  IS_AVG(const uint16_t *sec = CONVERT_TO_SHORTPTR(_second_pred);) \
+  int i;
+
+#define PROCESS_8x4(IS_AVG, i, j, k, v0, v1, v2, v3) \
+  v0 = _mm_loadu_si128((const __m128i*)(ref0 + i)); \
+  v1 = _mm_loadu_si128((const __m128i*)(ref1 + i)); \
+  v2 = _mm_loadu_si128((const __m128i*)(ref2 + i)); \
+  v3 = _mm_loadu_si128((const __m128i*)(ref3 + i)); \
+  IS_AVG( \
+    v4 = _mm_loadu_si128((const __m128i*)(sec + j)); \
+    v0 = _mm_avg_epu16(v0, v4); \
+    v1 = _mm_avg_epu16(v1, v4); \
+    v2 = _mm_avg_epu16(v2, v4); \
+    v3 = _mm_avg_epu16(v3, v4); \
+  ) \
+  v4 = _mm_loadu_si128((const __m128i*)(src + k)); \
+  v0 = _mm_abs_epi16(_mm_sub_epi16(v0, v4)); \
+  v1 = _mm_abs_epi16(_mm_sub_epi16(v1, v4)); \
+  v2 = _mm_abs_epi16(_mm_sub_epi16(v2, v4)); \
+  v3 = _mm_abs_epi16(_mm_sub_epi16(v3, v4));
+
+#define PROCESS_4x4(IS_AVG, i, j, k, v0, v1, v2, v3) \
+  v0 = *(const __m64*)(ref0 + i); \
+  v1 = *(const __m64*)(ref1 + i); \
+  v2 = *(const __m64*)(ref2 + i); \
+  v3 = *(const __m64*)(ref3 + i); \
+  IS_AVG( \
+    v4 = *(const __m64*)(sec + j); \
+    v0 = _mm_avg_pu16(v0, v4); \
+    v1 = _mm_avg_pu16(v1, v4); \
+    v2 = _mm_avg_pu16(v2, v4); \
+    v3 = _mm_avg_pu16(v3, v4); \
+  ) \
+  v4 = *(const __m64*)(src + k); \
+  v0 = _mm_abs_pi16(_mm_sub_pi16(v0, v4)); \
+  v1 = _mm_abs_pi16(_mm_sub_pi16(v1, v4)); \
+  v2 = _mm_abs_pi16(_mm_sub_pi16(v2, v4)); \
+  v3 = _mm_abs_pi16(_mm_sub_pi16(v3, v4));
+
+#define SAD64XN(h, IS_AVG, name, end) \
+void name##64x##h##end SAD_START(IS_AVG, __m128i, si128) \
+  SAD_LOOP(h, 1) { \
+    PROCESS_8x4(IS_AVG, 0, 0, 0, t0, t1, t2, t3) \
+    PROCESS_8x4(IS_AVG, 8, 8, 8, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    PROCESS_8x4(IS_AVG, 16, 16, 16, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    PROCESS_8x4(IS_AVG, 24, 24, 24, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    vsum0 = _mm_add_epi32(vsum0, _mm_unpacklo_epi16(t0, vzero)); \
+    vsum0 = _mm_add_epi32(vsum0, _mm_unpackhi_epi16(t0, vzero)); \
+    vsum1 = _mm_add_epi32(vsum1, _mm_unpacklo_epi16(t1, vzero)); \
+    vsum1 = _mm_add_epi32(vsum1, _mm_unpackhi_epi16(t1, vzero)); \
+    vsum2 = _mm_add_epi32(vsum2, _mm_unpacklo_epi16(t2, vzero)); \
+    vsum2 = _mm_add_epi32(vsum2, _mm_unpackhi_epi16(t2, vzero)); \
+    vsum3 = _mm_add_epi32(vsum3, _mm_unpacklo_epi16(t3, vzero)); \
+    vsum3 = _mm_add_epi32(vsum3, _mm_unpackhi_epi16(t3, vzero)); \
+    \
+    PROCESS_8x4(IS_AVG, 32, 32, 32, t0, t1, t2, t3) \
+    PROCESS_8x4(IS_AVG, 40, 40, 40, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    PROCESS_8x4(IS_AVG, 48, 48, 48, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    PROCESS_8x4(IS_AVG, 56, 56, 56, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    vsum0 = _mm_add_epi32(vsum0, _mm_unpacklo_epi16(t0, vzero)); \
+    vsum0 = _mm_add_epi32(vsum0, _mm_unpackhi_epi16(t0, vzero)); \
+    vsum1 = _mm_add_epi32(vsum1, _mm_unpacklo_epi16(t1, vzero)); \
+    vsum1 = _mm_add_epi32(vsum1, _mm_unpackhi_epi16(t1, vzero)); \
+    vsum2 = _mm_add_epi32(vsum2, _mm_unpacklo_epi16(t2, vzero)); \
+    vsum2 = _mm_add_epi32(vsum2, _mm_unpackhi_epi16(t2, vzero)); \
+    vsum3 = _mm_add_epi32(vsum3, _mm_unpacklo_epi16(t3, vzero)); \
+    vsum3 = _mm_add_epi32(vsum3, _mm_unpackhi_epi16(t3, vzero)); \
+    IS_AVG(second_pred += 64;) \
+    ref0 += ref_stride; \
+    ref1 += ref_stride; \
+    ref2 += ref_stride; \
+    ref3 += ref_stride; \
+    src += src_stride; \
+  } \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum1); \
+  vsum2 = _mm_hadd_epi32(vsum2, vsum3); \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum2); \
+  _mm_storeu_si128((__m128i*)sad_array, RESULT_SHL(vsum0, epi)); \
+}
+
+#define SAD32XN(h, IS_AVG, name, end) \
+void name##32x##h##end SAD_START(IS_AVG, __m128i, si128) \
+  SAD_LOOP(h, 1) { \
+    PROCESS_8x4(IS_AVG, 0, 0, 0, t0, t1, t2, t3) \
+    PROCESS_8x4(IS_AVG, 8, 8, 8, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    PROCESS_8x4(IS_AVG, 16, 16, 16, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    PROCESS_8x4(IS_AVG, 24, 24, 24, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    vsum0 = _mm_add_epi32(vsum0, _mm_unpacklo_epi16(t0, vzero)); \
+    vsum0 = _mm_add_epi32(vsum0, _mm_unpackhi_epi16(t0, vzero)); \
+    vsum1 = _mm_add_epi32(vsum1, _mm_unpacklo_epi16(t1, vzero)); \
+    vsum1 = _mm_add_epi32(vsum1, _mm_unpackhi_epi16(t1, vzero)); \
+    vsum2 = _mm_add_epi32(vsum2, _mm_unpacklo_epi16(t2, vzero)); \
+    vsum2 = _mm_add_epi32(vsum2, _mm_unpackhi_epi16(t2, vzero)); \
+    vsum3 = _mm_add_epi32(vsum3, _mm_unpacklo_epi16(t3, vzero)); \
+    vsum3 = _mm_add_epi32(vsum3, _mm_unpackhi_epi16(t3, vzero)); \
+    IS_AVG(second_pred += 32;) \
+    ref0 += ref_stride; \
+    ref1 += ref_stride; \
+    ref2 += ref_stride; \
+    ref3 += ref_stride; \
+    src += src_stride; \
+  } \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum1); \
+  vsum2 = _mm_hadd_epi32(vsum2, vsum3); \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum2); \
+  _mm_storeu_si128((__m128i*)sad_array, RESULT_SHL(vsum0, epi)); \
+}
+
+#define SAD16XN(h, IS_AVG, name, end) \
+void name##16x##h##end SAD_START(IS_AVG, __m128i, si128) \
+  SAD_LOOP(h, 2) { \
+    PROCESS_8x4(IS_AVG, 0, 0, 0, t0, t1, t2, t3) \
+    PROCESS_8x4(IS_AVG, 8, 8, 8, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    PROCESS_8x4(IS_AVG, ref_stride, 16, src_stride, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    PROCESS_8x4(IS_AVG, ref_stride + 8, 24, src_stride + 8, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    vsum0 = _mm_add_epi32(vsum0, _mm_unpacklo_epi16(t0, vzero)); \
+    vsum0 = _mm_add_epi32(vsum0, _mm_unpackhi_epi16(t0, vzero)); \
+    vsum1 = _mm_add_epi32(vsum1, _mm_unpacklo_epi16(t1, vzero)); \
+    vsum1 = _mm_add_epi32(vsum1, _mm_unpackhi_epi16(t1, vzero)); \
+    vsum2 = _mm_add_epi32(vsum2, _mm_unpacklo_epi16(t2, vzero)); \
+    vsum2 = _mm_add_epi32(vsum2, _mm_unpackhi_epi16(t2, vzero)); \
+    vsum3 = _mm_add_epi32(vsum3, _mm_unpacklo_epi16(t3, vzero)); \
+    vsum3 = _mm_add_epi32(vsum3, _mm_unpackhi_epi16(t3, vzero)); \
+    IS_AVG(second_pred += 32;) \
+    ref0 += ref_stride << 1; \
+    ref1 += ref_stride << 1; \
+    ref2 += ref_stride << 1; \
+    ref3 += ref_stride << 1; \
+    src += src_stride << 1; \
+  } \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum1); \
+  vsum2 = _mm_hadd_epi32(vsum2, vsum3); \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum2); \
+  _mm_storeu_si128((__m128i*)sad_array, RESULT_SHL(vsum0, epi)); \
+}
+
+#define SAD8XN(h, IS_AVG, name, end) \
+void name##8x##h##end SAD_START(IS_AVG, __m128i, si128) \
+  SAD_LOOP(h, 4) { \
+    PROCESS_8x4(IS_AVG, 0, 0, 0, t0, t1, t2, t3) \
+    PROCESS_8x4(IS_AVG, ref_stride, 8, src_stride, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    PROCESS_8x4(IS_AVG, ref_stride * 2, 16, src_stride * 2, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    PROCESS_8x4(IS_AVG, ref_stride * 3, 24, src_stride * 3, v0, v1, v2, v3) \
+    t0 = _mm_add_epi16(t0, v0); \
+    t1 = _mm_add_epi16(t1, v1); \
+    t2 = _mm_add_epi16(t2, v2); \
+    t3 = _mm_add_epi16(t3, v3); \
+    vsum0 = _mm_add_epi32(vsum0, _mm_unpacklo_epi16(t0, vzero)); \
+    vsum0 = _mm_add_epi32(vsum0, _mm_unpackhi_epi16(t0, vzero)); \
+    vsum1 = _mm_add_epi32(vsum1, _mm_unpacklo_epi16(t1, vzero)); \
+    vsum1 = _mm_add_epi32(vsum1, _mm_unpackhi_epi16(t1, vzero)); \
+    vsum2 = _mm_add_epi32(vsum2, _mm_unpacklo_epi16(t2, vzero)); \
+    vsum2 = _mm_add_epi32(vsum2, _mm_unpackhi_epi16(t2, vzero)); \
+    vsum3 = _mm_add_epi32(vsum3, _mm_unpacklo_epi16(t3, vzero)); \
+    vsum3 = _mm_add_epi32(vsum3, _mm_unpackhi_epi16(t3, vzero)); \
+    IS_AVG(second_pred += 32;) \
+    ref0 += ref_stride << 2; \
+    ref1 += ref_stride << 2; \
+    ref2 += ref_stride << 2; \
+    ref3 += ref_stride << 2; \
+    src += src_stride << 2; \
+  } \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum1); \
+  vsum2 = _mm_hadd_epi32(vsum2, vsum3); \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum2); \
+  _mm_storeu_si128((__m128i*)sad_array, RESULT_SHL(vsum0, epi)); \
+}
+
+#define SAD4XN(h, IS_AVG, name, end) \
+void name##4x##h##end SAD_START(IS_AVG, __m64, si64) \
+  SAD_LOOP(h, 4) { \
+    PROCESS_4x4(IS_AVG, 0, 0, 0, t0, t1, t2, t3) \
+    PROCESS_4x4(IS_AVG, ref_stride, 4, src_stride, v0, v1, v2, v3) \
+    t0 = _mm_add_pi16(t0, v0); \
+    t1 = _mm_add_pi16(t1, v1); \
+    t2 = _mm_add_pi16(t2, v2); \
+    t3 = _mm_add_pi16(t3, v3); \
+    PROCESS_4x4(IS_AVG, ref_stride * 2, 8, src_stride * 2, v0, v1, v2, v3) \
+    t0 = _mm_add_pi16(t0, v0); \
+    t1 = _mm_add_pi16(t1, v1); \
+    t2 = _mm_add_pi16(t2, v2); \
+    t3 = _mm_add_pi16(t3, v3); \
+    PROCESS_4x4(IS_AVG, ref_stride * 3, 12, src_stride * 3, v0, v1, v2, v3) \
+    t0 = _mm_add_pi16(t0, v0); \
+    t1 = _mm_add_pi16(t1, v1); \
+    t2 = _mm_add_pi16(t2, v2); \
+    t3 = _mm_add_pi16(t3, v3); \
+    vsum0 = _mm_add_pi32(vsum0, _mm_unpacklo_pi16(t0, vzero)); \
+    vsum0 = _mm_add_pi32(vsum0, _mm_unpackhi_pi16(t0, vzero)); \
+    vsum1 = _mm_add_pi32(vsum1, _mm_unpacklo_pi16(t1, vzero)); \
+    vsum1 = _mm_add_pi32(vsum1, _mm_unpackhi_pi16(t1, vzero)); \
+    vsum2 = _mm_add_pi32(vsum2, _mm_unpacklo_pi16(t2, vzero)); \
+    vsum2 = _mm_add_pi32(vsum2, _mm_unpackhi_pi16(t2, vzero)); \
+    vsum3 = _mm_add_pi32(vsum3, _mm_unpacklo_pi16(t3, vzero)); \
+    vsum3 = _mm_add_pi32(vsum3, _mm_unpackhi_pi16(t3, vzero)); \
+    IS_AVG(second_pred += 16;) \
+    ref0 += ref_stride << 2; \
+    ref1 += ref_stride << 2; \
+    ref2 += ref_stride << 2; \
+    ref3 += ref_stride << 2; \
+    src += src_stride << 2; \
+  } \
+  *(__m64*)sad_array = RESULT_SHL(_mm_hadd_pi32(vsum0, vsum1), pi); \
+  *(__m64*)(sad_array + 2) = RESULT_SHL(_mm_hadd_pi32(vsum2, vsum3), pi); \
+}
+
+FN(64, 64) FN(64, 32)
+FN(32, 64) FN(32, 32) FN(32, 16)
+FN(16, 32) FN(16, 16) FN(16, 8)
+FN(8, 16) FN(8, 8) FN(8, 4)
+FN(4, 8) FN(4, 4)
+
diff --git a/vpx_dsp/e2k/highbd_sad_e2k.c b/vpx_dsp/e2k/highbd_sad_e2k.c
new file mode 100644
index 0000000..9187b24
--- /dev/null
+++ b/vpx_dsp/e2k/highbd_sad_e2k.c
@@ -0,0 +1,182 @@
+/*
+ *  Copyright (c) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *  Copyright (c) 2016 The WebM project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "./vpx_config.h"
+#include "./vpx_dsp_rtcd.h"
+#include "vpx_ports/mem.h"
+
+#define NO_AVG(...)
+#define WITH_AVG(...) __VA_ARGS__
+
+#define FN(w, h) \
+  SAD##w##XN(h, NO_AVG, vpx_highbd_sad, _sse2) \
+  SAD##w##XN(h, WITH_AVG, vpx_highbd_sad, _avg_sse2)
+#define SAD_LOOP(h, n) \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < h; i += n)
+#define RESULT_SHL(x) x
+
+#define SAD_START(IS_AVG, vec, si) (const uint8_t *_src, int src_stride, \
+                                    const uint8_t *_ref, int ref_stride \
+                                    IS_AVG(, const uint8_t *_second_pred)) { \
+  vec v0, v1, v2, v3, vzero = _mm_setzero_##si(), vsum = vzero; \
+  const uint16_t *src = CONVERT_TO_SHORTPTR(_src); \
+  const uint16_t *ref = CONVERT_TO_SHORTPTR(_ref); \
+  IS_AVG(const uint16_t *sec = CONVERT_TO_SHORTPTR(_second_pred);) \
+  int i;
+
+#define SAD64XN(h, IS_AVG, name, end) \
+unsigned int name##64x##h##end SAD_START(IS_AVG, __m128i, si128) \
+  SAD_LOOP(h, 1) { \
+    v0 = _mm_loadu_si128((const __m128i*)ref); \
+    v1 = _mm_loadu_si128((const __m128i*)(ref + 8)); \
+    v2 = _mm_loadu_si128((const __m128i*)(ref + 16)); \
+    v3 = _mm_loadu_si128((const __m128i*)(ref + 24)); \
+    IS_AVG( \
+      v0 = _mm_avg_epu16(v0, _mm_loadu_si128((const __m128i*)sec)); \
+      v1 = _mm_avg_epu16(v1, _mm_loadu_si128((const __m128i*)(sec + 8))); \
+      v2 = _mm_avg_epu16(v2, _mm_loadu_si128((const __m128i*)(sec + 16))); \
+      v3 = _mm_avg_epu16(v3, _mm_loadu_si128((const __m128i*)(sec + 24))); \
+    ) \
+    v0 = _mm_sub_epi16(v0, _mm_loadu_si128((const __m128i*)src)); \
+    v1 = _mm_sub_epi16(v1, _mm_loadu_si128((const __m128i*)(src + 8))); \
+    v2 = _mm_sub_epi16(v2, _mm_loadu_si128((const __m128i*)(src + 16))); \
+    v3 = _mm_sub_epi16(v3, _mm_loadu_si128((const __m128i*)(src + 24))); \
+    v0 = _mm_add_epi16(_mm_abs_epi16(v0), _mm_abs_epi16(v1)); \
+    v2 = _mm_add_epi16(_mm_abs_epi16(v2), _mm_abs_epi16(v3)); \
+    v0 = _mm_add_epi16(v0, v2); \
+    vsum = _mm_add_epi32(vsum, _mm_unpacklo_epi16(v0, vzero)); \
+    vsum = _mm_add_epi32(vsum, _mm_unpackhi_epi16(v0, vzero)); \
+    \
+    v0 = _mm_loadu_si128((const __m128i*)(ref + 32)); \
+    v1 = _mm_loadu_si128((const __m128i*)(ref + 40)); \
+    v2 = _mm_loadu_si128((const __m128i*)(ref + 48)); \
+    v3 = _mm_loadu_si128((const __m128i*)(ref + 56)); \
+    IS_AVG( \
+      v0 = _mm_avg_epu16(v0, _mm_loadu_si128((const __m128i*)(sec + 32))); \
+      v1 = _mm_avg_epu16(v1, _mm_loadu_si128((const __m128i*)(sec + 40))); \
+      v2 = _mm_avg_epu16(v2, _mm_loadu_si128((const __m128i*)(sec + 48))); \
+      v3 = _mm_avg_epu16(v3, _mm_loadu_si128((const __m128i*)(sec + 56))); \
+      sec += 64; \
+    ) \
+    v0 = _mm_sub_epi16(v0, _mm_loadu_si128((const __m128i*)(src + 32))); \
+    v1 = _mm_sub_epi16(v1, _mm_loadu_si128((const __m128i*)(src + 40))); \
+    v2 = _mm_sub_epi16(v2, _mm_loadu_si128((const __m128i*)(src + 48))); \
+    v3 = _mm_sub_epi16(v3, _mm_loadu_si128((const __m128i*)(src + 56))); \
+    v0 = _mm_add_epi16(_mm_abs_epi16(v0), _mm_abs_epi16(v1)); \
+    v2 = _mm_add_epi16(_mm_abs_epi16(v2), _mm_abs_epi16(v3)); \
+    v0 = _mm_add_epi16(v0, v2); \
+    vsum = _mm_add_epi32(vsum, _mm_unpacklo_epi16(v0, vzero)); \
+    vsum = _mm_add_epi32(vsum, _mm_unpackhi_epi16(v0, vzero)); \
+    ref += ref_stride; \
+    src += src_stride; \
+  } \
+  vsum = _mm_hadd_epi32(vsum, vsum); \
+  return RESULT_SHL(_mm_cvtsi128_si32(vsum) + _mm_extract_epi32(vsum, 1)); \
+}
+
+#define SAD32XN(h, IS_AVG, name, end) \
+unsigned int name##32x##h##end SAD_START(IS_AVG, __m128i, si128) \
+  SAD_LOOP(h, 1) { \
+    v0 = _mm_loadu_si128((const __m128i*)ref); \
+    v1 = _mm_loadu_si128((const __m128i*)(ref + 8)); \
+    v2 = _mm_loadu_si128((const __m128i*)(ref + 16)); \
+    v3 = _mm_loadu_si128((const __m128i*)(ref + 24)); \
+    IS_AVG( \
+      v0 = _mm_avg_epu16(v0, _mm_loadu_si128((const __m128i*)sec)); \
+      v1 = _mm_avg_epu16(v1, _mm_loadu_si128((const __m128i*)(sec + 8))); \
+      v2 = _mm_avg_epu16(v2, _mm_loadu_si128((const __m128i*)(sec + 16))); \
+      v3 = _mm_avg_epu16(v3, _mm_loadu_si128((const __m128i*)(sec + 24))); \
+      sec += 32; \
+    ) \
+    v0 = _mm_sub_epi16(v0, _mm_loadu_si128((const __m128i*)src)); \
+    v1 = _mm_sub_epi16(v1, _mm_loadu_si128((const __m128i*)(src + 8))); \
+    v2 = _mm_sub_epi16(v2, _mm_loadu_si128((const __m128i*)(src + 16))); \
+    v3 = _mm_sub_epi16(v3, _mm_loadu_si128((const __m128i*)(src + 24))); \
+    v0 = _mm_add_epi16(_mm_abs_epi16(v0), _mm_abs_epi16(v1)); \
+    v2 = _mm_add_epi16(_mm_abs_epi16(v2), _mm_abs_epi16(v3)); \
+    v0 = _mm_add_epi16(v0, v2); \
+    vsum = _mm_add_epi32(vsum, _mm_unpacklo_epi16(v0, vzero)); \
+    vsum = _mm_add_epi32(vsum, _mm_unpackhi_epi16(v0, vzero)); \
+    ref += ref_stride; \
+    src += src_stride; \
+  } \
+  vsum = _mm_hadd_epi32(vsum, vsum); \
+  return RESULT_SHL(_mm_cvtsi128_si32(vsum) + _mm_extract_epi32(vsum, 1)); \
+}
+
+#define SAD16XN(h, IS_AVG, name, end) \
+unsigned int name##16x##h##end SAD_START(IS_AVG, __m128i, si128) \
+  SAD_LOOP(h, 2) { \
+    v0 = _mm_loadu_si128((const __m128i*)ref); \
+    v1 = _mm_loadu_si128((const __m128i*)(ref + 8)); \
+    v2 = _mm_loadu_si128((const __m128i*)(ref + ref_stride)); \
+    v3 = _mm_loadu_si128((const __m128i*)(ref + ref_stride + 8)); \
+    IS_AVG( \
+      v0 = _mm_avg_epu16(v0, _mm_loadu_si128((const __m128i*)sec)); \
+      v1 = _mm_avg_epu16(v1, _mm_loadu_si128((const __m128i*)(sec + 8))); \
+      v2 = _mm_avg_epu16(v2, _mm_loadu_si128((const __m128i*)(sec + 16))); \
+      v3 = _mm_avg_epu16(v3, _mm_loadu_si128((const __m128i*)(sec + 24))); \
+      sec += 32; \
+    ) \
+    v0 = _mm_sub_epi16(v0, _mm_loadu_si128((const __m128i*)src)); \
+    v1 = _mm_sub_epi16(v1, _mm_loadu_si128((const __m128i*)(src + 8))); \
+    v2 = _mm_sub_epi16(v2, _mm_loadu_si128((const __m128i*)(src + src_stride))); \
+    v3 = _mm_sub_epi16(v3, _mm_loadu_si128((const __m128i*)(src + src_stride + 8))); \
+    v0 = _mm_add_epi16(_mm_abs_epi16(v0), _mm_abs_epi16(v1)); \
+    v2 = _mm_add_epi16(_mm_abs_epi16(v2), _mm_abs_epi16(v3)); \
+    v0 = _mm_add_epi16(v0, v2); \
+    vsum = _mm_add_epi32(vsum, _mm_unpacklo_epi16(v0, vzero)); \
+    vsum = _mm_add_epi32(vsum, _mm_unpackhi_epi16(v0, vzero)); \
+    ref += ref_stride << 1; \
+    src += src_stride << 1; \
+  } \
+  vsum = _mm_hadd_epi32(vsum, vsum); \
+  return RESULT_SHL(_mm_cvtsi128_si32(vsum) + _mm_extract_epi32(vsum, 1)); \
+}
+
+#define SAD8XN(h, IS_AVG, name, end) \
+unsigned int name##8x##h##end SAD_START(IS_AVG, __m128i, si128) \
+  SAD_LOOP(h, 4) { \
+    v0 = _mm_loadu_si128((const __m128i*)ref); \
+    v1 = _mm_loadu_si128((const __m128i*)(ref + ref_stride)); \
+    v2 = _mm_loadu_si128((const __m128i*)(ref + ref_stride * 2)); \
+    v3 = _mm_loadu_si128((const __m128i*)(ref + ref_stride * 3)); \
+    IS_AVG( \
+      v0 = _mm_avg_epu16(v0, _mm_loadu_si128((const __m128i*)sec)); \
+      v1 = _mm_avg_epu16(v1, _mm_loadu_si128((const __m128i*)(sec + 8))); \
+      v2 = _mm_avg_epu16(v2, _mm_loadu_si128((const __m128i*)(sec + 16))); \
+      v3 = _mm_avg_epu16(v3, _mm_loadu_si128((const __m128i*)(sec + 24))); \
+      sec += 32; \
+    ) \
+    v0 = _mm_sub_epi16(v0, _mm_loadu_si128((const __m128i*)src)); \
+    v1 = _mm_sub_epi16(v1, _mm_loadu_si128((const __m128i*)(src + src_stride))); \
+    v2 = _mm_sub_epi16(v2, _mm_loadu_si128((const __m128i*)(src + src_stride * 2))); \
+    v3 = _mm_sub_epi16(v3, _mm_loadu_si128((const __m128i*)(src + src_stride * 3))); \
+    v0 = _mm_add_epi16(_mm_abs_epi16(v0), _mm_abs_epi16(v1)); \
+    v2 = _mm_add_epi16(_mm_abs_epi16(v2), _mm_abs_epi16(v3)); \
+    v0 = _mm_add_epi16(v0, v2); \
+    vsum = _mm_add_epi32(vsum, _mm_unpacklo_epi16(v0, vzero)); \
+    vsum = _mm_add_epi32(vsum, _mm_unpackhi_epi16(v0, vzero)); \
+    ref += ref_stride << 2; \
+    src += src_stride << 2; \
+  } \
+  vsum = _mm_hadd_epi32(vsum, vsum); \
+  return RESULT_SHL(_mm_cvtsi128_si32(vsum) + _mm_extract_epi32(vsum, 1)); \
+}
+
+FN(64, 64) FN(64, 32)
+FN(32, 64) FN(32, 32) FN(32, 16)
+FN(16, 32) FN(16, 16) FN(16, 8)
+FN(8, 16) FN(8, 8) FN(8, 4)
+
diff --git a/vpx_dsp/e2k/highbd_subpel_variance_e2k.c b/vpx_dsp/e2k/highbd_subpel_variance_e2k.c
new file mode 100644
index 0000000..b7a3845
--- /dev/null
+++ b/vpx_dsp/e2k/highbd_subpel_variance_e2k.c
@@ -0,0 +1,254 @@
+/*
+ *  Copyright (c) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *  Copyright (c) 2016 The WebM project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "./vpx_config.h"
+#include "./vpx_dsp_rtcd.h"
+
+#define SEC_NONE
+#define SEC_AVG16 \
+  t0 = _mm_loadu_si128((__m128i const *)sec); \
+  t1 = _mm_loadu_si128((__m128i const *)(sec + 8)); \
+  v0 = _mm_avg_epu16(v0, t0); \
+  v1 = _mm_avg_epu16(v1, t1); \
+  sec += sec_stride;
+#define SEC_AVG8 \
+  t0 = _mm_loadu_si128((__m128i const *)sec); \
+  t1 = _mm_loadu_si128((__m128i const *)(sec + sec_stride)); \
+  v0 = _mm_avg_epu16(v0, t0); \
+  v1 = _mm_avg_epu16(v1, t1); \
+  sec += sec_stride << 1;
+
+#define UPDATE_SUM_SSE(SEC_AVG, dst_offset, dst_stride) \
+  SEC_AVG \
+  t0 = _mm_loadu_si128((__m128i const *)(dst)); \
+  t1 = _mm_loadu_si128((__m128i const *)(dst + dst_offset)); \
+  dst += dst_stride; \
+  v0 = _mm_sub_epi16(v0, t0); \
+  v1 = _mm_sub_epi16(v1, t1); \
+  t0 = _mm_add_epi16(v0, v1); \
+  t1 = _mm_srai_epi16(t0, 16); \
+  vsse = _mm_add_epi32(vsse, _mm_madd_epi16(v0, v0)); \
+  vsse = _mm_add_epi32(vsse, _mm_madd_epi16(v1, v1)); \
+  vsum = _mm_add_epi32(vsum, _mm_unpacklo_epi16(t0, t1)); \
+  vsum = _mm_add_epi32(vsum, _mm_unpackhi_epi16(t0, t1));
+
+#define RETURN_SUM_SSE \
+  vsse = _mm_add_epi32(vsse, _mm_bsrli_si128(vsse, 8)); \
+  vsum = _mm_add_epi32(vsum, _mm_bsrli_si128(vsum, 8)); \
+  *sse = _mm_cvtsi128_si32(vsse) + _mm_extract_epi32(vsse, 1); \
+  return _mm_cvtsi128_si32(vsum) + _mm_extract_epi32(vsum, 1);
+
+#define LOADX(v0, v1) \
+  v0 = _mm_loadu_si128((__m128i const *)(src)); \
+  v1 = _mm_loadu_si128((__m128i const *)(src + 8));
+
+#define AVGX(v0, v1) \
+  LOADX(t0, t1) \
+  t2 = _mm_loadu_si128((__m128i const *)(src + 1)); \
+  t3 = _mm_loadu_si128((__m128i const *)(src + 8 + 1)); \
+  v0 = _mm_avg_epu16(t0, t2); \
+  v1 = _mm_avg_epu16(t1, t3);
+
+#define FILTERX(v0, v1) \
+  LOADX(t0, t1) \
+  t2 = _mm_loadu_si128((__m128i const *)(src + 1)); \
+  t3 = _mm_loadu_si128((__m128i const *)(src + 8 + 1)); \
+  t2 = _mm_mulhrs_epi16(_mm_sub_epi16(t2, t0), xfilter); \
+  t3 = _mm_mulhrs_epi16(_mm_sub_epi16(t3, t1), xfilter); \
+  v0 = _mm_add_epi16(t0, t2); \
+  v1 = _mm_add_epi16(t1, t3);
+
+#define AVGY(v0, v1, v2, v3) \
+  v0 = _mm_avg_epu16(v0, v2); \
+  v1 = _mm_avg_epu16(v1, v3);
+
+#define FILTERY(v0, v1, v2, v3) \
+  t2 = _mm_mulhrs_epi16(_mm_sub_epi16(v2, v0), yfilter); \
+  t3 = _mm_mulhrs_epi16(_mm_sub_epi16(v3, v1), yfilter); \
+  v0 = _mm_add_epi16(v0, t2); \
+  v1 = _mm_add_epi16(v1, t3);
+
+#define VAR16XH(LOADX, SEC_AVG) \
+  if (y_offset == 0) { \
+    for (i = 0; i < height; i++) { \
+      LOADX(v0, v1) \
+      UPDATE_SUM_SSE(SEC_AVG, 8, dst_stride) \
+      src += src_stride; \
+    } \
+  } else if (y_offset == 4) { \
+    LOADX(v0, v1) \
+    for (i = 0; i < height; i++) { \
+      src += src_stride; \
+      LOADX(v2, v3) \
+      AVGY(v0, v1, v2, v3) \
+      UPDATE_SUM_SSE(SEC_AVG, 8, dst_stride) \
+      v0 = v2; v1 = v3; \
+    } \
+  } else { \
+    yfilter = _mm_set1_epi16(y_offset << 12); \
+    LOADX(v0, v1) \
+    for (i = 0; i < height; i++) { \
+      src += src_stride; \
+      LOADX(v2, v3) \
+      FILTERY(v0, v1, v2, v3) \
+      UPDATE_SUM_SSE(SEC_AVG, 8, dst_stride) \
+      v0 = v2; v1 = v3; \
+    } \
+  }
+
+int vpx_highbd_sub_pixel_variance16xh_sse2(
+    const uint16_t *src, ptrdiff_t src_stride, int x_offset, int y_offset,
+    const uint16_t *dst, ptrdiff_t dst_stride, int height,
+    unsigned int *sse, void *unused0, void *unused) {
+  __m128i v0, v1, v2, v3, t0, t1, t2, t3, vzero = _mm_setzero_si128();
+  __m128i vsum = vzero, vsse = vzero, xfilter, yfilter;
+  int i;
+  (void)unused0; (void)unused;
+
+  if (x_offset == 0) {
+    VAR16XH(LOADX, SEC_NONE)
+  } else if (x_offset == 4) {
+    VAR16XH(AVGX, SEC_NONE)
+  } else {
+    xfilter = _mm_set1_epi16(x_offset << 12);
+    VAR16XH(FILTERX, SEC_NONE)
+  }
+  RETURN_SUM_SSE
+}
+
+int vpx_highbd_sub_pixel_avg_variance16xh_sse2(
+    const uint16_t *src, ptrdiff_t src_stride, int x_offset, int y_offset,
+    const uint16_t *dst, ptrdiff_t dst_stride, const uint16_t *sec,
+    ptrdiff_t sec_stride, int height, unsigned int *sse, void *unused0,
+    void *unused) {
+  __m128i v0, v1, v2, v3, t0, t1, t2, t3, vzero = _mm_setzero_si128();
+  __m128i vsum = vzero, vsse = vzero, xfilter, yfilter;
+  int i;
+  (void)unused0; (void)unused;
+
+  if (x_offset == 0) {
+    VAR16XH(LOADX, SEC_AVG16)
+  } else if (x_offset == 4) {
+    VAR16XH(AVGX, SEC_AVG16)
+  } else {
+    xfilter = _mm_set1_epi16(x_offset << 12);
+    VAR16XH(FILTERX, SEC_AVG16)
+  }
+  RETURN_SUM_SSE
+}
+
+#define LOADX1 \
+  v0 = _mm_loadu_si128((__m128i const *)src);
+
+#define AVGX1 \
+  LOADX1 \
+  t2 = _mm_loadu_si128((__m128i const *)(src + 1)); \
+  v0 = _mm_avg_epu16(v0, t2);
+
+#define FILTERX1 \
+  LOADX1 \
+  t2 = _mm_loadu_si128((__m128i const *)(src + 1)); \
+  t2 = _mm_mulhrs_epi16(_mm_sub_epi16(t2, v0), xfilter); \
+  v0 = _mm_add_epi16(v0, t2);
+
+#define LOADX2(v0, v1) \
+  v0 = _mm_loadu_si128((__m128i const *)src); \
+  v1 = _mm_loadu_si128((__m128i const *)(src + src_stride));
+
+#define AVGX2(v0, v1) \
+  LOADX2(t0, t1) \
+  t2 = _mm_loadu_si128((__m128i const *)(src + 1)); \
+  t3 = _mm_loadu_si128((__m128i const *)(src + src_stride + 1)); \
+  v0 = _mm_avg_epu16(t0, t2); \
+  v1 = _mm_avg_epu16(t1, t3);
+
+#define FILTERX2(v0, v1) \
+  LOADX2(t0, t1) \
+  t2 = _mm_loadu_si128((__m128i const *)(src + 1)); \
+  t3 = _mm_loadu_si128((__m128i const *)(src + src_stride + 1)); \
+  t2 = _mm_mulhrs_epi16(_mm_sub_epi16(t2, t0), xfilter); \
+  t3 = _mm_mulhrs_epi16(_mm_sub_epi16(t3, t1), xfilter); \
+  v0 = _mm_add_epi16(t0, t2); \
+  v1 = _mm_add_epi16(t1, t3);
+
+#define VAR8XH(LOADX, SEC_AVG) \
+  if (y_offset == 0) { \
+    for (i = 0; i < height; i += 2) { \
+      LOADX##2(v0, v1) \
+      UPDATE_SUM_SSE(SEC_AVG, dst_stride, dst_stride << 1) \
+      src += src_stride << 1; \
+    } \
+  } else if (y_offset == 4) { \
+    LOADX##1 \
+    src += src_stride; \
+    for (i = 0; i < height; i += 2) { \
+      LOADX##2(v1, v2) \
+      src += src_stride << 1; \
+      AVGY(v0, v1, v1, v2) \
+      UPDATE_SUM_SSE(SEC_AVG, dst_stride, dst_stride << 1) \
+      v0 = v2; \
+    } \
+  } else { \
+    yfilter = _mm_set1_epi16(y_offset << 12); \
+    LOADX##1 \
+    src += src_stride; \
+    for (i = 0; i < height; i += 2) { \
+      LOADX##2(v1, v2) \
+      src += src_stride << 1; \
+      FILTERY(v0, v1, v1, v2) \
+      UPDATE_SUM_SSE(SEC_AVG, dst_stride, dst_stride << 1) \
+      v0 = v2; \
+    } \
+  }
+
+int vpx_highbd_sub_pixel_variance8xh_sse2(
+    const uint16_t *src, ptrdiff_t src_stride, int x_offset, int y_offset,
+    const uint16_t *dst, ptrdiff_t dst_stride, int height,
+    unsigned int *sse, void *unused0, void *unused) {
+  __m128i v0, v1, v2, t0, t1, t2, t3, vzero = _mm_setzero_si128();
+  __m128i vsum = vzero, vsse = vzero, xfilter, yfilter;
+  int i;
+  (void)unused0; (void)unused;
+
+  if (x_offset == 0) {
+    VAR8XH(LOADX, SEC_NONE)
+  } else if (x_offset == 4) {
+    VAR8XH(AVGX, SEC_NONE)
+  } else {
+    xfilter = _mm_set1_epi16(x_offset << 12);
+    VAR8XH(FILTERX, SEC_NONE)
+  }
+  RETURN_SUM_SSE
+}
+
+int vpx_highbd_sub_pixel_avg_variance8xh_sse2(
+    const uint16_t *src, ptrdiff_t src_stride, int x_offset, int y_offset,
+    const uint16_t *dst, ptrdiff_t dst_stride, const uint16_t *sec,
+    ptrdiff_t sec_stride, int height, unsigned int *sse, void *unused0,
+    void *unused) {
+  __m128i v0, v1, v2, t0, t1, t2, t3, vzero = _mm_setzero_si128();
+  __m128i vsum = vzero, vsse = vzero, xfilter, yfilter;
+  int i;
+  (void)unused0; (void)unused;
+
+  if (x_offset == 0) {
+    VAR8XH(LOADX, SEC_AVG8)
+  } else if (x_offset == 4) {
+    VAR8XH(AVGX, SEC_AVG8)
+  } else {
+    xfilter = _mm_set1_epi16(x_offset << 12);
+    VAR8XH(FILTERX, SEC_AVG8)
+  }
+  RETURN_SUM_SSE
+}
+
diff --git a/vpx_dsp/e2k/highbd_variance_impl_e2k.c b/vpx_dsp/e2k/highbd_variance_impl_e2k.c
new file mode 100644
index 0000000..d240bf1
--- /dev/null
+++ b/vpx_dsp/e2k/highbd_variance_impl_e2k.c
@@ -0,0 +1,104 @@
+/*
+ *  Copyright (c) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *  Copyright (c) 2016 The WebM project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "./vpx_config.h"
+#include "./vpx_dsp_rtcd.h"
+
+void vpx_highbd_calc16x16var_sse2(const uint16_t *src, int src_stride,
+                                  const uint16_t *ref, int ref_stride,
+                                  uint32_t *sse, int *sum) {
+  __m128i v0, v1, v2, vzero = _mm_setzero_si128();
+  __m128i vsse = vzero, vsum = vzero;
+  int i;
+
+  for (i = 0; i < 16; i += 2) {
+      v0 = _mm_loadu_si128((const __m128i *)src);
+      v1 = _mm_loadu_si128((const __m128i *)ref);
+      v2 = _mm_sub_epi16(v0, v1);
+      vsse = _mm_add_epi32(vsse, _mm_madd_epi16(v2, v2));
+
+      v0 = _mm_loadu_si128((const __m128i *)(src + 8));
+      v1 = _mm_loadu_si128((const __m128i *)(ref + 8));
+      v0 = _mm_sub_epi16(v0, v1);
+      v2 = _mm_add_epi16(v2, v0);
+      vsse = _mm_add_epi32(vsse, _mm_madd_epi16(v0, v0));
+
+      v0 = _mm_loadu_si128((const __m128i *)(src + src_stride));
+      v1 = _mm_loadu_si128((const __m128i *)(ref + ref_stride));
+      v0 = _mm_sub_epi16(v0, v1);
+      v2 = _mm_add_epi16(v2, v0);
+      vsse = _mm_add_epi32(vsse, _mm_madd_epi16(v0, v0));
+
+      v0 = _mm_loadu_si128((const __m128i *)(src + src_stride + 8));
+      v1 = _mm_loadu_si128((const __m128i *)(ref + ref_stride + 8));
+      v0 = _mm_sub_epi16(v0, v1);
+      v2 = _mm_add_epi16(v2, v0);
+      vsse = _mm_add_epi32(vsse, _mm_madd_epi16(v0, v0));
+
+      v0 = _mm_srai_epi16(v2, 15);
+      vsum = _mm_add_epi32(vsum, _mm_unpacklo_epi16(v2, v0));
+      vsum = _mm_add_epi32(vsum, _mm_unpackhi_epi16(v2, v0));
+      src += src_stride << 1;
+      ref += ref_stride << 1;
+  }
+
+  vsse = _mm_add_epi32(vsse, _mm_bsrli_si128(vsse, 8));
+  vsum = _mm_add_epi32(vsum, _mm_bsrli_si128(vsum, 8));
+  *sse = _mm_cvtsi128_si32(vsse) + _mm_extract_epi32(vsse, 1);
+  *sum = _mm_cvtsi128_si32(vsum) + _mm_extract_epi32(vsum, 1);
+}
+
+void vpx_highbd_calc8x8var_sse2(const uint16_t *src, int src_stride,
+                                const uint16_t *ref, int ref_stride,
+                                uint32_t *sse, int *sum) {
+  __m128i v0, v1, v2, vzero = _mm_setzero_si128();
+  __m128i vsse = vzero, vsum = vzero;
+  int i;
+
+  for (i = 0; i < 8; i += 4) {
+      v0 = _mm_loadu_si128((const __m128i *)src);
+      v1 = _mm_loadu_si128((const __m128i *)ref);
+      v2 = _mm_sub_epi16(v0, v1);
+      vsse = _mm_add_epi32(vsse, _mm_madd_epi16(v2, v2));
+
+      v0 = _mm_loadu_si128((const __m128i *)(src + src_stride));
+      v1 = _mm_loadu_si128((const __m128i *)(ref + ref_stride));
+      v0 = _mm_sub_epi16(v0, v1);
+      v2 = _mm_add_epi16(v2, v0);
+      vsse = _mm_add_epi32(vsse, _mm_madd_epi16(v0, v0));
+
+      v0 = _mm_loadu_si128((const __m128i *)(src + src_stride * 2));
+      v1 = _mm_loadu_si128((const __m128i *)(ref + ref_stride * 2));
+      v0 = _mm_sub_epi16(v0, v1);
+      v2 = _mm_add_epi16(v2, v0);
+      vsse = _mm_add_epi32(vsse, _mm_madd_epi16(v0, v0));
+
+      v0 = _mm_loadu_si128((const __m128i *)(src + src_stride * 3));
+      v1 = _mm_loadu_si128((const __m128i *)(ref + ref_stride * 3));
+      v0 = _mm_sub_epi16(v0, v1);
+      v2 = _mm_add_epi16(v2, v0);
+      vsse = _mm_add_epi32(vsse, _mm_madd_epi16(v0, v0));
+
+      v0 = _mm_srai_epi16(v2, 15);
+      vsum = _mm_add_epi32(vsum, _mm_unpacklo_epi16(v2, v0));
+      vsum = _mm_add_epi32(vsum, _mm_unpackhi_epi16(v2, v0));
+      src += src_stride << 2;
+      ref += ref_stride << 2;
+  }
+
+  vsse = _mm_add_epi32(vsse, _mm_bsrli_si128(vsse, 8));
+  vsum = _mm_add_epi32(vsum, _mm_bsrli_si128(vsum, 8));
+  *sse = _mm_cvtsi128_si32(vsse) + _mm_extract_epi32(vsse, 1);
+  *sum = _mm_cvtsi128_si32(vsum) + _mm_extract_epi32(vsum, 1);
+}
+
diff --git a/vpx_dsp/e2k/intrapred_e2k.c b/vpx_dsp/e2k/intrapred_e2k.c
new file mode 100644
index 0000000..f4b2c06
--- /dev/null
+++ b/vpx_dsp/e2k/intrapred_e2k.c
@@ -0,0 +1,408 @@
+/*
+ *  Copyright (c) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *  Copyright (c) 2016 The WebM project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "./vpx_config.h"
+#include "./vpx_dsp_rtcd.h"
+
+#define DC_SUM_8(ref, v0) \
+  t0 = *(__m64 const *)ref; \
+  v0 = _mm_sad_pu8(t0, vzero); \
+
+#define DC_SUM_16(ref, v0) \
+  t0 = _mm_load_si128((__m128i const *)ref); \
+  v0 = _mm_sad_epu8(t0, vzero); \
+
+#define DC_SUM_32(ref, v0) \
+  t0 = _mm_load_si128((__m128i const *)ref); \
+  t1 = _mm_load_si128((__m128i const *)(ref + 16)); \
+  t0 = _mm_sad_epu8(t0, vzero); \
+  t1 = _mm_sad_epu8(t1, vzero); \
+  v0 = _mm_add_epi64(t0, t1);
+
+#define DC_STORE_4XH(val, h, dst, stride) \
+  sum = _mm_cvtsi64_si32(_mm_set1_pi8(val)); \
+  for (i = 0; i < h; i++, dst += stride) { \
+    *(uint32_t*)dst = sum; \
+  }
+
+#define DC_STORE_8XH(val, h, dst, stride) \
+  v0 = _mm_set1_pi8(val); \
+  for (i = 0; i < h; i++, dst += stride) { \
+    *(__m64*)dst = v0; \
+  }
+
+#define DC_STORE_16XH(val, h, dst, stride) \
+  v0 = _mm_set1_epi8(val); \
+  for (i = 0; i < h; i++, dst += stride) { \
+    _mm_store_si128((__m128i *)dst, v0); \
+  }
+
+#define DC_STORE_32XH(val, h, dst, stride) \
+  v0 = _mm_set1_epi8(val); \
+  for (i = 0; i < h; i++, dst += stride) { \
+    _mm_store_si128((__m128i *)dst, v0); \
+    _mm_store_si128((__m128i *)(dst + 16), v0); \
+  }
+
+#define FUNC(name) void vpx_##name( \
+  uint8_t *dst, ptrdiff_t stride, const uint8_t *above, const uint8_t *left)
+
+FUNC(dc_128_predictor_4x4_sse2) {
+  int32_t sum = 0x80808080; int i;
+  (void)above; (void)left;
+  for (i = 0; i < 4; i++, dst += stride) {
+    *(uint32_t*)dst = sum;
+  }
+}
+
+FUNC(dc_128_predictor_8x8_sse2) {
+  __m64 v0; int i;
+  (void)above; (void)left;
+  DC_STORE_8XH(128, 8, dst, stride);
+}
+
+FUNC(dc_128_predictor_16x16_sse2) {
+  __m128i v0; int i;
+  (void)above; (void)left;
+  DC_STORE_16XH(128, 16, dst, stride);
+}
+
+FUNC(dc_128_predictor_32x32_sse2) {
+  __m128i v0; int i;
+  (void)above; (void)left;
+  DC_STORE_32XH(128, 32, dst, stride);
+}
+
+FUNC(dc_top_predictor_4x4_sse2) {
+  __m64 v0, vzero = _mm_setzero_si64();
+  int32_t sum; int i;
+  (void)left;
+  v0 = _mm_cvtsi32_si64(*(uint32_t const *)above);
+  v0 = _mm_sad_pu8(v0, vzero);
+  sum = _mm_cvtsi64_si32(v0);
+  sum = (sum + 2) >> 2;
+  DC_STORE_4XH(sum, 4, dst, stride);
+}
+
+FUNC(dc_top_predictor_8x8_sse2) {
+  __m64 t0, v0, vzero = _mm_setzero_si64();
+  int32_t sum; int i;
+  (void)left;
+  DC_SUM_8(above, v0);
+  sum = _mm_cvtsi64_si32(v0);
+  sum = (sum + 4) >> 3;
+  DC_STORE_8XH(sum, 8, dst, stride);
+}
+
+FUNC(dc_top_predictor_16x16_sse2) {
+  __m128i t0, v0, vzero = _mm_setzero_si128();
+  int32_t sum; int i;
+  (void)left;
+  DC_SUM_16(above, v0);
+  sum = _mm_cvtsi128_si32(v0) + _mm_extract_epi32(v0, 2);;
+  sum = (sum + 8) >> 4;
+  DC_STORE_16XH(sum, 16, dst, stride);
+}
+
+FUNC(dc_top_predictor_32x32_sse2) {
+  __m128i t0, t1, v0, vzero = _mm_setzero_si128();
+  int32_t sum; int i;
+  (void)left;
+  DC_SUM_32(above, v0);
+  sum = _mm_cvtsi128_si32(v0) + _mm_extract_epi32(v0, 2);;
+  sum = (sum + 16) >> 5;
+  DC_STORE_32XH(sum, 32, dst, stride);
+}
+
+FUNC(dc_left_predictor_4x4_sse2) {
+  __m64 v0, vzero = _mm_setzero_si64();
+  int32_t sum; int i;
+  (void)above;
+  v0 = _mm_cvtsi32_si64(*(uint32_t const *)left);
+  v0 = _mm_sad_pu8(v0, vzero);
+  sum = _mm_cvtsi64_si32(v0);
+  sum = (sum + 2) >> 2;
+  DC_STORE_4XH(sum, 4, dst, stride);
+}
+
+FUNC(dc_left_predictor_8x8_sse2) {
+  __m64 t0, v0, vzero = _mm_setzero_si64();
+  int32_t sum; int i;
+  (void)above;
+  DC_SUM_8(left, v0);
+  sum = _mm_cvtsi64_si32(v0);
+  sum = (sum + 4) >> 3;
+  DC_STORE_8XH(sum, 8, dst, stride);
+}
+
+FUNC(dc_left_predictor_16x16_sse2) {
+  __m128i t0, v0, vzero = _mm_setzero_si128();
+  int32_t sum; int i;
+  (void)above;
+  DC_SUM_16(left, v0);
+  sum = _mm_cvtsi128_si32(v0) + _mm_extract_epi32(v0, 2);;
+  sum = (sum + 8) >> 4;
+  DC_STORE_16XH(sum, 16, dst, stride);
+}
+
+FUNC(dc_left_predictor_32x32_sse2) {
+  __m128i t0, t1, v0, vzero = _mm_setzero_si128();
+  int32_t sum; int i;
+  (void)above;
+  DC_SUM_32(left, v0);
+  sum = _mm_cvtsi128_si32(v0) + _mm_extract_epi32(v0, 2);;
+  sum = (sum + 16) >> 5;
+  DC_STORE_32XH(sum, 32, dst, stride);
+}
+
+FUNC(dc_predictor_4x4_sse2) {
+  __m64 v0, v1, vzero = _mm_setzero_si64();
+  int32_t sum; int i;
+  v0 = _mm_cvtsi32_si64(*(uint32_t const *)above);
+  v1 = _mm_cvtsi32_si64(*(uint32_t const *)left);
+  v0 = _mm_sad_pu8(_mm_unpacklo_pi32(v0, v1), vzero);
+  sum = _mm_cvtsi64_si32(v0);
+  sum = (sum + 4) >> 3;
+  DC_STORE_4XH(sum, 4, dst, stride);
+}
+
+FUNC(dc_predictor_8x8_sse2) {
+  __m64 t0, v0, v1, vzero = _mm_setzero_si64();
+  int32_t sum; int i;
+  DC_SUM_8(above, v0);
+  DC_SUM_8(left, v1);
+  v0 = _mm_add_pi32(v0, v1);
+  sum = _mm_cvtsi64_si32(v0);
+  sum = (sum + 8) >> 4;
+  DC_STORE_8XH(sum, 8, dst, stride);
+}
+
+FUNC(dc_predictor_16x16_sse2) {
+  __m128i t0, v0, v1, vzero = _mm_setzero_si128();
+  int32_t sum; int i;
+  DC_SUM_16(above, v0);
+  DC_SUM_16(left, v1);
+  v0 = _mm_add_epi64(v0, v1);
+  sum = _mm_cvtsi128_si32(v0) + _mm_extract_epi32(v0, 2);
+  sum = (sum + 16) >> 5;
+  DC_STORE_16XH(sum, 16, dst, stride);
+}
+
+FUNC(dc_predictor_32x32_sse2) {
+  __m128i t0, t1, v0, v1, vzero = _mm_setzero_si128();
+  int32_t sum; int i;
+  DC_SUM_32(above, v0);
+  DC_SUM_32(left, v1);
+  v0 = _mm_add_epi64(v0, v1);
+  sum = _mm_cvtsi128_si32(v0) + _mm_extract_epi32(v0, 2);
+  sum = (sum + 32) >> 6;
+  DC_STORE_32XH(sum, 32, dst, stride);
+}
+
+FUNC(v_predictor_4x4_sse2) {
+  uint32_t sum = *(uint32_t const *)above;
+  int i; (void)left;
+  for (i = 0; i < 4; i++, dst += stride) {
+    *(uint32_t*)dst = sum;
+  }
+}
+
+FUNC(v_predictor_8x8_sse2) {
+  __m64 v0 = *(__m64 const *)above; int i;
+  (void)left;
+  for (i = 0; i < 8; i++, dst += stride) {
+    *(__m64*)dst = v0;
+  }
+}
+
+FUNC(v_predictor_16x16_sse2) {
+  __m128i v0 = _mm_load_si128((__m128i const *)above);
+  int i; (void)left;
+  for (i = 0; i < 16; i++, dst += stride) {
+    _mm_store_si128((__m128i *)dst, v0);
+  }
+}
+
+FUNC(v_predictor_32x32_sse2) {
+  __m128i v0 = _mm_load_si128((__m128i const *)above);
+  __m128i v1 = _mm_load_si128((__m128i const *)(above + 16));
+  int i; (void)left;
+  for (i = 0; i < 32; i++, dst += stride) {
+    _mm_store_si128((__m128i *)dst, v0);
+    _mm_store_si128((__m128i *)(dst + 16), v1);
+  }
+}
+
+FUNC(h_predictor_4x4_sse2) {
+  __m128i v0; (void)above;
+  v0 = _mm_cvtsi32_si128(*(uint32_t const *)left);
+  v0 = _mm_unpacklo_epi8(v0, v0);
+  v0 = _mm_unpacklo_epi8(v0, v0);
+  *(uint32_t*)dst = _mm_cvtsi128_si32(v0); dst += stride;
+  *(uint32_t*)dst = _mm_extract_epi32(v0, 1); dst += stride;
+  *(uint32_t*)dst = _mm_extract_epi32(v0, 2); dst += stride;
+  *(uint32_t*)dst = _mm_extract_epi32(v0, 3);
+}
+
+FUNC(h_predictor_8x8_sse2) {
+  __m64 v0; int i; (void)above;
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < 8; i++, dst += stride) {
+    v0 = _mm_set1_pi8(left[i]);
+    *(__m64*)dst = v0;
+  }
+}
+
+FUNC(h_predictor_16x16_sse2) {
+  __m128i v0; int i; (void)above;
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < 16; i++, dst += stride) {
+    v0 = _mm_set1_epi8(left[i]);
+    _mm_store_si128((__m128i *)dst, v0);
+  }
+}
+
+FUNC(h_predictor_32x32_sse2) {
+  __m128i v0; int i; (void)above;
+  PRAGMA_E2K("ivdep")
+  for (i = 0; i < 32; i++, dst += stride) {
+    v0 = _mm_set1_epi8(left[i]);
+    _mm_store_si128((__m128i *)dst, v0);
+    _mm_store_si128((__m128i *)(dst + 16), v0);
+  }
+}
+
+FUNC(tm_predictor_4x4_sse2) {
+  __m128i v0, v1, v2, v3, c0 = _mm_setzero_si128();
+  v0 = _mm_set1_epi16(above[-1]);
+  v2 = _mm_cvtsi32_si128(*(const int32_t*)above);
+  v3 = _mm_cvtsi32_si128(*(const int32_t*)left);
+  v2 = _mm_sub_epi16(_mm_unpacklo_epi8(v2, c0), v0);
+  v3 = _mm_unpacklo_epi8(_mm_unpacklo_epi8(v3, v3), c0);
+  v2 = _mm_unpacklo_epi64(v2, v2);
+  v0 = _mm_add_epi16(_mm_unpacklo_epi16(v3, v3), v2);
+  v1 = _mm_add_epi16(_mm_unpackhi_epi16(v3, v3), v2);
+  v0 = _mm_packus_epi16(v0, v1);
+  *(int32_t*)dst = _mm_cvtsi128_si32(v0);
+  *(int32_t*)(dst + stride) = _mm_extract_epi32(v0, 1);
+  *(int32_t*)(dst + stride * 2) = _mm_extract_epi32(v0, 2);
+  *(int32_t*)(dst + stride * 3) = _mm_extract_epi32(v0, 3);
+}
+
+#define TM_SAVE8(i1, i2) \
+  v0 = _mm_add_epi16(_mm_shuffle_epi32(v4, i1), v2); \
+  v1 = _mm_add_epi16(_mm_shuffle_epi32(v4, i2), v2); \
+  v0 = _mm_packus_epi16(v0, v1); \
+  _mm_storel_epi64((__m128i*)dst, v0); \
+  _mm_storel_epi64((__m128i*)(dst + stride), _mm_bsrli_si128(v0, 8)); \
+  dst += stride << 1;
+
+FUNC(tm_predictor_8x8_sse2) {
+  __m128i v0, v1, v2, v3, v4, c0 = _mm_setzero_si128();
+  v0 = _mm_set1_epi16(above[-1]);
+  v2 = _mm_loadl_epi64((const __m128i*)above);
+  v3 = _mm_loadl_epi64((const __m128i*)left);
+  v2 = _mm_sub_epi16(_mm_unpacklo_epi8(v2, c0), v0);
+  v3 = _mm_unpacklo_epi8(v3, c0);
+  v4 = _mm_unpacklo_epi16(v3, v3);
+  TM_SAVE8(0x00, 0x55) TM_SAVE8(0xaa, 0xff)
+  v4 = _mm_unpackhi_epi16(v3, v3);
+  TM_SAVE8(0x00, 0x55) TM_SAVE8(0xaa, 0xff)
+}
+
+#define VEC_AVG3(p, si, v0, v1, v2, v3, c1) \
+  v3 = _mm_avg_##p##u8(v0, v2); \
+  v2 = _mm_and_##si(_mm_xor_##si(v2, v0), c1); \
+  v3 = _mm_avg_##p##u8(_mm_sub_##p##i8(v3, v2), v1);
+
+FUNC(d63_predictor_4x4_ssse3) {
+  __m64 v0, v1, v2, v3; (void)left;
+  v0 = *(const __m64*)above;
+  v1 = _mm_srli_si64(v0, 8);
+  v2 = _mm_srli_si64(v0, 16);
+  VEC_AVG3(p, si64, v0, v1, v2, v3, _mm_set1_pi8(1))
+  v0 = _mm_avg_pu8(v0, v1);
+
+  *(int32_t*)dst = _mm_cvtsi64_si32(v0);
+  *(int32_t*)(dst + stride) = _mm_cvtsi64_si32(v3);
+  *(int32_t*)(dst + stride * 2) = _mm_cvtsi64_si32(_mm_srli_si64(v0, 8));
+  *(int32_t*)(dst + stride * 3) = _mm_cvtsi64_si32(_mm_srli_si64(v3, 8));
+}
+
+FUNC(d63_predictor_8x8_ssse3) {
+  __m64 v0, v1, v2, v3, v4; (void)left;
+  v0 = *(const __m64*)above;
+  v4 = _mm_shuffle_pi8(v0, _mm_set1_pi8(7));
+  v1 = _mm_alignr_pi8(v4, v0, 1);
+  v2 = _mm_alignr_pi8(v4, v0, 2);
+  VEC_AVG3(p, si64, v0, v1, v2, v3, _mm_set1_pi8(1))
+  v0 = _mm_avg_pu8(v0, v1);
+
+  *(__m64*)dst = v0;
+  *(__m64*)(dst + stride) = v3;
+  *(__m64*)(dst + stride * 2) = _mm_alignr_pi8(v4, v0, 1);
+  *(__m64*)(dst + stride * 3) = _mm_alignr_pi8(v4, v3, 1);
+  dst += stride << 2;
+  *(__m64*)dst = _mm_alignr_pi8(v4, v0, 2);
+  *(__m64*)(dst + stride) = _mm_alignr_pi8(v4, v3, 2);
+  *(__m64*)(dst + stride * 2) = _mm_alignr_pi8(v4, v0, 3);
+  *(__m64*)(dst + stride * 3) = _mm_alignr_pi8(v4, v3, 3);
+}
+
+FUNC(d63_predictor_16x16_ssse3) {
+  int i; __m128i v0, v1, v2, v3, m1, m2; (void)left;
+  m1 = _mm_setr_epi8(1,2,3,4,5,6,7,8, 9,10,11,12,13,14,15,15);
+  m2 = _mm_setr_epi8(2,3,4,5,6,7,8,9, 10,11,12,13,14,15,15,15);
+  v0 = _mm_load_si128((const __m128i*)above);
+  v1 = _mm_shuffle_epi8(v0, m1);
+  v2 = _mm_shuffle_epi8(v0, m2);
+  VEC_AVG3(ep, si128, v0, v1, v2, v3, _mm_set1_epi8(1))
+  v0 = _mm_avg_epu8(v0, v1);
+
+  for (i = 0; i < 16; i += 4, dst += stride << 2) {
+    _mm_store_si128((__m128i*)dst, v0);
+    _mm_store_si128((__m128i*)(dst + stride), v3);
+    _mm_store_si128((__m128i*)(dst + stride * 2), _mm_shuffle_epi8(v0, m1));
+    _mm_store_si128((__m128i*)(dst + stride * 3), _mm_shuffle_epi8(v3, m1));
+    v0 = _mm_shuffle_epi8(v0, m2);
+    v3 = _mm_shuffle_epi8(v3, m2);
+  }
+}
+
+FUNC(d63_predictor_32x32_ssse3) {
+  int i; __m128i v0, v1, v2, v3; (void)left;
+  __m128i v4, v5, v6, v7, m1, m2, c1 = _mm_set1_epi8(1);
+  m1 = _mm_setr_epi8(1,2,3,4,5,6,7,8, 9,10,11,12,13,14,15,15);
+  m2 = _mm_setr_epi8(2,3,4,5,6,7,8,9, 10,11,12,13,14,15,15,15);
+  v0 = _mm_load_si128((const __m128i*)above);
+  v4 = _mm_load_si128((const __m128i*)above + 1);
+  v1 = _mm_alignr_epi8(v4, v0, 1); v5 = _mm_shuffle_epi8(v4, m1);
+  v2 = _mm_alignr_epi8(v4, v0, 2); v6 = _mm_shuffle_epi8(v4, m2);
+  VEC_AVG3(ep, si128, v0, v1, v2, v3, c1)
+  VEC_AVG3(ep, si128, v4, v5, v6, v7, c1)
+  v0 = _mm_avg_epu8(v0, v1);
+  v4 = _mm_avg_epu8(v4, v5);
+
+  for (i = 0; i < 32; i += 4, dst += stride << 2) {
+    _mm_store_si128((__m128i*)dst, v0);
+    _mm_store_si128((__m128i*)dst + 1, v4);
+    _mm_store_si128((__m128i*)(dst + stride), v3);
+    _mm_store_si128((__m128i*)(dst + stride) + 1, v7);
+    _mm_store_si128((__m128i*)(dst + stride * 2), _mm_alignr_epi8(v4, v0, 1));
+    _mm_store_si128((__m128i*)(dst + stride * 2) + 1, _mm_shuffle_epi8(v4, m1));
+    _mm_store_si128((__m128i*)(dst + stride * 3), _mm_alignr_epi8(v7, v3, 1));
+    _mm_store_si128((__m128i*)(dst + stride * 3) + 1, _mm_shuffle_epi8(v7, m1));
+    v0 = _mm_alignr_epi8(v4, v0, 2); v4 = _mm_shuffle_epi8(v4, m2);
+    v3 = _mm_alignr_epi8(v7, v3, 2); v7 = _mm_shuffle_epi8(v7, m2);
+  }
+}
diff --git a/vpx_dsp/e2k/inv_wht_e2k.c b/vpx_dsp/e2k/inv_wht_e2k.c
new file mode 100644
index 0000000..ad7daf2
--- /dev/null
+++ b/vpx_dsp/e2k/inv_wht_e2k.c
@@ -0,0 +1,100 @@
+/*
+ *  Copyright (c) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *  Copyright (c) 2015 The WebM project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "./vpx_config.h"
+#include "./vpx_dsp_rtcd.h"
+#include "vpx_dsp/inv_txfm.h"
+
+#if CONFIG_VP9_HIGHBITDEPTH
+#define LOAD_TRAN_LOW(v0, p) \
+  v0 = _mm_load_si128((const __m128i*)(p)); \
+  v0 = _mm_packs_epi32(v0, _mm_load_si128((const __m128i*)(p) + 1));
+#else
+#define LOAD_TRAN_LOW(v0, p) \
+  v0 = _mm_load_si128((const __m128i*)(p));
+#endif
+
+#define TRANSFORM_COLS \
+  v3 = _mm_unpackhi_epi16(v0, v1); \
+  v0 = _mm_unpacklo_epi16(v0, v1); \
+  v1 = _mm_bsrli_si128(v3, 8); \
+  v2 = _mm_bsrli_si128(v0, 8); \
+  v3 = _mm_sub_epi16(v3, v1); \
+  v0 = _mm_add_epi16(v0, v2); \
+  /* wide subtract */ \
+  v5 = _mm_unpacklo_epi16(v3, v3); \
+  v4 = _mm_unpacklo_epi16(v0, v0); \
+  v4 = _mm_srai_epi32(v4, 16); \
+  v5 = _mm_srai_epi32(v5, 16); \
+  v4 = _mm_sub_epi32(v4, v5); \
+  v4 = _mm_srai_epi32(v4, 1); \
+  v4 = _mm_packs_epi32(v4, v4); /* e */ \
+  v1 = _mm_sub_epi16(v4, v1); /* b */ \
+  v2 = _mm_sub_epi16(v4, v2); /* c */ \
+  v0 = _mm_sub_epi16(v0, v1); \
+  v3 = _mm_add_epi16(v3, v2);
+
+#define ADD_STORE_4P_2X(src1, src2, tmp1, tmp2, zero) \
+  tmp1 = _mm_cvtsi32_si128(*(int32_t*)dest); \
+  tmp2 = _mm_cvtsi32_si128(*(int32_t*)(dest + stride)); \
+  tmp1 = _mm_unpacklo_epi8(tmp1, zero); \
+  tmp2 = _mm_unpacklo_epi8(tmp2, zero); \
+  src1 = _mm_add_epi16(src1, tmp1); \
+  src2 = _mm_add_epi16(src2, tmp2); \
+  src1 = _mm_packus_epi16(src1, src2); \
+  *(int32_t*)dest = _mm_cvtsi128_si32(src1); \
+  *(int32_t*)(dest + stride) = _mm_extract_epi32(src1, 2);
+
+#include <stdio.h>
+
+void vpx_iwht4x4_16_add_sse2(const tran_low_t *input, uint8_t *dest, int stride) {
+  __m128i v0, v1, v2, v3, v4, v5;
+
+  LOAD_TRAN_LOW(v0, input)
+  LOAD_TRAN_LOW(v3, input + 8)
+  v0 = _mm_srai_epi16(v0, 2);
+  v3 = _mm_srai_epi16(v3, 2);
+
+  v1 = _mm_unpackhi_epi16(v0, v3);
+  v0 = _mm_unpacklo_epi16(v0, v3);
+  TRANSFORM_COLS
+  v1 = _mm_unpacklo_epi16(v1, v3);
+  v0 = _mm_unpacklo_epi16(v0, v2);
+  TRANSFORM_COLS
+#if 0
+  v4 = _mm_setzero_si128();
+  ADD_STORE_4P_2X(v0, v1, v5, v6, v4)
+  dest += stride << 1;
+  ADD_STORE_4P_2X(v2, v3, v5, v6, v4)
+#else
+  v0 = _mm_unpacklo_epi64(v0, v1);
+  v2 = _mm_unpacklo_epi64(v2, v3);
+  v1 = _mm_cvtsi32_si128(*(int32_t*)dest);
+  v4 = _mm_cvtsi32_si128(*(int32_t*)(dest + stride));
+  v3 = _mm_cvtsi32_si128(*(int32_t*)(dest + stride * 2));
+  v5 = _mm_cvtsi32_si128(*(int32_t*)(dest + stride * 3));
+  v1 = _mm_unpacklo_epi32(v1, v4);
+  v4 = _mm_setzero_si128();
+  v3 = _mm_unpacklo_epi32(v3, v5);
+  v1 = _mm_unpacklo_epi8(v1, v4);
+  v3 = _mm_unpacklo_epi8(v3, v4);
+  v0 = _mm_add_epi16(v0, v1);
+  v2 = _mm_add_epi16(v2, v3);
+  v0 = _mm_packus_epi16(v0, v2);
+  *(int32_t*)dest = _mm_cvtsi128_si32(v0);
+  *(int32_t*)(dest + stride) = _mm_extract_epi32(v0, 1);
+  *(int32_t*)(dest + stride * 2) = _mm_extract_epi32(v0, 2);
+  *(int32_t*)(dest + stride * 3) = _mm_extract_epi32(v0, 3);
+#endif
+}
+
diff --git a/vpx_dsp/e2k/sad4d_e2k.c b/vpx_dsp/e2k/sad4d_e2k.c
new file mode 100644
index 0000000..365d6b5
--- /dev/null
+++ b/vpx_dsp/e2k/sad4d_e2k.c
@@ -0,0 +1,237 @@
+/*
+ *  Copyright (c) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *  Copyright (c) 2016 The WebM project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "./vpx_config.h"
+#include "./vpx_dsp_rtcd.h"
+#include "vpx_ports/mem.h"
+
+#define NO_AVG(...)
+#define WITH_AVG(...) __VA_ARGS__
+
+#define FN(w, h) \
+  SAD##w##XN(h, NO_AVG, vpx_sad, x4d_sse2) \
+  SAD##w##XN(h, WITH_AVG, vpx_sad, x4d_avg_sse2)
+#define SAD_LOOP(h, n) \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < h; i += n)
+#define RESULT_SHL(x, pi) x
+
+#define PROCESS_16x4(IS_AVG, i) \
+  v0 = _mm_loadu_si128((const __m128i*)(ref0 + i)); \
+  v1 = _mm_loadu_si128((const __m128i*)(ref1 + i)); \
+  v2 = _mm_loadu_si128((const __m128i*)(ref2 + i)); \
+  v3 = _mm_loadu_si128((const __m128i*)(ref3 + i)); \
+  IS_AVG( \
+    v4 = _mm_loadu_si128((const __m128i*)(second_pred + i)); \
+    v0 = _mm_avg_epu8(v0, v4); \
+    v1 = _mm_avg_epu8(v1, v4); \
+    v2 = _mm_avg_epu8(v2, v4); \
+    v3 = _mm_avg_epu8(v3, v4); \
+  ) \
+  v4 = _mm_loadu_si128((const __m128i*)(src + i)); \
+  vsum0 = _mm_add_epi32(vsum0, _mm_sad_epu8(v0, v4)); \
+  vsum1 = _mm_add_epi32(vsum1, _mm_sad_epu8(v1, v4)); \
+  vsum2 = _mm_add_epi32(vsum2, _mm_sad_epu8(v2, v4)); \
+  vsum3 = _mm_add_epi32(vsum3, _mm_sad_epu8(v3, v4)); \
+
+#define SAD128XN(h, IS_AVG, name, end) \
+void name##128x##h##end(const uint8_t *src, int src_stride, \
+                        const uint8_t * const ref_ptr[], int ref_stride \
+                        IS_AVG(, const uint8_t *second_pred), uint32_t *sad_array) { \
+  __m128i v0, v1, v2, v3, v4, vsum0 = _mm_setzero_si128(); \
+  __m128i vsum1 = vsum0, vsum2 = vsum0, vsum3 = vsum0; \
+  const uint8_t *ref0 = ref_ptr[0], *ref1 = ref_ptr[1], *ref2 = ref_ptr[2], *ref3 = ref_ptr[3]; \
+  int i; \
+  SAD_LOOP(h, 1) { \
+    PROCESS_16x4(IS_AVG, 0) \
+    PROCESS_16x4(IS_AVG, 16) \
+    PROCESS_16x4(IS_AVG, 32) \
+    PROCESS_16x4(IS_AVG, 48) \
+    PROCESS_16x4(IS_AVG, 64) \
+    PROCESS_16x4(IS_AVG, 80) \
+    PROCESS_16x4(IS_AVG, 96) \
+    PROCESS_16x4(IS_AVG, 112) \
+    IS_AVG(second_pred += 128;) \
+    ref0 += ref_stride; \
+    ref1 += ref_stride; \
+    ref2 += ref_stride; \
+    ref3 += ref_stride; \
+    src += src_stride; \
+  } \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum1); \
+  vsum2 = _mm_hadd_epi32(vsum2, vsum3); \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum2); \
+  _mm_storeu_si128((__m128i*)sad_array, RESULT_SHL(vsum0, epi)); \
+}
+
+#define SAD64XN(h, IS_AVG, name, end) \
+void name##64x##h##end(const uint8_t *src, int src_stride, \
+                       const uint8_t * const ref_ptr[], int ref_stride \
+                       IS_AVG(, const uint8_t *second_pred), uint32_t *sad_array) { \
+  __m128i v0, v1, v2, v3, v4, vsum0 = _mm_setzero_si128(); \
+  __m128i vsum1 = vsum0, vsum2 = vsum0, vsum3 = vsum0; \
+  const uint8_t *ref0 = ref_ptr[0], *ref1 = ref_ptr[1], *ref2 = ref_ptr[2], *ref3 = ref_ptr[3]; \
+  int i; \
+  SAD_LOOP(h, 1) { \
+    PROCESS_16x4(IS_AVG, 0) \
+    PROCESS_16x4(IS_AVG, 16) \
+    PROCESS_16x4(IS_AVG, 32) \
+    PROCESS_16x4(IS_AVG, 48) \
+    IS_AVG(second_pred += 64;) \
+    ref0 += ref_stride; \
+    ref1 += ref_stride; \
+    ref2 += ref_stride; \
+    ref3 += ref_stride; \
+    src += src_stride; \
+  } \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum1); \
+  vsum2 = _mm_hadd_epi32(vsum2, vsum3); \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum2); \
+  _mm_storeu_si128((__m128i*)sad_array, RESULT_SHL(vsum0, epi)); \
+}
+
+#define SAD32XN(h, IS_AVG, name, end) \
+void name##32x##h##end(const uint8_t *src, int src_stride, \
+                       const uint8_t * const ref_ptr[], int ref_stride \
+                       IS_AVG(, const uint8_t *second_pred), uint32_t *sad_array) { \
+  __m128i v0, v1, v2, v3, v4, vsum0 = _mm_setzero_si128(); \
+  __m128i vsum1 = vsum0, vsum2 = vsum0, vsum3 = vsum0; \
+  const uint8_t *ref0 = ref_ptr[0], *ref1 = ref_ptr[1], *ref2 = ref_ptr[2], *ref3 = ref_ptr[3]; \
+  int i; \
+  SAD_LOOP(h, 1) { \
+    PROCESS_16x4(IS_AVG, 0) \
+    PROCESS_16x4(IS_AVG, 16) \
+    IS_AVG(second_pred += 32;) \
+    ref0 += ref_stride; \
+    ref1 += ref_stride; \
+    ref2 += ref_stride; \
+    ref3 += ref_stride; \
+    src += src_stride; \
+  } \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum1); \
+  vsum2 = _mm_hadd_epi32(vsum2, vsum3); \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum2); \
+  _mm_storeu_si128((__m128i*)sad_array, RESULT_SHL(vsum0, epi)); \
+}
+
+#define SAD16XN(h, IS_AVG, name, end) \
+void name##16x##h##end(const uint8_t *src, int src_stride, \
+                       const uint8_t * const ref_ptr[], int ref_stride \
+                       IS_AVG(, const uint8_t *second_pred), uint32_t *sad_array) { \
+  __m128i v0, v1, v2, v3, v4, vsum0 = _mm_setzero_si128(); \
+  __m128i vsum1 = vsum0, vsum2 = vsum0, vsum3 = vsum0; \
+  const uint8_t *ref0 = ref_ptr[0], *ref1 = ref_ptr[1], *ref2 = ref_ptr[2], *ref3 = ref_ptr[3]; \
+  int i; \
+  SAD_LOOP(h, 1) { \
+    PROCESS_16x4(IS_AVG, 0) \
+    IS_AVG(second_pred += 16;) \
+    ref0 += ref_stride; \
+    ref1 += ref_stride; \
+    ref2 += ref_stride; \
+    ref3 += ref_stride; \
+    src += src_stride; \
+  } \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum1); \
+  vsum2 = _mm_hadd_epi32(vsum2, vsum3); \
+  vsum0 = _mm_hadd_epi32(vsum0, vsum2); \
+  _mm_storeu_si128((__m128i*)sad_array, RESULT_SHL(vsum0, epi)); \
+}
+
+#define SAD8XN(h, IS_AVG, name, end) \
+void name##8x##h##end(const uint8_t *src, int src_stride, \
+                      const uint8_t * const ref_ptr[], int ref_stride \
+                      IS_AVG(, const uint8_t *second_pred), uint32_t *sad_array) { \
+  __m64 v0, v1, v2, v3, v4, vsum0 = _mm_setzero_si64(); \
+  __m64 vsum1 = vsum0, vsum2 = vsum0, vsum3 = vsum0; \
+  const uint8_t *ref0 = ref_ptr[0], *ref1 = ref_ptr[1], *ref2 = ref_ptr[2], *ref3 = ref_ptr[3]; \
+  int i; \
+  SAD_LOOP(h, 1) { \
+    v0 = *(const __m64*)ref0; \
+    v1 = *(const __m64*)ref1; \
+    v2 = *(const __m64*)ref2; \
+    v3 = *(const __m64*)ref3; \
+    IS_AVG( \
+      v4 = *(const __m64*)second_pred; \
+      v0 = _mm_avg_pu8(v0, v4); \
+      v1 = _mm_avg_pu8(v1, v4); \
+      v2 = _mm_avg_pu8(v2, v4); \
+      v3 = _mm_avg_pu8(v3, v4); \
+      second_pred += 8; \
+    ) \
+    v4 = *(const __m64*)src; \
+    vsum0 = _mm_add_pi32(vsum0, _mm_sad_pu8(v0, v4)); \
+    vsum1 = _mm_add_pi32(vsum1, _mm_sad_pu8(v1, v4)); \
+    vsum2 = _mm_add_pi32(vsum2, _mm_sad_pu8(v2, v4)); \
+    vsum3 = _mm_add_pi32(vsum3, _mm_sad_pu8(v3, v4)); \
+    ref0 += ref_stride; \
+    ref1 += ref_stride; \
+    ref2 += ref_stride; \
+    ref3 += ref_stride; \
+    src += src_stride; \
+  } \
+  *(__m64*)sad_array = RESULT_SHL(_mm_unpacklo_pi32(vsum0, vsum1), pi); \
+  *(__m64*)(sad_array + 2) = RESULT_SHL(_mm_unpacklo_pi32(vsum2, vsum3), pi); \
+}
+
+#define SAD4XN(h, IS_AVG, name, end) \
+void name##4x##h##end(const uint8_t *src, int src_stride, \
+                      const uint8_t * const ref_ptr[], int ref_stride \
+                      IS_AVG(, const uint8_t *second_pred), uint32_t *sad_array) { \
+  __m64 v0, v1, v2, v3, v4, v5, vsum0 = _mm_setzero_si64(); \
+  __m64 vsum1 = vsum0, vsum2 = vsum0, vsum3 = vsum0; \
+  const uint8_t *ref0 = ref_ptr[0], *ref1 = ref_ptr[1], *ref2 = ref_ptr[2], *ref3 = ref_ptr[3]; \
+  int i; \
+  SAD_LOOP(h, 2) { \
+    v4 = _mm_cvtsi32_si64(*(uint32_t const *)ref0); \
+    v5 = _mm_cvtsi32_si64(*(uint32_t const *)(ref0 + ref_stride)); \
+    v0 = _mm_unpacklo_pi32(v4, v5); \
+    v4 = _mm_cvtsi32_si64(*(uint32_t const *)ref1); \
+    v5 = _mm_cvtsi32_si64(*(uint32_t const *)(ref1 + ref_stride)); \
+    v1 = _mm_unpacklo_pi32(v4, v5); \
+    v4 = _mm_cvtsi32_si64(*(uint32_t const *)ref2); \
+    v5 = _mm_cvtsi32_si64(*(uint32_t const *)(ref2 + ref_stride)); \
+    v2 = _mm_unpacklo_pi32(v4, v5); \
+    v4 = _mm_cvtsi32_si64(*(uint32_t const *)ref3); \
+    v5 = _mm_cvtsi32_si64(*(uint32_t const *)(ref3 + ref_stride)); \
+    v3 = _mm_unpacklo_pi32(v4, v5); \
+    IS_AVG( \
+      v4 = *(const __m64*)second_pred; \
+      v0 = _mm_avg_pu8(v0, v4); \
+      v1 = _mm_avg_pu8(v1, v4); \
+      v2 = _mm_avg_pu8(v2, v4); \
+      v3 = _mm_avg_pu8(v3, v4); \
+      second_pred += 8; \
+    ) \
+    v4 = _mm_cvtsi32_si64(*(uint32_t const *)src); \
+    v5 = _mm_cvtsi32_si64(*(uint32_t const *)(src + src_stride)); \
+    v4 = _mm_unpacklo_pi32(v4, v5); \
+    vsum0 = _mm_add_pi32(vsum0, _mm_sad_pu8(v0, v4)); \
+    vsum1 = _mm_add_pi32(vsum1, _mm_sad_pu8(v1, v4)); \
+    vsum2 = _mm_add_pi32(vsum2, _mm_sad_pu8(v2, v4)); \
+    vsum3 = _mm_add_pi32(vsum3, _mm_sad_pu8(v3, v4)); \
+    ref0 += ref_stride << 1; \
+    ref1 += ref_stride << 1; \
+    ref2 += ref_stride << 1; \
+    ref3 += ref_stride << 1; \
+    src += src_stride << 1; \
+  } \
+  *(__m64*)sad_array = RESULT_SHL(_mm_unpacklo_pi32(vsum0, vsum1), pi); \
+  *(__m64*)(sad_array + 2) = RESULT_SHL(_mm_unpacklo_pi32(vsum2, vsum3), pi); \
+}
+
+FN(64, 64) FN(64, 32)
+FN(32, 64) FN(32, 32) FN(32, 16)
+FN(16, 32) FN(16, 16) FN(16, 8)
+FN(8, 16) FN(8, 8) FN(8, 4)
+FN(4, 8) FN(4, 4)
+
diff --git a/vpx_dsp/e2k/sad_e2k.c b/vpx_dsp/e2k/sad_e2k.c
new file mode 100644
index 0000000..df0fc5f
--- /dev/null
+++ b/vpx_dsp/e2k/sad_e2k.c
@@ -0,0 +1,216 @@
+/*
+ *  Copyright (c) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *  Copyright (c) 2016 The WebM project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "./vpx_config.h"
+#include "./vpx_dsp_rtcd.h"
+#include "vpx_ports/mem.h"
+
+#define NO_AVG(...)
+#define WITH_AVG(...) __VA_ARGS__
+
+#define FN(w, h) \
+  SAD##w##XN(h, NO_AVG, vpx_sad, _sse2) \
+  SAD##w##XN(h, WITH_AVG, vpx_sad, _avg_sse2)
+#define SAD_LOOP(h, n) \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < h; i += n)
+#define RESULT_SHL(x) x
+
+#define SAD64XN(h, IS_AVG, name, end) \
+unsigned int name##64x##h##end(const uint8_t *src, int src_stride, \
+                               const uint8_t *ref, int ref_stride \
+                               IS_AVG(, const uint8_t *second_pred)) { \
+  __m128i v0, v1, v2, v3, vsum = _mm_setzero_si128(); \
+  int i; \
+  SAD_LOOP(h, 1) { \
+    v0 = _mm_loadu_si128((const __m128i*)ref); \
+    v1 = _mm_loadu_si128((const __m128i*)(ref + 16)); \
+    v2 = _mm_loadu_si128((const __m128i*)(ref + 32)); \
+    v3 = _mm_loadu_si128((const __m128i*)(ref + 48)); \
+    IS_AVG( \
+      v0 = _mm_avg_epu8(v0, _mm_loadu_si128((const __m128i*)second_pred)); \
+      v1 = _mm_avg_epu8(v1, _mm_loadu_si128((const __m128i*)(second_pred + 16))); \
+      v2 = _mm_avg_epu8(v2, _mm_loadu_si128((const __m128i*)(second_pred + 32))); \
+      v3 = _mm_avg_epu8(v3, _mm_loadu_si128((const __m128i*)(second_pred + 48))); \
+      second_pred += 64; \
+    ) \
+    v0 = _mm_sad_epu8(v0, _mm_loadu_si128((const __m128i*)src)); \
+    v1 = _mm_sad_epu8(v1, _mm_loadu_si128((const __m128i*)(src + 16))); \
+    v2 = _mm_sad_epu8(v2, _mm_loadu_si128((const __m128i*)(src + 32))); \
+    v3 = _mm_sad_epu8(v3, _mm_loadu_si128((const __m128i*)(src + 48))); \
+    v0 = _mm_add_epi32(v0, v1); \
+    v2 = _mm_add_epi32(v2, v3); \
+    vsum = _mm_add_epi32(vsum, v0); \
+    vsum = _mm_add_epi32(vsum, v2); \
+    ref += ref_stride; \
+    src += src_stride; \
+  } \
+  return RESULT_SHL(_mm_cvtsi128_si32(vsum) + _mm_extract_epi32(vsum, 2)); \
+}
+
+#define SAD32XN(h, IS_AVG, name, end) \
+unsigned int name##32x##h##end(const uint8_t *src, int src_stride, \
+                               const uint8_t *ref, int ref_stride \
+                               IS_AVG(, const uint8_t *second_pred)) { \
+  __m128i v0, v1, v2, v3, vsum = _mm_setzero_si128(); \
+  int i; \
+  SAD_LOOP(h, 2) { \
+    v0 = _mm_loadu_si128((const __m128i*)ref); \
+    v1 = _mm_loadu_si128((const __m128i*)(ref + 16)); \
+    v2 = _mm_loadu_si128((const __m128i*)(ref + ref_stride)); \
+    v3 = _mm_loadu_si128((const __m128i*)(ref + ref_stride + 16)); \
+    IS_AVG( \
+      v0 = _mm_avg_epu8(v0, _mm_loadu_si128((const __m128i*)second_pred)); \
+      v1 = _mm_avg_epu8(v1, _mm_loadu_si128((const __m128i*)(second_pred + 16))); \
+      v2 = _mm_avg_epu8(v2, _mm_loadu_si128((const __m128i*)(second_pred + 32))); \
+      v3 = _mm_avg_epu8(v3, _mm_loadu_si128((const __m128i*)(second_pred + 48))); \
+      second_pred += 64; \
+    ) \
+    v0 = _mm_sad_epu8(v0, _mm_loadu_si128((const __m128i*)src)); \
+    v1 = _mm_sad_epu8(v1, _mm_loadu_si128((const __m128i*)(src + 16))); \
+    v2 = _mm_sad_epu8(v2, _mm_loadu_si128((const __m128i*)(src + src_stride))); \
+    v3 = _mm_sad_epu8(v3, _mm_loadu_si128((const __m128i*)(src + src_stride + 16))); \
+    v0 = _mm_add_epi32(v0, v1); \
+    v2 = _mm_add_epi32(v2, v3); \
+    vsum = _mm_add_epi32(vsum, v0); \
+    vsum = _mm_add_epi32(vsum, v2); \
+    ref += ref_stride << 1; \
+    src += src_stride << 1; \
+  } \
+  return RESULT_SHL(_mm_cvtsi128_si32(vsum) + _mm_extract_epi32(vsum, 2)); \
+}
+
+
+#define SAD16XN(h, IS_AVG, name, end) \
+unsigned int name##16x##h##end(const uint8_t *src, int src_stride, \
+                               const uint8_t *ref, int ref_stride \
+                               IS_AVG(, const uint8_t *second_pred)) { \
+  __m128i v0, v1, v2, v3, vsum = _mm_setzero_si128(); \
+  int i; \
+  SAD_LOOP(h, 4) { \
+    v0 = _mm_loadu_si128((const __m128i*)ref); \
+    v1 = _mm_loadu_si128((const __m128i*)(ref + ref_stride)); \
+    v2 = _mm_loadu_si128((const __m128i*)(ref + ref_stride * 2)); \
+    v3 = _mm_loadu_si128((const __m128i*)(ref + ref_stride * 3)); \
+    IS_AVG( \
+      v0 = _mm_avg_epu8(v0, _mm_loadu_si128((const __m128i*)second_pred)); \
+      v1 = _mm_avg_epu8(v1, _mm_loadu_si128((const __m128i*)(second_pred + 16))); \
+      v2 = _mm_avg_epu8(v2, _mm_loadu_si128((const __m128i*)(second_pred + 32))); \
+      v3 = _mm_avg_epu8(v3, _mm_loadu_si128((const __m128i*)(second_pred + 48))); \
+      second_pred += 64; \
+    ) \
+    v0 = _mm_sad_epu8(v0, _mm_loadu_si128((const __m128i*)src)); \
+    v1 = _mm_sad_epu8(v1, _mm_loadu_si128((const __m128i*)(src + src_stride))); \
+    v2 = _mm_sad_epu8(v2, _mm_loadu_si128((const __m128i*)(src + src_stride * 2))); \
+    v3 = _mm_sad_epu8(v3, _mm_loadu_si128((const __m128i*)(src + src_stride * 3))); \
+    v0 = _mm_add_epi32(v0, v1); \
+    v2 = _mm_add_epi32(v2, v3); \
+    vsum = _mm_add_epi32(vsum, v0); \
+    vsum = _mm_add_epi32(vsum, v2); \
+    ref += ref_stride << 2; \
+    src += src_stride << 2; \
+  } \
+  return RESULT_SHL(_mm_cvtsi128_si32(vsum) + _mm_extract_epi32(vsum, 2)); \
+}
+
+#define SAD8XN(h, IS_AVG, name, end) \
+unsigned int name##8x##h##end(const uint8_t *src, int src_stride, \
+                              const uint8_t *ref, int ref_stride \
+                              IS_AVG(, const uint8_t *second_pred)) { \
+  __m64 v0, v1, v2, v3, vsum = _mm_setzero_si64(); \
+  int i; \
+  SAD_LOOP(h, 4) { \
+    v0 = *(const __m64*)ref; \
+    v1 = *(const __m64*)(ref + ref_stride); \
+    v2 = *(const __m64*)(ref + ref_stride * 2); \
+    v3 = *(const __m64*)(ref + ref_stride * 3); \
+    IS_AVG( \
+      v0 = _mm_avg_pu8(v0, *(const __m64*)second_pred); \
+      v1 = _mm_avg_pu8(v1, *(const __m64*)(second_pred + 8)); \
+      v2 = _mm_avg_pu8(v2, *(const __m64*)(second_pred + 16)); \
+      v3 = _mm_avg_pu8(v3, *(const __m64*)(second_pred + 24)); \
+      second_pred += 32; \
+    ) \
+    v0 = _mm_sad_pu8(v0, *(const __m64*)src); \
+    v1 = _mm_sad_pu8(v1, *(const __m64*)(src + src_stride)); \
+    v2 = _mm_sad_pu8(v2, *(const __m64*)(src + src_stride * 2)); \
+    v3 = _mm_sad_pu8(v3, *(const __m64*)(src + src_stride * 3)); \
+    v0 = _mm_add_pi32(v0, v1); \
+    v2 = _mm_add_pi32(v2, v3); \
+    vsum = _mm_add_pi32(vsum, v0); \
+    vsum = _mm_add_pi32(vsum, v2); \
+    ref += ref_stride << 2; \
+    src += src_stride << 2; \
+  } \
+  return RESULT_SHL(_mm_cvtsi64_si32(vsum)); \
+}
+
+#define SAD4XN(h, IS_AVG, name, end) \
+unsigned int name##4x##h##end(const uint8_t *src, int src_stride, \
+                              const uint8_t *ref, int ref_stride \
+                              IS_AVG(, const uint8_t *second_pred)) { \
+  __m64 v0, v1, v2, v3, vsum = _mm_setzero_si64(); \
+  int i; \
+  SAD_LOOP(h, 4) { \
+    v2 = _mm_cvtsi32_si64(*(uint32_t const *)ref); \
+    v3 = _mm_cvtsi32_si64(*(uint32_t const *)(ref + ref_stride)); \
+    v0 = _mm_unpacklo_pi32(v2, v3); \
+    v2 = _mm_cvtsi32_si64(*(uint32_t const *)(ref + ref_stride * 2)); \
+    v3 = _mm_cvtsi32_si64(*(uint32_t const *)(ref + ref_stride * 3)); \
+    v1 = _mm_unpacklo_pi32(v2, v3); \
+    IS_AVG( \
+      v0 = _mm_avg_pu8(v0, *(const __m64*)second_pred); \
+      v1 = _mm_avg_pu8(v1, *(const __m64*)(second_pred + 8)); \
+      second_pred += 16; \
+    ) \
+    v2 = _mm_cvtsi32_si64(*(uint32_t const *)src); \
+    v3 = _mm_cvtsi32_si64(*(uint32_t const *)(src + src_stride)); \
+    v0 = _mm_sad_pu8(v0, _mm_unpacklo_pi32(v2, v3)); \
+    v2 = _mm_cvtsi32_si64(*(uint32_t const *)(src + src_stride * 2)); \
+    v3 = _mm_cvtsi32_si64(*(uint32_t const *)(src + src_stride * 3)); \
+    v1 = _mm_sad_pu8(v1, _mm_unpacklo_pi32(v2, v3)); \
+    v0 = _mm_add_pi32(v0, v1); \
+    vsum = _mm_add_pi32(vsum, v0); \
+    ref += ref_stride << 2; \
+    src += src_stride << 2; \
+  } \
+  return RESULT_SHL(_mm_cvtsi64_si32(vsum)); \
+}
+
+FN(64, 64) FN(64, 32)
+FN(32, 64) FN(32, 32) FN(32, 16)
+FN(16, 32) FN(16, 16) FN(16, 8)
+FN(8, 16) FN(8, 8) FN(8, 4)
+FN(4, 8) FN(4, 4)
+
+// TODO: better implementation
+#define sadMxNxK(m, n, k, end) \
+void vpx_sad##m##x##n##x##k##end(const uint8_t *src_ptr, int src_stride, \
+                   const uint8_t *ref_ptr, int ref_stride, uint32_t *sad_array) { \
+  int i; \
+  for (i = 0; i < k; ++i) \
+    sad_array[i] = \
+        vpx_sad##m##x##n##_sse2(src_ptr, src_stride, ref_ptr + i, ref_stride); \
+}
+
+sadMxNxK(16, 16, 3, _ssse3)
+sadMxNxK(16, 16, 8, _sse4_1)
+sadMxNxK(16, 8, 3, _ssse3)
+sadMxNxK(16, 8, 8, _sse4_1)
+sadMxNxK(8, 16, 3, _sse3)
+sadMxNxK(8, 16, 8, _sse4_1)
+sadMxNxK(8, 8, 3, _sse3)
+sadMxNxK(8, 8, 8, _sse4_1)
+sadMxNxK(4, 4, 3, _sse3)
+sadMxNxK(4, 4, 8, _sse4_1)
+
diff --git a/vpx_dsp/e2k/subpel_variance_e2k.c b/vpx_dsp/e2k/subpel_variance_e2k.c
new file mode 100644
index 0000000..ff11a6e
--- /dev/null
+++ b/vpx_dsp/e2k/subpel_variance_e2k.c
@@ -0,0 +1,1760 @@
+/*
+ *  Copyright (c) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *  Copyright (c) 2016 The WebM project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "./vpx_config.h"
+#include "./vpx_dsp_rtcd.h"
+#include "vpx_ports/mem.h"
+
+/* clang-format off */
+DECLARE_ALIGNED(16, static const uint8_t, bilinear_filters_sse2[256]) = {
+  16,  0, 16,  0, 16,  0, 16,  0, 16,  0, 16,  0, 16,  0, 16,  0,
+  14,  2, 14,  2, 14,  2, 14,  2, 14,  2, 14,  2, 14,  2, 14,  2,
+  12,  4, 12,  4, 12,  4, 12,  4, 12,  4, 12,  4, 12,  4, 12,  4,
+  10,  6, 10,  6, 10,  6, 10,  6, 10,  6, 10,  6, 10,  6, 10,  6,
+   8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,
+   6, 10,  6, 10,  6, 10,  6, 10,  6, 10,  6, 10,  6, 10,  6, 10,
+   4, 12,  4, 12,  4, 12,  4, 12,  4, 12,  4, 12,  4, 12,  4, 12,
+   2, 14,  2, 14,  2, 14,  2, 14,  2, 14,  2, 14,  2, 14,  2, 14,
+};
+/* clang-format on */
+
+#define FILTER_SRC(filter)                               \
+  /* filter the source */                                \
+  exp_src_lo = _mm_maddubs_epi16(exp_src_lo, filter);    \
+  exp_src_hi = _mm_maddubs_epi16(exp_src_hi, filter);    \
+                                                         \
+  /* add 8 to source */                                  \
+  exp_src_lo = _mm_add_epi16(exp_src_lo, pw8);           \
+  exp_src_hi = _mm_add_epi16(exp_src_hi, pw8);           \
+                                                         \
+  /* divide source by 16 */                              \
+  exp_src_lo = _mm_srai_epi16(exp_src_lo, 4);            \
+  exp_src_hi = _mm_srai_epi16(exp_src_hi, 4);
+
+#define MERGE_WITH_SRC(src_reg, reg)               \
+  exp_src_lo = _mm_unpacklo_epi8(src_reg, reg);    \
+  exp_src_hi = _mm_unpackhi_epi8(src_reg, reg);
+
+#define LOAD_SRC_DST                                    \
+  /* load source and destination */                     \
+  src_reg = _mm_loadu_si128((__m128i const *)(src));    \
+  dst_reg = _mm_loadu_si128((__m128i const *)(dst));
+
+#define AVG_NEXT_SRC(src_reg, size_stride)                                 \
+  src_next_reg = _mm_loadu_si128((__m128i const *)(src + size_stride));    \
+  /* average between current and next stride source */                     \
+  src_reg = _mm_avg_epu8(src_reg, src_next_reg);
+
+#define MERGE_NEXT_SRC(src_reg, size_stride)                               \
+  src_next_reg = _mm_loadu_si128((__m128i const *)(src + size_stride));    \
+  MERGE_WITH_SRC(src_reg, src_next_reg)
+
+#define CALC_SUM_SSE_INSIDE_LOOP                          \
+  /* expand each byte to 2 bytes */                       \
+  exp_dst_lo = _mm_unpacklo_epi8(dst_reg, zero_reg);      \
+  exp_dst_hi = _mm_unpackhi_epi8(dst_reg, zero_reg);      \
+  /* source - dest */                                     \
+  exp_src_lo = _mm_sub_epi16(exp_src_lo, exp_dst_lo);     \
+  exp_src_hi = _mm_sub_epi16(exp_src_hi, exp_dst_hi);     \
+  /* caculate sum */                                      \
+  sum_reg = _mm_add_epi16(sum_reg, exp_src_lo);           \
+  exp_src_lo = _mm_madd_epi16(exp_src_lo, exp_src_lo);    \
+  sum_reg = _mm_add_epi16(sum_reg, exp_src_hi);           \
+  exp_src_hi = _mm_madd_epi16(exp_src_hi, exp_src_hi);    \
+  /* calculate sse */                                     \
+  sse_reg = _mm_add_epi32(sse_reg, exp_src_lo);           \
+  sse_reg = _mm_add_epi32(sse_reg, exp_src_hi);
+
+// final calculation to sum and sse
+#define CALC_SUM_AND_SSE                                                   \
+  res_cmp = _mm_cmpgt_epi16(zero_reg, sum_reg);                            \
+  sse_reg_hi = _mm_bsrli_si128(sse_reg, 8);                                \
+  sum_reg_lo = _mm_unpacklo_epi16(sum_reg, res_cmp);                       \
+  sum_reg_hi = _mm_unpackhi_epi16(sum_reg, res_cmp);                       \
+  sse_reg = _mm_add_epi32(sse_reg, sse_reg_hi);                            \
+  sum_reg = _mm_add_epi32(sum_reg_lo, sum_reg_hi);                         \
+                                                                           \
+  sse_reg_hi = _mm_bsrli_si128(sse_reg, 4);                                \
+  sum_reg_hi = _mm_bsrli_si128(sum_reg, 8);                                \
+                                                                           \
+  sse_reg = _mm_add_epi32(sse_reg, sse_reg_hi);                            \
+  sum_reg = _mm_add_epi32(sum_reg, sum_reg_hi);                            \
+  *((int *)sse) = _mm_cvtsi128_si32(sse_reg);                              \
+  sum_reg_hi = _mm_bsrli_si128(sum_reg, 4);                                \
+  sum_reg = _mm_add_epi32(sum_reg, sum_reg_hi);                            \
+  sum = _mm_cvtsi128_si32(sum_reg);
+
+// Functions related to sub pixel variance width 8
+#define LOAD_SRC_DST_INSERT(src_stride, dst_stride)              \
+  /* load source and destination of 2 rows and insert*/          \
+  src_reg = _mm_unpacklo_epi64(                                  \
+      _mm_loadl_epi64((__m128i const *)(src)),                         \
+      _mm_loadl_epi64((__m128i const *)(src + src_stride)));           \
+  dst_reg = _mm_unpacklo_epi64(                                  \
+      _mm_loadl_epi64((__m128i const *)(dst)),                         \
+      _mm_loadl_epi64((__m128i const *)(dst + dst_stride)));
+
+#define AVG_NEXT_SRC_INSERT(src_reg, size_stride)                              \
+  src_next_reg = _mm_unpacklo_epi64(                                           \
+      _mm_loadl_epi64((__m128i const *)(src + size_stride)),                         \
+      _mm_loadl_epi64((__m128i const *)(src + (size_stride << 1))));                 \
+  /* average between current and next stride source */                         \
+  src_reg = _mm_avg_epu8(src_reg, src_next_reg);
+
+#define MERGE_NEXT_SRC_INSERT(src_reg, size_stride)                            \
+  src_next_reg = _mm_unpacklo_epi64(                                           \
+      _mm_loadl_epi64((__m128i const *)(src + size_stride)),                         \
+      _mm_loadl_epi64((__m128i const *)(src + (src_stride + size_stride))));         \
+  MERGE_WITH_SRC(src_reg, src_next_reg)
+
+#define LOAD_SRC_NEXT_BYTE_INSERT                                    \
+  /* load source and another source from next row   */               \
+  src_reg = _mm_unpacklo_epi64(                                      \
+      _mm_loadl_epi64((__m128i const *)(src)),                             \
+      _mm_loadl_epi64((__m128i const *)(src + src_stride)));               \
+  /* load source and next row source from 1 byte onwards   */        \
+  src_next_reg = _mm_unpacklo_epi64(                                 \
+      _mm_loadl_epi64((__m128i const *)(src + 1)),                         \
+      _mm_loadl_epi64((__m128i const *)(src + src_stride + 1)));
+
+#define LOAD_DST_INSERT                                        \
+  dst_reg = _mm_unpacklo_epi64(                                \
+      _mm_loadl_epi64((__m128i const *)(dst)),                       \
+      _mm_loadl_epi64((__m128i const *)(dst + dst_stride)));
+
+#define LOAD_SRC_MERGE_HALF(filter)                          \
+  __m128i src_reg_0 = _mm_loadl_epi64((__m128i const *)(src));     \
+  __m128i src_reg_1 = _mm_loadl_epi64((__m128i const *)(src + 1)); \
+  __m128i src_lo = _mm_unpacklo_epi8(src_reg_0, src_reg_1);
+
+#define FILTER_SRC_HALF(filter)               \
+  /* filter the source */                     \
+  src_lo = _mm_maddubs_epi16(src_lo, filter); \
+                                              \
+  /* add 8 to source */                       \
+  src_lo = _mm_add_epi16(src_lo, pw8);        \
+                                              \
+  /* divide source by 16 */                   \
+  src_lo = _mm_srai_epi16(src_lo, 4);
+
+unsigned int vpx_sub_pixel_variance16xh_ssse3(const uint8_t *src, int src_stride,
+                                              int x_offset, int y_offset,
+                                              const uint8_t *dst, int dst_stride,
+                                              int height, unsigned int *sse) {
+  __m128i src_reg, dst_reg, exp_src_lo, exp_src_hi, exp_dst_lo, exp_dst_hi;
+  __m128i sse_reg, sum_reg, sse_reg_hi, res_cmp, sum_reg_lo, sum_reg_hi;
+  __m128i zero_reg;
+  int i, sum;
+  sum_reg = _mm_setzero_si128();
+  sse_reg = _mm_setzero_si128();
+  zero_reg = _mm_setzero_si128();
+
+  // x_offset = 0 and y_offset = 0
+  if (x_offset == 0) {
+    if (y_offset == 0) {
+      for (i = 0; i < height; i++) {
+        LOAD_SRC_DST
+        // expend each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride;
+        dst += dst_stride;
+      }
+      // x_offset = 0 and y_offset = 4
+    } else if (y_offset == 4) {
+      __m128i src_next_reg;
+      for (i = 0; i < height; i++) {
+        LOAD_SRC_DST
+        AVG_NEXT_SRC(src_reg, src_stride)
+        // expend each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride;
+        dst += dst_stride;
+      }
+      // x_offset = 0 and y_offset = bilin interpolation
+    } else {
+      __m128i filter, pw8, src_next_reg;
+
+      y_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + y_offset));
+      pw8 = _mm_set1_epi16(8);
+      for (i = 0; i < height; i++) {
+        LOAD_SRC_DST
+        MERGE_NEXT_SRC(src_reg, src_stride)
+        FILTER_SRC(filter)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride;
+        dst += dst_stride;
+      }
+    }
+    // x_offset = 4  and y_offset = 0
+  } else if (x_offset == 4) {
+    if (y_offset == 0) {
+      __m128i src_next_reg;
+      for (i = 0; i < height; i++) {
+        LOAD_SRC_DST
+        AVG_NEXT_SRC(src_reg, 1)
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride;
+        dst += dst_stride;
+      }
+      // x_offset = 4  and y_offset = 4
+    } else if (y_offset == 4) {
+      __m128i src_next_reg, src_avg;
+      // load source and another source starting from the next
+      // following byte
+      src_reg = _mm_loadu_si128((__m128i const *)(src));
+      AVG_NEXT_SRC(src_reg, 1)
+      for (i = 0; i < height; i++) {
+        src_avg = src_reg;
+        src += src_stride;
+        LOAD_SRC_DST
+        AVG_NEXT_SRC(src_reg, 1)
+        // average between previous average to current average
+        src_avg = _mm_avg_epu8(src_avg, src_reg);
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_avg, zero_reg)
+        // save current source average
+        CALC_SUM_SSE_INSIDE_LOOP
+        dst += dst_stride;
+      }
+      // x_offset = 4  and y_offset = bilin interpolation
+    } else {
+      __m128i filter, pw8, src_next_reg, src_avg;
+      y_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + y_offset));
+      pw8 = _mm_set1_epi16(8);
+      // load source and another source starting from the next
+      // following byte
+      src_reg = _mm_loadu_si128((__m128i const *)(src));
+      AVG_NEXT_SRC(src_reg, 1)
+      for (i = 0; i < height; i++) {
+        // save current source average
+        src_avg = src_reg;
+        src += src_stride;
+        LOAD_SRC_DST
+        AVG_NEXT_SRC(src_reg, 1)
+        MERGE_WITH_SRC(src_avg, src_reg)
+        FILTER_SRC(filter)
+        CALC_SUM_SSE_INSIDE_LOOP
+        dst += dst_stride;
+      }
+    }
+    // x_offset = bilin interpolation and y_offset = 0
+  } else {
+    if (y_offset == 0) {
+      __m128i filter, pw8, src_next_reg;
+      x_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + x_offset));
+      pw8 = _mm_set1_epi16(8);
+      for (i = 0; i < height; i++) {
+        LOAD_SRC_DST
+        MERGE_NEXT_SRC(src_reg, 1)
+        FILTER_SRC(filter)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride;
+        dst += dst_stride;
+      }
+      // x_offset = bilin interpolation and y_offset = 4
+    } else if (y_offset == 4) {
+      __m128i filter, pw8, src_next_reg, src_pack;
+      x_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + x_offset));
+      pw8 = _mm_set1_epi16(8);
+      src_reg = _mm_loadu_si128((__m128i const *)(src));
+      MERGE_NEXT_SRC(src_reg, 1)
+      FILTER_SRC(filter)
+      // convert each 16 bit to 8 bit to each low and high lane source
+      src_pack = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+      for (i = 0; i < height; i++) {
+        src += src_stride;
+        LOAD_SRC_DST
+        MERGE_NEXT_SRC(src_reg, 1)
+        FILTER_SRC(filter)
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        // average between previous pack to the current
+        src_pack = _mm_avg_epu8(src_pack, src_reg);
+        MERGE_WITH_SRC(src_pack, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src_pack = src_reg;
+        dst += dst_stride;
+      }
+      // x_offset = bilin interpolation and y_offset = bilin interpolation
+    } else {
+      __m128i xfilter, yfilter, pw8, src_next_reg, src_pack;
+      x_offset <<= 4;
+      xfilter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + x_offset));
+      y_offset <<= 4;
+      yfilter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + y_offset));
+      pw8 = _mm_set1_epi16(8);
+      // load source and another source starting from the next
+      // following byte
+      src_reg = _mm_loadu_si128((__m128i const *)(src));
+      MERGE_NEXT_SRC(src_reg, 1)
+
+      FILTER_SRC(xfilter)
+      // convert each 16 bit to 8 bit to each low and high lane source
+      src_pack = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+      for (i = 0; i < height; i++) {
+        src += src_stride;
+        LOAD_SRC_DST
+        MERGE_NEXT_SRC(src_reg, 1)
+        FILTER_SRC(xfilter)
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        // merge previous pack to current pack source
+        MERGE_WITH_SRC(src_pack, src_reg)
+        // filter the source
+        FILTER_SRC(yfilter)
+        src_pack = src_reg;
+        CALC_SUM_SSE_INSIDE_LOOP
+        dst += dst_stride;
+      }
+    }
+  }
+  CALC_SUM_AND_SSE
+  return sum;
+}
+
+unsigned int vpx_sub_pixel_variance8xh_ssse3(const uint8_t *src, int src_stride,
+                                             int x_offset, int y_offset,
+                                             const uint8_t *dst, int dst_stride,
+                                             int height, unsigned int *sse) {
+  __m128i src_reg, dst_reg, exp_src_lo, exp_src_hi, exp_dst_lo, exp_dst_hi;
+  __m128i sse_reg, sum_reg, sse_reg_hi, res_cmp, sum_reg_lo, sum_reg_hi;
+  __m128i zero_reg;
+  int i, sum;
+  sum_reg = _mm_setzero_si128();
+  sse_reg = _mm_setzero_si128();
+  zero_reg = _mm_setzero_si128();
+
+  // x_offset = 0 and y_offset = 0
+  if (x_offset == 0) {
+    if (y_offset == 0) {
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        // expend each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = 0 and y_offset = 4
+    } else if (y_offset == 4) {
+      __m128i src_next_reg;
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        AVG_NEXT_SRC_INSERT(src_reg, src_stride)
+        // expend each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = 0 and y_offset = bilin interpolation
+    } else {
+      __m128i filter, pw8, src_next_reg;
+      y_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + y_offset));
+      pw8 = _mm_set1_epi16(8);
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        MERGE_NEXT_SRC_INSERT(src_reg, src_stride)
+        FILTER_SRC(filter)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+    }
+    // x_offset = 4  and y_offset = 0
+  } else if (x_offset == 4) {
+    if (y_offset == 0) {
+      __m128i src_next_reg;
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        LOAD_DST_INSERT
+        /* average between current and next stride source */
+        src_reg = _mm_avg_epu8(src_reg, src_next_reg);
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = 4  and y_offset = 4
+    } else if (y_offset == 4) {
+      __m128i src_next_reg, src_avg, src_temp;
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      src_avg = _mm_avg_epu8(src_reg, src_next_reg);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        src_next_reg = _mm_avg_epu8(src_reg, src_next_reg);
+        src_temp = _mm_castpd_si128(_mm_shuffle_pd(
+            _mm_castsi128_pd(src_avg), _mm_castsi128_pd(src_next_reg), 1));
+        src_temp = _mm_avg_epu8(src_avg, src_temp);
+        LOAD_DST_INSERT
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_temp, zero_reg)
+        // save current source average
+        src_avg = src_next_reg;
+        CALC_SUM_SSE_INSIDE_LOOP
+        dst += dst_stride << 1;
+        src += src_stride << 1;
+      }
+      // last 2 rows processing happens here
+      __m128i src_reg_0 = _mm_loadl_epi64((__m128i const *)(src));
+      __m128i src_reg_1 = _mm_loadl_epi64((__m128i const *)(src + 1));
+      src_reg_0 = _mm_avg_epu8(src_reg_0, src_reg_1);
+      src_next_reg = _mm_castpd_si128(_mm_shuffle_pd(
+          _mm_castsi128_pd(src_avg), _mm_castsi128_pd(src_reg_0), 1));
+      LOAD_DST_INSERT
+      src_avg = _mm_avg_epu8(src_avg, src_next_reg);
+      MERGE_WITH_SRC(src_avg, zero_reg)
+      CALC_SUM_SSE_INSIDE_LOOP
+    } else {
+      // x_offset = 4  and y_offset = bilin interpolation
+      __m128i filter, pw8, src_next_reg, src_avg, src_temp;
+      y_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + y_offset));
+      pw8 = _mm_set1_epi16(8);
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      src_avg = _mm_avg_epu8(src_reg, src_next_reg);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        src_next_reg = _mm_avg_epu8(src_reg, src_next_reg);
+        src_temp = _mm_castpd_si128(_mm_shuffle_pd(
+            _mm_castsi128_pd(src_avg), _mm_castsi128_pd(src_next_reg), 1));
+        LOAD_DST_INSERT
+        MERGE_WITH_SRC(src_avg, src_temp)
+        // save current source average
+        src_avg = src_next_reg;
+        FILTER_SRC(filter)
+        CALC_SUM_SSE_INSIDE_LOOP
+        dst += dst_stride << 1;
+        src += src_stride << 1;
+      }
+      // last 2 rows processing happens here
+      __m128i src_reg_0 = _mm_loadl_epi64((__m128i const *)(src));
+      __m128i src_reg_1 = _mm_loadl_epi64((__m128i const *)(src + 1));
+      src_reg_0 = _mm_avg_epu8(src_reg_0, src_reg_1);
+      src_next_reg = _mm_castpd_si128(_mm_shuffle_pd(
+          _mm_castsi128_pd(src_avg), _mm_castsi128_pd(src_reg_0), 1));
+      LOAD_DST_INSERT
+      MERGE_WITH_SRC(src_avg, src_next_reg)
+      FILTER_SRC(filter)
+      CALC_SUM_SSE_INSIDE_LOOP
+    }
+    // x_offset = bilin interpolation and y_offset = 0
+  } else {
+    if (y_offset == 0) {
+      __m128i filter, pw8, src_next_reg;
+      x_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + x_offset));
+      pw8 = _mm_set1_epi16(8);
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        MERGE_NEXT_SRC_INSERT(src_reg, 1)
+        FILTER_SRC(filter)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = bilin interpolation and y_offset = 4
+    } else if (y_offset == 4) {
+      __m128i filter, pw8, src_next_reg, src_pack;
+      x_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + x_offset));
+      pw8 = _mm_set1_epi16(8);
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      MERGE_WITH_SRC(src_reg, src_next_reg)
+      FILTER_SRC(filter)
+      // convert each 16 bit to 8 bit to each low and high lane source
+      src_pack = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        LOAD_DST_INSERT
+        MERGE_WITH_SRC(src_reg, src_next_reg)
+        FILTER_SRC(filter)
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        src_next_reg = _mm_castpd_si128(_mm_shuffle_pd(
+            _mm_castsi128_pd(src_pack), _mm_castsi128_pd(src_reg), 1));
+        // average between previous pack to the current
+        src_pack = _mm_avg_epu8(src_pack, src_next_reg);
+        MERGE_WITH_SRC(src_pack, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src_pack = src_reg;
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // last 2 rows processing happens here
+      LOAD_SRC_MERGE_HALF(filter)
+      LOAD_DST_INSERT
+      FILTER_SRC_HALF(filter)
+      src_reg_0 = _mm_packus_epi16(src_lo, src_lo);
+      src_next_reg = _mm_castpd_si128(_mm_shuffle_pd(
+          _mm_castsi128_pd(src_pack), _mm_castsi128_pd(src_reg_0), 1));
+      // average between previous pack to the current
+      src_pack = _mm_avg_epu8(src_pack, src_next_reg);
+      MERGE_WITH_SRC(src_pack, zero_reg)
+      CALC_SUM_SSE_INSIDE_LOOP
+    } else {
+      // x_offset = bilin interpolation and y_offset = bilin interpolation
+      __m128i xfilter, yfilter, pw8, src_next_reg, src_pack;
+      x_offset <<= 4;
+      xfilter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + x_offset));
+      y_offset <<= 4;
+      yfilter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + y_offset));
+      pw8 = _mm_set1_epi16(8);
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      MERGE_WITH_SRC(src_reg, src_next_reg)
+      FILTER_SRC(xfilter)
+      // convert each 16 bit to 8 bit to each low and high lane source
+      src_pack = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        LOAD_DST_INSERT
+        MERGE_WITH_SRC(src_reg, src_next_reg)
+        FILTER_SRC(xfilter)
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        src_next_reg = _mm_castpd_si128(_mm_shuffle_pd(
+            _mm_castsi128_pd(src_pack), _mm_castsi128_pd(src_reg), 1));
+        // average between previous pack to the current
+        MERGE_WITH_SRC(src_pack, src_next_reg)
+        // filter the source
+        FILTER_SRC(yfilter)
+        src_pack = src_reg;
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // last 2 rows processing happens here
+      LOAD_SRC_MERGE_HALF(xfilter)
+      LOAD_DST_INSERT
+      FILTER_SRC_HALF(xfilter)
+      src_reg_0 = _mm_packus_epi16(src_lo, src_lo);
+      src_next_reg = _mm_castpd_si128(_mm_shuffle_pd(
+          _mm_castsi128_pd(src_pack), _mm_castsi128_pd(src_reg_0), 1));
+      MERGE_WITH_SRC(src_pack, src_next_reg)
+      FILTER_SRC(yfilter)
+      CALC_SUM_SSE_INSIDE_LOOP
+    }
+  }
+  CALC_SUM_AND_SSE
+  return sum;
+}
+
+unsigned int vpx_sub_pixel_avg_variance16xh_ssse3(
+    const uint8_t *src, int src_stride, int x_offset, int y_offset,
+    const uint8_t *dst, int dst_stride, const uint8_t *sec, int sec_stride,
+    int height, unsigned int *sse) {
+  __m128i sec_reg;
+  __m128i src_reg, dst_reg, exp_src_lo, exp_src_hi, exp_dst_lo, exp_dst_hi;
+  __m128i sse_reg, sum_reg, sse_reg_hi, res_cmp, sum_reg_lo, sum_reg_hi;
+  __m128i zero_reg;
+  int i, sum;
+  sum_reg = _mm_setzero_si128();
+  sse_reg = _mm_setzero_si128();
+  zero_reg = _mm_setzero_si128();
+
+  // x_offset = 0 and y_offset = 0
+  if (x_offset == 0) {
+    if (y_offset == 0) {
+      for (i = 0; i < height; i++) {
+        LOAD_SRC_DST
+        sec_reg = _mm_loadu_si128((__m128i const *)(sec));
+        src_reg = _mm_avg_epu8(src_reg, sec_reg);
+        sec += sec_stride;
+        // expend each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride;
+        dst += dst_stride;
+      }
+    } else if (y_offset == 4) {
+      __m128i src_next_reg;
+      for (i = 0; i < height; i++) {
+        LOAD_SRC_DST
+        AVG_NEXT_SRC(src_reg, src_stride)
+        sec_reg = _mm_loadu_si128((__m128i const *)(sec));
+        src_reg = _mm_avg_epu8(src_reg, sec_reg);
+        sec += sec_stride;
+        // expend each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride;
+        dst += dst_stride;
+      }
+      // x_offset = 0 and y_offset = bilin interpolation
+    } else {
+      __m128i filter, pw8, src_next_reg;
+
+      y_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + y_offset));
+      pw8 = _mm_set1_epi16(8);
+      for (i = 0; i < height; i++) {
+        LOAD_SRC_DST
+        MERGE_NEXT_SRC(src_reg, src_stride)
+        FILTER_SRC(filter)
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        sec_reg = _mm_loadu_si128((__m128i const *)(sec));
+        src_reg = _mm_avg_epu8(src_reg, sec_reg);
+        sec += sec_stride;
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride;
+        dst += dst_stride;
+      }
+    }
+    // x_offset = 4  and y_offset = 0
+  } else if (x_offset == 4) {
+    if (y_offset == 0) {
+      __m128i src_next_reg;
+      for (i = 0; i < height; i++) {
+        LOAD_SRC_DST
+        AVG_NEXT_SRC(src_reg, 1)
+        sec_reg = _mm_loadu_si128((__m128i const *)(sec));
+        src_reg = _mm_avg_epu8(src_reg, sec_reg);
+        sec += sec_stride;
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride;
+        dst += dst_stride;
+      }
+      // x_offset = 4  and y_offset = 4
+    } else if (y_offset == 4) {
+      __m128i src_next_reg, src_avg;
+      // load source and another source starting from the next
+      // following byte
+      src_reg = _mm_loadu_si128((__m128i const *)(src));
+      AVG_NEXT_SRC(src_reg, 1)
+      for (i = 0; i < height; i++) {
+        // save current source average
+        src_avg = src_reg;
+        src += src_stride;
+        LOAD_SRC_DST
+        AVG_NEXT_SRC(src_reg, 1)
+        // average between previous average to current average
+        src_avg = _mm_avg_epu8(src_avg, src_reg);
+        sec_reg = _mm_loadu_si128((__m128i const *)(sec));
+        src_avg = _mm_avg_epu8(src_avg, sec_reg);
+        sec += sec_stride;
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_avg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        dst += dst_stride;
+      }
+      // x_offset = 4  and y_offset = bilin interpolation
+    } else {
+      __m128i filter, pw8, src_next_reg, src_avg;
+      y_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + y_offset));
+      pw8 = _mm_set1_epi16(8);
+      // load source and another source starting from the next
+      // following byte
+      src_reg = _mm_loadu_si128((__m128i const *)(src));
+      AVG_NEXT_SRC(src_reg, 1)
+      for (i = 0; i < height; i++) {
+        // save current source average
+        src_avg = src_reg;
+        src += src_stride;
+        LOAD_SRC_DST
+        AVG_NEXT_SRC(src_reg, 1)
+        MERGE_WITH_SRC(src_avg, src_reg)
+        FILTER_SRC(filter)
+        src_avg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        sec_reg = _mm_loadu_si128((__m128i const *)(sec));
+        src_avg = _mm_avg_epu8(src_avg, sec_reg);
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_avg, zero_reg)
+        sec += sec_stride;
+        CALC_SUM_SSE_INSIDE_LOOP
+        dst += dst_stride;
+      }
+    }
+    // x_offset = bilin interpolation and y_offset = 0
+  } else {
+    if (y_offset == 0) {
+      __m128i filter, pw8, src_next_reg;
+      x_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + x_offset));
+      pw8 = _mm_set1_epi16(8);
+      for (i = 0; i < height; i++) {
+        LOAD_SRC_DST
+        MERGE_NEXT_SRC(src_reg, 1)
+        FILTER_SRC(filter)
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        sec_reg = _mm_loadu_si128((__m128i const *)(sec));
+        src_reg = _mm_avg_epu8(src_reg, sec_reg);
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        sec += sec_stride;
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride;
+        dst += dst_stride;
+      }
+      // x_offset = bilin interpolation and y_offset = 4
+    } else if (y_offset == 4) {
+      __m128i filter, pw8, src_next_reg, src_pack;
+      x_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + x_offset));
+      pw8 = _mm_set1_epi16(8);
+      src_reg = _mm_loadu_si128((__m128i const *)(src));
+      MERGE_NEXT_SRC(src_reg, 1)
+      FILTER_SRC(filter)
+      // convert each 16 bit to 8 bit to each low and high lane source
+      src_pack = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+      for (i = 0; i < height; i++) {
+        src += src_stride;
+        LOAD_SRC_DST
+        MERGE_NEXT_SRC(src_reg, 1)
+        FILTER_SRC(filter)
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        // average between previous pack to the current
+        src_pack = _mm_avg_epu8(src_pack, src_reg);
+        sec_reg = _mm_loadu_si128((__m128i const *)(sec));
+        src_pack = _mm_avg_epu8(src_pack, sec_reg);
+        sec += sec_stride;
+        MERGE_WITH_SRC(src_pack, zero_reg)
+        src_pack = src_reg;
+        CALC_SUM_SSE_INSIDE_LOOP
+        dst += dst_stride;
+      }
+      // x_offset = bilin interpolation and y_offset = bilin interpolation
+    } else {
+      __m128i xfilter, yfilter, pw8, src_next_reg, src_pack;
+      x_offset <<= 4;
+      xfilter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + x_offset));
+      y_offset <<= 4;
+      yfilter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + y_offset));
+      pw8 = _mm_set1_epi16(8);
+      // load source and another source starting from the next
+      // following byte
+      src_reg = _mm_loadu_si128((__m128i const *)(src));
+      MERGE_NEXT_SRC(src_reg, 1)
+
+      FILTER_SRC(xfilter)
+      // convert each 16 bit to 8 bit to each low and high lane source
+      src_pack = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+      for (i = 0; i < height; i++) {
+        src += src_stride;
+        LOAD_SRC_DST
+        MERGE_NEXT_SRC(src_reg, 1)
+        FILTER_SRC(xfilter)
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        // merge previous pack to current pack source
+        MERGE_WITH_SRC(src_pack, src_reg)
+        // filter the source
+        FILTER_SRC(yfilter)
+        src_pack = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        sec_reg = _mm_loadu_si128((__m128i const *)(sec));
+        src_pack = _mm_avg_epu8(src_pack, sec_reg);
+        MERGE_WITH_SRC(src_pack, zero_reg)
+        src_pack = src_reg;
+        sec += sec_stride;
+        CALC_SUM_SSE_INSIDE_LOOP
+        dst += dst_stride;
+      }
+    }
+  }
+  CALC_SUM_AND_SSE
+  return sum;
+}
+
+unsigned int vpx_sub_pixel_avg_variance8xh_ssse3(
+    const uint8_t *src, int src_stride, int x_offset, int y_offset,
+    const uint8_t *dst, int dst_stride, const uint8_t *sec, int sec_stride,
+    int height, unsigned int *sse) {
+  __m128i sec_reg;
+  __m128i src_reg, dst_reg, exp_src_lo, exp_src_hi, exp_dst_lo, exp_dst_hi;
+  __m128i sse_reg, sum_reg, sse_reg_hi, res_cmp, sum_reg_lo, sum_reg_hi;
+  __m128i zero_reg;
+  int i, sum;
+  sum_reg = _mm_setzero_si128();
+  sse_reg = _mm_setzero_si128();
+  zero_reg = _mm_setzero_si128();
+
+  // x_offset = 0 and y_offset = 0
+  if (x_offset == 0) {
+    if (y_offset == 0) {
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        sec_reg = _mm_unpacklo_epi64(
+            _mm_loadl_epi64((__m128i const *)(sec)),
+            _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+        src_reg = _mm_avg_epu8(src_reg, sec_reg);
+        sec += sec_stride << 1;
+        // expend each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = 0 and y_offset = 4
+    } else if (y_offset == 4) {
+      __m128i src_next_reg;
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        AVG_NEXT_SRC_INSERT(src_reg, src_stride)
+        sec_reg = _mm_unpacklo_epi64(
+            _mm_loadl_epi64((__m128i const *)(sec)),
+            _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+        src_reg = _mm_avg_epu8(src_reg, sec_reg);
+        sec += sec_stride << 1;
+        // expend each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = 0 and y_offset = bilin interpolation
+    } else {
+      __m128i filter, pw8, src_next_reg;
+      y_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + y_offset));
+      pw8 = _mm_set1_epi16(8);
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        MERGE_NEXT_SRC_INSERT(src_reg, src_stride)
+        FILTER_SRC(filter)
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        sec_reg = _mm_unpacklo_epi64(
+            _mm_loadl_epi64((__m128i const *)(sec)),
+            _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+        src_reg = _mm_avg_epu8(src_reg, sec_reg);
+        sec += sec_stride << 1;
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+    }
+    // x_offset = 4  and y_offset = 0
+  } else if (x_offset == 4) {
+    if (y_offset == 0) {
+      __m128i src_next_reg;
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        LOAD_DST_INSERT
+        /* average between current and next stride source */
+        src_reg = _mm_avg_epu8(src_reg, src_next_reg);
+        sec_reg = _mm_unpacklo_epi64(
+            _mm_loadl_epi64((__m128i const *)(sec)),
+            _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+        src_reg = _mm_avg_epu8(src_reg, sec_reg);
+        sec += sec_stride << 1;
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = 4  and y_offset = 4
+    } else if (y_offset == 4) {
+      __m128i src_next_reg, src_avg, src_temp;
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      src_avg = _mm_avg_epu8(src_reg, src_next_reg);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        src_next_reg = _mm_avg_epu8(src_reg, src_next_reg);
+        src_temp = _mm_castpd_si128(_mm_shuffle_pd(
+            _mm_castsi128_pd(src_avg), _mm_castsi128_pd(src_next_reg), 1));
+        src_temp = _mm_avg_epu8(src_avg, src_temp);
+        sec_reg = _mm_unpacklo_epi64(
+            _mm_loadl_epi64((__m128i const *)(sec)),
+            _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+        src_temp = _mm_avg_epu8(src_temp, sec_reg);
+        sec += sec_stride << 1;
+        LOAD_DST_INSERT
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_temp, zero_reg)
+        // save current source average
+        src_avg = src_next_reg;
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // last 2 rows processing happens here
+      __m128i src_reg_0 = _mm_loadl_epi64((__m128i const *)(src));
+      __m128i src_reg_1 = _mm_loadl_epi64((__m128i const *)(src + 1));
+      src_reg_0 = _mm_avg_epu8(src_reg_0, src_reg_1);
+      src_next_reg = _mm_castpd_si128(_mm_shuffle_pd(
+          _mm_castsi128_pd(src_avg), _mm_castsi128_pd(src_reg_0), 1));
+      LOAD_DST_INSERT
+      src_avg = _mm_avg_epu8(src_avg, src_next_reg);
+      sec_reg = _mm_unpacklo_epi64(
+          _mm_loadl_epi64((__m128i const *)(sec)),
+          _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+      src_avg = _mm_avg_epu8(src_avg, sec_reg);
+      MERGE_WITH_SRC(src_avg, zero_reg)
+      CALC_SUM_SSE_INSIDE_LOOP
+    } else {
+      // x_offset = 4  and y_offset = bilin interpolation
+      __m128i filter, pw8, src_next_reg, src_avg, src_temp;
+      y_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + y_offset));
+      pw8 = _mm_set1_epi16(8);
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      src_avg = _mm_avg_epu8(src_reg, src_next_reg);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        src_next_reg = _mm_avg_epu8(src_reg, src_next_reg);
+        src_temp = _mm_castpd_si128(_mm_shuffle_pd(
+            _mm_castsi128_pd(src_avg), _mm_castsi128_pd(src_next_reg), 1));
+        LOAD_DST_INSERT
+        MERGE_WITH_SRC(src_avg, src_temp)
+        FILTER_SRC(filter)
+        src_avg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        sec_reg = _mm_unpacklo_epi64(
+            _mm_loadl_epi64((__m128i const *)(sec)),
+            _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+        src_avg = _mm_avg_epu8(src_avg, sec_reg);
+        sec += sec_stride << 1;
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_avg, zero_reg)
+        // save current source average
+        src_avg = src_next_reg;
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // last 2 rows processing happens here
+      __m128i src_reg_0 = _mm_loadl_epi64((__m128i const *)(src));
+      __m128i src_reg_1 = _mm_loadl_epi64((__m128i const *)(src + 1));
+      src_reg_0 = _mm_avg_epu8(src_reg_0, src_reg_1);
+      src_next_reg = _mm_castpd_si128(_mm_shuffle_pd(
+          _mm_castsi128_pd(src_avg), _mm_castsi128_pd(src_reg_0), 1));
+      LOAD_DST_INSERT
+      MERGE_WITH_SRC(src_avg, src_next_reg)
+      FILTER_SRC(filter)
+      src_avg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+      sec_reg = _mm_unpacklo_epi64(
+          _mm_loadl_epi64((__m128i const *)(sec)),
+          _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+      src_avg = _mm_avg_epu8(src_avg, sec_reg);
+      MERGE_WITH_SRC(src_avg, zero_reg)
+      CALC_SUM_SSE_INSIDE_LOOP
+    }
+    // x_offset = bilin interpolation and y_offset = 0
+  } else {
+    if (y_offset == 0) {
+      __m128i filter, pw8, src_next_reg;
+      x_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + x_offset));
+      pw8 = _mm_set1_epi16(8);
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        MERGE_NEXT_SRC_INSERT(src_reg, 1)
+        FILTER_SRC(filter)
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        sec_reg = _mm_unpacklo_epi64(
+            _mm_loadl_epi64((__m128i const *)(sec)),
+            _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+        src_reg = _mm_avg_epu8(src_reg, sec_reg);
+        sec += sec_stride << 1;
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = bilin interpolation and y_offset = 4
+    } else if (y_offset == 4) {
+      __m128i filter, pw8, src_next_reg, src_pack;
+      x_offset <<= 4;
+      filter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + x_offset));
+      pw8 = _mm_set1_epi16(8);
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      MERGE_WITH_SRC(src_reg, src_next_reg)
+      FILTER_SRC(filter)
+      // convert each 16 bit to 8 bit to each low and high lane source
+      src_pack = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        LOAD_DST_INSERT
+        MERGE_WITH_SRC(src_reg, src_next_reg)
+        FILTER_SRC(filter)
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        src_next_reg = _mm_castpd_si128(_mm_shuffle_pd(
+            _mm_castsi128_pd(src_pack), _mm_castsi128_pd(src_reg), 1));
+        // average between previous pack to the current
+        src_pack = _mm_avg_epu8(src_pack, src_next_reg);
+        sec_reg = _mm_unpacklo_epi64(
+            _mm_loadl_epi64((__m128i const *)(sec)),
+            _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+        src_pack = _mm_avg_epu8(src_pack, sec_reg);
+        sec += sec_stride << 1;
+        MERGE_WITH_SRC(src_pack, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src_pack = src_reg;
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // last 2 rows processing happens here
+      LOAD_SRC_MERGE_HALF(filter)
+      LOAD_DST_INSERT
+      FILTER_SRC_HALF(filter)
+      src_reg_0 = _mm_packus_epi16(src_lo, src_lo);
+      src_next_reg = _mm_castpd_si128(_mm_shuffle_pd(
+          _mm_castsi128_pd(src_pack), _mm_castsi128_pd(src_reg_0), 1));
+      // average between previous pack to the current
+      src_pack = _mm_avg_epu8(src_pack, src_next_reg);
+      sec_reg = _mm_unpacklo_epi64(
+          _mm_loadl_epi64((__m128i const *)(sec)),
+          _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+      src_pack = _mm_avg_epu8(src_pack, sec_reg);
+      MERGE_WITH_SRC(src_pack, zero_reg)
+      CALC_SUM_SSE_INSIDE_LOOP
+    } else {
+      // x_offset = bilin interpolation and y_offset = bilin interpolation
+      __m128i xfilter, yfilter, pw8, src_next_reg, src_pack;
+      x_offset <<= 4;
+      xfilter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + x_offset));
+      y_offset <<= 4;
+      yfilter = _mm_load_si128(
+          (__m128i const *)(bilinear_filters_sse2 + y_offset));
+      pw8 = _mm_set1_epi16(8);
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      MERGE_WITH_SRC(src_reg, src_next_reg)
+      FILTER_SRC(xfilter)
+      // convert each 16 bit to 8 bit to each low and high lane source
+      src_pack = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        LOAD_DST_INSERT
+        MERGE_WITH_SRC(src_reg, src_next_reg)
+        FILTER_SRC(xfilter)
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        src_next_reg = _mm_castpd_si128(_mm_shuffle_pd(
+            _mm_castsi128_pd(src_pack), _mm_castsi128_pd(src_reg), 1));
+        // average between previous pack to the current
+        MERGE_WITH_SRC(src_pack, src_next_reg)
+        // filter the source
+        FILTER_SRC(yfilter)
+        src_pack = src_reg;
+        src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+        sec_reg = _mm_unpacklo_epi64(
+            _mm_loadl_epi64((__m128i const *)(sec)),
+            _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+        src_reg = _mm_avg_epu8(src_reg, sec_reg);
+        sec += sec_stride << 1;
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // last 2 rows processing happens here
+      LOAD_SRC_MERGE_HALF(xfilter)
+      LOAD_DST_INSERT
+      FILTER_SRC_HALF(xfilter)
+      src_reg_0 = _mm_packus_epi16(src_lo, src_lo);
+      src_next_reg = _mm_castpd_si128(_mm_shuffle_pd(
+          _mm_castsi128_pd(src_pack), _mm_castsi128_pd(src_reg_0), 1));
+      MERGE_WITH_SRC(src_pack, src_next_reg)
+      FILTER_SRC(yfilter)
+      src_reg = _mm_packus_epi16(exp_src_lo, exp_src_hi);
+      sec_reg = _mm_unpacklo_epi64(
+          _mm_loadl_epi64((__m128i const *)(sec)),
+          _mm_loadl_epi64((__m128i const *)(sec + sec_stride)));
+      src_reg = _mm_avg_epu8(src_reg, sec_reg);
+      MERGE_WITH_SRC(src_reg, zero_reg)
+      CALC_SUM_SSE_INSIDE_LOOP
+    }
+  }
+  CALC_SUM_AND_SSE
+  return sum;
+}
+
+#undef FILTER_SRC
+#undef MERGE_WITH_SRC
+#undef LOAD_SRC_DST
+#undef AVG_NEXT_SRC
+#undef MERGE_NEXT_SRC
+#undef CALC_SUM_SSE_INSIDE_LOOP
+#undef CALC_SUM_AND_SSE
+#undef LOAD_SRC_DST_INSERT
+#undef AVG_NEXT_SRC_INSERT
+#undef MERGE_NEXT_SRC_INSERT
+#undef LOAD_SRC_NEXT_BYTE_INSERT
+#undef LOAD_DST_INSERT
+#undef LOAD_SRC_MERGE_HALF
+#undef FILTER_SRC_HALF
+
+#define FILTER_SRC(filter)                               \
+  /* filter the source */                                \
+  exp_src_lo = _mm_maddubs_pi16(exp_src_lo, filter);     \
+  exp_src_hi = _mm_maddubs_pi16(exp_src_hi, filter);     \
+                                                         \
+  /* add 8 to source */                                  \
+  exp_src_lo = _mm_add_pi16(exp_src_lo, pw8);            \
+  exp_src_hi = _mm_add_pi16(exp_src_hi, pw8);            \
+                                                         \
+  /* divide source by 16 */                              \
+  exp_src_lo = _mm_srai_pi16(exp_src_lo, 4);             \
+  exp_src_hi = _mm_srai_pi16(exp_src_hi, 4);
+
+#define MERGE_WITH_SRC(src_reg, reg)               \
+  exp_src_lo = _mm_unpacklo_pi8(src_reg, reg);     \
+  exp_src_hi = _mm_unpackhi_pi8(src_reg, reg);
+
+#define CALC_SUM_SSE_INSIDE_LOOP                          \
+  /* expand each byte to 2 bytes */                       \
+  exp_dst_lo = _mm_unpacklo_pi8(dst_reg, zero_reg);       \
+  exp_dst_hi = _mm_unpackhi_pi8(dst_reg, zero_reg);       \
+  /* source - dest */                                     \
+  exp_src_lo = _mm_sub_pi16(exp_src_lo, exp_dst_lo);      \
+  exp_src_hi = _mm_sub_pi16(exp_src_hi, exp_dst_hi);      \
+  /* caculate sum */                                      \
+  sum_reg = _mm_add_pi16(sum_reg, exp_src_lo);            \
+  exp_src_lo = _mm_madd_pi16(exp_src_lo, exp_src_lo);     \
+  sum_reg = _mm_add_pi16(sum_reg, exp_src_hi);            \
+  exp_src_hi = _mm_madd_pi16(exp_src_hi, exp_src_hi);     \
+  /* calculate sse */                                     \
+  sse_reg = _mm_add_pi32(sse_reg, exp_src_lo);            \
+  sse_reg = _mm_add_pi32(sse_reg, exp_src_hi);
+
+// final calculation to sum and sse
+#define CALC_SUM_AND_SSE                                                   \
+  res_cmp = _mm_cmpgt_pi16(zero_reg, sum_reg);                             \
+  sum_reg_lo = _mm_unpacklo_pi16(sum_reg, res_cmp);                        \
+  sum_reg_hi = _mm_unpackhi_pi16(sum_reg, res_cmp);                        \
+  sum_reg = _mm_add_pi32(sum_reg_lo, sum_reg_hi);                          \
+                                                                           \
+  sse_reg_hi = _mm_srli_si64(sse_reg, 32);                                 \
+  sse_reg = _mm_add_pi32(sse_reg, sse_reg_hi);                             \
+  *((int *)sse) = _mm_cvtsi64_si32(sse_reg);                               \
+  sum_reg_hi = _mm_srli_si64(sum_reg, 32);                                 \
+  sum_reg = _mm_add_pi32(sum_reg, sum_reg_hi);                             \
+  sum = _mm_cvtsi64_si32(sum_reg);
+
+// Functions related to sub pixel variance width 8
+#define LOAD_SRC_DST_INSERT(src_stride, dst_stride)              \
+  /* load source and destination of 2 rows and insert*/          \
+  src_reg = _mm_unpacklo_pi32(                                   \
+      _mm_cvtsi32_si64(*(uint32_t const *)(src)),                \
+      _mm_cvtsi32_si64(*(uint32_t const *)(src + src_stride)));  \
+  dst_reg = _mm_unpacklo_pi32(                                   \
+      _mm_cvtsi32_si64(*(uint32_t const *)(dst)),                \
+      _mm_cvtsi32_si64(*(uint32_t const *)(dst + dst_stride)));
+
+#define AVG_NEXT_SRC_INSERT(src_reg, size_stride)                              \
+  src_next_reg = _mm_unpacklo_pi32(                                            \
+      _mm_cvtsi32_si64(*(uint32_t const *)(src + size_stride)),                \
+      _mm_cvtsi32_si64(*(uint32_t const *)(src + (size_stride << 1))));        \
+  /* average between current and next stride source */                         \
+  src_reg = _mm_avg_pu8(src_reg, src_next_reg);
+
+#define MERGE_NEXT_SRC_INSERT(src_reg, size_stride)                            \
+  src_next_reg = _mm_unpacklo_pi32(                                            \
+      _mm_cvtsi32_si64(*(uint32_t const *)(src + size_stride)),                \
+      _mm_cvtsi32_si64(*(uint32_t const *)(src + src_stride + size_stride)));  \
+  MERGE_WITH_SRC(src_reg, src_next_reg)
+
+#define LOAD_SRC_NEXT_BYTE_INSERT                                    \
+  /* load source and another source from next row   */               \
+  src_reg = _mm_unpacklo_pi32(                                       \
+      _mm_cvtsi32_si64(*(uint32_t const *)(src)),                    \
+      _mm_cvtsi32_si64(*(uint32_t const *)(src + src_stride)));      \
+  /* load source and next row source from 1 byte onwards   */        \
+  src_next_reg = _mm_unpacklo_pi32(                                  \
+      _mm_cvtsi32_si64(*(uint32_t const *)(src + 1)),                \
+      _mm_cvtsi32_si64(*(uint32_t const *)(src + src_stride + 1)));
+
+#define LOAD_DST_INSERT                                        \
+  dst_reg = _mm_unpacklo_pi32(                                 \
+      _mm_cvtsi32_si64(*(uint32_t const *)(dst)),              \
+      _mm_cvtsi32_si64(*(uint32_t const *)(dst + dst_stride)));
+
+#define LOAD_SRC_MERGE_HALF(filter)                                 \
+  __m64 src_reg_0 = _mm_cvtsi32_si64(*(uint32_t const *)(src));     \
+  __m64 src_reg_1 = _mm_cvtsi32_si64(*(uint32_t const *)(src + 1)); \
+  __m64 src_lo = _mm_unpacklo_pi8(src_reg_0, src_reg_1);
+
+#define FILTER_SRC_HALF(filter)               \
+  /* filter the source */                     \
+  src_lo = _mm_maddubs_pi16(src_lo, filter);  \
+                                              \
+  /* add 8 to source */                       \
+  src_lo = _mm_add_pi16(src_lo, pw8);         \
+                                              \
+  /* divide source by 16 */                   \
+  src_lo = _mm_srai_pi16(src_lo, 4);
+
+unsigned int vpx_sub_pixel_variance4xh_ssse3(const uint8_t *src, int src_stride,
+                                             int x_offset, int y_offset,
+                                             const uint8_t *dst, int dst_stride,
+                                             int height, unsigned int *sse) {
+  __m64 src_reg, dst_reg, exp_src_lo, exp_src_hi, exp_dst_lo, exp_dst_hi;
+  __m64 sse_reg, sum_reg, sse_reg_hi, res_cmp, sum_reg_lo, sum_reg_hi;
+  __m64 zero_reg;
+  int i, sum;
+  sum_reg = _mm_setzero_si64();
+  sse_reg = _mm_setzero_si64();
+  zero_reg = _mm_setzero_si64();
+
+  // x_offset = 0 and y_offset = 0
+  if (x_offset == 0) {
+    if (y_offset == 0) {
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        // expend each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = 0 and y_offset = 4
+    } else if (y_offset == 4) {
+      __m64 src_next_reg;
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        AVG_NEXT_SRC_INSERT(src_reg, src_stride)
+        // expend each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = 0 and y_offset = bilin interpolation
+    } else {
+      __m64 filter, pw8, src_next_reg;
+      y_offset <<= 4;
+      filter = *(__m64 const *)(bilinear_filters_sse2 + y_offset);
+      pw8 = _mm_set1_pi16(8);
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        MERGE_NEXT_SRC_INSERT(src_reg, src_stride)
+        FILTER_SRC(filter)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+    }
+    // x_offset = 4  and y_offset = 0
+  } else if (x_offset == 4) {
+    if (y_offset == 0) {
+      __m64 src_next_reg;
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        LOAD_DST_INSERT
+        /* average between current and next stride source */
+        src_reg = _mm_avg_pu8(src_reg, src_next_reg);
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = 4  and y_offset = 4
+    } else if (y_offset == 4) {
+      __m64 src_next_reg, src_avg, src_temp;
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      src_avg = _mm_avg_pu8(src_reg, src_next_reg);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        src_next_reg = _mm_avg_pu8(src_reg, src_next_reg);
+        src_temp = _mm_alignr_pi8(src_next_reg, src_avg, 4);
+        src_temp = _mm_avg_pu8(src_avg, src_temp);
+        LOAD_DST_INSERT
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_temp, zero_reg)
+        // save current source average
+        src_avg = src_next_reg;
+        CALC_SUM_SSE_INSIDE_LOOP
+        dst += dst_stride << 1;
+        src += src_stride << 1;
+      }
+      // last 2 rows processing happens here
+      __m64 src_reg_0 = _mm_cvtsi32_si64(*(uint32_t const *)(src));
+      __m64 src_reg_1 = _mm_cvtsi32_si64(*(uint32_t const *)(src + 1));
+      src_reg_0 = _mm_avg_pu8(src_reg_0, src_reg_1);
+      src_next_reg = _mm_alignr_pi8(src_reg_0, src_avg, 4);
+      LOAD_DST_INSERT
+      src_avg = _mm_avg_pu8(src_avg, src_next_reg);
+      MERGE_WITH_SRC(src_avg, zero_reg)
+      CALC_SUM_SSE_INSIDE_LOOP
+    } else {
+      // x_offset = 4  and y_offset = bilin interpolation
+      __m64 filter, pw8, src_next_reg, src_avg, src_temp;
+      y_offset <<= 4;
+      filter = *(__m64 const *)(bilinear_filters_sse2 + y_offset);
+      pw8 = _mm_set1_pi16(8);
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      src_avg = _mm_avg_pu8(src_reg, src_next_reg);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        src_next_reg = _mm_avg_pu8(src_reg, src_next_reg);
+        src_temp = _mm_alignr_pi8(src_next_reg, src_avg, 4);
+        LOAD_DST_INSERT
+        MERGE_WITH_SRC(src_avg, src_temp)
+        // save current source average
+        src_avg = src_next_reg;
+        FILTER_SRC(filter)
+        CALC_SUM_SSE_INSIDE_LOOP
+        dst += dst_stride << 1;
+        src += src_stride << 1;
+      }
+      // last 2 rows processing happens here
+      __m64 src_reg_0 = _mm_cvtsi32_si64(*(uint32_t const *)(src));
+      __m64 src_reg_1 = _mm_cvtsi32_si64(*(uint32_t const *)(src + 1));
+      src_reg_0 = _mm_avg_pu8(src_reg_0, src_reg_1);
+      src_next_reg = _mm_alignr_pi8(src_reg_0, src_avg, 4);
+      LOAD_DST_INSERT
+      MERGE_WITH_SRC(src_avg, src_next_reg)
+      FILTER_SRC(filter)
+      CALC_SUM_SSE_INSIDE_LOOP
+    }
+    // x_offset = bilin interpolation and y_offset = 0
+  } else {
+    if (y_offset == 0) {
+      __m64 filter, pw8, src_next_reg;
+      x_offset <<= 4;
+      filter = *(__m64 const *)(bilinear_filters_sse2 + x_offset);
+      pw8 = _mm_set1_pi16(8);
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        MERGE_NEXT_SRC_INSERT(src_reg, 1)
+        FILTER_SRC(filter)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = bilin interpolation and y_offset = 4
+    } else if (y_offset == 4) {
+      __m64 filter, pw8, src_next_reg, src_pack;
+      x_offset <<= 4;
+      filter = *(__m64 const *)(bilinear_filters_sse2 + x_offset);
+      pw8 = _mm_set1_pi16(8);
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      MERGE_WITH_SRC(src_reg, src_next_reg)
+      FILTER_SRC(filter)
+      // convert each 16 bit to 8 bit to each low and high lane source
+      src_pack = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        LOAD_DST_INSERT
+        MERGE_WITH_SRC(src_reg, src_next_reg)
+        FILTER_SRC(filter)
+        src_reg = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+        src_next_reg = _mm_alignr_pi8(src_reg, src_pack, 4);
+        // average between previous pack to the current
+        src_pack = _mm_avg_pu8(src_pack, src_next_reg);
+        MERGE_WITH_SRC(src_pack, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src_pack = src_reg;
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // last 2 rows processing happens here
+      LOAD_SRC_MERGE_HALF(filter)
+      LOAD_DST_INSERT
+      FILTER_SRC_HALF(filter)
+      src_reg_0 = _mm_packs_pu16(src_lo, src_lo);
+      src_next_reg = _mm_alignr_pi8(src_reg_0, src_pack, 4);
+      // average between previous pack to the current
+      src_pack = _mm_avg_pu8(src_pack, src_next_reg);
+      MERGE_WITH_SRC(src_pack, zero_reg)
+      CALC_SUM_SSE_INSIDE_LOOP
+    } else {
+      // x_offset = bilin interpolation and y_offset = bilin interpolation
+      __m64 xfilter, yfilter, pw8, src_next_reg, src_pack;
+      x_offset <<= 4;
+      xfilter = *(__m64 const *)(bilinear_filters_sse2 + x_offset);
+      y_offset <<= 4;
+      yfilter = *(__m64 const *)(bilinear_filters_sse2 + y_offset);
+      pw8 = _mm_set1_pi16(8);
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      MERGE_WITH_SRC(src_reg, src_next_reg)
+      FILTER_SRC(xfilter)
+      // convert each 16 bit to 8 bit to each low and high lane source
+      src_pack = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        LOAD_DST_INSERT
+        MERGE_WITH_SRC(src_reg, src_next_reg)
+        FILTER_SRC(xfilter)
+        src_reg = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+        src_next_reg = _mm_alignr_pi8(src_reg, src_pack, 4);
+        // average between previous pack to the current
+        MERGE_WITH_SRC(src_pack, src_next_reg)
+        // filter the source
+        FILTER_SRC(yfilter)
+        src_pack = src_reg;
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // last 2 rows processing happens here
+      LOAD_SRC_MERGE_HALF(xfilter)
+      LOAD_DST_INSERT
+      FILTER_SRC_HALF(xfilter)
+      src_reg_0 = _mm_packs_pu16(src_lo, src_lo);
+      src_next_reg = _mm_alignr_pi8(src_reg_0, src_pack, 4);
+      MERGE_WITH_SRC(src_pack, src_next_reg)
+      FILTER_SRC(yfilter)
+      CALC_SUM_SSE_INSIDE_LOOP
+    }
+  }
+  CALC_SUM_AND_SSE
+  return sum;
+}
+
+unsigned int vpx_sub_pixel_avg_variance4xh_ssse3(
+    const uint8_t *src, int src_stride, int x_offset, int y_offset,
+    const uint8_t *dst, int dst_stride, const uint8_t *sec, int sec_stride,
+    int height, unsigned int *sse) {
+  __m64 sec_reg;
+  __m64 src_reg, dst_reg, exp_src_lo, exp_src_hi, exp_dst_lo, exp_dst_hi;
+  __m64 sse_reg, sum_reg, sse_reg_hi, res_cmp, sum_reg_lo, sum_reg_hi;
+  __m64 zero_reg;
+  int i, sum;
+  sum_reg = _mm_setzero_si64();
+  sse_reg = _mm_setzero_si64();
+  zero_reg = _mm_setzero_si64();
+
+  // x_offset = 0 and y_offset = 0
+  if (x_offset == 0) {
+    if (y_offset == 0) {
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        sec_reg = _mm_unpacklo_pi32(
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+        src_reg = _mm_avg_pu8(src_reg, sec_reg);
+        sec += sec_stride << 1;
+        // expend each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = 0 and y_offset = 4
+    } else if (y_offset == 4) {
+      __m64 src_next_reg;
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        AVG_NEXT_SRC_INSERT(src_reg, src_stride)
+        sec_reg = _mm_unpacklo_pi32(
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+        src_reg = _mm_avg_pu8(src_reg, sec_reg);
+        sec += sec_stride << 1;
+        // expend each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = 0 and y_offset = bilin interpolation
+    } else {
+      __m64 filter, pw8, src_next_reg;
+      y_offset <<= 4;
+      filter = *(__m64 const *)(bilinear_filters_sse2 + y_offset);
+      pw8 = _mm_set1_pi16(8);
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        MERGE_NEXT_SRC_INSERT(src_reg, src_stride)
+        FILTER_SRC(filter)
+        src_reg = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+        sec_reg = _mm_unpacklo_pi32(
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+        src_reg = _mm_avg_pu8(src_reg, sec_reg);
+        sec += sec_stride << 1;
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+    }
+    // x_offset = 4  and y_offset = 0
+  } else if (x_offset == 4) {
+    if (y_offset == 0) {
+      __m64 src_next_reg;
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        LOAD_DST_INSERT
+        /* average between current and next stride source */
+        src_reg = _mm_avg_pu8(src_reg, src_next_reg);
+        sec_reg = _mm_unpacklo_pi32(
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+        src_reg = _mm_avg_pu8(src_reg, sec_reg);
+        sec += sec_stride << 1;
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = 4  and y_offset = 4
+    } else if (y_offset == 4) {
+      __m64 src_next_reg, src_avg, src_temp;
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      src_avg = _mm_avg_pu8(src_reg, src_next_reg);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        src_next_reg = _mm_avg_pu8(src_reg, src_next_reg);
+        src_temp = _mm_alignr_pi8(src_next_reg, src_avg, 4);
+        src_temp = _mm_avg_pu8(src_avg, src_temp);
+        sec_reg = _mm_unpacklo_pi32(
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+        src_temp = _mm_avg_pu8(src_temp, sec_reg);
+        sec += sec_stride << 1;
+        LOAD_DST_INSERT
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_temp, zero_reg)
+        // save current source average
+        src_avg = src_next_reg;
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // last 2 rows processing happens here
+      __m64 src_reg_0 = _mm_cvtsi32_si64(*(uint32_t const *)(src));
+      __m64 src_reg_1 = _mm_cvtsi32_si64(*(uint32_t const *)(src + 1));
+      src_reg_0 = _mm_avg_pu8(src_reg_0, src_reg_1);
+      src_next_reg = _mm_alignr_pi8(src_reg_0, src_avg, 4);
+      LOAD_DST_INSERT
+      src_avg = _mm_avg_pu8(src_avg, src_next_reg);
+      sec_reg = _mm_unpacklo_pi32(
+          _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+          _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+      src_avg = _mm_avg_pu8(src_avg, sec_reg);
+      MERGE_WITH_SRC(src_avg, zero_reg)
+      CALC_SUM_SSE_INSIDE_LOOP
+    } else {
+      // x_offset = 4  and y_offset = bilin interpolation
+      __m64 filter, pw8, src_next_reg, src_avg, src_temp;
+      y_offset <<= 4;
+      filter = *(__m64 const *)(bilinear_filters_sse2 + y_offset);
+      pw8 = _mm_set1_pi16(8);
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      src_avg = _mm_avg_pu8(src_reg, src_next_reg);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        src_next_reg = _mm_avg_pu8(src_reg, src_next_reg);
+        src_temp = _mm_alignr_pi8(src_next_reg, src_avg, 4);
+        LOAD_DST_INSERT
+        MERGE_WITH_SRC(src_avg, src_temp)
+        FILTER_SRC(filter)
+        src_avg = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+        sec_reg = _mm_unpacklo_pi32(
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+        src_avg = _mm_avg_pu8(src_avg, sec_reg);
+        sec += sec_stride << 1;
+        // expand each byte to 2 bytes
+        MERGE_WITH_SRC(src_avg, zero_reg)
+        // save current source average
+        src_avg = src_next_reg;
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // last 2 rows processing happens here
+      __m64 src_reg_0 = _mm_cvtsi32_si64(*(uint32_t const *)(src));
+      __m64 src_reg_1 = _mm_cvtsi32_si64(*(uint32_t const *)(src + 1));
+      src_reg_0 = _mm_avg_pu8(src_reg_0, src_reg_1);
+      src_next_reg = _mm_alignr_pi8(src_reg_0, src_avg, 4);
+      LOAD_DST_INSERT
+      MERGE_WITH_SRC(src_avg, src_next_reg)
+      FILTER_SRC(filter)
+      src_avg = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+      sec_reg = _mm_unpacklo_pi32(
+          _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+          _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+      src_avg = _mm_avg_pu8(src_avg, sec_reg);
+      MERGE_WITH_SRC(src_avg, zero_reg)
+      CALC_SUM_SSE_INSIDE_LOOP
+    }
+    // x_offset = bilin interpolation and y_offset = 0
+  } else {
+    if (y_offset == 0) {
+      __m64 filter, pw8, src_next_reg;
+      x_offset <<= 4;
+      filter = *(__m64 const *)(bilinear_filters_sse2 + x_offset);
+      pw8 = _mm_set1_pi16(8);
+      for (i = 0; i < height; i += 2) {
+        LOAD_SRC_DST_INSERT(src_stride, dst_stride)
+        MERGE_NEXT_SRC_INSERT(src_reg, 1)
+        FILTER_SRC(filter)
+        src_reg = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+        sec_reg = _mm_unpacklo_pi32(
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+        src_reg = _mm_avg_pu8(src_reg, sec_reg);
+        sec += sec_stride << 1;
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // x_offset = bilin interpolation and y_offset = 4
+    } else if (y_offset == 4) {
+      __m64 filter, pw8, src_next_reg, src_pack;
+      x_offset <<= 4;
+      filter = *(__m64 const *)(bilinear_filters_sse2 + x_offset);
+      pw8 = _mm_set1_pi16(8);
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      MERGE_WITH_SRC(src_reg, src_next_reg)
+      FILTER_SRC(filter)
+      // convert each 16 bit to 8 bit to each low and high lane source
+      src_pack = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        LOAD_DST_INSERT
+        MERGE_WITH_SRC(src_reg, src_next_reg)
+        FILTER_SRC(filter)
+        src_reg = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+        src_next_reg = _mm_alignr_pi8(src_reg, src_pack, 4);
+        // average between previous pack to the current
+        src_pack = _mm_avg_pu8(src_pack, src_next_reg);
+        sec_reg = _mm_unpacklo_pi32(
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+        src_pack = _mm_avg_pu8(src_pack, sec_reg);
+        sec += sec_stride << 1;
+        MERGE_WITH_SRC(src_pack, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src_pack = src_reg;
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // last 2 rows processing happens here
+      LOAD_SRC_MERGE_HALF(filter)
+      LOAD_DST_INSERT
+      FILTER_SRC_HALF(filter)
+      src_reg_0 = _mm_packs_pu16(src_lo, src_lo);
+      src_next_reg = _mm_alignr_pi8(src_reg_0, src_pack, 4);
+      // average between previous pack to the current
+      src_pack = _mm_avg_pu8(src_pack, src_next_reg);
+      sec_reg = _mm_unpacklo_pi32(
+          _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+          _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+      src_pack = _mm_avg_pu8(src_pack, sec_reg);
+      MERGE_WITH_SRC(src_pack, zero_reg)
+      CALC_SUM_SSE_INSIDE_LOOP
+    } else {
+      // x_offset = bilin interpolation and y_offset = bilin interpolation
+      __m64 xfilter, yfilter, pw8, src_next_reg, src_pack;
+      x_offset <<= 4;
+      xfilter = *(__m64 const *)(bilinear_filters_sse2 + x_offset);
+      y_offset <<= 4;
+      yfilter = *(__m64 const *)(bilinear_filters_sse2 + y_offset);
+      pw8 = _mm_set1_pi16(8);
+      // load and insert source and next row source
+      LOAD_SRC_NEXT_BYTE_INSERT
+      MERGE_WITH_SRC(src_reg, src_next_reg)
+      FILTER_SRC(xfilter)
+      // convert each 16 bit to 8 bit to each low and high lane source
+      src_pack = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+      src += src_stride << 1;
+      for (i = 0; i < height - 2; i += 2) {
+        LOAD_SRC_NEXT_BYTE_INSERT
+        LOAD_DST_INSERT
+        MERGE_WITH_SRC(src_reg, src_next_reg)
+        FILTER_SRC(xfilter)
+        src_reg = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+        src_next_reg = _mm_alignr_pi8(src_reg, src_pack, 4);
+        // average between previous pack to the current
+        MERGE_WITH_SRC(src_pack, src_next_reg)
+        // filter the source
+        FILTER_SRC(yfilter)
+        src_pack = src_reg;
+        src_reg = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+        sec_reg = _mm_unpacklo_pi32(
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+            _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+        src_reg = _mm_avg_pu8(src_reg, sec_reg);
+        sec += sec_stride << 1;
+        MERGE_WITH_SRC(src_reg, zero_reg)
+        CALC_SUM_SSE_INSIDE_LOOP
+        src += src_stride << 1;
+        dst += dst_stride << 1;
+      }
+      // last 2 rows processing happens here
+      LOAD_SRC_MERGE_HALF(xfilter)
+      LOAD_DST_INSERT
+      FILTER_SRC_HALF(xfilter)
+      src_reg_0 = _mm_packs_pu16(src_lo, src_lo);
+      src_next_reg = _mm_alignr_pi8(src_reg_0, src_pack, 4);
+      MERGE_WITH_SRC(src_pack, src_next_reg)
+      FILTER_SRC(yfilter)
+      src_reg = _mm_packs_pu16(exp_src_lo, exp_src_hi);
+      sec_reg = _mm_unpacklo_pi32(
+          _mm_cvtsi32_si64(*(uint32_t const *)(sec)),
+          _mm_cvtsi32_si64(*(uint32_t const *)(sec + sec_stride)));
+      src_reg = _mm_avg_pu8(src_reg, sec_reg);
+      MERGE_WITH_SRC(src_reg, zero_reg)
+      CALC_SUM_SSE_INSIDE_LOOP
+    }
+  }
+  CALC_SUM_AND_SSE
+  return sum;
+}
+
diff --git a/vpx_dsp/e2k/subtract_e2k.c b/vpx_dsp/e2k/subtract_e2k.c
new file mode 100644
index 0000000..6751e11
--- /dev/null
+++ b/vpx_dsp/e2k/subtract_e2k.c
@@ -0,0 +1,111 @@
+/*
+ *  Copyright (c) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *  Copyright (c) 2016 The WebM project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "./vpx_config.h"
+#include "./vpx_dsp_rtcd.h"
+
+#define PROCESS16(i) \
+  v2 = _mm_load_si128((const __m128i *)(src + i)); \
+  v3 = _mm_load_si128((const __m128i *)(pred + i)); \
+  v0 = _mm_unpacklo_epi8(v2, vzero); \
+  v1 = _mm_unpackhi_epi8(v2, vzero); \
+  v0 = _mm_sub_epi16(v0, _mm_unpacklo_epi8(v3, vzero)); \
+  v1 = _mm_sub_epi16(v1, _mm_unpackhi_epi8(v3, vzero)); \
+  _mm_store_si128((__m128i *)(diff + i), v0); \
+  _mm_store_si128((__m128i *)(diff + i + 8), v1);
+
+void vpx_subtract_block_sse2(int rows, int cols, int16_t *diff,
+                             ptrdiff_t diff_stride, const uint8_t *src,
+                             ptrdiff_t src_stride, const uint8_t *pred,
+                             ptrdiff_t pred_stride) {
+  __m128i v0, v1, v2, v3, vzero = _mm_setzero_si128();
+  __m64 h0, h1, h2, h3, hzero;
+  int i;
+
+  switch (cols) {
+    case 128:
+      PRAGMA_E2K("ivdep")
+      for (i = 0; i < rows; i++) {
+        PROCESS16(0) PROCESS16(16) PROCESS16(32) PROCESS16(48)
+        PROCESS16(64) PROCESS16(80) PROCESS16(96) PROCESS16(112)
+        src += src_stride;
+        pred += pred_stride;
+        diff += diff_stride;
+      }
+      break;
+
+    case 64:
+      PRAGMA_E2K("ivdep")
+      for (i = 0; i < rows; i++) {
+        PROCESS16(0) PROCESS16(16) PROCESS16(32) PROCESS16(48)
+        src += src_stride;
+        pred += pred_stride;
+        diff += diff_stride;
+      }
+      break;
+
+    case 32:
+      PRAGMA_E2K("ivdep")
+      for (i = 0; i < rows; i++) {
+        PROCESS16(0) PROCESS16(16)
+        src += src_stride;
+        pred += pred_stride;
+        diff += diff_stride;
+      }
+      break;
+
+    case 16:
+      PRAGMA_E2K("ivdep")
+      for (i = 0; i < rows; i++) {
+        PROCESS16(0)
+        src += src_stride;
+        pred += pred_stride;
+        diff += diff_stride;
+      }
+      break;
+
+    case 8:
+      hzero = _mm_setzero_si64();
+      PRAGMA_E2K("ivdep")
+      for (i = 0; i < rows; i++) {
+        h2 = *(const __m64 *)src;
+        h3 = *(const __m64 *)pred;
+        h0 = _mm_unpacklo_pi8(h2, hzero);
+        h1 = _mm_unpackhi_pi8(h2, hzero);
+        h0 = _mm_sub_pi16(h0, _mm_unpacklo_pi8(h3, hzero));
+        h1 = _mm_sub_pi16(h1, _mm_unpackhi_pi8(h3, hzero));
+        *(__m64 *)diff = h0;
+        *(__m64 *)(diff + 4) = h1;
+        src += src_stride;
+        pred += pred_stride;
+        diff += diff_stride;
+      }
+      break;
+
+    case 4:
+      hzero = _mm_setzero_si64();
+      PRAGMA_E2K("ivdep")
+      for (i = 0; i < rows; i++) {
+        h2 = _mm_cvtsi32_si64(*(uint32_t const *)src);
+        h3 = _mm_cvtsi32_si64(*(uint32_t const *)pred);
+        h0 = _mm_unpacklo_pi8(h2, hzero);
+        h0 = _mm_sub_pi16(h0, _mm_unpacklo_pi8(h3, hzero));
+        *(__m64 *)diff = h0;
+        src += src_stride;
+        pred += pred_stride;
+        diff += diff_stride;
+      }
+      break;
+  }
+}
+
diff --git a/vpx_dsp/e2k/vpx_convolve_copy_e2k.c b/vpx_dsp/e2k/vpx_convolve_copy_e2k.c
new file mode 100644
index 0000000..2504781
--- /dev/null
+++ b/vpx_dsp/e2k/vpx_convolve_copy_e2k.c
@@ -0,0 +1,125 @@
+/*
+ *  Copyright (c) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *  Copyright (c) 2014 The WebM project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "./vpx_dsp_rtcd.h"
+#include "vpx/vpx_integer.h"
+
+#define EXTRA8 ) {
+#define EXTRA16 , int bd) { (void)bd;
+
+#define COPY(name, n, avg) \
+void vpx_##name##_sse2(const uint##n##_t *src, ptrdiff_t src_stride, \
+                       uint##n##_t *dst, ptrdiff_t dst_stride, \
+                       const InterpKernel *filter, int x0_q4, \
+                       int x_step_q4, int y0_q4, int y_step_q4, int w, int h EXTRA##n \
+  __m64 h0; __m128i v0, v1, v2, v3, v4, v5, v6, v7; (void)filter; \
+  (void)x0_q4; (void)x_step_q4; (void)y0_q4; (void)y_step_q4; \
+  if (n == 8 && w < 8 * 8 / n) {  /* copy4 */ \
+    PRAGMA_E2K("ivdep") PRAGMA_E2K("unroll(2)") \
+    do { \
+      uint32_t s0 = *(const uint32_t*)src; \
+      if (avg) { \
+        h0 = _mm_cvtsi32_si64(s0); \
+        h0 = _mm_avg_pu##n(h0, _mm_cvtsi32_si64(*(uint32_t*)dst)); \
+        s0 = _mm_cvtsi64_si32(h0); \
+      } \
+      *(uint32_t*)dst = s0; \
+      src += src_stride; dst += dst_stride; \
+    } while (--h); \
+  } else if (w == 8 * 8 / n) {  /* copy8 */ \
+    PRAGMA_E2K("ivdep") PRAGMA_E2K("unroll(2)") \
+    do { \
+      h0 = *(const __m64*)src; \
+      if (avg) h0 = _mm_avg_pu##n(h0, *(const __m64*)dst); \
+      *(__m64*)dst = h0; \
+      src += src_stride; dst += dst_stride; \
+    } while (--h); \
+  } else if (w < 32 * 8 / n) {  /* copy16 */ \
+    PRAGMA_E2K("ivdep") PRAGMA_E2K("unroll(2)") \
+    do { \
+      v0 = _mm_loadu_si128((const __m128i*)src); \
+      if (avg) v0 = _mm_avg_epu##n(v0, _mm_load_si128((__m128i*)dst)); \
+      _mm_store_si128((__m128i*)dst, v0); \
+      src += src_stride; dst += dst_stride; \
+    } while (--h); \
+  } else if (w == 32 * 8 / n) {  /* copy32 */ \
+    PRAGMA_E2K("ivdep") PRAGMA_E2K("unroll(2)") \
+    do { \
+      v0 = _mm_loadu_si128((const __m128i*)src); \
+      v1 = _mm_loadu_si128((const __m128i*)src + 1); \
+      if (avg) { \
+        v0 = _mm_avg_epu##n(v0, _mm_load_si128((__m128i*)dst)); \
+        v1 = _mm_avg_epu##n(v1, _mm_load_si128((__m128i*)dst + 1)); \
+      } \
+      _mm_store_si128((__m128i*)dst, v0); \
+      _mm_store_si128((__m128i*)dst + 1, v1); \
+      src += src_stride; dst += dst_stride; \
+    } while (--h); \
+  } else if (n == 8 || w == 64 * 8 / n) {  /* copy64 */ \
+    PRAGMA_E2K("ivdep") \
+    do { \
+      v0 = _mm_loadu_si128((const __m128i*)src); \
+      v1 = _mm_loadu_si128((const __m128i*)src + 1); \
+      v2 = _mm_loadu_si128((const __m128i*)src + 2); \
+      v3 = _mm_loadu_si128((const __m128i*)src + 3); \
+      if (avg) { \
+        v0 = _mm_avg_epu##n(v0, _mm_load_si128((__m128i*)dst)); \
+        v1 = _mm_avg_epu##n(v1, _mm_load_si128((__m128i*)dst + 1)); \
+        v2 = _mm_avg_epu##n(v2, _mm_load_si128((__m128i*)dst + 2)); \
+        v3 = _mm_avg_epu##n(v3, _mm_load_si128((__m128i*)dst + 3)); \
+      } \
+      _mm_store_si128((__m128i*)dst, v0); \
+      _mm_store_si128((__m128i*)dst + 1, v1); \
+      _mm_store_si128((__m128i*)dst + 2, v2); \
+      _mm_store_si128((__m128i*)dst + 3, v3); \
+      src += src_stride; dst += dst_stride; \
+    } while (--h); \
+  } else {  /* copy128 */ \
+    PRAGMA_E2K("ivdep") \
+    do { \
+      v0 = _mm_loadu_si128((const __m128i*)src); \
+      v1 = _mm_loadu_si128((const __m128i*)src + 1); \
+      v2 = _mm_loadu_si128((const __m128i*)src + 2); \
+      v3 = _mm_loadu_si128((const __m128i*)src + 3); \
+      v4 = _mm_loadu_si128((const __m128i*)src + 4); \
+      v5 = _mm_loadu_si128((const __m128i*)src + 5); \
+      v6 = _mm_loadu_si128((const __m128i*)src + 6); \
+      v7 = _mm_loadu_si128((const __m128i*)src + 7); \
+      if (avg) { \
+        v0 = _mm_avg_epu##n(v0, _mm_load_si128((__m128i*)dst)); \
+        v1 = _mm_avg_epu##n(v1, _mm_load_si128((__m128i*)dst + 1)); \
+        v2 = _mm_avg_epu##n(v2, _mm_load_si128((__m128i*)dst + 2)); \
+        v3 = _mm_avg_epu##n(v3, _mm_load_si128((__m128i*)dst + 3)); \
+        v4 = _mm_avg_epu##n(v4, _mm_load_si128((__m128i*)dst + 4)); \
+        v5 = _mm_avg_epu##n(v5, _mm_load_si128((__m128i*)dst + 5)); \
+        v6 = _mm_avg_epu##n(v6, _mm_load_si128((__m128i*)dst + 6)); \
+        v7 = _mm_avg_epu##n(v7, _mm_load_si128((__m128i*)dst + 7)); \
+      } \
+      _mm_store_si128((__m128i*)dst, v0); \
+      _mm_store_si128((__m128i*)dst + 1, v1); \
+      _mm_store_si128((__m128i*)dst + 2, v2); \
+      _mm_store_si128((__m128i*)dst + 3, v3); \
+      _mm_store_si128((__m128i*)dst + 4, v4); \
+      _mm_store_si128((__m128i*)dst + 5, v5); \
+      _mm_store_si128((__m128i*)dst + 6, v6); \
+      _mm_store_si128((__m128i*)dst + 7, v7); \
+      src += src_stride; dst += dst_stride; \
+    } while (--h); \
+  } \
+}
+
+COPY(convolve_copy, 8, 0)
+COPY(highbd_convolve_copy, 16, 0)
+COPY(convolve_avg, 8, 1)
+COPY(highbd_convolve_avg, 16, 1)
+
diff --git a/vpx_dsp/e2k/vpx_high_subpixel_8t_e2k.c b/vpx_dsp/e2k/vpx_high_subpixel_8t_e2k.c
new file mode 100644
index 0000000..b0ad2f2
--- /dev/null
+++ b/vpx_dsp/e2k/vpx_high_subpixel_8t_e2k.c
@@ -0,0 +1,390 @@
+/*
+ *  Copyright (c) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *  Copyright (c) 2016 The WebM project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "./vpx_config.h"
+#include "./vpx_dsp_rtcd.h"
+
+DECLARE_ALIGNED(16, static const uint8_t, filt_high_sse2[]) = {
+   0,  1,   2,  3,   2,  3,   4,  5,   4,  5,   6,  7,   6,  7,   8,  9,
+   4,  5,   6,  7,   6,  7,   8,  9,   8,  9,  10, 11,  10, 11,  12, 13,
+   2,  3,   4,  5,   4,  5,   6,  7,   6,  7,   8,  9,   8,  9,  10, 11, /* -6 */
+   6,  7,   8,  9,   8,  9,  10, 11,  10, 11,  12, 13,  12, 13,  14, 15,
+};
+
+#define INIT_H8 \
+  i = 0x10001; \
+  vmax = _mm_set1_epi32((i << bd) - i); \
+  round = _mm_set1_epi32(64); \
+  v0 = _mm_loadu_si128((const __m128i *)filter); \
+  firstFilters = _mm_shuffle_epi32(v0, 0); \
+  secondFilters = _mm_shuffle_epi32(v0, 0x55); \
+  thirdFilters = _mm_shuffle_epi32(v0, 0xaa); \
+  forthFilters = _mm_shuffle_epi32(v0, 0xff); \
+  filt1Reg = _mm_load_si128((__m128i const *)filt_high_sse2); \
+  filt2Reg = _mm_load_si128((__m128i const *)(filt_high_sse2 + 16)); \
+  filt3Reg = _mm_load_si128((__m128i const *)(filt_high_sse2 + 16 * 2)); \
+  filt4Reg = _mm_load_si128((__m128i const *)(filt_high_sse2 + 16 * 3));
+
+#define FILTER_H8(src, v0) \
+  v1 = _mm_loadu_si128((const __m128i *)(src - 3)); \
+  v3 = _mm_loadu_si128((const __m128i *)(src)); \
+  v0 = _mm_shuffle_epi8(v1, filt1Reg); \
+  v1 = _mm_shuffle_epi8(v1, filt2Reg); \
+  v0 = _mm_madd_epi16(v0, firstFilters); \
+  v1 = _mm_madd_epi16(v1, secondFilters); \
+  v0 = _mm_add_epi32(v0, v1); \
+  v2 = _mm_shuffle_epi8(v3, filt3Reg); \
+  v3 = _mm_shuffle_epi8(v3, filt4Reg); \
+  v2 = _mm_madd_epi16(v2, thirdFilters); \
+  v3 = _mm_madd_epi16(v3, forthFilters); \
+  v0 = _mm_add_epi32(v0, _mm_add_epi32(v2, v3)); \
+  v0 = _mm_add_epi32(v0, round); \
+  v0 = _mm_srai_epi32(v0, 7);
+
+#define INIT_X2 \
+  i = 0x10001; \
+  vmax = _mm_set1_epi32((i << bd) - i); \
+  round = _mm_set1_epi32(64); \
+  v0 = _mm_cvtsi32_si128(*(const uint32_t *)(filter + 3)); \
+  firstFilters = _mm_shuffle_epi32(v0, 0);
+
+#define FILTER_H2(src, v0) \
+  v2 = _mm_loadu_si128((const __m128i *)(src)); \
+  v3 = _mm_loadu_si128((const __m128i *)(src + 1)); \
+  v0 = _mm_unpacklo_epi16(v2, v3); \
+  v1 = _mm_unpackhi_epi16(v2, v3); \
+  v0 = _mm_madd_epi16(v0, firstFilters); \
+  v1 = _mm_madd_epi16(v1, firstFilters); \
+  v0 = _mm_add_epi32(v0, round); \
+  v1 = _mm_add_epi32(v1, round); \
+  v0 = _mm_srai_epi32(v0, 7); \
+  v1 = _mm_srai_epi32(v1, 7); \
+  v0 = _mm_packus_epi32(v0, v1); \
+  v0 = _mm_min_epu16(v0, vmax);
+
+#define INIT_V8(LOAD) \
+  i = 0x10001; \
+  vmax = _mm_set1_epi32((i << bd) - i); \
+  round = _mm_set1_epi32(64); \
+  v0 = _mm_loadu_si128((const __m128i *)filter); \
+  firstFilters = _mm_shuffle_epi32(v0, 0); \
+  secondFilters = _mm_shuffle_epi32(v0, 0x55); \
+  thirdFilters = _mm_shuffle_epi32(v0, 0xaa); \
+  forthFilters = _mm_shuffle_epi32(v0, 0xff); \
+  i0 = LOAD((const __m128i *)src_ptr); \
+  i1 = LOAD((const __m128i *)(src_ptr + src_pixels_per_line)); \
+  i2 = LOAD((const __m128i *)(src_ptr + src_pixels_per_line * 2)); \
+  i3 = LOAD((const __m128i *)(src_ptr + src_pixels_per_line * 3)); \
+  i4 = LOAD((const __m128i *)(src_ptr + src_pixels_per_line * 4)); \
+  i5 = LOAD((const __m128i *)(src_ptr + src_pixels_per_line * 5)); \
+  i6 = LOAD((const __m128i *)(src_ptr + src_pixels_per_line * 6)); \
+  src_ptr += src_pixels_per_line * 7;
+
+#define FILTER_V8(lo, v0, i) \
+  v0 = _mm_unpack##lo##_epi16(i##0, i##1); \
+  v1 = _mm_unpack##lo##_epi16(i##6, i##7); \
+  v0 = _mm_madd_epi16(v0, firstFilters); \
+  v1 = _mm_madd_epi16(v1, forthFilters); \
+  v0 = _mm_add_epi32(v0, v1); \
+  v2 = _mm_unpack##lo##_epi16(i##2, i##3); \
+  v3 = _mm_unpack##lo##_epi16(i##4, i##5); \
+  v2 = _mm_madd_epi16(v2, secondFilters); \
+  v3 = _mm_madd_epi16(v3, thirdFilters); \
+  v0 = _mm_add_epi32(v0, _mm_add_epi32(v2, v3)); \
+  v0 = _mm_add_epi32(v0, round); \
+  v0 = _mm_srai_epi32(v0, 7);
+
+#define FILTER_V2(src, v2, v3, v0) \
+  v3 = _mm_loadu_si128((const __m128i *)(src)); \
+  v0 = _mm_unpacklo_epi16(v2, v3); \
+  v2 = _mm_unpackhi_epi16(v2, v3); \
+  v0 = _mm_madd_epi16(v0, firstFilters); \
+  v2 = _mm_madd_epi16(v2, firstFilters); \
+  v0 = _mm_add_epi32(v0, round); \
+  v2 = _mm_add_epi32(v2, round); \
+  v0 = _mm_srai_epi32(v0, 7); \
+  v2 = _mm_srai_epi32(v2, 7); \
+  v0 = _mm_packus_epi32(v0, v2); \
+  v0 = _mm_min_epu16(v0, vmax); \
+  v2 = v3;
+
+#define STORE64 \
+  _mm_storel_epi64((__m128i*)output_ptr, v0);
+#define AVG_STORE64 \
+  v0 = _mm_avg_epu16(v0, _mm_loadl_epi64((__m128i*)output_ptr)); \
+  STORE64
+
+#define STORE128 \
+  _mm_store_si128((__m128i*)output_ptr, v0);
+#define AVG_STORE128 \
+  v0 = _mm_avg_epu16(v0, _mm_load_si128((__m128i*)output_ptr)); \
+  STORE128
+
+#define STORE128X2(v0, v1) \
+  _mm_store_si128((__m128i*)output_ptr, v0); \
+  _mm_store_si128((__m128i*)output_ptr + 1, v1);
+#define AVG_STORE128X2(v0, v1) \
+  v0 = _mm_avg_epu16(v0, _mm_load_si128((__m128i*)output_ptr)); \
+  v1 = _mm_avg_epu16(v1, _mm_load_si128((__m128i*)output_ptr + 1)); \
+  STORE128X2(v0, v1)
+
+#define FUNC(name, end) \
+void vpx_highbd_filter_##name##_##end( \
+    const uint16_t *src_ptr, ptrdiff_t src_pixels_per_line, uint16_t *output_ptr, \
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter, int bd)
+
+#define FILTER(end, STORE) \
+FUNC(block1d4_h8, end) { \
+  __m128i round, vmax, filt1Reg, filt2Reg, filt3Reg, filt4Reg, v0, v1, v2, v3; \
+  __m128i firstFilters, secondFilters, thirdFilters, forthFilters; \
+  unsigned int i; \
+  INIT_H8 \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < output_height; i++) { \
+    FILTER_H8(src_ptr, v0) \
+    v0 = _mm_packus_epi32(v0, v0); \
+    v0 = _mm_min_epu16(v0, vmax); \
+    STORE##64 \
+    src_ptr += src_pixels_per_line; \
+    output_ptr += output_pitch; \
+  } \
+} \
+\
+FUNC(block1d8_h8, end) { \
+  __m128i round, vmax, filt1Reg, filt2Reg, filt3Reg, filt4Reg, v0, v1, v2, v3, v4; \
+  __m128i firstFilters, secondFilters, thirdFilters, forthFilters; \
+  unsigned int i; \
+  INIT_H8 \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < output_height; i++) { \
+    FILTER_H8(src_ptr, v4) \
+    FILTER_H8(src_ptr + 4, v0) \
+    v0 = _mm_packus_epi32(v4, v0); \
+    v0 = _mm_min_epu16(v0, vmax); \
+    STORE##128 \
+    src_ptr += src_pixels_per_line; \
+    output_ptr += output_pitch; \
+  } \
+} \
+\
+FUNC(block1d16_h8, end) { \
+  __m128i round, vmax, filt1Reg, filt2Reg, filt3Reg, filt4Reg, v0, v1, v2, v3, v4, v5; \
+  __m128i firstFilters, secondFilters, thirdFilters, forthFilters; \
+  unsigned int i; \
+  INIT_H8 \
+  PRAGMA_E2K_NOAVG("ivdep") \
+  for (i = 0; i < output_height; i++) { \
+    FILTER_H8(src_ptr, v4) \
+    FILTER_H8(src_ptr + 4, v0) \
+    v4 = _mm_packus_epi32(v4, v0); \
+    v4 = _mm_min_epu16(v4, vmax); \
+    FILTER_H8(src_ptr + 8, v5) \
+    FILTER_H8(src_ptr + 12, v0) \
+    v0 = _mm_packus_epi32(v5, v0); \
+    v0 = _mm_min_epu16(v0, vmax); \
+    STORE##128X2(v4, v0) \
+    src_ptr += src_pixels_per_line; \
+    output_ptr += output_pitch; \
+  } \
+} \
+\
+FUNC(block1d4_h2, end) { \
+  __m128i round, vmax, firstFilters, v0, v1; \
+  unsigned int i; \
+  INIT_X2 \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < output_height; i++) { \
+    v0 = _mm_loadl_epi64((const __m128i *)src_ptr); \
+    v1 = _mm_loadl_epi64((const __m128i *)(src_ptr + 1)); \
+    v0 = _mm_unpacklo_epi16(v0, v1); \
+    v0 = _mm_madd_epi16(v0, firstFilters); \
+    v0 = _mm_add_epi32(v0, round); \
+    v0 = _mm_srai_epi32(v0, 7); \
+    v0 = _mm_packus_epi32(v0, v0); \
+    v0 = _mm_min_epu16(v0, vmax); \
+    STORE##64 \
+    src_ptr += src_pixels_per_line; \
+    output_ptr += output_pitch; \
+  } \
+} \
+\
+FUNC(block1d8_h2, end) { \
+  __m128i round, vmax, firstFilters, v0, v1, v2, v3; \
+  unsigned int i; \
+  INIT_X2 \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < output_height; i++) { \
+    FILTER_H2(src_ptr, v0) \
+    STORE##128 \
+    src_ptr += src_pixels_per_line; \
+    output_ptr += output_pitch; \
+  } \
+} \
+\
+FUNC(block1d16_h2, end) { \
+  __m128i round, vmax, firstFilters, v0, v1, v2, v3, v4; \
+  unsigned int i; \
+  INIT_X2 \
+  PRAGMA_E2K_NOAVG("ivdep") \
+  for (i = 0; i < output_height; i++) { \
+    FILTER_H2(src_ptr, v4) \
+    FILTER_H2(src_ptr + 8, v0) \
+    STORE##128X2(v4, v0) \
+    src_ptr += src_pixels_per_line; \
+    output_ptr += output_pitch; \
+  } \
+} \
+\
+FUNC(block1d4_v8, end) { \
+  __m128i round, vmax, i0, i1, i2, i3, i4, i5, i6, i7, v0, v1, v2, v3; \
+  __m128i firstFilters, secondFilters, thirdFilters, forthFilters; \
+  unsigned int i; \
+  INIT_V8(_mm_loadl_epi64) \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < output_height; i++) { \
+    i7 = _mm_loadl_epi64((const __m128i *)src_ptr); \
+    src_ptr += src_pixels_per_line; \
+    FILTER_V8(lo, v0, i) \
+    v0 = _mm_packus_epi32(v0, v0); \
+    v0 = _mm_min_epu16(v0, vmax); \
+    STORE##64 \
+    output_ptr += output_pitch; \
+    i0 = i1; i1 = i2; i2 = i3; i3 = i4; i4 = i5; i5 = i6; i6 = i7; \
+  } \
+} \
+\
+FUNC(block1d8_v8, end) { \
+  __m128i round, vmax, i0, i1, i2, i3, i4, i5, i6, i7, v0, v1, v2, v3, v4; \
+  __m128i firstFilters, secondFilters, thirdFilters, forthFilters; \
+  unsigned int i; \
+  INIT_V8(_mm_loadu_si128) \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < output_height; i++) { \
+    i7 = _mm_loadu_si128((const __m128i *)src_ptr); \
+    src_ptr += src_pixels_per_line; \
+    FILTER_V8(lo, v4, i) \
+    FILTER_V8(hi, v0, i) \
+    v0 = _mm_packus_epi32(v4, v0); \
+    v0 = _mm_min_epu16(v0, vmax); \
+    STORE##128 \
+    output_ptr += output_pitch; \
+    i0 = i1; i1 = i2; i2 = i3; i3 = i4; i4 = i5; i5 = i6; i6 = i7; \
+  } \
+} \
+\
+FUNC(block1d16_v8, end) { \
+  __m128i round, vmax, i0, i1, i2, i3, i4, i5, i6, i7, v0, v1, v2, v3, v4, v5; \
+  __m128i j0, j1, j2, j3, j4, j5, j6, j7; \
+  __m128i firstFilters, secondFilters, thirdFilters, forthFilters; \
+  unsigned int i; \
+  i = 0x10001; \
+  vmax = _mm_set1_epi32((i << bd) - i); \
+  round = _mm_set1_epi32(64); \
+  v0 = _mm_loadu_si128((const __m128i *)filter); \
+  firstFilters = _mm_shuffle_epi32(v0, 0); \
+  secondFilters = _mm_shuffle_epi32(v0, 0x55); \
+  thirdFilters = _mm_shuffle_epi32(v0, 0xaa); \
+  forthFilters = _mm_shuffle_epi32(v0, 0xff); \
+  i0 = _mm_loadu_si128((const __m128i *)src_ptr); \
+  j0 = _mm_loadu_si128((const __m128i *)(src_ptr + 8)); \
+  i1 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pixels_per_line)); \
+  j1 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pixels_per_line + 8)); \
+  i2 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pixels_per_line * 2)); \
+  j2 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pixels_per_line * 2 + 8)); \
+  i3 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pixels_per_line * 3)); \
+  j3 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pixels_per_line * 3 + 8)); \
+  i4 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pixels_per_line * 4)); \
+  j4 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pixels_per_line * 4 + 8)); \
+  i5 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pixels_per_line * 5)); \
+  j5 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pixels_per_line * 5 + 8)); \
+  i6 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pixels_per_line * 6)); \
+  j6 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pixels_per_line * 6 + 8)); \
+  src_ptr += src_pixels_per_line * 7; \
+  PRAGMA_E2K_NOAVG("ivdep") \
+  for (i = 0; i < output_height; i++) { \
+    i7 = _mm_loadu_si128((const __m128i *)src_ptr); \
+    j7 = _mm_loadu_si128((const __m128i *)(src_ptr + 8)); \
+    src_ptr += src_pixels_per_line; \
+    FILTER_V8(lo, v4, i) \
+    FILTER_V8(hi, v0, i) \
+    v4 = _mm_packus_epi32(v4, v0); \
+    v4 = _mm_min_epu16(v4, vmax); \
+    FILTER_V8(lo, v5, j) \
+    FILTER_V8(hi, v0, j) \
+    v0 = _mm_packus_epi32(v5, v0); \
+    v0 = _mm_min_epu16(v0, vmax); \
+    STORE##128X2(v4, v0) \
+    output_ptr += output_pitch; \
+    i0 = i1; i1 = i2; i2 = i3; i3 = i4; i4 = i5; i5 = i6; i6 = i7; \
+    j0 = j1; j1 = j2; j2 = j3; j3 = j4; j4 = j5; j5 = j6; j6 = j7; \
+  } \
+} \
+\
+FUNC(block1d4_v2, end) { \
+  __m128i round, vmax, firstFilters, v0, v2, v3; \
+  unsigned int i; \
+  INIT_X2 \
+  v2 = _mm_loadl_epi64((const __m128i *)src_ptr); \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < output_height; i++) { \
+    src_ptr += src_pixels_per_line; \
+    v3 = _mm_loadl_epi64((const __m128i *)src_ptr); \
+    v0 = _mm_unpacklo_epi16(v2, v3); \
+    v0 = _mm_madd_epi16(v0, firstFilters); \
+    v0 = _mm_add_epi32(v0, round); \
+    v0 = _mm_srai_epi32(v0, 7); \
+    v0 = _mm_packus_epi32(v0, v0); \
+    v0 = _mm_min_epu16(v0, vmax); \
+    STORE##64 \
+    output_ptr += output_pitch; \
+    v2 = v3; \
+  } \
+} \
+\
+FUNC(block1d8_v2, end) { \
+  __m128i round, vmax, firstFilters, v0, v2, v3; \
+  unsigned int i; \
+  INIT_X2 \
+  v2 = _mm_loadu_si128((const __m128i *)src_ptr); \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < output_height; i++) { \
+    src_ptr += src_pixels_per_line; \
+    FILTER_V2(src_ptr, v2, v3, v0) \
+    STORE##128 \
+    output_ptr += output_pitch; \
+  } \
+} \
+\
+FUNC(block1d16_v2, end) { \
+  __m128i round, vmax, firstFilters, v0, v1, v2, v3, v4, v5; \
+  unsigned int i; \
+  INIT_X2 \
+  v2 = _mm_loadu_si128((const __m128i *)src_ptr); \
+  v4 = _mm_loadu_si128((const __m128i *)(src_ptr + 8)); \
+  PRAGMA_E2K_NOAVG("ivdep") \
+  for (i = 0; i < output_height; i++) { \
+    src_ptr += src_pixels_per_line; \
+    FILTER_V2(src_ptr, v2, v3, v0) \
+    FILTER_V2(src_ptr + 8, v4, v5, v1) \
+    STORE##128X2(v0, v1) \
+    output_ptr += output_pitch; \
+  } \
+}
+
+#define PRAGMA_E2K_NOAVG PRAGMA_E2K
+FILTER(sse2, STORE)
+// wrong results SSE2/ConvolveTest.MatchesReferenceSubpixelFilter*
+#undef PRAGMA_E2K_NOAVG
+#define PRAGMA_E2K_NOAVG(x)
+FILTER(avg_sse2, AVG_STORE)
+
diff --git a/vpx_dsp/e2k/vpx_subpixel_8t_e2k.c b/vpx_dsp/e2k/vpx_subpixel_8t_e2k.c
new file mode 100644
index 0000000..a05d36a
--- /dev/null
+++ b/vpx_dsp/e2k/vpx_subpixel_8t_e2k.c
@@ -0,0 +1,404 @@
+/*
+ *  Copyright (c) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *  Copyright (c) 2016 The WebM project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#include <smmintrin.h>  // SSE4.1
+
+#include "./vpx_config.h"
+#include "./vpx_dsp_rtcd.h"
+
+DECLARE_ALIGNED(16, static const uint8_t, filt_sse2[]) = {
+  0,  1,  1,  2,  2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  7,   8,
+  2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  7,  8,  8,  9,  9,  10,
+  4,  5,  5,  6,  6,  7,  7,  8,  8,  9,  9,  10, 10, 11, 11, 12,
+  6,  7,  7,  8,  8,  9,  9,  10, 10, 11, 11, 12, 12, 13, 13, 14
+};
+
+DECLARE_ALIGNED(16, static const uint8_t, filt_d4_sse2[]) = {
+  0, 1, 2, 3,  1, 2, 3, 4,  2, 3, 4, 5,  3, 4, 5, 6,
+  4, 5, 6, 7,  5, 6, 7, 8,  6, 7, 8, 9,  7, 8, 9, 10
+};
+
+#define INIT_H8 \
+  round = _mm_set1_epi16(64); \
+  v0 = _mm_loadu_si128((const __m128i *)filter); \
+  v0 = _mm_packs_epi16(v0, v0); \
+  firstFilters = _mm_shuffle_epi8(v0, _mm_set1_epi16(0x100)); \
+  secondFilters = _mm_shuffle_epi8(v0, _mm_set1_epi16(0x302)); \
+  thirdFilters = _mm_shuffle_epi8(v0, _mm_set1_epi16(0x504)); \
+  forthFilters = _mm_shuffle_epi8(v0, _mm_set1_epi16(0x706)); \
+  filt1Reg = _mm_load_si128((__m128i const *)filt_sse2); \
+  filt2Reg = _mm_load_si128((__m128i const *)(filt_sse2 + 16)); \
+  filt3Reg = _mm_load_si128((__m128i const *)(filt_sse2 + 16 * 2)); \
+  filt4Reg = _mm_load_si128((__m128i const *)(filt_sse2 + 16 * 3)); \
+  src_ptr -= 3;
+
+#define FILTER_H8(src, v0) \
+  v3 = _mm_loadu_si128((const __m128i *)(src)); \
+  v0 = _mm_shuffle_epi8(v3, filt1Reg); \
+  v1 = _mm_shuffle_epi8(v3, filt3Reg); \
+  v0 = _mm_maddubs_epi16(v0, firstFilters); \
+  v1 = _mm_maddubs_epi16(v1, thirdFilters); \
+  v0 = _mm_add_epi16(v0, v1); \
+  v2 = _mm_shuffle_epi8(v3, filt2Reg); \
+  v3 = _mm_shuffle_epi8(v3, filt4Reg); \
+  v2 = _mm_maddubs_epi16(v2, secondFilters); \
+  v3 = _mm_maddubs_epi16(v3, forthFilters); \
+  v0 = _mm_add_epi16(v0, round); \
+  v0 = _mm_adds_epi16(v0, _mm_add_epi16(v2, v3)); \
+  v0 = _mm_srai_epi16(v0, 7);
+
+#define INIT_V8(LOAD) \
+  round = _mm_set1_epi16(64); \
+  v0 = _mm_loadu_si128((const __m128i *)filter); \
+  v0 = _mm_packs_epi16(v0, v0); \
+  firstFilters = _mm_shuffle_epi8(v0, _mm_set1_epi16(0x100)); \
+  secondFilters = _mm_shuffle_epi8(v0, _mm_set1_epi16(0x302)); \
+  thirdFilters = _mm_shuffle_epi8(v0, _mm_set1_epi16(0x504)); \
+  forthFilters = _mm_shuffle_epi8(v0, _mm_set1_epi16(0x706)); \
+  i0 = LOAD((const __m128i *)src_ptr); \
+  i1 = LOAD((const __m128i *)(src_ptr + src_pixels_per_line)); \
+  i2 = LOAD((const __m128i *)(src_ptr + src_pixels_per_line * 2)); \
+  i3 = LOAD((const __m128i *)(src_ptr + src_pixels_per_line * 3)); \
+  i4 = LOAD((const __m128i *)(src_ptr + src_pixels_per_line * 4)); \
+  i5 = LOAD((const __m128i *)(src_ptr + src_pixels_per_line * 5)); \
+  i6 = LOAD((const __m128i *)(src_ptr + src_pixels_per_line * 6)); \
+  src_ptr += src_pixels_per_line * 7;
+
+#define FILTER_V8(lo, v0) \
+  v0 = _mm_unpack##lo##_epi8(i0, i1); \
+  v1 = _mm_unpack##lo##_epi8(i4, i5); \
+  v0 = _mm_maddubs_epi16(v0, firstFilters); \
+  v1 = _mm_maddubs_epi16(v1, thirdFilters); \
+  v0 = _mm_add_epi16(v0, v1); \
+  v2 = _mm_unpack##lo##_epi8(i2, i3); \
+  v3 = _mm_unpack##lo##_epi8(i6, i7); \
+  v2 = _mm_maddubs_epi16(v2, secondFilters); \
+  v3 = _mm_maddubs_epi16(v3, forthFilters); \
+  v0 = _mm_add_epi16(v0, round); \
+  v0 = _mm_adds_epi16(v0, _mm_add_epi16(v2, v3)); \
+  v0 = _mm_srai_epi16(v0, 7);
+
+#define STORE32w \
+  *(uint32_t *)output_ptr = _mm_cvtsi128_si32(v0);
+#define AVG_STORE32w \
+  v0 = _mm_avg_epu8(v0, _mm_cvtsi32_si128(*(uint32_t *)output_ptr)); \
+  STORE32w
+
+#define STORE32 \
+  *(uint32_t *)output_ptr = _mm_cvtsi64_si32(v0);
+#define AVG_STORE32 \
+  v0 = _mm_avg_pu8(v0, _mm_cvtsi32_si64(*(uint32_t *)output_ptr)); \
+  STORE32
+
+#define STORE64 \
+  _mm_storel_epi64((__m128i *)output_ptr, v0);
+#define AVG_STORE64 \
+  v0 = _mm_avg_epu8(v0, _mm_loadl_epi64((const __m128i *)output_ptr)); \
+  STORE64
+
+#define STORE128 \
+  _mm_store_si128((__m128i *)output_ptr, v0);
+#define AVG_STORE128 \
+  v0 = _mm_avg_epu8(v0, _mm_load_si128((__m128i *)output_ptr)); \
+  STORE128
+
+#define FUNC(name, end) \
+void vpx_filter_##name##_##end( \
+    const uint8_t *src_ptr, ptrdiff_t src_pixels_per_line, uint8_t *output_ptr, \
+    ptrdiff_t output_pitch, uint32_t output_height, const int16_t *filter)
+
+#define FILTER(end, STORE) \
+FUNC(block1d4_h8, end) { \
+  __m128i round, filt1Reg, filt2Reg, v0, v1; \
+  __m128i firstFilters, secondFilters; \
+  unsigned int i; \
+  round = _mm_set1_epi16(64); \
+  v0 = _mm_loadu_si128((const __m128i *)filter); \
+  v0 = _mm_packs_epi16(v0, v0); \
+  firstFilters = _mm_shuffle_epi32(v0, 0); \
+  secondFilters = _mm_shuffle_epi32(v0, 0x55); \
+  filt1Reg = _mm_load_si128((__m128i const *)filt_d4_sse2); \
+  filt2Reg = _mm_load_si128((__m128i const *)(filt_d4_sse2 + 16)); \
+  src_ptr -= 3; \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < output_height; i++) { \
+    v1 = _mm_loadu_si128((const __m128i *)(src_ptr)); \
+    v0 = _mm_shuffle_epi8(v1, filt1Reg); \
+    v1 = _mm_shuffle_epi8(v1, filt2Reg); \
+    v0 = _mm_maddubs_epi16(v0, firstFilters); \
+    v1 = _mm_maddubs_epi16(v1, secondFilters); \
+    v0 = _mm_add_epi16(v0, v1); \
+    v0 = _mm_hadds_epi16(v0, v0); \
+    v0 = _mm_adds_epi16(v0, round); \
+    v0 = _mm_srai_epi16(v0, 7); \
+    v0 = _mm_packus_epi16(v0, v0); \
+    STORE##32w \
+    src_ptr += src_pixels_per_line; \
+    output_ptr += output_pitch; \
+  } \
+} \
+\
+FUNC(block1d8_h8, end) { \
+  __m128i round, filt1Reg, filt2Reg, filt3Reg, filt4Reg, v0, v1, v2, v3; \
+  __m128i firstFilters, secondFilters, thirdFilters, forthFilters; \
+  unsigned int i; \
+  INIT_H8 \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < output_height; i++) { \
+    FILTER_H8(src_ptr, v0) \
+    v0 = _mm_packus_epi16(v0, v0); \
+    STORE##64 \
+    src_ptr += src_pixels_per_line; \
+    output_ptr += output_pitch; \
+  } \
+} \
+\
+FUNC(block1d16_h8, end) { \
+  __m128i round, filt1Reg, filt2Reg, filt3Reg, filt4Reg, v0, v1, v2, v3, v4; \
+  __m128i firstFilters, secondFilters, thirdFilters, forthFilters; \
+  unsigned int i; \
+  INIT_H8 \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < output_height; i++) { \
+    FILTER_H8(src_ptr, v4) \
+    FILTER_H8(src_ptr + 8, v0) \
+    v0 = _mm_packus_epi16(v4, v0); \
+    STORE##128 \
+    src_ptr += src_pixels_per_line; \
+    output_ptr += output_pitch; \
+  } \
+} \
+\
+FUNC(block1d4_h2, end) { \
+  __m64 round, filt1Reg, firstFilters, v0; \
+  unsigned int i; \
+  round = _mm_set1_pi16(64); \
+  v0 = _mm_cvtsi32_si64(*(const uint32_t *)(filter + 3)); \
+  v0 = _mm_packs_pi16(v0, v0); \
+  firstFilters = _mm_shuffle_pi16(v0, 0); \
+  filt1Reg = _mm_setr_pi8(3, 4, 4, 5, 5, 6, 6, 7); \
+  src_ptr -= 3; \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < output_height; i++) { \
+    v0 = *(const __m64 *)src_ptr; \
+    v0 = _mm_shuffle_pi8(v0, filt1Reg); \
+    v0 = _mm_maddubs_pi16(v0, firstFilters); \
+    v0 = _mm_adds_pi16(v0, round); \
+    v0 = _mm_srai_pi16(v0, 7); \
+    v0 = _mm_packs_pu16(v0, v0); \
+    STORE##32 \
+    src_ptr += src_pixels_per_line; \
+    output_ptr += output_pitch; \
+  } \
+} \
+\
+FUNC(block1d8_h2, end) { \
+  __m128i round, firstFilters, v0, v1; \
+  unsigned int i; \
+  round = _mm_set1_epi16(64); \
+  v0 = _mm_cvtsi32_si128(*(const uint32_t *)(filter + 3)); \
+  v0 = _mm_packs_epi16(v0, v0); \
+  firstFilters = _mm_shuffle_epi8(v0, _mm_set1_epi16(0x100)); \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < output_height; i++) { \
+    v0 = _mm_loadl_epi64((const __m128i *)src_ptr); \
+    v1 = _mm_loadl_epi64((const __m128i *)(src_ptr + 1)); \
+    v0 = _mm_unpacklo_epi8(v0, v1); \
+    v0 = _mm_maddubs_epi16(v0, firstFilters); \
+    v0 = _mm_adds_epi16(v0, round); \
+    v0 = _mm_srai_epi16(v0, 7); \
+    v0 = _mm_packus_epi16(v0, v0); \
+    STORE##64 \
+    src_ptr += src_pixels_per_line; \
+    output_ptr += output_pitch; \
+  } \
+} \
+\
+FUNC(block1d16_h2, end) { \
+  __m128i round, firstFilters, v0, v1, v2, v3; \
+  unsigned int i; \
+  round = _mm_set1_epi16(64); \
+  v0 = _mm_cvtsi32_si128(*(const uint32_t *)(filter + 3)); \
+  v0 = _mm_packs_epi16(v0, v0); \
+  firstFilters = _mm_shuffle_epi8(v0, _mm_set1_epi16(0x100)); \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < output_height; i++) { \
+    v2 = _mm_loadu_si128((const __m128i *)src_ptr); \
+    v3 = _mm_loadu_si128((const __m128i *)(src_ptr + 1)); \
+    v0 = _mm_unpacklo_epi8(v2, v3); \
+    v1 = _mm_unpackhi_epi8(v2, v3); \
+    v0 = _mm_maddubs_epi16(v0, firstFilters); \
+    v1 = _mm_maddubs_epi16(v1, firstFilters); \
+    v0 = _mm_adds_epi16(v0, round); \
+    v1 = _mm_adds_epi16(v1, round); \
+    v0 = _mm_srai_epi16(v0, 7); \
+    v1 = _mm_srai_epi16(v1, 7); \
+    v0 = _mm_packus_epi16(v0, v1); \
+    STORE##128 \
+    src_ptr += src_pixels_per_line; \
+    output_ptr += output_pitch; \
+  } \
+} \
+\
+FUNC(block1d4_v8, end) { \
+  __m64 round, i0, i1, i2, i3, i4, i5, i6, i7, v0, v1, v2, v3; \
+  __m64 firstFilters, secondFilters, thirdFilters, forthFilters; \
+  unsigned int i; \
+  round = _mm_set1_pi16(64); \
+  v0 = *(const __m64 *)filter; \
+  v1 = *(const __m64 *)(filter + 4); \
+  v0 = _mm_packs_pi16(v0, v1); \
+  firstFilters = _mm_shuffle_pi16(v0, 0); \
+  secondFilters = _mm_shuffle_pi16(v0, 0x55); \
+  thirdFilters = _mm_shuffle_pi16(v0, 0xaa); \
+  forthFilters = _mm_shuffle_pi16(v0, 0xff); \
+  i0 = _mm_cvtsi32_si64(*(const uint32_t *)src_ptr); \
+  i1 = _mm_cvtsi32_si64(*(const uint32_t *)(src_ptr + src_pixels_per_line)); \
+  i2 = _mm_cvtsi32_si64(*(const uint32_t *)(src_ptr + src_pixels_per_line * 2)); \
+  i3 = _mm_cvtsi32_si64(*(const uint32_t *)(src_ptr + src_pixels_per_line * 3)); \
+  i4 = _mm_cvtsi32_si64(*(const uint32_t *)(src_ptr + src_pixels_per_line * 4)); \
+  i5 = _mm_cvtsi32_si64(*(const uint32_t *)(src_ptr + src_pixels_per_line * 5)); \
+  i6 = _mm_cvtsi32_si64(*(const uint32_t *)(src_ptr + src_pixels_per_line * 6)); \
+  src_ptr += src_pixels_per_line * 7; \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < output_height; i++) { \
+    i7 = _mm_cvtsi32_si64(*(const uint32_t *)src_ptr); \
+    src_ptr += src_pixels_per_line; \
+    v0 = _mm_unpacklo_pi8(i0, i1); \
+    v1 = _mm_unpacklo_pi8(i4, i5); \
+    v0 = _mm_maddubs_pi16(v0, firstFilters); \
+    v1 = _mm_maddubs_pi16(v1, thirdFilters); \
+    v0 = _mm_add_pi16(v0, v1); \
+    v2 = _mm_unpacklo_pi8(i2, i3); \
+    v3 = _mm_unpacklo_pi8(i6, i7); \
+    v2 = _mm_maddubs_pi16(v2, secondFilters); \
+    v3 = _mm_maddubs_pi16(v3, forthFilters); \
+    v0 = _mm_add_pi16(v0, round); \
+    v0 = _mm_adds_pi16(v0, _mm_add_pi16(v2, v3)); \
+    v0 = _mm_srai_pi16(v0, 7); \
+    v0 = _mm_packs_pu16(v0, v0); \
+    STORE##32 \
+    output_ptr += output_pitch; \
+    i0 = i1; i1 = i2; i2 = i3; i3 = i4; i4 = i5; i5 = i6; i6 = i7; \
+  } \
+} \
+\
+FUNC(block1d8_v8, end) { \
+  __m128i round, i0, i1, i2, i3, i4, i5, i6, i7, v0, v1, v2, v3; \
+  __m128i firstFilters, secondFilters, thirdFilters, forthFilters; \
+  unsigned int i; \
+  INIT_V8(_mm_loadl_epi64) \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < output_height; i++) { \
+    i7 = _mm_loadl_epi64((const __m128i *)src_ptr); \
+    src_ptr += src_pixels_per_line; \
+    FILTER_V8(lo, v0) \
+    v0 = _mm_packus_epi16(v0, v0); \
+    STORE##64 \
+    output_ptr += output_pitch; \
+    i0 = i1; i1 = i2; i2 = i3; i3 = i4; i4 = i5; i5 = i6; i6 = i7; \
+  } \
+} \
+\
+FUNC(block1d16_v8, end) { \
+  __m128i round, i0, i1, i2, i3, i4, i5, i6, i7, v0, v1, v2, v3, v4; \
+  __m128i firstFilters, secondFilters, thirdFilters, forthFilters; \
+  unsigned int i; \
+  INIT_V8(_mm_loadu_si128) \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < output_height; i++) { \
+    i7 = _mm_loadu_si128((const __m128i *)src_ptr); \
+    src_ptr += src_pixels_per_line; \
+    FILTER_V8(lo, v4) \
+    FILTER_V8(hi, v0) \
+    v0 = _mm_packus_epi16(v4, v0); \
+    STORE##128 \
+    output_ptr += output_pitch; \
+    i0 = i1; i1 = i2; i2 = i3; i3 = i4; i4 = i5; i5 = i6; i6 = i7; \
+  } \
+} \
+\
+FUNC(block1d4_v2, end) { \
+  __m64 round, firstFilters, v0, v2, v3; \
+  unsigned int i; \
+  round = _mm_set1_pi16(64); \
+  v0 = _mm_cvtsi32_si64(*(const uint32_t *)(filter + 3)); \
+  v0 = _mm_packs_pi16(v0, v0); \
+  firstFilters = _mm_shuffle_pi16(v0, 0); \
+  v2 = _mm_cvtsi32_si64(*(const uint32_t *)src_ptr); \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < output_height; i++) { \
+    src_ptr += src_pixels_per_line; \
+    v3 = _mm_cvtsi32_si64(*(const uint32_t *)src_ptr); \
+    v0 = _mm_unpacklo_pi8(v2, v3); \
+    v0 = _mm_maddubs_pi16(v0, firstFilters); \
+    v0 = _mm_adds_pi16(v0, round); \
+    v0 = _mm_srai_pi16(v0, 7); \
+    v0 = _mm_packs_pu16(v0, v0); \
+    STORE##32 \
+    output_ptr += output_pitch; \
+    v2 = v3; \
+  } \
+} \
+\
+FUNC(block1d8_v2, end) { \
+  __m128i round, firstFilters, v0, v2, v3; \
+  unsigned int i; \
+  round = _mm_set1_epi16(64); \
+  v0 = _mm_cvtsi32_si128(*(const uint32_t *)(filter + 3)); \
+  v0 = _mm_packs_epi16(v0, v0); \
+  firstFilters = _mm_shuffle_epi8(v0, _mm_set1_epi16(0x100)); \
+  v2 = _mm_loadl_epi64((const __m128i *)src_ptr); \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < output_height; i++) { \
+    src_ptr += src_pixels_per_line; \
+    v3 = _mm_loadl_epi64((const __m128i *)src_ptr); \
+    v0 = _mm_unpacklo_epi8(v2, v3); \
+    v0 = _mm_maddubs_epi16(v0, firstFilters); \
+    v0 = _mm_adds_epi16(v0, round); \
+    v0 = _mm_srai_epi16(v0, 7); \
+    v0 = _mm_packus_epi16(v0, v0); \
+    STORE##64 \
+    output_ptr += output_pitch; \
+    v2 = v3; \
+  } \
+} \
+\
+FUNC(block1d16_v2, end) { \
+  __m128i round, firstFilters, v0, v1, v2, v3; \
+  unsigned int i; \
+  round = _mm_set1_epi16(64); \
+  v0 = _mm_cvtsi32_si128(*(const uint32_t *)(filter + 3)); \
+  v0 = _mm_packs_epi16(v0, v0); \
+  firstFilters = _mm_shuffle_epi8(v0, _mm_set1_epi16(0x100)); \
+  v2 = _mm_loadu_si128((const __m128i *)src_ptr); \
+  PRAGMA_E2K("ivdep") \
+  for (i = 0; i < output_height; i++) { \
+    src_ptr += src_pixels_per_line; \
+    v3 = _mm_loadu_si128((const __m128i *)src_ptr); \
+    v0 = _mm_unpacklo_epi8(v2, v3); \
+    v1 = _mm_unpackhi_epi8(v2, v3); \
+    v0 = _mm_maddubs_epi16(v0, firstFilters); \
+    v1 = _mm_maddubs_epi16(v1, firstFilters); \
+    v0 = _mm_adds_epi16(v0, round); \
+    v1 = _mm_adds_epi16(v1, round); \
+    v0 = _mm_srai_epi16(v0, 7); \
+    v1 = _mm_srai_epi16(v1, 7); \
+    v0 = _mm_packus_epi16(v0, v1); \
+    STORE##128 \
+    output_ptr += output_pitch; \
+    v2 = v3; \
+  } \
+}
+
+FILTER(ssse3, STORE)
+FILTER(avg_ssse3, AVG_STORE)
+
diff --git a/vpx_dsp/vpx_dsp.mk b/vpx_dsp/vpx_dsp.mk
index 0165310..92818a4 100644
--- a/vpx_dsp/vpx_dsp.mk
+++ b/vpx_dsp/vpx_dsp.mk
@@ -49,12 +49,14 @@ DSP_SRCS-yes += intrapred.c
 
 DSP_SRCS-$(HAVE_SSE2) += x86/intrapred_sse2.asm
 DSP_SRCS-$(HAVE_SSSE3) += x86/intrapred_ssse3.asm
+DSP_SRCS-$(VPX_ARCH_E2K) += e2k/intrapred_e2k.c
 DSP_SRCS-$(HAVE_VSX) += ppc/intrapred_vsx.c
 
 ifeq ($(CONFIG_VP9_HIGHBITDEPTH),yes)
 DSP_SRCS-$(HAVE_SSE2) += x86/highbd_intrapred_sse2.asm
 DSP_SRCS-$(HAVE_SSE2) += x86/highbd_intrapred_intrin_sse2.c
 DSP_SRCS-$(HAVE_SSSE3) += x86/highbd_intrapred_intrin_ssse3.c
+DSP_SRCS-$(VPX_ARCH_E2K) += e2k/highbd_intrapred_e2k.c
 DSP_SRCS-$(HAVE_NEON) += arm/highbd_intrapred_neon.c
 endif  # CONFIG_VP9_HIGHBITDEPTH
 
@@ -99,10 +101,12 @@ DSP_SRCS-$(HAVE_SSSE3) += x86/vpx_subpixel_8t_ssse3.asm
 DSP_SRCS-$(HAVE_SSSE3) += x86/vpx_subpixel_bilinear_ssse3.asm
 DSP_SRCS-$(HAVE_AVX2)  += x86/vpx_subpixel_8t_intrin_avx2.c
 DSP_SRCS-$(HAVE_SSSE3) += x86/vpx_subpixel_8t_intrin_ssse3.c
+DSP_SRCS-$(VPX_ARCH_E2K) += e2k/vpx_subpixel_8t_e2k.c
 ifeq ($(CONFIG_VP9_HIGHBITDEPTH),yes)
 DSP_SRCS-$(HAVE_SSE2)  += x86/vpx_high_subpixel_8t_sse2.asm
 DSP_SRCS-$(HAVE_SSE2)  += x86/vpx_high_subpixel_bilinear_sse2.asm
 DSP_SRCS-$(HAVE_AVX2)  += x86/highbd_convolve_avx2.c
+DSP_SRCS-$(VPX_ARCH_E2K) += e2k/vpx_high_subpixel_8t_e2k.c
 DSP_SRCS-$(HAVE_NEON)  += arm/highbd_vpx_convolve_copy_neon.c
 DSP_SRCS-$(HAVE_NEON)  += arm/highbd_vpx_convolve_avg_neon.c
 DSP_SRCS-$(HAVE_NEON)  += arm/highbd_vpx_convolve8_neon.c
@@ -110,6 +114,7 @@ DSP_SRCS-$(HAVE_NEON)  += arm/highbd_vpx_convolve_neon.c
 endif
 
 DSP_SRCS-$(HAVE_SSE2)  += x86/vpx_convolve_copy_sse2.asm
+DSP_SRCS-$(VPX_ARCH_E2K) += e2k/vpx_convolve_copy_e2k.c
 DSP_SRCS-$(HAVE_NEON)  += arm/vpx_scaled_convolve8_neon.c
 
 ifeq ($(HAVE_NEON_ASM),yes)
@@ -234,6 +239,7 @@ DSP_SRCS-$(HAVE_SSE2)   += x86/inv_txfm_sse2.c
 DSP_SRCS-$(HAVE_SSE2)   += x86/inv_wht_sse2.asm
 DSP_SRCS-$(HAVE_SSSE3)  += x86/inv_txfm_ssse3.h
 DSP_SRCS-$(HAVE_SSSE3)  += x86/inv_txfm_ssse3.c
+DSP_SRCS-$(VPX_ARCH_E2K) += e2k/inv_wht_e2k.c
 
 DSP_SRCS-$(HAVE_NEON_ASM) += arm/save_reg_neon$(ASM)
 
@@ -356,12 +362,18 @@ DSP_SRCS-$(HAVE_SSE2)   += x86/sad4d_sse2.asm
 DSP_SRCS-$(HAVE_SSE2)   += x86/sad_sse2.asm
 DSP_SRCS-$(HAVE_SSE2)   += x86/subtract_sse2.asm
 
+DSP_SRCS-$(VPX_ARCH_E2K) += e2k/sad4d_e2k.c
+DSP_SRCS-$(VPX_ARCH_E2K) += e2k/sad_e2k.c
+DSP_SRCS-$(VPX_ARCH_E2K) += e2k/subtract_e2k.c
+
 DSP_SRCS-$(HAVE_VSX) += ppc/sad_vsx.c
 DSP_SRCS-$(HAVE_VSX) += ppc/subtract_vsx.c
 
 ifeq ($(CONFIG_VP9_HIGHBITDEPTH),yes)
 DSP_SRCS-$(HAVE_SSE2) += x86/highbd_sad4d_sse2.asm
 DSP_SRCS-$(HAVE_SSE2) += x86/highbd_sad_sse2.asm
+DSP_SRCS-$(VPX_ARCH_E2K) += e2k/highbd_sad4d_e2k.c
+DSP_SRCS-$(VPX_ARCH_E2K) += e2k/highbd_sad_e2k.c
 endif  # CONFIG_VP9_HIGHBITDEPTH
 
 endif  # CONFIG_ENCODERS
@@ -389,11 +401,14 @@ DSP_SRCS-$(HAVE_SSE2)   += x86/ssim_opt_x86_64.asm
 endif  # VPX_ARCH_X86_64
 
 DSP_SRCS-$(HAVE_SSE2)   += x86/subpel_variance_sse2.asm  # Contains SSE2 and SSSE3
+DSP_SRCS-$(VPX_ARCH_E2K) += e2k/subpel_variance_e2k.c
 
 ifeq ($(CONFIG_VP9_HIGHBITDEPTH),yes)
 DSP_SRCS-$(HAVE_SSE2)   += x86/highbd_variance_sse2.c
 DSP_SRCS-$(HAVE_SSE2)   += x86/highbd_variance_impl_sse2.asm
 DSP_SRCS-$(HAVE_SSE2)   += x86/highbd_subpel_variance_impl_sse2.asm
+DSP_SRCS-$(VPX_ARCH_E2K) += e2k/highbd_variance_impl_e2k.c
+DSP_SRCS-$(VPX_ARCH_E2K) += e2k/highbd_subpel_variance_e2k.c
 endif  # CONFIG_VP9_HIGHBITDEPTH
 endif  # CONFIG_ENCODERS || CONFIG_POSTPROC || CONFIG_VP9_POSTPROC
 
diff --git a/vpx_dsp/vpx_dsp_rtcd_defs.pl b/vpx_dsp/vpx_dsp_rtcd_defs.pl
index fd7eefd..79f46a8 100644
--- a/vpx_dsp/vpx_dsp_rtcd_defs.pl
+++ b/vpx_dsp/vpx_dsp_rtcd_defs.pl
@@ -24,7 +24,7 @@ forward_decls qw/vpx_dsp_forward_decls/;
 
 # functions that are 64 bit only.
 $mmx_x86_64 = $sse2_x86_64 = $ssse3_x86_64 = $avx_x86_64 = $avx2_x86_64 = '';
-if ($opts{arch} eq "x86_64") {
+if ($opts{arch} eq "x86_64" or $opts{arch} eq "e2k") {
   $mmx_x86_64 = 'mmx';
   $sse2_x86_64 = 'sse2';
   $ssse3_x86_64 = 'ssse3';
diff --git a/vpx_dsp/x86/variance_sse2.c b/vpx_dsp/x86/variance_sse2.c
index 37ef64e..6f1e30f 100644
--- a/vpx_dsp/x86/variance_sse2.c
+++ b/vpx_dsp/x86/variance_sse2.c
@@ -433,7 +433,9 @@ unsigned int vpx_mse16x16_sse2(const uint8_t *src_ptr, int src_stride,
   DECL(8, opt1);          \
   DECL(16, opt1)
 
+#if !VPX_ARCH_E2K
 DECLS(sse2, sse2);
+#endif
 DECLS(ssse3, ssse3);
 #undef DECLS
 #undef DECL
@@ -486,7 +488,9 @@ DECLS(ssse3, ssse3);
   FN(4, 8, 4, 2, 3, opt1, (int32_t), (int32_t));     \
   FN(4, 4, 4, 2, 2, opt1, (int32_t), (int32_t))
 
+#if !VPX_ARCH_E2K
 FNS(sse2, sse2);
+#endif
 FNS(ssse3, ssse3);
 
 #undef FNS
@@ -504,7 +508,9 @@ FNS(ssse3, ssse3);
   DECL(8, opt1);          \
   DECL(16, opt1)
 
+#if !VPX_ARCH_E2K
 DECLS(sse2, sse2);
+#endif
 DECLS(ssse3, ssse3);
 #undef DECL
 #undef DECLS
@@ -558,7 +564,9 @@ DECLS(ssse3, ssse3);
   FN(4, 8, 4, 2, 3, opt1, (uint32_t), (int32_t));    \
   FN(4, 4, 4, 2, 2, opt1, (uint32_t), (int32_t))
 
+#if !VPX_ARCH_E2K
 FNS(sse2, sse);
+#endif
 FNS(ssse3, ssse3);
 
 #undef FNS
diff --git a/vpx_dsp/x86/vpx_subpixel_4t_intrin_sse2.c b/vpx_dsp/x86/vpx_subpixel_4t_intrin_sse2.c
index 2391790..7499aa9 100644
--- a/vpx_dsp/x86/vpx_subpixel_4t_intrin_sse2.c
+++ b/vpx_dsp/x86/vpx_subpixel_4t_intrin_sse2.c
@@ -19,6 +19,7 @@
 #define CONV8_ROUNDING_BITS (7)
 #define CONV8_ROUNDING_NUM (1 << (CONV8_ROUNDING_BITS - 1))
 
+#if !VPX_ARCH_E2K
 static void vpx_filter_block1d16_h4_sse2(const uint8_t *src_ptr,
                                          ptrdiff_t src_stride, uint8_t *dst_ptr,
                                          ptrdiff_t dst_stride, uint32_t height,
@@ -601,8 +602,9 @@ static void vpx_filter_block1d4_v4_sse2(const uint8_t *src_ptr,
     src_reg_1 = src_reg_3;
   }
 }
+#endif  // !VPX_ARCH_E2K
 
-#if CONFIG_VP9_HIGHBITDEPTH && VPX_ARCH_X86_64
+#if CONFIG_VP9_HIGHBITDEPTH && (VPX_ARCH_X86_64 || VPX_ARCH_E2K)
 static void vpx_highbd_filter_block1d4_h4_sse2(
     const uint16_t *src_ptr, ptrdiff_t src_stride, uint16_t *dst_ptr,
     ptrdiff_t dst_stride, uint32_t height, const int16_t *kernel, int bd) {
@@ -982,8 +984,9 @@ static void vpx_highbd_filter_block1d16_v4_sse2(
   vpx_highbd_filter_block1d8_v4_sse2(src_ptr + 8, src_stride, dst_ptr + 8,
                                      dst_stride, height, kernel, bd);
 }
-#endif  // CONFIG_VP9_HIGHBITDEPTH && VPX_ARCH_X86_64
+#endif  // CONFIG_VP9_HIGHBITDEPTH && (VPX_ARCH_X86_64 || VPX_ARCH_E2K)
 
+#if !VPX_ARCH_E2K
 // From vpx_subpixel_8t_sse2.asm.
 filter8_1dfunction vpx_filter_block1d16_v8_sse2;
 filter8_1dfunction vpx_filter_block1d16_h8_sse2;
@@ -1059,8 +1062,9 @@ FUN_CONV_1D(avg_vert, y0_q4, y_step_q4, v,
 //                             int w, int h);
 FUN_CONV_2D(, sse2, 0);
 FUN_CONV_2D(avg_, sse2, 1);
+#endif  // !VPX_ARCH_E2K
 
-#if CONFIG_VP9_HIGHBITDEPTH && VPX_ARCH_X86_64
+#if CONFIG_VP9_HIGHBITDEPTH && (VPX_ARCH_X86_64 || VPX_ARCH_E2K)
 // From vpx_dsp/x86/vpx_high_subpixel_8t_sse2.asm.
 highbd_filter8_1dfunction vpx_highbd_filter_block1d16_v8_sse2;
 highbd_filter8_1dfunction vpx_highbd_filter_block1d16_h8_sse2;
@@ -1158,4 +1162,4 @@ HIGH_FUN_CONV_1D(avg_vert, y0_q4, y_step_q4, v,
 //                                    int y_step_q4, int w, int h, int bd);
 HIGH_FUN_CONV_2D(, sse2, 0);
 HIGH_FUN_CONV_2D(avg_, sse2, 1);
-#endif  // CONFIG_VP9_HIGHBITDEPTH && VPX_ARCH_X86_64
+#endif  // CONFIG_VP9_HIGHBITDEPTH && (VPX_ARCH_X86_64 || VPX_ARCH_E2K)
diff --git a/vpx_ports/bitops.h b/vpx_ports/bitops.h
index 5b2f31c..1b5cdaa 100644
--- a/vpx_ports/bitops.h
+++ b/vpx_ports/bitops.h
@@ -26,20 +26,32 @@
 extern "C" {
 #endif
 
-// These versions of get_msb() are only valid when n != 0 because all
-// of the optimized versions are undefined when n == 0:
+// These versions of get_lsb() and get_msb() are only valid when n != 0
+// because all of the optimized versions are undefined when n == 0:
 // https://gcc.gnu.org/onlinedocs/gcc/Other-Builtins.html
 
 // use GNU builtins where available.
 #if defined(__GNUC__) && \
     ((__GNUC__ == 3 && __GNUC_MINOR__ >= 4) || __GNUC__ >= 4)
+static INLINE int get_lsb(unsigned int n) {
+  assert(n != 0);
+  return __builtin_ctz(n);
+}
+
 static INLINE int get_msb(unsigned int n) {
   assert(n != 0);
   return 31 ^ __builtin_clz(n);
 }
 #elif defined(USE_MSC_INTRINSICS)
+#pragma intrinsic(_BitScanForward)
 #pragma intrinsic(_BitScanReverse)
 
+static INLINE int get_lsb(unsigned int n) {
+  unsigned long first_set_bit;  // NOLINT(runtime/int)
+  _BitScanForward(&first_set_bit, n);
+  return first_set_bit;
+}
+
 static INLINE int get_msb(unsigned int n) {
   unsigned long first_set_bit;
   assert(n != 0);
@@ -48,6 +60,13 @@ static INLINE int get_msb(unsigned int n) {
 }
 #undef USE_MSC_INTRINSICS
 #else
+static INLINE int get_lsb(unsigned int n) {
+  int i;
+  assert(n != 0);
+  for (i = 0; i < 32 && !(n & 1); ++i) n >>= 1;
+  return i;
+}
+
 // Returns (int)floor(log2(n)). n must be > 0.
 static INLINE int get_msb(unsigned int n) {
   int log = 0;
diff --git a/vpx_ports/e2k.h b/vpx_ports/e2k.h
new file mode 100644
index 0000000..7cc4862
--- /dev/null
+++ b/vpx_ports/e2k.h
@@ -0,0 +1,133 @@
+/*
+ *  Copyright (c) 2021 Ilya Kurdyukov <jpegqs@gmail.com> for BaseALT, Ltd
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#ifndef VPX_VPX_PORTS_E2K_H_
+#define VPX_VPX_PORTS_E2K_H_
+#include <stdlib.h>
+
+#include "vpx_config.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#ifdef __e2k__
+#define PRAGMA_E2K _Pragma
+#else
+#define PRAGMA_E2K(x)
+#endif
+
+#define _mm_extract_pi32(a, b) _mm_extract_epi32(_mm_movpi64_epi64(a), b)
+
+#ifdef RTCD_C
+#define HAS_MMX 1
+#define HAS_SSE 2
+#define HAS_SSE2 2
+#define HAS_SSE3 2
+#define HAS_SSSE3 2
+#define HAS_SSE4_1 2
+#define HAS_AVX 4
+#define HAS_AVX2 4
+#define HAS_AVX512 8
+
+#define e2k_simd_caps() 3
+#endif
+
+#define vp8_bilinear_predict16x16_ssse3 vp8_bilinear_predict16x16_c
+#define vp8_bilinear_predict8x8_ssse3 vp8_bilinear_predict8x8_c
+#define vp8_copy_mem16x16_sse2 vp8_copy_mem16x16_c
+#define vp8_copy_mem8x4_mmx vp8_copy_mem8x4_c
+#define vp8_copy_mem8x8_mmx vp8_copy_mem8x8_c
+#define vp8_dc_only_idct_add_mmx vp8_dc_only_idct_add_c
+#define vp8_dequant_idct_add_mmx vp8_dequant_idct_add_c
+#define vp8_short_idct4x4llm_mmx vp8_short_idct4x4llm_c
+#define vp8_short_inv_walsh4x4_sse2 vp8_short_inv_walsh4x4_c
+#define vp8_short_walsh4x4_sse2 vp8_short_walsh4x4_c
+#define vp8_temporal_filter_apply_sse2 vp8_temporal_filter_apply_c
+#define vp8_dequant_idct_add_uv_block_sse2 vp8_dequant_idct_add_uv_block_c
+#define vp8_dequant_idct_add_y_block_sse2 vp8_dequant_idct_add_y_block_c
+#define vp8_dequantize_b_mmx vp8_dequantize_b_c
+
+#define vp9_quantize_fp_32x32_ssse3 vp9_quantize_fp_32x32_c
+#define vp9_quantize_fp_ssse3 vp9_quantize_fp_c
+#define vp9_fwht4x4_sse2 vp9_fwht4x4_c
+
+#define vpx_fdct8x8_ssse3 vpx_fdct8x8_c
+#define vpx_hadamard_8x8_ssse3 vpx_hadamard_8x8_c
+
+// TODO: intrapred_e2k.c
+#define vpx_d153_predictor_4x4_ssse3 vpx_d153_predictor_4x4_c
+#define vpx_d153_predictor_8x8_ssse3 vpx_d153_predictor_8x8_c
+#define vpx_d153_predictor_16x16_ssse3 vpx_d153_predictor_16x16_c
+#define vpx_d153_predictor_32x32_ssse3 vpx_d153_predictor_32x32_c
+#define vpx_d207_predictor_4x4_sse2 vpx_d207_predictor_4x4_c
+#define vpx_d207_predictor_8x8_ssse3 vpx_d207_predictor_8x8_c
+#define vpx_d207_predictor_16x16_ssse3 vpx_d207_predictor_16x16_c
+#define vpx_d207_predictor_32x32_ssse3 vpx_d207_predictor_32x32_c
+#define vpx_d45_predictor_4x4_sse2 vpx_d45_predictor_4x4_c
+#define vpx_d45_predictor_8x8_sse2 vpx_d45_predictor_8x8_c
+#define vpx_d45_predictor_16x16_ssse3 vpx_d45_predictor_16x16_c
+#define vpx_d45_predictor_32x32_ssse3 vpx_d45_predictor_32x32_c
+#define vpx_tm_predictor_16x16_sse2 vpx_tm_predictor_16x16_c
+#define vpx_tm_predictor_32x32_sse2 vpx_tm_predictor_32x32_c
+
+// TODO: highbd_intrapred_e2k.c
+#define vpx_highbd_tm_predictor_4x4_sse2 vpx_highbd_tm_predictor_4x4_c
+#define vpx_highbd_tm_predictor_8x8_sse2 vpx_highbd_tm_predictor_8x8_c
+#define vpx_highbd_tm_predictor_16x16_sse2 vpx_highbd_tm_predictor_16x16_c
+#define vpx_highbd_tm_predictor_32x32_sse2 vpx_highbd_tm_predictor_32x32_c
+
+// vp8_sixtap_predict*_{mmx,sse2} -> vp8_sixtap_predict4x4_ssse3
+#define vp8_sixtap_predict4x4_mmx vp8_sixtap_predict4x4_ssse3
+#define vp8_sixtap_predict8x4_sse2 vp8_sixtap_predict8x4_ssse3
+#define vp8_sixtap_predict8x8_sse2 vp8_sixtap_predict8x8_ssse3
+#define vp8_sixtap_predict16x16_sse2 vp8_sixtap_predict16x16_ssse3
+
+// vpx_convolve8_*_sse2 -> vpx_convolve8_*_ssse3
+#define vpx_convolve8_avg_horiz_sse2 vpx_convolve8_avg_horiz_ssse3
+#define vpx_convolve8_avg_vert_sse2 vpx_convolve8_avg_vert_ssse3
+#define vpx_convolve8_horiz_sse2 vpx_convolve8_horiz_ssse3
+#define vpx_convolve8_vert_sse2 vpx_convolve8_vert_ssse3
+
+// vpx_sub_pixel_variance*_sse2 -> vpx_sub_pixel_variance*_ssse3
+#define vpx_sub_pixel_variance16x16_sse2 vpx_sub_pixel_variance16x16_ssse3
+#define vpx_sub_pixel_variance16x32_sse2 vpx_sub_pixel_variance16x32_ssse3
+#define vpx_sub_pixel_variance16x8_sse2 vpx_sub_pixel_variance16x8_ssse3
+#define vpx_sub_pixel_variance32x16_sse2 vpx_sub_pixel_variance32x16_ssse3
+#define vpx_sub_pixel_variance32x32_sse2 vpx_sub_pixel_variance32x32_ssse3
+#define vpx_sub_pixel_variance32x64_sse2 vpx_sub_pixel_variance32x64_ssse3
+#define vpx_sub_pixel_variance4x4_sse2 vpx_sub_pixel_variance4x4_ssse3
+#define vpx_sub_pixel_variance4x8_sse2 vpx_sub_pixel_variance4x8_ssse3
+#define vpx_sub_pixel_variance64x32_sse2 vpx_sub_pixel_variance64x32_ssse3
+#define vpx_sub_pixel_variance64x64_sse2 vpx_sub_pixel_variance64x64_ssse3
+#define vpx_sub_pixel_variance8x16_sse2 vpx_sub_pixel_variance8x16_ssse3
+#define vpx_sub_pixel_variance8x4_sse2 vpx_sub_pixel_variance8x4_ssse3
+#define vpx_sub_pixel_variance8x8_sse2 vpx_sub_pixel_variance8x8_ssse3
+
+// vpx_sub_pixel_avg_variance*_sse2 -> vpx_sub_pixel_avg_variance*_ssse3
+#define vpx_sub_pixel_avg_variance16x16_sse2 vpx_sub_pixel_avg_variance16x16_ssse3
+#define vpx_sub_pixel_avg_variance16x32_sse2 vpx_sub_pixel_avg_variance16x32_ssse3
+#define vpx_sub_pixel_avg_variance16x8_sse2 vpx_sub_pixel_avg_variance16x8_ssse3
+#define vpx_sub_pixel_avg_variance32x16_sse2 vpx_sub_pixel_avg_variance32x16_ssse3
+#define vpx_sub_pixel_avg_variance32x32_sse2 vpx_sub_pixel_avg_variance32x32_ssse3
+#define vpx_sub_pixel_avg_variance32x64_sse2 vpx_sub_pixel_avg_variance32x64_ssse3
+#define vpx_sub_pixel_avg_variance4x4_sse2 vpx_sub_pixel_avg_variance4x4_ssse3
+#define vpx_sub_pixel_avg_variance4x8_sse2 vpx_sub_pixel_avg_variance4x8_ssse3
+#define vpx_sub_pixel_avg_variance64x32_sse2 vpx_sub_pixel_avg_variance64x32_ssse3
+#define vpx_sub_pixel_avg_variance64x64_sse2 vpx_sub_pixel_avg_variance64x64_ssse3
+#define vpx_sub_pixel_avg_variance8x16_sse2 vpx_sub_pixel_avg_variance8x16_ssse3
+#define vpx_sub_pixel_avg_variance8x4_sse2 vpx_sub_pixel_avg_variance8x4_ssse3
+#define vpx_sub_pixel_avg_variance8x8_sse2 vpx_sub_pixel_avg_variance8x8_ssse3
+
+#ifdef __cplusplus
+}  // extern "C"
+#endif
+
+#endif  // VPX_VPX_PORTS_E2K_H_
diff --git a/vpx_ports/vpx_ports.mk b/vpx_ports/vpx_ports.mk
index e5001be..2a58f07 100644
--- a/vpx_ports/vpx_ports.mk
+++ b/vpx_ports/vpx_ports.mk
@@ -36,6 +36,8 @@ PORTS_SRCS-yes += x86.h
 PORTS_SRCS-yes += x86_abi_support.asm
 endif
 
+PORTS_SRCS-$(VPX_ARCH_E2K) += e2k.h
+
 PORTS_SRCS-$(VPX_ARCH_ARM) += arm_cpudetect.c
 PORTS_SRCS-$(VPX_ARCH_ARM) += arm.h
 
diff --git a/vpx_ports/x86.h b/vpx_ports/x86.h
index 4d5391b..aa463b1 100644
--- a/vpx_ports/x86.h
+++ b/vpx_ports/x86.h
@@ -19,6 +19,8 @@
 #include "vpx_config.h"
 #include "vpx/vpx_integer.h"
 
+#if !VPX_ARCH_E2K
+
 #ifdef __cplusplus
 extern "C" {
 #endif
@@ -399,4 +401,6 @@ static INLINE unsigned int x87_set_double_precision(void) {
 }  // extern "C"
 #endif
 
+#endif  // !VPX_ARCH_E2K
+
 #endif  // VPX_VPX_PORTS_X86_H_
-- 
2.17.1

